<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on Kontronn</title>
    <link>https://www.kontronn.com/tags/llm/</link>
    <description>Recent content in LLM on Kontronn</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 25 Sep 2024 00:05:33 +0800</lastBuildDate><atom:link href="https://www.kontronn.com/tags/llm/rss.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>洞察之路变革：AI-HPC范式转移</title>
      <link>https://www.kontronn.com/post/datacenter/the-path-to-insight-is-changing-the-ai-hpc-paradigm-shift.html</link>
      <pubDate>Wed, 25 Sep 2024 00:05:33 +0800</pubDate>
      
      <guid>https://www.kontronn.com/post/datacenter/the-path-to-insight-is-changing-the-ai-hpc-paradigm-shift.html</guid>
      <description>&lt;p&gt;在一篇近期发布的论文中，作者提出了一种令人担忧的现象：当AI模型在先前LLM模型递归生成的数据点上进行训练时，可能会发生模型崩溃，即所谓的“蛇吞尾”现象。生成式AI在挖掘互联网数据，构建大型LLM模型方面取得了显著的成功，这些模型能够对许多问题提供类人的回答。&lt;/p&gt;
&lt;p&gt;特别是，生成新文本一直是许多LLM的强项。从文档摘要到报告生成，LLM能够创造大量文本。然而，不可避免地，这些LLM生成的文本会出现在互联网上，并被下一代LLM模型所采集。论文中的研究人员认为，这种递归训练会导致模型性能退化，最终引发模型崩溃。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LLM训练的存储需求：训练数据和检查点</title>
      <link>https://www.kontronn.com/post/ai/a-checkpoint-on-checkpoints-in-llms.html</link>
      <pubDate>Sat, 31 Aug 2024 22:07:26 +0800</pubDate>
      
      <guid>https://www.kontronn.com/post/ai/a-checkpoint-on-checkpoints-in-llms.html</guid>
      <description>&lt;p&gt;LLM训练的存储需求主要涉及两个方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;训练数据：用于更新模型权重和促进模型收敛。&lt;/li&gt;
&lt;li&gt;检查点：将模型权重从GPU内存保存至持久化存储。&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>大型语言模型入门必知术语汇总</title>
      <link>https://www.kontronn.com/post/datacenter/terminology-of-large-scale-language-models.html</link>
      <pubDate>Sat, 18 Nov 2023 22:19:49 +0800</pubDate>
      
      <guid>https://www.kontronn.com/post/datacenter/terminology-of-large-scale-language-models.html</guid>
      <description>&lt;p&gt;在本文中，我将以一种非数据科学家易于理解的方式分解与LLM和AI相关的一些基本术语和概念。我将涵盖从神经网络到数据增强的所有内容，并为每个术语提供简单的解释和示例。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
