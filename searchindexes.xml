<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<search>
  
  <entry>
    <title>欢迎加入 VxWorks 俱乐部！</title>
    <url>/post/welcome-to-vxworks-club/</url>
    <categories><category>Announce</category>
    </categories>
    <tags>
      <tag>VxWorks</tag>
      <tag>WindRiver</tag>
    </tags>
    <content type="html"><![CDATA[欢迎来到 北南南北 文档站点！ 相关文章来源于 VxWorks 俱乐部  ，也可能发布于 AI 嵌入式开发  ，专注于技术分享和交流。
免责声明 所有资源均来自网络，版权归原作者，如有侵权，请联系删除！
欢迎投稿  欢迎广大网友投稿 欢迎加入网友微信群  ]]></content>
  </entry>
  
  <entry>
    <title>风河Linux为何能在汽车智驾领域成为主流</title>
    <url>/post/linux/wind-river-linux-become-mainstream-in-the-field-of-intelligent-driving.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Wind River Linux</tag>
      <tag>Intelligent Drive</tag>
    </tags>
    <content type="html"><![CDATA[汽车行业越来越认同这样一个理念：决定未来汽车价值的是以 人工智能  为核心的软件技术，而不再是传统的机械性能或硬件配置。一场围绕着智驾软件的竞争因此已经在更高更广层面展开，并且每一个汽车行业人都明白，这是一场不能输的战争，因为在软件定义汽车时代，汽车主要通过软件为用户提供价值。
一辆今天的 “数字” 汽车所包含的代码量，甚至将要接近7亿行。这些智能驾驶底层软件的核心是其运行的操作系统，当 Linux操作系统  成为汽车智驾领域的主流操作系统之后，其在汽车行业的地位也不断凸显。
而回顾Linux操作系统发展的初期阶段，风河公司的 VxWorks  才是当时嵌入式市场的真正王者，只是因为 ARM  、StrongARM、MIPS等多种架构的存在，才给了Linux生存和发展空间，让其作为“游击队”的角色而发展了起来。那么，从“游击队”到主流操作系统，Linux操作系统在汽车智驾领域又经历了怎样的演变呢?
智驾领域操作系统的基本要求 智能汽车的代码量一直处于变化之中。如果回到2015年，当时一辆“数字”汽车软件代码量能只有1亿行,虽然这已经远高于Facebook、战斗机、人造卫星等高科技产品的代码量，但这个数字放在今天，却只能说是小巫见大巫。
行业专家预测，到2025年,一辆智能汽车代码量预计将达到7亿行。这个数字相较于2022年的代码量，将要提升2.3倍。从中不难看出，智能汽车的代码量一直处于高速增长期，因此汽车行业对于高效开发能力的追求，可以说是达到了如饥似渴的程度。
与智能汽车代码数量同步上升的，还有安全风险的高速同步上涨，因为每一行代码都有可能成为黑客的攻击点。自2022年上半年针对车联网平台的网络恶意行为突破百万之后，以汽车数据为核心的新安全问题给智能驾驶底层操作系统提出了更高安全性要求。
操作系统在智能汽车生态中发挥着承上启下的作用，不仅负责智能汽车对内管理、对外交互，同时也负责对上构建繁荣生态环境、对下协调硬件资源。对汽车内部，操作系统的作用是管理，对用户操作系统则要起到交互的作用。因此，操作系统的稳定直接决定着汽车的表现。也正是因为这个原因，智驾领域同样将稳定性作为操作系统的重要考量因素。
由此可见，安全、高效开发、稳定，就构成了智能汽车对于操作系统的基本要求，只有通过了这些门槛，才有可能在这个领域越走越顺。
Linux的长短板 应对安全性、高效开发、稳定性等几个基本要求，Linux操作系统有着天然的优势。
首先从高效开发的角度来看，Linux操作系统有多种程序语言与开发工具，对于不同的开发平台的接口和软件适配度将更好。此外，由于Linux内核大部分是用C语言编写，并采用可移植Unix标准应用程序接口，因此可以很好地支持车载智能汽车等相关嵌入式设备的控制。这些因素都为高效开发的实现提供了保障。
再从稳定性视角来看，Linux是一个真正多任务多用户的操作系统，开发人员可以对自己需要的资源配置需求的权限，这种方式允许多应用程序在同一时间调用操作系统资源且互不影响。并且，由于Linux内的源代码是以标准的32位计算机来做的最佳设计，因此基本可以确保其系统的稳定性，不易宕机。
然而，再比较下去，却会发现Linux中存在的一些安全短板。虽然Linux内部自带的防火墙、入侵检测和安全认证等工具也能在很多情况下满足对网络安全性的需求，但Linux却还是不可避免地表现出一些在智驾设计中的安全缺陷。
首先是许可协议方面的麻烦，Linux采用的是Open Source/GPL2/3; Monolithic kernel。GPL协议允许将使用产品最大化地授权给用户，确保用户可以获得自由运行、复制、研究和改进的分发产品。因此， Linux允许开发社区工程师任意注入不稳定或存在一定bug的程序代码，可能导致系统的突然崩溃，自然无法满足功能安全需求。
此外，从操作系统功能安全性来看，Linux这类操作系统相当于基于宏内核架构设计而成，其硬实时性问题及开源版本分支维护问题都相当明显，如果不经过深度定制和裁减，不可能满足功能安全中的暴露度、严重度、可控性要求。然而，深度定制对团队要求却极高，在这点上，对于智驾系统研发团队来说几乎是不可能做到。
这还带来了另一个问题，Linux源码多达2500万行，一般智驾系统公司的项目开发团队不具备裁剪Linux的能力，整体鉴定难度较大，同时对软件架构师的要求也比较高，需要对Linux系统进行软件安全分析及设计过程难度就会加大。
表面看来，Linux由于存在这些缺陷，不会在智能驾驶业界走得太远，但结果却是Linux操作系统依然是当前智能驾驶业界内部使用最为广泛的系统。要了解产生这种局面的原因，就得不聊聊开源产业中的中兴力量。
中兴力量的崛起 在开源运动的历史进程中，Eric S·Raymond所著的《大教堂与集市》无疑可以称得上是开源运动的《圣经》，甚至说它颠覆了传统的软件开发思路，影响了整个软件开发领域，也不为过。原因在于大家都喜欢“集市”带来的多样化与自由度，同时从用户的应用角度出发，却希望使用“大教堂式”的有质量保证和后期服务的产品。
正是从这样的需求出发，帮助把开源产品从野外的自然水加工成可以直接饮用的矿泉水的公司，都取得了成功。同时，这些公司的成功又让开源产品一步步从“游击队”转化成主流“正规军”。在嵌入式开源软件领域，风河(Wind River)公司恰好就是这样一股让嵌入式Linux中兴的力量。
相较于“集市”般的Linux社区，以稳健著称的风河公司却是“大教堂”一般的存在。创立于1981年的风河公司，最早产品就是为核电站这类重要设施提供实时控制软件，以后更是借助VxWorks深度参与了美国宇航局太空项目。以后，这种“集市”与“大教堂”风格的融合，更是让风河首款完全具备Linux文件系统的平台产品——2005年内置丰富网络中间件和应用组件的Linux版本网络设备平台(PNE)参考设计版本，一经问世就广受当时国内消费电子产品设计人员青睐。
具体到汽车行业，Linux操作系统天然具备稳定、高效的能力，而风河公事凭借着自己稳健的流程控制能力和强大的技术实力，让应用于汽车行业的Linux操作系统变得安全而可控。
经过多年的技术积累，风河完全有能力深度定制和裁减Linux操作系统，从而满足功能安全中的暴露度、严重度、可控性要求，也完全可以杜绝社区工程师注入不稳定代码的问题。因此，风河Linux使企业能够在专门构建的Linux操作系统上开发、部署和运行强大、可靠且安全的嵌入式解决方案。
风河公司对于Linux操作系统的安全保障，我们可以从Wind River Studio Linux Services解决软件维护问题中一窥究竟。
由于智能汽车性能的不断创新，底层控制软件始终处于高速开发进程中，开发团队因此而面临着开发和部署新功能的压力，但另一方面，软件的维护工作必须同步跟上，否则新的漏洞会不断出现，技术债会持续累积，未修复的安全问题最终会影响合规性、与最终客户签订的服务水平协议(SLA)以及按时为系统和设备部署新软件的能力。在这样的条件之下，Wind River Studio Linux Services却可以为用户的既有Linux或Yocto项目平台确保全生命周期安全。
一家全球知名的网络设备供应商非常专注于创新，以至于在Linux平台的日常维护方面有所滞后，而且严重影响到发布新软件的能力，他们急需迅速发现并修复Linux操作系统中所有的严重安全风险。为此，风河公司为其提供全生命周期安全服务。我们在今天可以通过还原这个问题的解决过程，来体会风河技术工具的强大。
通过Wind River Studio Linux Services，风河提供了CVE(通用漏洞披露)识别、CVE优先级、安全生命周期管理等服务。应用CVE扫描工具Wind River Studio Security Scanning，用户的既有Linux平台上被识别出超过1500个CVE，其中超过80个属于严重风险等级。
在分析了CVE的真实影响之后，风河的技术专家与客户的工程师协作优先解决了需要立即关注的漏洞。接下来，在CVE被修复并且平台稳定的条件下，Wind River Studio的Linux团队开始负责提供平台的持续维护和管理。基于用户硬件的质量检查和测试，通过每日构建以确保对其操作系统平台和BSP进行持续性高质量修复。此外，Studio Linux Services团队还提供在线发布仪表板和报告，以便跟踪修复和进展，连同发布说明和工件，来捕获发布中修复的CVE和漏洞。
最终，在软件维护瓶颈被突破之后，这家网络设备供应商的开发团队的创新能力得到了充分释放。并且这些工作应用Studio Security Lifecycle Assurance服务来完成，要比原来自行处理的方法节省了大量成本。
当稳定性、高性能、安全性同时具备时，也就不难解释风河Linux在汽车行业一路顺利攻城拔寨的原因了。合众汽车选用风河Wind River Linux开发最新软件定义汽车平台、极氪选用风河Wind River Linux用于未来EEA开发、哪吒选用风河Wind River Linux开发智能域控制器XPC-S32G推进软件定义汽车。
更为重要的，是风河在汽车行业没有仅仅止步于操作系统层面，也开始延伸到整个汽车基础架构软件领域，在先进的高性能计算、车辆内部及其与基础设施之间(V2X)各类网络以及高级驾驶辅助系统(ADAS)等领域越走越远。
记得Linux操作系统在服务器端成功的原因吗?当Linux操作系统单打独斗时，用户会因为它缺少应用支持而和它保持距离;只有当LAMP支撑起一个生态时，Linux操作系统才开始了成长之旅。今天，当Linux操作系统在汽车行业成为主流之时，就可能依托强大的平台，实现更为迅速的开源生态成长。这可能才是如风河这样的Linux中兴力量让Linux操作系统变得更为健硕的终极原因。
]]></content>
  </entry>
  
  <entry>
    <title>如何计算串口的传输速率</title>
    <url>/post/hardware/how-to-calculate-uart-rate.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>UART</tag>
      <tag>Baud Rate</tag>
    </tags>
    <content type="html"><![CDATA[1960年代，DEC（Digital Equipment Corp）公司的Gordon Bell采用大约50个分离元件设计了一个电路板，发明了 UART串口  。
前言 时至今日，已经60多年，虽然在个人消费类电子产品中，UART已近乎绝迹；但在工业、科研、国防、航空/航天等领域，UART却无处不在。
很多从事多年 嵌入式  开发的朋友居然还不会根据波特率计算UART的速率，下面和大家介绍一下计算方法。
uart帧格式 要证明速率的计算，必须搞清楚uart数据帧格式
其中各位的意义如下：
 起始位：先发出一个逻辑 &ldquo;0&rdquo; 信号，表示传输字符的开始 数据位：可以是5~8位逻辑 &ldquo;0&rdquo; 或 &ldquo;1&rdquo;，如ASCII码（7位），扩展BCD码（8位），小端传输 校验位：数据位加上这一位后，使得 &ldquo;1&rdquo; 的位数应为偶数(偶校验)或奇数(奇校验) 停止位：它是一个字符数据的结束标志。可以是1位、1.5位、2位的高电平[通常为1位] 空闲位：处于逻辑&quot;1&quot;状态，表示当前线路上没有资料传送  例如我们传输数据0X33（00110011），那么对应的波形就是如下这样，因为是LSB在前，所以8位数据依次是11001100。
由上图可得，
假设没有奇偶校验位，停止位为1位 要传输1个字节（8个bit）数据， 那么实际硬件上需要传输10位（1个起始位，8个数据位，1个停止位）
如果有奇偶校验位，停止位为1位 要传输1个字节（8个bit）数据， 那么实际硬件上需要传输11位（1个起始位，8个数据位，1个奇偶校验位，1个停止位）
波特率、比特率 波特率 UART速率，也称为波特率，是指数据在串行通信中每秒钟传输的位数（包含起始位、数据位、校验位、停止位）。
比如：115200波特率意思就是在二进制传输条件下每秒传输115200个比特。
比特率 比特率也叫信息传输速率，单位时间传递的平均信息量或比特数（只包含有效的数据位）。
比特率通常以bps（bits per second）表示。
速率计算 比特率与波特率计算公式是：
比如没有奇偶校验位，停止位为1位，波特率115200对应的比特率为
比特率 = （115200*8 ）/(1个起始位 + 8个数据位 + 0个奇偶校验位 + 1个停止位) = 115200*8/10 = 11520*8 b/s 转换为字节 就是11520 B/s，即每秒传输11520个字节（帧） ]]></content>
  </entry>
  
  <entry>
    <title>Marvell Teralynx 10 51.2Tbps 交换机</title>
    <url>/post/datacenter/marvell-teralynx-10-51.2-tbps-switch.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Marvell</tag>
      <tag>Teralynx</tag>
    </tags>
    <content type="html"><![CDATA[随着 人工智能（AI）  技术的迅猛发展，对数据处理和传输的需求日益增加，网络基础设施也必须随之升级。
Marvell Teralynx 10 作为一款51.2Tbps的交换机，正是为了满足这些高要求而诞生的。
本文将深入探讨这款革命性的交换机内部结构和技术细节，揭示其在未来AI集群中的重要作用。
2021年，Marvell收购了Innovium，这家公司以其Teralynx 7交换机闻名，后者具备32个400GbE端口和12.8Tbps的吞吐量。Innovium是当时最成功的初创公司之一，成功打入了超大规模数据中心市场。
相比之下，英特尔在收购 Barefoot Networks  后未能达到预期效果，反而在2022年宣布退出以太网交换业务。博通在商用交换机芯片市场占据了重要地位，而Innovium/Marvell则在超大规模数据中心取得了显著进展。
Marvell Teralynx 10 的技术细节 外部结构 从交换机的正面来看，Teralynx 10采用了一个2U机箱，正面布满了64个OSFP（Octal Small Form-factor Pluggable）端口，每个端口支持800Gbps的速率。每个接口都装有OSFP光学器件，这些器件比传统的QSFP+/QSFP28光学器件更大，并且配备了集成散热器，从而提高了散热效率。
在交换机的右侧，可以看到管理和控制台端口，而在背面则是风扇和电源模块。为了支持这些高功率的光学器件和交换机芯片，Teralynx 10的电源额定功率超过2kW。
内部结构 打开交换机，我们首先看到的是大型散热器，覆盖了Teralynx 10的核心芯片。这款芯片采用了5nm工艺，功耗高达500W。散热器的设计考虑到了芯片的高热量输出，确保在高负载下仍能保持稳定运行。
在散热器下方是交换机的PCB板，这里布局了32个OSFP笼子，整个交换机内部共有64个OSFP笼子分布在两个块中。Teralynx 10的交换芯片位于OSFP笼子的后方，这个芯片是整个系统的核心，负责处理所有的数据传输任务。
交换机还配备了基于Marvell Octeon的管理控制器，并且主配电板上还安装了一个M.2 SSD，用于存储管理数据。为了便于诊断和维护，交换机内部还留有PCIe插槽和10Gbase-T管理端口。
冷却系统和产品迭代展望 Teralynx 10的冷却系统设计相对简单，但高效。机箱后部安装了四个风扇模块，这些风扇能够提供足够的气流，确保芯片和光学器件在高负载下也能保持低温运行。
在Marvell的实验室中，Teralynx 10被放置在机架中，并与Keysight Ixia AresONE 800GbE测试设备连接。这些设备能够模拟800GbE的高负载网络流量，验证交换机在实际使用中的性能。
在测试中，Teralynx 10表现出色，能够在多个端口上同时处理高带宽数据流，线路速率达到99.3%。
随着AI技术的发展，数据中心对网络交换机的性能要求将越来越高。Marvell Teralynx 10凭借其51.2Tbps的高吞吐量和64个800GbE端口，完美契合了未来AI集群的需求。其先进的芯片工艺、强大的散热系统和灵活的管理控制器，确保了在高负载下的稳定性和高效性。
小结 Marvell Teralynx 10不仅是一款技术先进的交换机，更是未来 数据中心  和AI集群的核心组件。随着更多新技术的引入，这款交换机将在未来几年内发挥越来越重要的作用。
]]></content>
  </entry>
  
  <entry>
    <title>了解Linux定时任务Cron</title>
    <url>/post/linux/understanding-linux-scheduled-tasks-cron.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Cron</tag>
    </tags>
    <content type="html"><![CDATA[Cron是 Linux  和Unix系统中的任务调度程序，用于在指定的时间或间隔自动执行脚本或命令。它的主要作用是帮助系统管理员和用户自动化一些重复性的任务，从而节省时间和避免人为错误。无论是定时备份数据、定期清理日志文件，还是自动发送报告，Cron都能胜任。
在Linux系统中，自动化是提高效率的关键，而Cron作为最常用的自动化工具之一，对于每个Linux用户来说都至关重要。掌握Cron，不仅能让你轻松处理日常任务，还能让你在系统管理和维护中游刃有余，减少手动操作的负担。
Cron基础知识 Cron是由Ken Thompson在20世纪70年代为Unix系统开发的，最初的目的是简化系统管理任务。随着时间的推移，Cron逐渐演变成了现代Linux系统中不可或缺的定时任务工具。它以crond守护进程的形式运行在后台，定期检查和执行预定义的任务。
Cron的最早版本出现在Unix V7中，由于其简单有效的设计，很快被其他Unix系统采纳。随着Linux的发展，Cron也得到了广泛应用。现代Linux发行版几乎都内置了Cron，并且经过多次优化，功能和性能都得到了显著提升。
Cron的工作原理非常简单：它通过解析用户或系统定义的Crontab文件，确定任务的执行时间和频率。当当前时间与Crontab文件中的时间条件匹配时，Cron便会启动相应的命令或脚本。Cron的整个过程都是自动化的，用户只需定义好任务和时间规则，剩下的工作交给Cron即可。
Cron的核心组件 Cron守护进程（crond） crond是一个后台运行的守护进程，负责执行所有由Cron安排的任务。它会定期检查系统和用户的Crontab文件，并根据文件中的时间表执行任务。crond通常在系统启动时自动启动，并一直运行，确保所有定时任务能够按时执行。
Crontab文件 Crontab文件是Cron调度任务的核心配置文件。它包含了用户或系统定义的任务和时间表。Crontab文件分为用户Crontab和系统Crontab。
  用户Crontab：每个用户都可以拥有自己的Crontab文件，存放在/var/spool/cron/crontabs/目录下。用户可以通过crontab -e命令来编辑自己的Crontab文件。
  系统Crontab：系统级Crontab文件通常位于/etc/crontab，由系统管理员管理，主要用于安排系统级任务。
  # /etc/crontab: system-wide crontab # Unlike any other crontab you don&#39;t have to run the `crontab&#39; # command to install the new version when you edit this file # and files in /etc/cron.d. These files also have username fields, # that none of the other crontabs do. SHELL=/bin/sh # You can also override PATH, but by default, newer versions inherit it from the environment #PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin # Example of job definition: # .---------------- minute (0 - 59) # | .------------- hour (0 - 23) # | | .---------- day of month (1 - 31) # | | | .------- month (1 - 12) OR jan,feb,mar,apr ... # | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat # | | | | | # * * * * * user-name command to be executed 17 *	* * *	root cd / &amp;&amp; run-parts --report /etc/cron.hourly 25 6	* * *	root	test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.daily ) 47 6	* * 7	root	test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.weekly ) 52 6	1 * *	root	test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.monthly ) # Cron的目录结构 Cron的目录结构是其灵活性和可扩展性的体现。除了用户和系统Crontab文件外，Cron还支持其他几种配置方式。
 /etc/crontab：系统级Crontab文件，通常由系统管理员配置，用于安排与系统维护相关的任务。 /etc/cron.d/：该目录下可以存放多个独立的Crontab文件，每个文件都可以定义自己的任务和时间表。适用于需要多个独立任务配置的场景。 /var/spool/cron/：这是用户Crontab文件的存放目录。每个用户的Crontab文件都会存储在这里，文件名与用户名对应。  Cron语法 Cron语法由五个时间字段和一个命令字段组成。每个时间字段表示一个特定的时间单位，用户可以通过这些字段精确定义任务的执行时间。
 分钟（0-59）：指定任务在某分钟执行。例如，0表示整点执行。 小时（0-23）：指定任务在某小时执行。例如，14表示下午2点执行。 日期（1-31）：指定任务在某天执行。例如，15表示每月15日执行。 月份（1-12）：指定任务在某月执行。例如，7表示7月份执行。 星期（0-6）：指定任务在某星期几执行。0和7都表示星期天，1表示星期一，以此类推。  为了实现更复杂的时间调度，Cron语法支持多种特殊字符。
 星号（）：表示任何值。例如， * * * *表示每分钟执行一次任务。 逗号（,）：用于分隔多个值。例如，0,15,30,45 * * * *表示在每小时的0、15、30、45分钟执行任务。 连字符（-）：用于定义一个范围。例如，1-5表示从1到5的所有值。 斜杠（/）：用于定义增量。例如，*/5表示每5个单位执行一次任务。  Cron还支持一些快捷字符串，用于简化常见的时间调度需求。
 @reboot：在系统启动后执行任务。 @yearly：每年执行一次，等价于0 0 1 1 *。 @monthly：每月执行一次，等价于0 0 1 * *。 @weekly：每周执行一次，等价于0 0 * * 0。 @daily：每天执行一次，等价于0 0 * * *。 @hourly：每小时执行一次，等价于0 * * * *。  Cron的工作机制 Cron任务的启动时机由Crontab文件中的时间字段决定。crond守护进程会每分钟检查一次Crontab文件，并在时间匹配时启动相应的任务。任务启动后，Cron会在后台执行任务，执行完成后将结果返回到指定的日志文件或发送至用户邮箱。
由于Cron任务在后台运行，因此环境变量的设置非常重要。默认情况下，Cron任务使用的环境变量与用户登录时的环境变量不同。如果任务需要使用特定的环境变量（如路径变量），则应在Crontab文件中显式设置这些变量。可以通过在Crontab文件的顶部添加环境变量设置来实现这一点。
PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin Cron任务通常在sh shell中执行，如果任务脚本依赖于其他shell（如bash或zsh），需要在Crontab中指定使用的shell。例如，在Crontab文件的开头添加SHELL=/bin/bash，即可让Cron任务在bash环境中执行。
Cron任务的设置与管理 用户可以通过crontab -e命令编辑自己的Crontab文件，添加或修改定时任务。编辑器通常是系统默认的文本编辑器，如vim或nano。每个任务一行，格式为时间字段 命令。
# 每天凌晨2点执行备份脚本 0 2 * * * /home/user/backup.sh 系统管理员可以编辑/etc/crontab文件，定义系统级的定时任务。系统Crontab文件格式与用户Crontab略有不同，时间字段后面还需要指定运行任务的用户。
# 每天凌晨3点执行系统日志清理 0 3 * * * root /usr/bin/logrotate 用户可以使用crontab -l命令查看自己的Cron任务列表。系统管理员可以通过查看/etc/crontab和/etc/cron.d/目录下的文件，查看系统级的Cron任务。
# 列出当前用户的Cron任务 crontab -l 编辑Cron任务可以通过crontab -e命令进行，删除任务则可以通过crontab -r命令完成。
# 删除当前用户的所有Cron任务 crontab -r Cron任务的执行状态可以通过日志文件查看。通常，Cron的执行日志记录在/var/log/syslog或/var/log/cron文件中。可以使用grep命令过滤日志，查找特定任务的执行记录。
# 查找所有与Cron相关的日志记录 grep CRON /var/log/syslog Cron的高级使用 Cron中的时间调度技巧 高级用户可以使用复杂的时间调度技巧，如结合多个时间字段、使用特殊字符和范围，来实现精确的任务调度。例如，每隔10分钟从早上8点到晚上8点执行任务，可以使用以下Crontab规则：
# 每隔10分钟执行一次任务，从8:00到20:00 */10 8-20 * * * /path/to/command Cron的调试与故障排除 当Cron任务没有按预期执行时，可以通过以下几种方式进行调试和故障排除：
 检查Crontab语法：确保Crontab文件中没有语法错误，可以通过在线Crontab语法检查工具验证。 查看日志文件：通过检查Cron日志文件，查看任务是否有执行记录，是否有报错信息。 添加调试信息：在Crontab中添加调试信息，如将输出重定向到文件，记录任务执行时的输出内容。  # 将任务的输出重定向到文件，便于调试 * * * * * /path/to/command &gt;&gt; /tmp/cron_debug.log 2&gt;&amp;1 常见Cron使用场景 定时备份与数据同步 定时备份是Cron最常见的应用场景之一。可以使用Cron安排每天定时备份数据库、文件或整个系统，并将备份文件同步到远程服务器。
# 每天凌晨3点备份数据库，并同步到远程服务器 0 3 * * * /usr/bin/mysqldump -u root -p password database | gzip &gt; /backups/db_backup.sql.gz &amp;&amp; rsync -avz /backups/db_backup.sql.gz user@remote:/backup/ 自动清理与维护任务 系统维护任务，如清理临时文件、压缩日志文件、删除过期数据等，也可以通过Cron自动化。
# 每周一凌晨4点清理/tmp目录中的过期文件 0 4 * * 1 find /tmp -type f -mtime +7 -exec rm {} \; 自动发送报告与提醒 Cron还可以用于定期生成和发送报告，如系统状态报告、网站流量报告等。
# 每天早上7点生成系统状态报告并发送到指定邮箱 0 7 * * * /usr/local/bin/system_report.sh | mail -s &#34;Daily System Report&#34; user@example.com 监控与报警 通过Cron定时执行监控脚本，可以实时检测系统或服务状态，并在出现异常时发送报警通知。
# 每5分钟检查web服务是否运行，如果停止则重启并发送报警邮件 */5 * * * * /usr/local/bin/check_web_service.sh || (/usr/bin/systemctl restart httpd &amp;&amp; echo &#34;Web Service Restarted&#34; | mail -s &#34;Web Service Alert&#34; user@example.com) Cron的安全性 限制用户使用Cron 为了防止未经授权的用户滥用Cron，可以通过编辑/etc/cron.allow和/etc/cron.deny文件来控制哪些用户可以使用Cron。
 /etc/cron.allow：列出允许使用Cron的用户。如果文件存在，只有文件中列出的用户可以使用Cron。 /etc/cron.deny：列出禁止使用Cron的用户。如果文件存在，列在其中的用户将被禁止使用Cron。  设置Cron任务的权限 在设置Cron任务时，确保脚本和命令的权限设置正确，防止未授权用户访问或修改任务脚本。
# 设置脚本的权限为仅所有者可读写执行 chmod 700 /path/to/script.sh 在使用Cron执行一些需要提升权限的任务时，可以通过SUID位设置，确保任务在正确的权限下执行。然而，需要谨慎使用SUID，避免引入安全风险。
]]></content>
  </entry>
  
  <entry>
    <title>详细解读UCIe 2.0</title>
    <url>/post/datacenter/a-detailed-explanation-of-UCIe-2.0.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>UCIe 2.0</tag>
    </tags>
    <content type="html"><![CDATA[因为摩尔定律的失效，半导体行业过去多年正在寻找提升芯片性能的方法，而Chiplet正在成为几乎所有巨头的共同目标。然而，因为Chiplet的理念是将芯片的不同功能模块变成一个die，如何保证这些die能够更通用地连接到一起就成为了行业的头等大事。
于是，UCIe（ Universal Chiplet Interconnect Express ）便顺势成立。据介绍，UCIe是一种开放的行业架构标准，可在不同chiplet之间提供die-to-die之间的接口，解决物理芯片间 I/O 层、芯片间协议和软件堆栈问题。
UCIe为die提供了标准接口
而继之前的UCIe 1.0和UCIe 1.1之后，新的UCIe 2.0标准正式发布。
UCIe 2.0，发布 UCIe联盟日前宣布发布其 2.0 规范。UCIe 2.0 规范增加了对标准化系统架构的支持，以实现可管理性，并全面解决了跨多个chiplets的 SIP 生命周期（从分类到现场管理）的可测试性、可管理性和调试 (DFx) 设计挑战。可选的可管理性功能和 UCIe DFx Architecture (UDA) 的引入，包括每个芯片内的管理结构，用于测试、遥测和调试功能，允许通过灵活统一的 SIP 管理和 DFx 操作方法实现与供应商无关的芯片互操作性。
此外，2.0 规范支持 3D 封装，与 2D 和 2.5D 架构相比，可提供更高的带宽密度和更高的功率效率。UCIe-3D 针对混合键合进行了优化，凸块间距可适用于大至 10-25 微米、小至 1 微米或更小的凸块间距，从而提供灵活性和可扩展性。
另一个功能是针对互操作性和合规性测试优化的封装设计。合规性测试的目标是根据已知良好的参考 UCIe 实现来Device Under Test (DUT) 的主频带支持功能。UCIe 2.0 为物理、适配器和协议合规性测试建立了初始框架。
UCIe 联盟总裁兼三星电子公司副总裁 Cheolmin Park 表示：“UCIe 联盟支持多种芯片，以满足快速变化的半导体行业的需求。UCIe 2.0 规范在之前的版本基础上开发了全面的解决方案堆栈，并鼓励芯片解决方案之间的互操作性。这又是联盟致力于蓬勃发展的开放芯片生态系统的又一例证。”
UCIe 2.0 规范的亮点：
 全面支持具有多个chiplets的任何系统级封装 (SiP) 结构的可管理性、调试和测试。 支持3D封装，显著提升带宽密度和功率效率。 改进的系统级解决方案，其可管理性被定义为chiplet堆栈的一部分。 针对互操作性和合规性测试优化的封装设计。 完全向后兼容 UCIe 1.1 和 UCIe 1.0。  与此同时，我们也带来了UCIe联盟同步发布的UCIe 2.0白皮书，以飨读者。
以下为白皮书正文：
UCIe 2.0 规范：持续创新，推动开放 Chiplet 生态系统 Universal Chiplet Interconnect Express (UCIe) 是一种开放的行业标准互连，可在 Chiplet 之间提供高带宽、低延迟、节能且经济高效的封装内连接。它满足了整个计算领域（涵盖云、边缘、企业、5G、汽车、高性能计算和手持设备）对计算、内存、存储和连接的预计不断增长的需求。UCIe 能够封装来自各种来源的die，包括不同的代工厂、设计和封装技术。
UCIe 2.0 规范涉及两个广泛的领域，以推动蓬勃发展的开放 Chiplet 生态系统。第一个规范以整体方式解决了任何具有多个 Chiplet 的系统级封装 (SiP) 结构中出现的可管理性、调试和测试挑战。该解决方案超越了 UCIe 接口，使用 UCIe 增强功能，以完全向后兼容的方式进行扩展；第二个领域涉及使用混合键合互连等技术（我们将其称为 UCIe-3D）的间距非常细（9 µm 到大约 1 µm，甚至更低）的垂直集成芯片。
在整个芯片生命周期中解决 SiP 级别的可管理性、调试和测试挑战 可测试性、可管理性和调试是需要持续创新的三个主要方面。UCIe 1.0 和 1.1 规范有几种机制来处理互连级别的可管理性和测试/调试/遥测（统称为 DFx）设计的各个方面。示例包括通道裕度（lane margining）、合规性测试、故障报告、边带访问（sideband access）等。然而，在芯片和 SiP 级别仍有许多具有挑战性的问题必须解决，才能实现开放、即插即用的基于chiplet的生态系统的愿景。
UCIe 联盟正在全面解决这些挑战，超越接口级别，解决从die分类、封装/键合到现场级别的挑战——这涵盖整个硅片生命周期，这些增强功能将使我们的成员能够应用这些学习成果并改进上游。
在本文中，我们提供了实现广泛的、即插即用的基于小芯片的生态系统所需克服的挑战的示例。
在分类芯片测试期间，虽然我们可以探测凸块，但无法对微凸块进行探测；尤其是当我们转向 25µ 凸块间距时。因此，我们必须创新，使用其他凸块。同样，我们应该能够在现场无缝管理维修或固件升级。
对于在封装级别可控性和可观察性有限的Chiplet，调试提出了独特的挑战（例如，无法在封装内插入逻辑分析仪或示波器）。行业应如何处理 SiP 中芯片的可管理性？最重要的是，我们如何安全地解决这些问题？一些chiplets可能无法从封装引脚直接访问（见图 1a），这一事实使这些问题变得更加困难。我们还需要处理各种带宽需求。例如，不同的chiplets对扫描链、调试、可管理性等所需的带宽范围不同。
我们对 UCIe 2.0 规范的方法是定义一个通用基础设施，该基础设施可在使用现有 IP 构建块（building blocks ）以及封装级别的外部接口的同时解决所有已确定的挑战。我们认为这些功能是互补的，我们的方法适用于现有 IP（甚至非 UCIe IP），并对 UCIe PHY 进行了增强。我们还使用外部封装引脚来访问芯片集，以通过规范中定义的桥接机制进行管理、调试或测试。这些接口和 IP 必须与封装上的 UCIe 2.0 链路无缝协作，以提供所需的外部和内部访问。图 1b 列出了不同接口可用的带宽，为 SiP 设计人员提供了多种选择。
在 UCIe 2.0 规范中，可管理性是可选的。支持的机制包括发现chiplet集及其配置；初始化芯片集结构（initialization of chiplet structures）和参数（即串行 EEPROM 替换）；固件下载；电源和热管理；错误报告；遥测；检索日志和崩溃转储信息；测试和调试；启动和报告自检状态；以及芯片安全的各个方面。这些机制利用现有的适用行业标准，并且与chiplets上的底层协议无关。这些机制旨在跨来自不同供应商的chiplet工作，并支持特定于供应商的扩展。这些功能是可发现和可配置的，允许在 SiP 之间快速部署通用固件库。UCIe 可管理性所需的核心功能可以通过硬件和/或固件实现，从而提高灵活性。
UCIe 2.0 可管理性基线架构（manageability baseline architecture，如图 2）定义了一种桥接功能，用于连接到外部接口（例如 SMBus 或 PCIe），从而实现封装外连接。每个chiplet组中的管理结构由多个管理元素组成，其中一个元素充当管理主管，负责发现、配置和协调 SiP 的整体管理，并充当可管理性信任根。
UCIe 管理传输被定义为一种独立于媒体（media-independent）的协议，用于芯片组中管理实体之间以及 SiP 中芯片组之间的通信。安全机制被定义为根据功能提供所需的保护级别。定义了两种管理链路封装机制，以使用边带和主带传输 UCIe 管理传输数据包。UCIe 定义了最多八个独立的虚拟通道来提供服务质量，每个通道都具有有序或无序语义。数据包基于信用（credits）进行交换，信用最初是在链路训练期间协商的。
UCIe DFx 架构 (UDA：UCIe DFx architecture) 包含测试、遥测和调试，并通过管理结构进行覆盖。UDA 基于每个chiplet内的 Hub-Spoke 模型（图 2）。每个chiplet都支持一个 DFx 管理中心 (DMH：DFx Management Hub )，这是一个管理元素，可充当访问芯片内测试、调试和遥测功能的网关。DMH 允许发现这些功能，并将与这些功能相关的管理传输数据包路由到各种连接的 DFx 管理“辐条”(DMS：DFx Management “Spokes” )。辐条（Spokes）是实现给定测试、调试或遥测功能的实体。一些示例包括扫描控制器、MEM BIST、SoC（片上系统）结构调试、跟踪协议引擎、核心调试、遥测等。
架构配置寄存器（图 3）在现有寄存器之上具有 UCIe-wrapper，为软件提供了一个通用框架。对于系统级使用，可以根据 Spoke 的（UCIe 联盟分配的）供应商 ID (VID) 和（供应商分配的）Device ID (DID) 加载特定于供应商的驱动程序以支持每个独特的功能。UDA 的管理数据包可以作为内存访问协议数据包（例如，用于发现chiplet中的 DMH/DMS）和/或以供应商定义的 UCIe DFx 消息格式（例如，用于通过chiplet将调试信号发送到 PCIe 等封装引脚，以便使用逻辑分析仪进行观察）发送。图 4 演示了其他使用模型。
虽然管理数据包可以在现有 UCIe 端口上进行时分复用，但 UCIe 2.0 还提供了添加专用 UCIe-S 端口以实现可管理性和 UDA 功能的额外功能。这些端口可以是简单的边带（sideband），以 4 个凸块或半宽 (x8) 提供 800 Mb/s/方向，或更高，UCIe-S 以 32 GT/s 的速度为每个 x8 提供 256 Gb/s/方向。
垂直集成芯片组可显著提高功率性能，并采用 UCIe-3D UCIe 联盟于 2022 年 3 月成立，我们发布了定义明确的 UCIe 1.0 规范，解决了平面连接（2D 和 2.5D）问题。我们认识到垂直集成的重要性，并表示我们打算研究 3D 芯片组。UCIe 2.0 规范通过完全定义的规范（涵盖平面和垂直连接）兑现了这一承诺。
十多年来，随着封装内存和计算的商业化，提供垂直连接的 3D 互连芯片组的技术取得了显著进步，证实了需求的存在。现在是时候通过一系列选项来标准化接口，以满足生态系统中的各种需求。
3D 封装技术（例如混合键合 (HB：Hybrid Bonding)）的最新趋势是大幅缩Chiplet之间的凸块间距。UCIe-3D 的目标是将凸块间距从 9 µm 缩小到 1 µm，甚至可能更低。3D 互连将Chiplet之间的距离缩小到几乎为 0。因此，互操作性需要限制在相同的凸块间距内。虽然这不是一种广泛的即插即用（即，凸块间距为 1 µm 的芯片只能与凸块间距为 1 µm 的另一个chiplet混合键合，而不能与凸块间距为 9 µm 的chiplet混合键合），但关键性能指标 (KPI：key performance indicator) 的改进（例如带宽密度、功率效率等）是巨大的。如表 1 所示。
UCIe-3D 的第一大优势是带宽密度增加。这是双重优势。首先，减小的凸块间距（从 9 µm 降至 1µm 以下）意味着给定面积的导线数量与平方成反比；例如，将 2.5D 的 25 µm 与 3D 的 5 µm 进行比较，可得出相同面积的导线数量增加 25 倍；其次是面积本身。与 UCIe 2D/2.5D 相比，UCIe-3D 具有真实连接与海岸线消耗的优势。这意味着外围 PHY 上不会浪费任何面积，并且整个芯片组都可用于 3D 连接。
图 5 显示了使用 UCIe-3D 连接的两个Chiplet组和九个片上网络控制器 (NOC)。要获得凸块间距缩放的好处，必须保持相关电路简单，限制凸块。随着带宽密度的增加，无需驱动更高的频率。如表 1 所示，即使在 4 GT/s 频率下，带宽密度也比 32 GT/s 的 UCIe 2.5D 提高了几个数量级（例如，凸块间距为 1 µm 的 UCIe-3D 为 300 TB/s/mm²，而凸块间距为 25 µm 的 UCIe-2.5D 为 1.35 TB/s/mm²）。为了适应减小的凸块间距，我们通过选择适当的误码率 (BER：bit error rate ) 消除了对 (反) 序列化、CRC、重放等的需求（如表 1 所示）。同样，ESD 保护电路必须先降低至 5V CDM，并从 3 µm 开始逐渐消除。
UCIe-3D 的第二个主要优势是功耗更低。随着距离减小（~0），相关的电寄生效应也随之减小。随着 SoC 频率（&lt;= 4 GT/s），电路变得简单 - 由简单的逆变器组成。再加上频率降低，功耗甚至更低（至少低一个数量级）。
结论 UCIe 技术发展势头强劲！自 UCIe 联盟成立以来，UCIe 联盟成员已宣布了产品开发，并提供了基于 UCIe 1.0 和 1.1 规范的可操作硅片演示。我们正处于与其他成功标准（包括 PCIe、CXL 和 USB）类似的数十年历程的早期阶段。随着技术的普及，我们的成员致力于对未来规范进行必要的改进；
UCIe 2.0 是我们承诺的体现。可管理性和 DFx 增强功能表明我们不断致力于改进现有方法，而 UCIe-3D 则表明我们愿意接受必要的挑战，以实现能效性能的指数级改进。
最后，我想描绘一个系统级封装的愿景，其中使用现有的 UCIe-2.5D 和 UCIe-2D 平面互连连接多个 UCIe-3D 芯片组堆栈，以及所有即将推出的增强功能。如今的芯片级封装就像是小城市，其密度高于十年前的单片芯片，而后者可以比作小村庄。未来采用 UCIe-3D 的 SiP 将像一座摩天大楼林立的大都市，密度极高。计算和内存元件紧密封装在一起的高密度意味着比特传输距离更短，从而实现卓越的性能和更低的功耗。换句话说，未来确实非常光明。
值得一提的的是，作为芯片行业的重要参与者，NVIDIA 硬件工程副总裁 Ashish Karandikar在评价UCIe新标准时候谈到：“UCIe 2.0 规范的发布标志着基于芯片的系统设计发展的一个重要里程碑，它提供了一种初始化、管理和调试片上系统的标准化方法。作为 UCIe 联盟的成员，NVIDIA 致力于推进该规范的各个方面，以帮助推动下一代计算系统的创新和性能。”
]]></content>
  </entry>
  
  <entry>
    <title>传高通考虑收购英特尔芯片设计业务</title>
    <url>/post/news/qualcomm-reportedly-considering-buying-Intel-chip-design-business.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Intel</tag>
      <tag>Qualcomm</tag>
    </tags>
    <content type="html"><![CDATA[据路透社今日报道， 高通  已探索收购英特尔部分股权的可能。据两位知情人士透露，高通正在寻求收购英特尔的设计业务，以丰富其产品组合。
知情人士称，高通已经考虑收购英特尔的不同业务； 英特尔  正努力创收，希望剥离业务部门并出售其他资产。
其中一位消息人士说，高通高管对英特尔的客户端PC（个人电脑）设计业务非常感兴趣，但他们正在关注英特尔的所有设计部门。
另一位了解高通运营情况的消息人士认为，高通收购英特尔的其他业务，比如服务器业务，意义不大。
英特尔发言人回应称，高通尚未就收购事宜与英特尔接洽，拒绝就其计划发表评论，谈到英特尔“非常重视PC业务”。高通拒绝发表评论。
市值1824亿美元的高通以智能手机芯片而闻名。几个月来，高通一直在制定收购英特尔部分业务的计划。消息人士称，高通的兴趣和计划尚未最终确定，可能会发生变化。
英特尔上个月公布了灾难性的第二季度业绩，裁员15%，并暂停派发股息。其高管们正在努力思考如何继续为公司的制造计划提供资金并创收。目前英特尔的总市值已经跌至830亿美元。
由于PC市场整体疲软，去年英特尔PC客户端业务收入下降8%至293亿美元。高管们认为AI PC的推出将推动消费者升级他们的电脑，从而带动销售。本周早些时候，英特尔推出了一款名为Lunar Lake的新型PC芯片，并声称这款芯片为AI应用提供了出色性能。
英特尔董事会将于下周召开会议，讨论英特尔CEO帕特·基辛格和其他高管提出的削减业务建议。据路透社报道，潜在的选择包括出售其可编程芯片部门Altera。
]]></content>
  </entry>
  
  <entry>
    <title>传英特尔拟出售Altera，Marvell或成潜在买家</title>
    <url>/post/news/intel-ceo-pitch-board-plans-shed-assets-cut-costs.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Intel</tag>
      <tag>Altera</tag>
      <tag>Marvell</tag>
    </tags>
    <content type="html"><![CDATA[9月1日，据路透社报道，英特尔CEO帕特·基辛格（Pat Gelsinger）及其核心管理团队正紧锣密鼓地筹备一项重大战略调整方案，计划于本月末正式提交给公司董事会审议。该方案旨在通过剥离非核心业务与削减成本，重振英特尔作为昔日芯片业巨头的市场地位。
方案核心聚焦于两大方面：一是通过出售包括可编程芯片 Altera  在内的部分业务部门，以减轻财务负担并优化资源配置。鉴于英特尔当前难以持续支撑这些业务的高昂运营成本，此举被视为必要之举。二是调整资本支出策略，特别是在工厂扩建项目上或将采取更为谨慎的态度，甚至可能暂停或取消德国价值320亿美元的工厂建设计划，该计划此前已传出延期消息。
据知情人士进一步透露，基辛格及其团队计划于9月中旬的董事会上详细阐述这一方案，这将是基辛格提案内容的首次公开披露。值得注意的是，英特尔目前正积极寻求外部咨询支持，已聘请摩根士丹利与高盛两大投行作为顾问，协助评估哪些业务适合保留，哪些应被剥离。
此外，随着8月份公布的第二季度财报表现不佳，英特尔已宣布暂停派息、裁员15%并设定100亿美元的节省目标，预计2025年资本支出将削减至215亿美元，较今年下降17%。这一系列举措表明公司正面临前所未有的挑战，尤其是在与Nvidia等AI芯片制造巨头的竞争中显得力不从心。英特尔市值也因此跌落至千亿美元以下，与Nvidia高达3万亿美元的市值形成鲜明对比。
值得注意的是，Altera作为英特尔2015年以167亿美元收购的可编程芯片业务，其未来命运也悬而未决。英特尔虽已考虑将其分拆并部分上市，但据最新消息，Altera也可能整体出售给如Marvell等寻求扩张的芯片制造商。
9月中旬的董事会会议对于英特尔而言意义非凡，它不仅关乎公司短期内的财务健康，更将决定其长期发展战略的方向。随着市场环境的不断变化，英特尔能否通过此次战略调整成功转型，重新赢得市场与投资者的信任，将成为业界关注的焦点。
截至目前，英特尔官方对此次报道未予置评，但市场普遍期待基辛格及其团队能够带来一份令人信服的重振计划。
]]></content>
  </entry>
  
  <entry>
    <title>IBM发布了新一代处理器Telum II</title>
    <url>/post/datacenter/IBM-releases-new-generation-processor-Telum-II.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>IBM</tag>
      <tag>Telum II</tag>
    </tags>
    <content type="html"><![CDATA[IBM发布了新一代处理器Telum II，面向下一代IBM Z大型主机，可用于任务关键工作、 AI  负载。
Telum II采用三星5HPP 5nm工艺制造，包含430亿个晶体管，集成了8个高性能核心，改进了分支预测、存储写回、寻址转换等。
它的主频达5.5GHz，集成了36MB二级缓存(增加40％)、360MB三级缓存、2.88GB四级缓存——IBM处理器一向以海量多级缓存而闻名。
它改进了内置的AI加速器，INT8整数精度算力24 TOPS，四倍于上代产品，并针对低延迟实时AI负载进行了优化，可以从任何一个核心中接手AI任务，而在完整配置下每个机柜的算力可达192 TOPS。
Teum II
Teum II
2021年发布的初代Teum
同时，IBM还发布了新的Spyre AI加速卡，三星5LPE 5nm工艺制造，260亿个晶体管，包括32个AI加速核心，架构上与Telum II内置加速器基本一致。
它可以通过PCIe接入IBM Z主机的IO子系统，提供额外的AI加速。
Teum II处理器、Spyre加速器都将于2025年上市。
Teum II、Spyre加速卡
Spyre加速卡
]]></content>
  </entry>
  
  <entry>
    <title>RTOS正在缩小与Linux的差距</title>
    <url>/post/linux/the-rtos-renaissance-closing-the-os-gap-with-linux-in-iot.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>IoT</tag>
      <tag>RTOS</tag>
    </tags>
    <content type="html"><![CDATA[ RTOS  与 Linux  的物联网设备操作系统之争已经持续了很多年。Linux以其强大的计算能力和丰富的软件生态，在需要复杂处理和软件支持的物联网设备上占据一席之地；RTOS凭借实时响应和资源节约的特性，在对实时性和资源占用有严格要求的场景中独领风骚。
如果时间倒回五年前，那时候IoT彻底火了，但大型科技公司在面对RTOS和Linux的抉择中，对RTOS不感兴趣，更加青睐Linux，并把它推成主流。不过，在最近一段时间，风评反转了，实时操作系统 （RTOS）在嵌入式IoT领域开始重新复兴。
在开源计划、大型科技公司支持、对低延迟和资源受限应用程序不断增长的需求的推动下，RTOS 正在迅速缩小与Linux的差距。很多开发人员发现，在选择操作系统时，规模并非越大越好，随着更多传感器在时间关键型的应用程序中上线，越来越多人发现RTOS有着改变边缘世界的力量。
嵌入式的新变革 全面而精简，是RTOS被人注视的最大原因，毕竟谁又不喜欢“小而美”。并且，这几年，随着RTOS在保持实时响应性的同时，其在可用性、安全性和可扩展性等方面也取得进展。
目前，有将近三分之二的嵌入式系统都在使用RTOS，而RTOS迄今已经拥有了100多种不同的商业和开源产品，比如VxWorks、PX5、EmbOS、SafeRTOS等商业RTOS，再比如Eclipse ThreadX、RT-Thread、mbed OS、UCOS-II、NuttX、eCos、Zephyr等开源RTOS。
对于未来几年的市场，很多业内人士都很乐观吗，并表示RTOS在未来三年内每年或可增长10%。之所以能够有这种市场表现的原因在于大厂不断布局RTOS，帮助提升了RTOS开发环境，提供了增强的综合工具、强大的社区支持等，持续促进市场增长。
首先，是微软。2019年4月18日，ThreadX这一有名的RTOS被微软看中，其所有者Express Logic 被整体收购。从那时起，它的名字也被改为Azure RTOS。去年11月，微软宣布将Azure RTOS托管至Eclipse基金会，更名为Eclipse ThreadX，并过渡到开源模式。
其次，是亚马逊。2017年 FreeRTOS由亚马逊收购，并成为亚马逊Web Services（AWS）的一部分，进一步推动了其在物联网（IoT）领域的应用。同时，亚马逊宣布在FreeRTOS_V10内核基础上建立MIT licensed的Amazon FreeRTOS操作系统”。
最后，是MCU厂商。比如，乐鑫将FreeRTOS作为组件集成到ESP-IDF中，因为原生FreeRTOS是单核RTOS，而ESP32是双核的，因此乐鑫为了支持多核，将FreeRTOS 内核移植到ESP芯片的所有可用架构中；Linux基金会的Zephyr RTOS在Intel、NXP 和 Nordic等厂商的努力下，正在逐渐发展壮大，成为行业新主流；为嵌入式系统设计的lwIP（轻型IP）这样的RTOS IP堆栈正变得越来越强大；MbedTLS这样的加密库现在可以与OpenSSL的功能相媲美，OpenSSL长期以来一直是Linux系统的领导者。
RTOS和Linux的异同 RTOS和Linux都属于嵌入式领域，二者的根本区别在于它们的设计理念。
RTOS追求的是轻量级和极致的实时性（Real-time），通常是专为实时应用程序构建的，优先考虑任务执行的确定性和可预测性，使其在医疗、核能或航空航天等关键任务行业中得到广泛应用。同时，轻量化的设计，使得其在资源受限的嵌入式系统中非常受欢。
所谓实时性指的是操作系统（OS）在有限的响应时间内提供所需服务级别的能力。在实时系统中，计算的正确性不仅取决于结果的逻辑正确性，还取决于它产生结果的时间。
与通用操作系统不同，RTOS侧重于确定性响应时间和对任务调度的精确控制。相比来说，RTOS擅长管理任务优先级，允许关键任务优先于对时间不太敏感的流程，在系统必须保证运营在特定时间范围内时，这种优先级至关重要。企业通常在任务关键型场景中使用RTOS，此外，它同样适用于无法容忍任何形式系统故障的关键系统应用程序中。
Linux则更为通用，支持多用户和多任务处理，并提供了丰富的网络功能和设备独立性，也针对性地推出了实时Linux。不过，毕竟Linux需要处理大量系统调度和其它任务，实时性相对会差一些。
实时Linux的核心概念是“抢占”，也就是中断当前执行线程，立即处理优先级较高的事件。在没有内核抢占的情况下，在Linux中无法实现确定性响应时间。
RTOS和实时Linux之间的选择取决于系统特定延迟要求，以及对开销和资源效率的需求。虽然实时Linux为许多场景提供了强大的解决方案，但在某些关键的嵌入式系统中，专用 RTOS可能仍然更可取。如果项目需要在实时需求和运行各种应用程序的灵活性之间取得平衡，那么实时Linux将提供更通用的解决方案。
RTOS和Linux未来依然会共生 RTOS此时此刻的优势是明显的。对于构建时间敏感型应用程序的开发人员来说，RTOS的低延迟和可预测的实时任务执行比通用操作系统具有明显优势。此外，随着复杂软件堆栈中的安全漏洞不断浮出水面，RTOS的精简架构减少了攻击面。由于应用程序直接编译到操作系统中，因此与动态加载的程序相比，代码注入的可能性大大降低。
更重要的是，开发人员不再需要为了性能和安全性而牺牲可用性。从云到加密，RTOS解决方案正在迅速成熟，使开发人员能够构建安全、响应迅速且可扩展的嵌入式系统。
不过，要说RTOS彻底取代Linux，那也是完全不可能的。
毕竟，实时不会带来优化的性能，而且Linux在复杂的高端嵌入式系统中无法被取代。
RTOS通常是专业且资源密集型的，与通用操作系统相比，功能有限。在需要超出实时约束的不同应用程序的场景中，这可能是一个缺点。而且，现在实时Linux开始集成一些补丁（比如Ubuntu的PREEMPT_RT补丁），能够有效地为大多数实际的低延迟、低抖动工作负载提供服务，同时实现均衡的解决方案，从而减少开销并保持响应能力，最终减少维护成本，并使用有限的资源最大限度地提高。
总之，正是因为RTOS和Linux非常互补，开发人员才有了更多选择，能够自由地选择用什么系统，对开发者来说才是一次真正的胜利。
]]></content>
  </entry>
  
  <entry>
    <title>LLM训练的存储需求：训练数据和检查点</title>
    <url>/post/ai/a-checkpoint-on-checkpoints-in-llms.html</url>
    <categories><category>AI</category>
    </categories>
    <tags>
      <tag>LLM</tag>
    </tags>
    <content type="html"><![CDATA[LLM训练的存储需求主要涉及两个方面：
 训练数据：用于更新模型权重和促进模型收敛。 检查点：将模型权重从GPU内存保存至持久化存储。  训练数据 Transformer模型训练本质上是计算密集型的，这意味着所需的数据量相对较小。经过分词处理后的二进制训练数据，每个英语tokens约占4字节，每万亿tokens约需几TB存储空间。
当前领先的模型通常使用数万亿tokens进行训练，相当于数十TB的文本数据。
检查点 LLM检查点的存储需求与模型规模相关，而非训练集群大小。可以通过假设每个参数占用16字节来粗略估算检查点大小。
同样，检查点的性能需求也主要取决于模型规模。通过采用异步多级检查点策略，可进一步降低性能要求。
在大规模系统中，异步检查点机制被广泛采用。在此方案下，GPU仅在将检查点数据从GPU内存复制到主机内存时被阻塞。随后GPU继续计算，而CPU异步将数据刷新至非易失性存储。字节跳动的MegaScale系统采用了这种方法。
利用多级非易失性存储并在不同时间间隔进行检查点，可进一步减少带宽需求并提高可靠性。微软的Nebula框架采用以下策略：
 同步将检查点保存至CPU内存 异步复制到相邻节点的本地SSD，防止单节点故障 异步复制到对象存储，防止多节点故障、支持回滚及长期存储  在实际应用中，字节跳动使用HDFS进行检查点存储。
快速检查点与恢复（字节跳动MegaScale） 在识别并剔除故障机器后，驱动程序需要通过加载最近检查点的模型权重和优化器状态来恢复训练。确保最新检查点尽可能接近故障发生时的训练进度状态至关重要，以最小化计算和时间损失。这要求我们提高训练期间的检查点频率。然而，我们还需要减少检查点过程引入的延迟，特别是阻碍训练进度的关键路径时间，因为这会影响整体系统吞吐量。
为实现快速检查点，我们引入了一种优化的两阶段方法。第一阶段，每个 GPU  工作节点将其片上状态写入主机内存，然后继续训练过程。通过优化PyTorch的序列化机制并使用固定内存，借助高速PCIe带宽，该过程可缩短至几秒钟，从而最小化对持续训练过程的干扰。第二阶段，后台进程接管，异步地将状态从主机内存传输到分布式文件系统（在我们的部署中为HDFS）进行集中维护。这种将操作解耦为两个阶段的方法使GPU工作节点能在转储状态后几乎立即恢复训练，而更耗时的HDFS写入过程则被卸载到一个单独的非阻塞进程中。
在从检查点恢复的情境下，由于没有最新检查点就无法开始训练，因此这一过程处于关键路径上。瓶颈在于HDFS的带宽，特别是当每个GPU工作节点需要读取其对应的状态分区时。为缓解这一瓶颈，我们提出了一种优化的数据检索策略。我们认识到多个GPU工作节点通常共享相同的状态分区，例如同一数据并行组中的工作节点。因此，我们指定组内的单个工作节点从HDFS读取共享状态分区，从而线性减少负载。该工作节点随后将状态分区广播给所有共享相同数据的其他GPU工作节点。这种方法有效缓解了HDFS的带宽限制，显著缩短了恢复时间。
LLM中的检查点问题 随着深度学习领域的快速发展，数据集和模型规模呈现爆炸性增长。当前数据集规模可达PB级，单个模型参数量也可达数百GB。这使得模型无法完全加载到标准GPU显存中，凸显了高效并行化和可恢复性的重要性。
全维度并行化 对于LLM等大规模深度学习模型，数据和模型规模已超出单个设备的内存容量。以GPT-3为例，其模型大小超过500GB，而典型GPU仅有80GB显存。且单GPU训练GPT-3需耗时300余年。因此,多维度并行化成为训练和微调大模型的必然选择。
基于斯坦福大学、NVIDIA和微软研究院的研究，结合三种并行化方式可显著提升LLM训练效率：
 数据并行：在多个计算设备上复制整个模型，并分配训练数据。实现简单但内存消耗大。 模型并行：将模型划分为离散层或张量，分布到多个设备。实现复杂但更节省内存。 管道并行：将训练过程拆分为小步骤，在不同设备上执行。可能增加延迟,但能提高吞吐量。  检查点与可恢复性 即便采用并行化，完成一次模型训练仍可能耗时数月。因此，定期执行检查点操作以保证系统状态可恢复性至关重要。检查点通常在每个训练周期后或固定训练步数后执行。这不仅可以在训练中断时恢复进度，还允许回溯调整超参数，确保模型训练的可重复性。
在AI架构设计中，必须充分考虑检查点操作的支持。尽管AI模型训练主要受GPU限制，检查点过程却是I/O密集型操作。写入是检查点的主要负载，而读取则是恢复过程的瓶颈。因此，AI架构的I/O性能直接影响检查点和恢复效率。
检查点大小：深入分析 近年来，随着AI技术的快速发展，基础设施规模成为热点话题。以GPT-3为例，我们可以探讨如何为LLM配置适当的基础设施。
以下是一个具体配置示例：GPT-3拥有1750亿参数，假设部署于1024个GPU(128台NVIDIA DGX-A100，每台8个GPU)。采用三种并行化策略：
 张量并行：将500GB模型分散到8个GPU上。 管道并行：在8个DGX节点(八元组)上复制模型。 数据并行：16组8个管道并行系统(共128节点)。  每GPU配置1线程，每DGX 8线程，每八元组64线程。这种配置支持高效的检查点写入。
注意：常见误解是集群中每个GPU都需要进行检查点操作。实际上，只有一个八元组需要检查点。
该配置在Megatron模式下可实现线性扩展，适用于最大1万亿参数的模型，FLOP效率接近理论值的50%。
结论：检查点大小仅取决于模型大小，与数据量或GPU数量无关。配置LLM基础设施时，关键是了解模型大小并确保足够的I/O带宽。
检查点数学分析 下表展示了三个主流LLM的模型大小与检查点状态大小的线性关系：
检查点状态大小仅依赖模型大小，与数据集大小、检查点频率或GPU数量无关。
检查点操作详细分析 以3072个GPU(384台DGX)训练1万亿参数模型为例：
读取检查点：受限于存储系统1 TB/s读取带宽
写入13.8 TB检查点：273 GB/s，仅达存储系统写入带宽40%
每DGX约8 GB/s，受8线程限制
写入时间是关键考虑因素。示例中512个GPU需0.53 GB/s/GPU带宽，即4.3 GB/s/DGX。GPT-3检查点时间约50秒，每4小时一次，开销0.3%，远低于5%建议值。
恢复操作 所需读取带宽 = 数据并行度 × 写入带宽 = 6 × 273 GB/s = 1.638 TB/s
写入带宽为恢复所需读取带宽的17%
存储系统需提供1.64 TB/s读取和280 GB/s写入带宽，才能实现50秒的最优恢复时间(13.8 TB模型)。
检查点误区澄清 市场上存在每GPU需1 GB/s带宽用于全集群检查点的误解。实际上，仅管道并行集中的GPU需要此带宽。例如，1万亿参数模型的273 GB/s写入带宽仅对应0.53 GB/s/GPU(512个GPU并行)。
检查点只需记录一致的系统状态，不依赖数据，而依赖模型权重和偏差。
结论 构建 AI系统  时，应依靠严谨的数学和科学分析来驱动决策。考虑到系统规模，微小错误可能代价高昂。务必依赖准确的技术知识和计算，避免粗略估算或带有偏见的指导。
]]></content>
  </entry>
  
  <entry>
    <title>三星或将斥资709亿收购诺基亚移动网络资产</title>
    <url>/post/news/samsung-takeover-of-nokia-mobile-would-show-futility-of-open-ran.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Samsung</tag>
      <tag>Nokia</tag>
    </tags>
    <content type="html"><![CDATA[周四，彭博社援引知情人士的话报道称，电信设备制造商诺基亚正在与韩国三星等公司就出售其移动网络业务部门进行谈判。彭博社称，诺基亚已考虑多种选择，包括撤资、出售部分或全部移动网络业务（价值可能高达 100 亿美元），以及与竞争对手合并。
对诺基亚和整个电信行业来说，这将是一次重大且具有颠覆性的举措。如果三星成功收购诺基亚的整个移动部门，这将使诺基亚退出其最大的市场，该市场去年占公司总收入的44%。如果收购完成，三星将成为全球最大的 5G 网络供应商之一。
RAN领域步履维艰 当前，整个 RAN 行业都处于困境之中。Omdia 的数据显示，去年RAN 行业收入下降约 11%，降至约 400 亿美元。预计今年销售额将再下降 7% 至 9%。
不过，诺基亚遭受的损失比其两大竞争对手爱立信和华为都要大。
2024 年上半年，诺基亚移动网络业务部门的销售额同比下降近三分之一，降至约 35 亿欧元（39 亿美元），利润暴跌 62%，仅剩 1.29 亿欧元（1.43 亿美元）。相比之下，爱立信的同类业务部门销售额仅下降 16%，降至约 714 亿瑞典克朗（70 亿美元）。
在经历几次重击后，诺基亚摇摇欲坠。2020 年，在前任管理层的领导下诺基亚在争夺美国电信巨头Verizon的5G合同时输给了三星，失去了这一重要的美国客户。不过，很多人对此结果并不感到惊讶。由于英特尔未能按计划供应5G基站芯片，诺基亚不得不依赖现场可编程门阵列（FPGA），这是一种更昂贵的组件类型。诺基亚在竞争中举步维艰，利润率也因此缩水。不过，在新的管理层领导下，诺基亚已逐步淘汰这些 FPGA，并更新了整个产品组合。
然而，令整个市场措手不及的是AT&amp;T在去年12月宣布的决定，即将诺基亚从其约三分之一的网络覆盖中剔除，转而由另一家RAN供应商爱立信取而代之。AT&amp;T似乎认为，使用单一供应商为包括服务管理、编排和基带软件在内的关键平台提供支持，具有经济和技术上的吸引力。作为规模较小的供应商，诺基亚因此失利。
AT&amp;T称爱立信的技术将符合新的开放RAN规格，这些规格旨在确保设备能够与其他供应商的产品互操作。然而，所谓的Open RAN 让AT&amp;T对爱立信的依赖加重，同时也不可避免地削弱了诺基亚。如今，T-Mobile成为诺基亚在美国唯一剩下的大客户。
根据诺基亚自己的估计，AT&amp;T 在 2023 年占诺基亚移动网络销售额的 5% 至 8%，利润份额可能更大。为了应对各种市场困难，诺基亚已经大幅裁员，去年 9 月至今年 7 月期间裁员6000人。
超越爱立信 诺基亚方面尚未证实与三星的谈判。一位发言人通过电子邮件表示：“诺基亚不对市场传言或猜测发表评论。诺基亚致力于其移动网络业务的成功，这一业务对诺基亚及其客户来说都是高度战略性的资产。今年，该业务在优化成本基础、保护产品路线图、赢得新客户的订单和增加现有客户的份额方面取得了显著进展。”
但如果三星全面收购诺基亚，将立即跃升至顶级供应商行列。根据 Omdia 的数据，去年三星仅占全球 RAN 销售额的 6.1%。收购诺基亚将立即将这一比例提升至 25.6%，使三星成为全球第二大 RAN 供应商，仅次于华为的 31.3%，但略高于爱立信的 24.3%。
与爱立信和诺基亚一样，三星最近在 5G 设备市场也陷入困境。此前，三星网络业务第一季度收入同比下降 31%，至 7400 亿韩元（约5.55 亿美元）。第二季度收入下降 21%，同样降至 7400 亿韩元。
然而，与诺基亚不同的是，三星拥有更庞大的电子业务，由于芯片和智能手机需求旺盛，该业务近来发展良好。第二季度，该公司销售额增长 23%，超过 74 万亿韩元（约555 亿美元），利润增长近五倍，达到 9.84 万亿韩元（约73.8 亿美元）。相比之下，诺基亚在同一时期报告称，集团销售额整体下降 18%，降至约 45 亿欧元（约50 亿美元），利润下降五分之一，降至 3.28 亿欧元（约3.64 亿美元）。
鉴于三星的市场份额很小，监管机构可能不会反对任何交易。但对于运营商来说，情况就不同了，除非他们已经放弃了通过Open RAN来推动竞争的计划。三星 6.1% 的 RAN 市场份额将使它成为全球第四大 RAN 供应商。在许多国家，这笔交易实际上可能预示着双头垄断局面的到来。
]]></content>
  </entry>
  
  <entry>
    <title>AMD Zen5 EYPC 9755 速度比前代Zen4 9754 快2倍</title>
    <url>/post/soc/AMD-Zen5-EYPC-9755-is-2-times-faster-than-Zen4-9754.html</url>
    <categories><category>SOC</category>
    </categories>
    <tags>
      <tag>AMD</tag>
      <tag>EYPC</tag>
    </tags>
    <content type="html"><![CDATA[根据最新的基准测试结果， AMD  即将发布的第五代EPYC Turin 旗舰型号 EPYC 9755 在多项指标上均表现优异，其性能相比前代产品有了大幅提升，尤其是在 7-Zip 压缩与解压缩测试中的表现更是超越了同样拥有 128 核心的 EPYC 9754，显示出 Zen 5 架构相较于 Zen 4C 架构的巨大优势。
EPYC 9755 处理器基于 AMD 的 Zen 5 核心架构，拥有 128 个内核和 256 个线程。其基本时钟频率为 2.70 GHz，最高可提升至 4.10 GHz，频率较上一代产品提升了 11%。此外，该处理器配备了总计 650 MB 的缓存，包括 512 MB 的 L3 缓存、128 MB 的 L2 缓存和 10 MB 的 L1 缓存。与前代旗舰 EPYC 9654 相比，EPYC 9755 的缓存容量增加了 31%，而内核数量则从 96 个提升至 128 个。
在 7-Zip 的基准测试中，EPYC 9755 的压缩测试得分为 346,000 KB/s，解压缩测试得分为 4,445,000 KB/s，平均得分为 394.705 GIPS。这个成绩表明，EPYC 9755 的整体性能较 EPYC 9754 提升了两倍以上，而后者的压缩和解压缩得分分别为 171,000 KB/s 和 2,221,000 KB/s。
相比之下，EPYC 9754 作为 Bergamo 系列的旗舰芯片，采用 Zen 4C 核心架构，虽然同样拥有 128 个内核，但其性能远不及采用 Zen 5 架构的 EPYC 9755。这也说明了 AMD 在新一代 Zen 5 架构上的进步，使得同样核心数量的情况下，处理器性能得到了大幅度提升。
AMD 的 EPYC “Turin”系列处理器计划于 2024 年 10 月 10 日发布，届时将与英特尔即将推出的 Xeon 6“6900P” 处理器展开竞争。值得注意的是，英特尔的这款处理器同样配备了 128 个 P 核，更高密度的 288 核版本预计要到 2025 年第一季度才会与 Sierra Forest“Xeon 6900E”一起推出。
这次 AMD 的新一代 EPYC 处理器不仅在规格上有了大幅提升，更重要的是在实际应用中的性能表现得到了显著的提升，这将使其在高性能计算领域具有更强的竞争力。
]]></content>
  </entry>
  
  <entry>
    <title>英特尔或出售晶圆厂</title>
    <url>/post/news/intel-may-sell-its-wafer-fab.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Intel</tag>
      <tag>Wafer fab</tag>
    </tags>
    <content type="html"><![CDATA[8月30日消息，据知情人士透露，英特尔公司正与投资银行合作，帮助渡过公司56年历史上最困难的时期。
知情人士说，该公司正在讨论各种可能的情况，包括拆分或出售其芯片制造业务，以及哪些工厂项目可能会被关闭。
由于讨论是私下进行的，这些人士要求不具名。知情人士说，长期为英特尔服务的投行摩根士丹利和高盛集团一直在就各种可能性提供建议，其中可能还包括潜在的并购。上述知情人士说，这些方案预计将在9月份的董事会会议上宣布。
据报道，按照英特尔的财报，英特尔第二季度晶圆代工营收季减2％，年增4％，亏损扩大到28.3亿美元，相当于18个月，合计亏损了123.3亿美元，折合人民币是880亿元左右。
按照机构的数据，台积电每卖1片10000美元的晶圆，就赚4350美元，等于每片制造成本为5650美元，利润率高达43.5%。而英特尔每卖一片10000美元的晶圆，就要亏掉6550美元（约4.7万元），真的是妥妥的负担。
]]></content>
  </entry>
  
  <entry>
    <title>PCIe技术的新革命</title>
    <url>/post/hardware/a-new-revolution-in-PCIe-technology.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>PCIe</tag>
    </tags>
    <content type="html"><![CDATA[在高速数据传输和计算需求日益增长的今天， PCIe  正在经历一场前所未有的光互联革命。2024年，光互连技术发展势头迅猛，多家厂商纷纷推出与PCIe相关的解决方案，加速了光互连在数据中心中的应用。光互连技术也越来越有望迅速从实验室走向数据中心，成为数据传输领域的中坚力量。
铜缆PCIe发展吃力，光互连来接力 PCI-Express（PCIe）自2000年诞生以来，凭借其高带宽、低延迟等优势，一直是计算机系统中不可或缺的互连标准。尽管PCIe卡的外形尺寸在过去二十年基本保持不变（很大程度上是为了确保向后和向前兼容性），但其信号传输速度却实现了飞跃式发展。从最初的PCIe 1.0到如今的PCIe 6.0和即将发布的PCIe 7.0，单个通道的传输速率已经提高了32倍，而PCI-SIG将在2025年通过PCIe 7.0将这一速度再次提高一倍，达到512GB/s。
PCIe数据速率的演变（来源 PCI-SIG）
PCIe 7.0的创新还包括四级脉冲幅度调制 (PAM4)、轻量级前向纠错 (FEC)、循环冗余校验 (CRC) 和流量控制单元 (Flits)。PCIe 7.0 技术旨在成为人工智能/机器学习、数据中心、HPC、汽车、物联网和军事/航空航天等数据密集型市场的可扩展互连解决方案。
就当下以及未来的发展来看，大语言模型的快速迭代离不开海量GPU集群的强劲支持。这个海量已经到了万卡集群级别，成为AI标配，万卡集群之间的互连通常是基于GPU上原生的PCIe接口。要达到PCIe 7.0及更高速度的数据传输，几乎无法通过PCIe标准的铜缆实现机架间数十米的传输，传统的电气 PCIe 接口在传输数据时受限于电缆和板上的电气特性，带宽和距离受限，传输距离通常约为一米，使用重定时器（retimer）（关于重定时器可以参看《一颗芯片的新战争》一文）或光学传输将成为唯一的实际解决方案。
实际上，到PCIe 5.0 和 6.0标准时，铜缆就已经开始吃力了。这也是为什么PCI-SIG（PCIe标准的制定组织）在2024年5月1日宣布了新的CopprLink内部和外部电缆规范。CopprLink电缆规范具有相同的外形尺寸，可以提供32.0和 64.0 GT/s 的信号传输，并利用由SNIA维护的完善的行业标准连接器外形尺寸。但是CopprLink的传输距离仍然很有限，单个系统内仅为1米，机架到机架连接的最大可达 2米。
再加上考虑到重定时器的使用既复杂、昂贵又耗电，而且有其局限性，因为每个链路只能使用两个重定时器。接下来，光互连将成为PCIe架构继续演进的重要一部分。
这点可以从PCI-SIG的动作看出，2023年8月，PCI-SIG成立了一个光学工作组探索光学连接，计划采用多种光学技术来支持 PCIe，包括可插拔光收发器、板载光学器件、共封装光学器件和光学 I/O，标准化光纤上PCIe的工作和行为方式。光纤通信具有更长距离和更高数据速率的潜力，并且与日益耗电的铜线传输相比，可以显著降低功耗。
PCI-SIG布线工作分为三个不同的工作组：电气工作组 (EWG)、布线工作组 (CWG) 和光学工作组 (OWG)（图源：PCI-SIG）
总的来说，PCI-SIG正在采取两条腿走路的策略：一方面在为 128.0 GT/s 的 PCIe 7.0 架构开发 CopprLink 电缆；另一方面，在积极推动PCIe光纤互连的工作，PCIe光互连对于将基于PCIe的GPU集群扩展到多个机架和行、提高AI模型性能和提高GPU利用率至关重要。PCI-SIG希望CopprLink电缆和光学互连能够相互补充。
厂商奋进，PCIe光互连近在咫尺 在实现光互连的路上，已经有越来越多不同产业链的厂商参与进来，这为光互联的发展起到了很大的推动作用。
连接技术公司 在2024年的光纤通信会议 (OFC) 大会上，Alphawave与多家光学供应商合作开展了一项非重定时光学研究，使用Alphawave PipeCORE PCIe 6.0子系统 IP（适用于PCIe和CXL）在评估板上运行，以驱动使用PCIe 6.0数据的光学系统，并始终实现小于 1×10 -9的 BER ，这至少是性能裕度的3个数量级。Alphawave是一家提供用于数据中心、通信和人工智能应用的高速连接技术，专注于开发和制造高速接口芯片和解决方案，如 PCIe、CXL 和 Ethernet连接器。
Alphawave评估板将64Gbps驱动至Nubis光学引擎（图源：Alphawave）
6 月 11 日，专用连接解决方案厂商Astera Labs首次展示了数据中心 GPU 集群的端到端 PCIe 光纤传输技术。在演示中，他们组装了两种常见配置以扩展覆盖范围：从头节点到 GPU 集群，以及从头节点到远程分散的内存系统。系统通过单模光纤实现了全速率 PCIe传输，总带宽达到128GB/s，覆盖范围为 20 米。不过根据实际应用需求，该覆盖范围可以轻松扩展至 50 米或更长。
数据中心GPU集群端到端PCIe光纤传输的演示（图源：Astera Labs）
IP厂商：新思科技 &amp; Cadence 从最简单的构建块（如 GPIO）到最先进的高速接口，IP子系统是芯片制造生态系统的命脉。目前，新思科技和Cadence这两家业界领先的EDA公司正积极投入到PCIe 7.0光纤接口的研发中，力求为高速互联提供更具创新性的解决方案。
新思科技和OpenLight在OFC 2024期间展示了世界上首个采用线性驱动方法的PCIe 7.0光纤数据速率演示。该演示展示了端到端链路 BER 性能比 FEC 阈值高出几个数量级，证明了以128Gbps PAM4运行的PCIe 7.0光纤的可行性。值得一提的是，新思科技推出了首个PCIe 7.0 IP，通过正在进行的互操作性演示和 PCIe 7.0 数据速率和基于光纤的 PCIe 6.x 的出色现场结果，有助于降低集成和风险，并使一次通过硅片成功成为可能。
PCIe 7.x/6.x光纤演示（图源：新思科技）
Cadence在2024 年 PCI-SIG 开发者大会（PCI-SIG DevCon 2024）上演示了全球首个 PCIe 7.0 光纤连接方案。Cadence成功使用线性可插拔光学元件（LPO）演示了传输速度达128GT/s的光纤PCIe 7.0信号收发，无需DSP/Retimer。
（图片来源：Cadence）
芯片厂商：Intel 英特尔是光互连的多年研究者，在2024 OFC上，英特尔推出了其首款与计算处理器共同封装的光输入/输出 (I/O) 芯片组，该芯片组支持 64 个 PCIe 5.0 通道，每个通道双向传输速度为 32 GT/s，总计4Tbps，使用光纤传输距离可达100米。而且其功耗很低，据英特尔称，该芯片组使用密集波分复用 (DWDM) 波长，每比特仅消耗5皮焦耳，比每比特消耗约 15 皮焦耳的可插拔光收发器模块节能得多。
（图片来源：英特尔）
PCIe演进，CXL光互连的突破 虽然PCIe是一个出色的互连技术，但是近年来，随着AI和机器学习的迅猛发展，对计算、内存和互连都提出了新的要求，一种基于PCIe的全新的高速互连标准——CXL，正在成为AI时代的“运力”引擎。
 CXL（Compute Express Link）  是由英特尔于2019年发起的一项开放性行业标准，可增强处理器、内存扩展和加速器之间的通信。CXL建立在PCIe框架之上，从技术上看，CXL是通过PCIe物理层传输信号，但在协议层面上引入了新的特性和改进，以显著提升系统中处理器、加速器和内存设备之间的数据交换效率和一致性，使得资源共享具有更低的延迟，减少了软件堆栈的复杂性，并降低了整体系统成本，为高性能计算和大规模数据处理提供了更为强大的支持。
Rambus近期成功演示了CXL与光纤的无缝对接。Rambus利用Samtec Firefly光缆技术，将CXL端点设备与Viavi Xgig 6P4训练器连接，成功构建了一个远程“CXL内存扩展”模块。具体而言，Rambus的被测设备（DUT）搭载了CXL 2.0控制器，以四通道16 GT/s的速度运行。Viavi Xgig 6P4则模拟根复合设备，通过支持16 GT/s速率的Samtec Firefly PCUO G4光缆与DUT连接。测试结果表明，DUT在四倍速率下稳定运行，达到了预期性能。更重要的是，在设备发现阶段和CXL 2.0合规性测试中，DUT表现出色，顺利通过了所有标准测试。
Rambus 展示利用 CXL Over Optics 实现的先进数据中心功能(图源：Rambus)
国内方面，2024年8月2日，曦智科技与紫光股份旗下新华三集团合作，成功将曦智科技片间光网络技术(Optical inter-chip Networking, oNET)应用于新华三集团CXL-O光互连解决方案，实现服务器作为主机读写挂载于CXL 2.0交换机后的内存资源，并顺利完成了相关带宽、延时和压力等测试内容。曦智科技自成立以来，专注于光电混合算力新范式，oNET是曦智科技原创核心技术之一。
作为近几年才诞生的互联技术——CXL，发展迅速，据Yole Intelligence称，CXL市场预计从 2022年的170万美元增长到2026年的21亿美元，其中 70%（即 15 亿美元）将由 CXL 内存解决方案构成。
值得一提的是，8月初，Kioxia（铠侠）推出了具有光学接口的宽带SSD，通过用光学接口取代电线接口，该 SSD 技术显著增加了计算和存储设备之间的物理距离，减少了接线，同时保持了能源效率和高信号质量。目前，Kioxia已经能够将存储驱动器放置在距离CPU最远40米的距离，但计划在未来将这一距离增加到100米。
来源：anandtech
写在最后 长远来看，PCIe 架构在中长期内在各种高增长垂直领域依然展现出强大的增长潜力。根据 ABI Research 的《PCI Express市场垂直机会》报告，汽车和网络边缘领域为 PCIe 技术提供了最高的增长机会，预测期内的总潜在市场 (TAM) 和复合年增长率 (CAGR) 分别达到 53% 和 38%。
汽车行业能够从 PCIe 技术的广泛应用中获得巨大价值，因为它可以整合电气/电子 (E/E) 系统，并帮助解决自动驾驶汽车在安全性和效率方面的挑战。而在数据中心等高性能应用领域，对新 PCIe 技术的需求将保持长期的持续增长。PCIe 技术的前向和后向兼容性为决策者提供了灵活性，缩短了价值实现时间并降低了部署风险，这也促使 AI 行业的采用率不断提升。除了性能之外，PCIe 技术的关键驱动因素还包括能效、安全性和“价值实现时间”。
而PCIe的未来演进路线中，光学必然是一块重要的拼图。光学PCIe的发展契合了当前大数据、人工智能等领域对高性能计算的需求。在高速数据传输的需求驱动下，光学技术不仅在理论上展现出巨大潜力，更在实践中不断突破极限。随着更多厂商的加入和技术的不断演进，未来的数据互联将会迎来更加高效和高速的新时代。
]]></content>
  </entry>
  
  <entry>
    <title>RAID级别介绍</title>
    <url>/post/datacenter/raid-class-introduction.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Raid</tag>
    </tags>
    <content type="html"><![CDATA[RAID（Redundant Array of Independent Disks，独立磁盘冗余阵列）是一种 数据存储  虚拟化技术，它将多个物理硬盘组合成一个或多个逻辑单元，以提高存储性能、容量和数据冗余。RAID技术在现代数据存储系统中得到了广泛应用，其多样化的RAID级别可以满足不同的性能和可靠性需求。
RAID的基本概念是通过将多个物理硬盘组合在一起，形成一个逻辑存储单元，以实现以下目标：
 提高存储性能：通过并行读写多个硬盘，可以显著提高数据传输速率。 增加存储容量：将多个硬盘的存储容量合并为一个更大的逻辑存储单元。 增强数据可靠性：通过数据冗余技术（如镜像和奇偶校验），在硬盘故障时仍能保持数据的完整性。  RAID技术可以在硬件层面或软件层面实现。硬件RAID通常通过专用的RAID控制器来管理，而软件RAID则依靠操作系统或专用软件进行管理。
RAID概念最早由加州大学伯克利分校的三位研究人员David A. Patterson、Garth Gibson和Randy H. Katz在1987年的论文《A Case for Redundant Arrays of Inexpensive Disks (RAID)》中提出。这篇论文指出，通过使用廉价的磁盘驱动器并将它们组合成一个冗余阵列，可以显著提高存储系统的性能和可靠性。最初的RAID级别包括RAID 0到RAID 5，这些级别定义了不同的数据存储和保护方法。随着存储技术的发展，更多的RAID级别被引入，以满足不同的需求。
RAID 0：条带化 RAID 0是一种数据条带化（striping）技术，它将数据分成多个块，并将这些块分布在多个硬盘上。RAID 0不提供数据冗余，因此在数据保护方面较为薄弱，但它可以显著提高存储性能。
在RAID 0中，数据被分割成相等的块（通常称为条带），这些条带依次写入不同的硬盘。例如，如果有两个硬盘，数据块A的第一个部分写入硬盘1，第二部分写入硬盘2，依此类推。通过这种方式，读写操作可以并行进行，从而提高了存储性能。
假设有一个4个硬盘的RAID 0阵列，每个数据块大小为64KB。当一个256KB的文件被写入RAID 0阵列时，文件会被分割成4个64KB的数据块。第一个64KB块被写入第一个硬盘，第二个64KB块被写入第二个硬盘，依此类推。这意味着在读取该文件时，4个硬盘可以同时读取各自的64KB块，从而提高了读取速度。
优点  提高读写性能：由于多个硬盘可以并行读写数据，RAID 0显著提高了存储性能。 容量利用率高：RAID 0不需要存储奇偶校验或镜像数据，因此所有硬盘的存储容量都可以用来存储数据。  缺点  无数据冗余：RAID 0没有任何数据冗余机制，一旦任意一个硬盘发生故障，整个RAID阵列中的数据都将丢失。 数据恢复困难：RAID 0中数据丢失后很难恢复，需要借助专业的数据恢复服务。  RAID 0适用于那些对性能要求极高但对数据保护要求较低的场景，例如视频编辑、临时数据存储、非关键应用程序和游戏等。
RAID 1：镜像 RAID 1是一种数据镜像（mirroring）技术，它将相同的数据同时写入两个或多个硬盘，以实现数据冗余。当一个硬盘发生故障时，可以从镜像硬盘中恢复数据。RAID 1提供了较高的数据可靠性，但在存储效率方面有所牺牲。
在RAID 1中，所有数据都会被完全复制到一个或多个额外的硬盘上。这样，每个硬盘上都存储有相同的数据。一旦一个硬盘发生故障，系统可以从另一个硬盘读取数据，从而保证数据的完整性和可用性。
假设有一个2个硬盘的RAID 1阵列。当一个文件被写入RAID 1阵列时，该文件的每一个数据块都会被同时写入两个硬盘。如果硬盘1上的数据块发生损坏，可以从硬盘2上的镜像数据块中读取相同的数据，从而确保数据的完整性。
优点  高数据可靠性：RAID 1通过数据镜像提供了高数据可靠性，即使一个硬盘发生故障，数据仍然是安全的。 数据恢复简单：当一个硬盘发生故障时，可以直接从镜像硬盘中恢复数据，无需复杂的数据恢复过程。  缺点  存储效率低：由于每个数据块都需要被复制到一个或多个镜像硬盘上，因此存储效率较低。例如，在两个硬盘组成的RAID 1阵列中，只有50%的存储容量可用。 写性能略低：由于需要将数据写入多个硬盘，写入性能可能略低于单个硬盘。  RAID 1适用于那些对数据保护要求极高的场景，例如关键数据存储、数据库存储、操作系统驱动器和其他需要高数据可靠性的应用程序。
RAID 2：位级条带化（带海明码校验） RAID 2采用位级条带化（bit-level striping）并结合海明码（Hamming code）校验来实现数据冗余和错误检测。它使用专门的硬盘来存储校验码，可以提供较高的错误检测和纠错能力。
在RAID 2中，数据被分割为位级单位，然后分布到不同的硬盘上。同时，使用海明码校验生成校验位，这些校验位存储在专用的校验硬盘上。海明码是一种纠错码，可以检测和纠正单比特错误。
假设有一个5个硬盘的RAID 2阵列，其中3个用于存储数据，2个用于存储校验码。数据被分割为位级单位后，依次写入3个数据硬盘中。海明码校验位根据数据位生成，并写入2个校验硬盘中。在读取数据时，可以通过海明码校验检测和纠正数据中的单比特错误。
优点  高错误检测和纠错能力：使用海明码校验可以检测并纠正单比特错误，提高数据可靠性。 提高存储性能：通过并行读写多个硬盘，RAID 2可以提高数据传输速率。  缺点  硬件复杂性高：需要专用硬件支持海明码校验和位级条带化。 存储效率低：部分存储容量用于存储校验码，降低了整体存储效率。  RAID 2适用于那些对数据完整性要求极高的场景，例如科学计算、工程计算和高可靠性存储系统。
RAID 3：字节级条带化（带独立奇偶校验） RAID 3使用字节级条带化（byte-level striping）并结合独立奇偶校验来实现数据冗余。它将数据分割为字节单位，并将这些字节分布到不同的硬盘上，同时使用一个专用硬盘存储奇偶校验数据。
在RAID 3中，数据被分割为字节单位，然后依次写入多个数据硬盘。同时，计算每个字节的奇偶校验位，并将这些奇偶校验位存储在一个专用的校验硬盘上。在读取数据时，奇偶校验硬盘可以用于检测和恢复损坏的数据。
假设有一个4个硬盘的RAID 3阵列，其中3个用于存储数据，1个用于存储奇偶校验数据。数据被分割为字节单位后，依次写入3个数据硬盘中。奇偶校验位根据每个字节的数据计算，并写入奇偶校验硬盘中。在读取数据时，如果一个数据硬盘发生故障，可以通过奇偶校验数据恢复丢失的数据。
优点  提高存储性能：通过并行读写多个硬盘，RAID 3可以提高数据传输速率。 数据恢复能力强：使用奇偶校验可以在一个硬盘故障时恢复数据。  缺点  写性能瓶颈：由于所有奇偶校验数据都存储在一个硬盘上，写操作时需要频繁更新奇偶校验硬盘，可能导致性能瓶颈。 存储效率较低：部分存储容量用于存储奇偶校验数据，降低了整体存储效率。  RAID 3适用于那些对顺序数据访问要求较高的场景，例如视频编辑、音频编辑和流媒体服务器。
RAID 4：块级条带化（带独立奇偶校验） RAID 4使用块级条带化（block-level striping）并结合独立奇偶校验来实现数据冗余。与RAID 3类似，RAID 4将数据分割为块单位，并将这些块分布到不同的硬盘上，同时使用一个专用硬盘存储奇偶校验数据。
在RAID 4中，数据被分割为块单位，然后依次写入多个数据硬盘。每个数据块都有对应的奇偶校验位，这些奇偶校验位存储在一个专用的校验硬盘上。在读取数据时，可以使用奇偶校验硬盘来检测和恢复损坏的数据。
假设有一个5个硬盘的RAID 4阵列，其中4个用于存储数据，1个用于存储奇偶校验数据。数据被分割为块单位后，依次写入4个数据硬盘中。奇偶校验位根据每个数据块计算，并写入奇偶校验硬盘中。在读取数据时，如果一个数据硬盘发生故障，可以通过奇偶校验数据恢复丢失的数据。
优点  提高存储性能：通过并行读写多个硬盘，RAID 4可以提高数据传输速率。 数据恢复能力强：使用奇偶校验可以在一个硬盘故障时恢复数据。  缺点  写性能瓶颈：由于所有奇偶校验数据都存储在一个硬盘上，写操作时需要频繁更新奇偶校验硬盘，可能导致性能瓶颈。 存储效率较低：部分存储容量用于存储奇偶校验数据，降低了整体存储效率。  RAID 4适用于那些对数据读性能要求较高但写性能要求较低的场景，例如数据仓库、备份服务器和大容量存储系统。
RAID 5：分布式奇偶校验 RAID 5采用块级条带化（block-level striping）并结合分布式奇偶校验来实现数据冗余。与RAID 4不同，RAID 5将奇偶校验数据分布在所有硬盘上，而不是集中存储在一个硬盘上。这种方式可以避免单一奇偶校验硬盘的性能瓶颈，提高了系统的整体性能。
在RAID 5中，数据被分割为块单位，并分布在多个硬盘上。每个数据块的奇偶校验位根据其他数据块计算，并分布在不同的硬盘上。这样，当一个硬盘发生故障时，可以通过奇偶校验数据恢复丢失的数据。
假设有一个4个硬盘的RAID 5阵列，数据被分割为块单位并分布在硬盘1、硬盘2和硬盘3上。奇偶校验数据分布在硬盘4上。下一个数据块的奇偶校验位可能存储在硬盘3上，而数据块则分布在硬盘1、硬盘2和硬盘4上。这种分布方式可以避免奇偶校验硬盘的性能瓶颈。
优点  高存储效率：与RAID 1相比，RAID 5的存储效率更高，因为只需要额外的奇偶校验数据。 提高读写性能：通过分布式奇偶校验，RAID 5可以避免奇偶校验硬盘的性能瓶颈，提高系统的整体性能。 数据恢复能力强：可以在一个硬盘故障时通过奇偶校验数据恢复丢失的数据。  缺点  写性能略低：由于每次写操作都需要更新奇偶校验数据，写性能可能略低于读取性能。 数据恢复时间长：在一个硬盘故障后恢复数据需要较长时间，尤其是当数据量较大时。  RAID 5适用于那些对存储性能和数据保护都有较高要求的场景，例如文件服务器、数据库服务器和企业级存储系统。
RAID 6：双重分布式奇偶校验 RAID 6在RAID 5的基础上增加了第二层奇偶校验，从而提供更高的数据可靠性。RAID 6采用块级条带化并结合双重分布式奇偶校验，可以在两个硬盘同时发生故障时依然保持数据的完整性。
在RAID 6中，数据被分割为块单位，并分布在多个硬盘上。每个数据块的奇偶校验位根据其他数据块计算，并分布在不同的硬盘上。RAID 6有两层奇偶校验：P奇偶校验和Q奇偶校验。P奇偶校验和RAID 5的奇偶校验类似，而Q奇偶校验使用更复杂的算法来提供额外的数据保护。
假设有一个5个硬盘的RAID 6阵列，数据被分割为块单位并分布在硬盘1、硬盘2和硬盘3上。P奇偶校验数据分布在硬盘4上，Q奇偶校验数据分布在硬盘5上。下一个数据块的P奇偶校验位和Q奇偶校验位会分布在不同的硬盘上。这种双重分布式奇偶校验可以在两个硬盘同时发生故障时恢复数据。
优点  极高的数据可靠性：RAID 6通过双重分布式奇偶校验提供了极高的数据可靠性，可以在两个硬盘同时发生故障时保持数据的完整性。 高存储效率：与RAID 1相比，RAID 6的存储效率更高，因为只需要额外的奇偶校验数据。  缺点  写性能较低：由于每次写操作都需要更新两层奇偶校验数据，写性能可能较低。 实现复杂度高：RAID 6的实现比RAID 5复杂，需要更多的计算和存储资源。  RAID 6适用于那些对数据可靠性要求极高的场景，例如企业级存储系统、数据库服务器和关键数据存储。
RAID 10：条带化镜像 RAID 10（也称RAID 1+0）是一种将RAID 1和RAID 0结合起来的混合RAID级别。它首先将数据镜像到多个硬盘，然后将这些镜像硬盘组成一个RAID 0阵列。RAID 10同时提供了RAID 0的高性能和RAID 1的数据冗余。
在RAID 10中，数据首先被完全复制到一组镜像硬盘上，然后这些镜像硬盘被条带化，以提高存储性能。每个数据块被写入一个镜像组，然后在该组内进行条带化。
假设有一个由4个硬盘组成的RAID 10阵列，数据首先被镜像到两个硬盘（硬盘1和硬盘2），然后条带化到另外两个镜像硬盘（硬盘3和硬盘4）。这意味着数据块A的副本被存储在硬盘1和硬盘3上，而数据块B的副本被存储在硬盘2和硬盘4上。
优点  高数据可靠性：RAID 10通过数据镜像提供高数据可靠性，即使多个硬盘发生故障，数据仍然可以被恢复。 高存储性能：RAID 10结合了RAID 0的高性能和RAID 1的数据冗余，提供了快速的读写性能。 快速恢复：当一个硬盘发生故障时，数据可以迅速从镜像硬盘中恢复。  缺点  存储效率较低：由于每个数据块都需要镜像到一个或多个硬盘上，存储效率较低。例如，在4个硬盘的RAID 10阵列中，只有50%的存储容量可用。  RAID 10适用于那些既需要高性能又需要高数据可靠性的场景，例如数据库服务器、虚拟化环境和高性能计算应用。
RAID 50：分布式奇偶校验条带化 RAID 50（也称RAID 5+0）是一种将RAID 5和RAID 0结合起来的混合RAID级别。它首先将数据条带化并使用分布式奇偶校验（RAID 5）保护数据，然后将这些RAID 5阵列组成一个RAID 0阵列。RAID 50提供了较高的存储性能和数据冗余。
在RAID 50中，数据首先被分割为块单位，并使用分布式奇偶校验存储在多个RAID 5阵列中。然后，这些RAID 5阵列被条带化，以提高存储性能。每个RAID 5阵列都提供数据冗余，而RAID 0条带化提高了系统的整体性能。
假设有一个由6个硬盘组成的RAID 50阵列，首先将硬盘分为两个RAID 5阵列，每个阵列包含3个硬盘。每个RAID 5阵列使用分布式奇偶校验保护数据，然后这两个RAID 5阵列被条带化组成一个RAID 0阵列。
优点  高存储性能：通过RAID 0条带化和RAID 5的分布式奇偶校验，RAID 50提供了较高的存储性能。 高存储效率：与RAID 10相比，RAID 50的存储效率更高，因为只需要额外的奇偶校验数据。 数据恢复能力强：每个RAID 5阵列可以在一个硬盘故障时恢复数据，而RAID 0条带化提高了系统的整体性能。  缺点  实现复杂度高：RAID 50的实现比RAID 10复杂，需要更多的硬盘和控制器。 恢复时间较长：在一个硬盘故障后恢复数据需要较长时间，尤其是当数据量较大时。  RAID 50适用于那些需要高存储性能和数据冗余的场景，例如企业级存储系统、数据仓库和高性能计算应用。
RAID 60：双重分布式奇偶校验条带化 RAID 60（也称RAID 6+0）是一种将RAID 6和RAID 0结合起来的混合RAID级别。它首先将数据条带化并使用双重分布式奇偶校验（RAID 6）保护数据，然后将这些RAID 6阵列组成一个RAID 0阵列。RAID 60提供了极高的数据可靠性和存储性能。
在RAID 60中，数据首先被分割为块单位，并使用双重分布式奇偶校验存储在多个RAID 6阵列中。然后，这些RAID 6阵列被条带化，以提高存储性能。每个RAID 6阵列都提供双重数据冗余，而RAID 0条带化提高了系统的整体性能。
假设有一个由8个硬盘组成的RAID 60阵列，首先将硬盘分为两个RAID 6阵列，每个阵列包含4个硬盘。每个RAID 6阵列使用双重分布式奇偶校验保护数据，然后这两个RAID 6阵列被条带化组成一个RAID 0阵列。
优点  极高的数据可靠性：RAID 60通过双重分布式奇偶校验提供极高的数据可靠性，可以在两个硬盘同时发生故障时保持数据的完整性。 高存储性能：通过RAID 0条带化和RAID 6的双重分布式奇偶校验，RAID 60提供了较高的存储性能。 高存储效率：与RAID 10相比，RAID 60的存储效率更高，因为只需要额外的奇偶校验数据。  缺点  实现复杂度高：RAID 60的实现比RAID 50复杂，需要更多的硬盘和控制器。 恢复时间较长：在一个硬盘故障后恢复数据需要较长时间，尤其是当数据量较大时。  RAID 60适用于那些对数据可靠性和存储性能都有极高要求的场景，例如企业级存储系统、关键数据存储和高性能计算应用。
总结    RAID 级别 工作原理 优点 缺点 适用场景      RAID 0 数据条带化，无冗余 提高存储性能，所有存储容量可用 无数据冗余，任意一个硬盘故障导致数据丢失 高性能但数据可靠性要求低的场景，如缓存和临时存储    RAID 1 数据镜像，每个数据块复制到两个或多个硬盘 提供高数据可靠性，快速数据恢复 存储效率低，只有50%存储容量可用 需要高数据可靠性的场景，如操作系统和关键业务数据    RAID 2 位级条带化，带海明码校验 高错误检测和纠错能力，提高存储性能 硬件复杂性高，存储效率低 科学计算、工程计算等对数据完整性要求高的场景    RAID 3 字节级条带化，带独立奇偶校验 提高存储性能，数据恢复能力强 写性能瓶颈，存储效率较低 顺序数据访问要求高的场景，如视频编辑和流媒体服务器    RAID 4 块级条带化，带独立奇偶校验 提高存储性能，数据恢复能力强 写性能瓶颈，存储效率较低 数据读性能要求高但写性能要求较低的场景，如数据仓库和备份服务器    RAID 5 块级条带化，带分布式奇偶校验 高存储效率，提高读写性能，数据恢复能力强 写性能略低，数据恢复时间长 存储性能和数据保护要求高的场景，如文件服务器和数据库服务器    RAID 6 块级条带化，带双重分布式奇偶校验 极高的数据可靠性，高存储效率 写性能较低，实现复杂度高 数据可靠性要求极高的场景，如企业级存储系统和关键数据存储    RAID 10 条带化镜像 高数据可靠性，高存储性能，快速恢复 存储效率较低，仅50%存储容量可用 需要高性能和高数据可靠性的场景，如数据库服务器和虚拟化环境    RAID 50 分布式奇偶校验条带化 高存储性能，高存储效率，数据恢复能力强 实现复杂度高，恢复时间较长 高存储性能和数据冗余要求的场景，如企业级存储系统和数据仓库    RAID 60 双重分布式奇偶校验条带化 极高的数据可靠性，高存储性能，高存储效率 实现复杂度高，恢复时间较长 数据可靠性和存储性能要求极高的场景，如企业级存储系统和高性能计算    ]]></content>
  </entry>
  
  <entry>
    <title>Shell脚本中的彩色输出</title>
    <url>/post/linux/colored-output-in-shell-script.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Shell</tag>
    </tags>
    <content type="html"><![CDATA[Shell脚本是一种在Unix和 Linux  操作系统上用于自动化任务的强大工具。它可以执行各种命令、管理文件系统、处理文本和系统管理等。
为了提高脚本的可读性和用户交互体验，常常需要在终端中以不同颜色显示文本。
本文将介绍如何在Shell脚本中使用echo命令显示彩色文本。
使用ANSI转义码实现彩色输出 要在终端中实现彩色输出，我们可以使用ANSI转义码。这些转义码是一系列控制字符，用于改变终端文本的颜色和样式。以下是一些常用的ANSI转义码：
ANSI转义码颜色列表 前景色（文本颜色）
 黑色：\033[30m 红色：\033[31m 绿色：\033[32m 黄色：\033[33m 蓝色：\033[34m 紫色：\033[35m 青色：\033[36m 白色：\033[37m  背景色
 黑色：\033[40m 红色：\033[41m 绿色：\033[42m 黄色：\033[43m 蓝色：\033[44m 紫色：\033[45m 青色：\033[46m 白色：\033[47m  示例脚本
文章提供了一个示例脚本，定义了输出不同颜色文本和背景色的函数。以下是脚本的主要内容：
#!/bin/bash  # 定义前景色输出函数 black_text(){ echo -e &#34;\033[30m $1\033[0m&#34; } red_text(){ echo -e &#34;\033[31m $1\033[0m&#34; } green_text(){ echo -e &#34;\033[32m $1\033[0m&#34; } yellow_text(){ echo -e &#34;\033[33m $1\033[0m&#34; } blue_text(){ echo -e &#34;\033[34m $1\033[0m&#34; } purple_text(){ echo -e &#34;\033[35m $1\033[0m&#34; } cyan_text(){ echo -e &#34;\033[36m $1\033[0m&#34; } white_text(){ echo -e &#34;\033[37m $1\033[0m&#34; } # 定义背景色输出函数 black_bg(){ echo -e &#34;\033[40m $1\033[0m&#34; } red_bg(){ echo -e &#34;\033[41m $1\033[0m&#34; } green_bg(){ echo -e &#34;\033[42m $1\033[0m&#34; } yellow_bg(){ echo -e &#34;\033[43m $1\033[0m&#34; } blue_bg(){ echo -e &#34;\033[44m $1\033[0m&#34; } purple_bg(){ echo -e &#34;\033[45m $1\033[0m&#34; } cyan_bg(){ echo -e &#34;\033[46m $1\033[0m&#34; } white_bg(){ echo -e &#34;\033[47m $1\033[0m&#34; } # 显示前景色 echo &#34;前景色示例：&#34; black_text &#34;这是黑色的文本&#34; red_text &#34;这是红色的文本&#34; green_text &#34;这是绿色的文本&#34; yellow_text &#34;这是黄色的文本&#34; blue_text &#34;这是蓝色的文本&#34; purple_text &#34;这是紫色的文本&#34; cyan_text &#34;这是青色的文本&#34; white_text &#34;这是白色的文本&#34; # 显示背景色 echo &#34;背景色示例：&#34; black_bg &#34;这是黑色背景的文本&#34; red_bg &#34;这是红色背景的文本&#34; green_bg &#34;这是绿色背景的文本&#34; yellow_bg &#34;这是黄色背景的文本&#34; blue_bg &#34;这是蓝色背景的文本&#34; purple_bg &#34;这是紫色背景的文本&#34; cyan_bg &#34;这是青色背景的文本&#34; white_bg &#34;这是白色背景的文本&#34; 在Shell脚本中使用彩色输出不仅可以提高脚本的可读性，还能在提示信息中突出重点，提高用户体验。通过结合使用echo命令和ANSI转义码，我们可以方便地在终端中实现各种彩色输出。希望本文对你了解和使用Shell脚本中的彩色输出有所帮助。
]]></content>
  </entry>
  
  <entry>
    <title>思科取消独立的网络部门</title>
    <url>/post/news/cisco-to-slash-more-jobs-merge-networking-unit.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>CISCO</tag>
    </tags>
    <content type="html"><![CDATA[近日，思科宣布了一项新的的企业重组计划，除了裁减约 7% 的员工外，长期担任 网络  部门负责人的Jonathan Davidson将离职，他之前所在的网络部门将并入思科的安全和协作团队。
此次重组是在思科发布最新财务业绩时宣布的。在随后的财报电话会议上，思科首席执行官Chuck Robbins强调了思科在整合其斥资280亿美元收购的Splunk方面的进展，以及在制定更强大的人工智能（AI）战略和云平台持续增长方面的进度。
Robbins指出，这三个运营部门将是思科未来的关注重点。“展望未来，我们将继续专注于增长和一致性的执行，投资于人工智能、云和网络安全等关键领域。为了专注于这些关键优先领域，我们宣布了重组计划，以便我们既能投资关键增长机会，又能提高业务效率。”
部门重组 此次重组将涉及网络部门的整合，Davidson将离开思科，由Jeetu Patel担任首席产品官一职，领导新组建的团队。
Davidson在思科工作了近 21 年。Patel 于 2020 年加入思科，当时被任命负责公司新成立的安全与应用业务集团。在此之前，他在Box和EMC担任过高管。
除了离职的Davidson外，思科还将裁减约7%的员工。思科首席财务官Scott Herren表示，思科将因此次裁员而承担 10 亿美元的费用。Herren称此次“裁员”更多是一种“重新分配”，而非简单的“节省人力成本”。
此前，在Cisco Live 2024活动上，重组的迹象已经初现端倪。当时在会议上思科并未对网络业务进行深入探讨，反而大部分的演讲内容都集中在网络安全、云和人工智能方面。
分析师如何看待思科的重组计划？ 大多数分析师对思科的最新举措持积极态度。
Dell&rsquo;Oro Group企业安全和网络高级总监 Mauricio Sanchez认为，整合业务重点对于思科来说是一个很好的策略。“我一直觉得思科的安全业务部门与网络业务部门之间存在矛盾。当涉及到网络和安全的交集时，就会出现重复，以及产品定位的混乱。一边是 Catalyst，另一边是防火墙和 Meraki，这种情况会让用户感到困惑。希望这次重组将有助于思科产品和定位的一致性。”
其他分析师则特别指出了行业对人工智能日益增长的关注，这将为思科带来新的增长机会。
Futurum Group 在报告中指出：“总体而言，我们认为思科在加强其人工智能、安全性和云能力方面的产品整合，可以让其更好地把握人工智能大趋势。此外，产品组合调整和组织重组也为思科在2025年及以后的人工智能时代重返增长轨道奠定了坚实基础。”
Robbins强调，思科正在“向人工智能、面向云的人工智能网络、人工智能基础设施、芯片和网络安全等领域投入数亿美元。市场正以惊人的速度变化，我们必须紧跟时代步伐，积极行动。”
思科的网络业务将如何？ Herren补充称，尽管思科采取了最新举措，但网络仍然是思科的重要市场。
“网络业务对我们而言，其重要性不言而喻，我们也将继续支持这一领域。但同时我们也在积极探索如何提升公司整体效率，以便更有效地调配资源，并将其投入到增长最快的领域。”
Sanchez指出，思科减少对网络业务的关注只是跟随市场的趋势。
在谈及整个网络领域时，Sanchez表示：“这些传统的网络业务是公司稳定收益的基石，它们将继续为公司带来盈利并维持运营，然而，它们并不属于高增长市场。这就引出了一个问题：你是满足于仅仅实现个位数的增长，还是希望在快速崛起的新领域中展现出影响力？毕竟，这些新领域未来有可能成为推动公司飞跃式发展的强大引擎。”
]]></content>
  </entry>
  
  <entry>
    <title>Linux系统SSH免密认证的完整操作指南</title>
    <url>/post/linux/a-complete-guide-to-SSH-password-free-authentication-in-Linux.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>SSH</tag>
    </tags>
    <content type="html"><![CDATA[在 Linux  系统中，免密认证（Passwordless Authentication）可以通过多种方式实现，最常见的就是使用SSH密钥认证。本文将详细介绍如何在Linux系统中设置SSH免密登录，并提供具体的操作步骤示范。
什么是SSH免密认证？ SSH（Secure Shell）是一种加密的网络协议，用于在不安全的网络上安全地进行远程登录和其他安全网络服务。免密认证的核心思想是利用公钥和私钥进行认证，而不是每次都输入密码。只要在客户端和服务器之间正确配置了公钥和私钥，用户可以直接登录而无需输入密码。
生成SSH密钥对 在开始配置免密认证之前，需要先生成一对SSH密钥，即公钥和私钥。这对密钥将在认证过程中使用。
 在客户端生成密钥对  打开终端，输入以下命令生成密钥对：
ssh-keygen -t rsa -b 4096 -C &#34;admin@vxworks.net&#34; 系统会提示你选择存储密钥的位置。默认情况下，密钥将存储在~/.ssh/id_rsa。按下Enter键以接受默认位置。
接下来，系统会询问你是否要设置一个密钥密码（passphrase）。可以选择不设置，这样在使用密钥时不需要输入密码。如果选择设置密码，请输入密码并再次确认。
检查生成的密钥文件  默认情况下，生成的密钥文件将位于~/.ssh/目录下。可以使用以下命令查看：
ls ~/.ssh/ 你应该能看到两个文件：id_rsa（私钥）和id_rsa.pub（公钥）。
将公钥复制到目标服务器 为了实现免密认证，需要将生成的公钥复制到目标服务器上。
 使用ssh-copy-id命令  最简单的方法是使用ssh-copy-id命令，它会自动将公钥添加到目标服务器的~/.ssh/authorized_keys文件中。
ssh-copy-id user@remote_host user@remote_host是目标服务器的用户名和IP地址。输入后，系统会要求你输入目标服务器用户的密码。
手动复制公钥  如果不能使用ssh-copy-id，可以手动将公钥复制到目标服务器上。首先，查看并复制公钥的内容：
cat ~/.ssh/id_rsa.pub 然后，在目标服务器上，将公钥内容追加到~/.ssh/authorized_keys文件中：
echo &#34;your_public_key_content&#34; &gt;&gt; ~/.ssh/authorized_keys 将~/.ssh/authorized_keys文件的权限设置为600，即只有所有者可读写：
chmod 600 ~/.ssh/authorized_keys 测试免密登录 完成以上配置后，可以测试免密登录是否成功。
 执行SSH登录  在客户端，尝试通过SSH登录到目标服务器：
ssh user@remote_host 如果配置正确，你应该能够直接登录到服务器而无需输入密码。
处理可能的错误  如果仍然提示输入密码，可能是权限配置有问题，或者公钥没有正确复制。可以重新检查~/.ssh目录及其中文件的权限设置。
设置其他用户的免密认证 除了自己使用免密认证外，还可以为其他用户设置。步骤与上文类似，只需确保公钥正确添加到目标用户的~/.ssh/authorized_keys文件中即可。
使用SSH配置文件简化连接 如果需要频繁连接多个服务器，可以在~/.ssh/config文件中配置SSH连接参数，以简化连接过程。
 编辑SSH配置文件  在~/.ssh/目录下创建或编辑config文件：
nano ~/.ssh/config 添加如下内容：
Host server_alias HostName remote_host User user IdentityFile ~/.ssh/id_rsa 其中server_alias是服务器的别名，HostName是目标服务器的IP地址或域名，User是登录的用户名。
使用别名进行登录  配置完成后，可以使用别名进行SSH登录：
ssh server_alias 总结 通过上述步骤，我们就可以在Linux系统中轻松实现SSH免密认证，提升远程管理的便利性和安全性。免密认证不仅能减少输入密码的麻烦，还能通过密钥的复杂性提高系统的安全性。建议定期更换密钥并注意保护私钥的安全，确保系统免受未授权访问的威胁。
]]></content>
  </entry>
  
  <entry>
    <title>Rust 为什么适合嵌入式开发</title>
    <url>/post/programming/why-rust-is-suitable-for-embedded-development.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>Rust</tag>
      <tag>Embedded</tag>
    </tags>
    <content type="html"><![CDATA[Rust 作为一门年轻的语言，聚焦与安全、并发、高性能等特点，号称能替代 C/C++，那么究竟有多少优点能值得我们来切换呢？本文将告诉你为什么 Rust 适合嵌入式开发。
工具链  轻松搭建各种不同类型的芯片编译环境，使用 cargo、rustup 等命令快速搭建新的环境。 统一的编译、调试工具、下载工具, 如cargo build,cargo falsh,probe-rs等 快速生成代码文档,cargo doc直接生成网页文档，能让新手快速了解整个项目的模块、接口框架。 内置代码格式化工具 fmt，轻松就各个代码文件统一格式，标准规范，团队作战无需在吐槽队友的代码风格，cargo fmt 后格式都会统一好，新手无需学习新公司的编码格式规范，公司也无需过多培训编码格式规范。  库的集成  移植优势  可移植性移植是嵌入式开发的一个大问题，每个工作可能或多或少会考虑一些模块的可移植性，想进来在不同框架不同平台上能够共有这些模块，避免重复造轮子。Rust 则提供了高效的方法来保证库能够轻松移植，避免库接口域业务接口杂糅到一起。trait 等特性让 Rust 的库能在不同的 CPU 如 ARM 或 RISV-V 甚至操作系统上方便得使用，无需过多关注库的文件数量、无需手动添加库的每个文件，仅仅只需在toml文件中添加库的名字、版本、开启所需的featues即可。在 Rust 中你可以轻松得将各种 IIC 的传感器库添加到自己的工程，很少花时间在适配上。
[dependencies] panic-halt = &#34;0.2.0&#34; ufmt = &#34;0.2.0&#34; nb = &#34;1.1.0&#34; embedded-hal = &#34;1.0&#34; pwm-pca9685 = &#34;1.0.0&#34; infrared = &#34;0.14.1&#34; embedded-storage = &#34;0.2&#34; [dependencies.embedded-hal-v0] version = &#34;0.2.3&#34; package = &#34;embedded-hal&#34; Rust 官方发布了许多标准的库，基于这些库能简化开发、指导用户完成统一的适配接口。Rust 社区也非常活跃，发布了大量的开源库。   As part of the Rust open source project, support for embedded systems is driven by a best-in-class open source community with support from commercial partners.  调试 Rust 生成的固件能使用 openocd 来轻松 gdb 调试，与 C/C++ 完全一样，单步、断电、查看等操作都支持。无需担心调试障碍。
(gdb) break main Breakpoint 1 at 0x8000d18: file examples/hello.rs, line 15. (gdb) continue Continuing. Note: automatically using hardware breakpoints for read-only addresses. Breakpoint 1, main () at examples/hello.rs:15 15 let mut stdout = hio::hstdout().unwrap(); 语言优势  内存安全优势。  C/C++嵌入式工程师肯定知道，经常在编码完成后，烧录程序到芯片测试运行，经常会出现莫名的内存泄露、异常退出甚至死机的现象，这种内存问题有时非常难怕查，也许编码十分钟调试两小时。当然目前也有一些先进的工具用来辅助调试，如 ASan，Valgrind、Memcheck 等工具，但这些工具本身就需要复杂的调试手段，需要仔细查看日志才能得出结果，但是对于某些资源受限嵌入式设备，很难使用这些工具来辅助排查。对于新手来说学习这些工具的使用就让人头疼。而 Rust 天生保证内存安全，没有丰富的 Rust 经验很难写出能让内存异常的代码，是的你没听错，Rust 对于新手保护特别友好，需要有经验的人才能故意写出不安全的代码。Rust 的生命周期的约束使得实现内存安全而且零成本，也就是无需在时间和空间上浪费资源。
语法优势：语法中新的枚举，闭包、异步、流控、变量生命周期控制、安全宏等，基于这些基础语法能最方便、便捷的表达问题的逻辑，无需使用太多的技巧。让编程更加简洁和优雅。  Rust 作为强类型的语言，但是在使用时无需过多指定变量类型，Rust 编译器会自动推导变量的类型，并基于生命周期的约束可以重复使用变量名，原理上保证使用这些语法就像 C 语言一样安全，但是用起来像 Python 一样方便。
线程安全，无畏并发   Rust makes it impossible to accidentally share state between threads. Use any concurrency approach you like and you’ll still get Rust’s strong guarantees  一般来说 Rust 与其他语言也会面临同一样的并发问题。对于嵌入式软件环境，包括：
 多线程 多核处理器 中断处理  对于以上三种常见并发的问题，Rust 也提供了高效的解决方案，如定义原子类型、临界类型、互斥体防止被中断影响。同时也在编译期间检测多线程引起死锁风险，让风险扼杀在编译期间。目前已经有优秀的embassy,rtic等框架提供异步操作系统。
智能的编译提示，对于编译时的错误，给出详细的原因，对于有风险的代码段给出解决的意见，从代码编写阶段提高代码质量，无需在调试时去发现再优化，让程序员花给多的时间来考虑代码逻辑，业务逻辑，避免低效的调试过程。 轻松衔接 C/C++的代码，零成本接口绑定   Integrate Rust into your existing C codebase or leverage an existing SDK to write a Rust application.  如果目前你的项目无法使用 Rust 来完成所有的模块，你也可以使用 FFF 机制来轻松绑定原项目的 C/C++接口，能够轻松与 C/C++互相操作。可以使用bindgen命令来轻松构建外部接口，也可在build.rs中编译 C/C++ 文件，也能 C/C++ 调用库文件如*.a。轻松集成。
/* File: cool_bindings.rs */ #[repr(C)] pub struct CoolStruct { pub x: cty::c_int, pub y: cty::c_int, } pub extern &#34;C&#34; fn cool_function( i: cty::c_int, c: cty::c_char, cs: *mut CoolStruct ); pub extern &#34;C&#34; fn cool_function( ... ); extern crate cc; fn main() { cc::Build::new() .file(&#34;foo.c&#34;) .compile(&#34;libfoo.a&#34;); } 底层控制能力  Rust 也提供了接口能尽可能安全访问地访问底层接口，如 PAC 包用来抽象微控制器的外设寄存器的访问，能编译成高效的二进制代码且接口容易使用，用户无需太多关注寄存器各域的位置，只需聚焦于芯片手册来操作各域的值，不会出现移位错或写错的低级错误。
#![no_std] #![no_main] extern crate panic_halt; // panic handler use cortex_m_rt::entry; use tm4c123x; #[entry] pub fn init() -&gt; (Delay, Leds) { let cp = cortex_m::Peripherals::take().unwrap(); let p = tm4c123x::Peripherals::take().unwrap(); let pwm = p.PWM0; pwm.ctl.write(|w| w.globalsync0().clear_bit()); // Mode = 1 =&gt; Count up/down mode pwm._2_ctl.write(|w| w.enable().set_bit().mode().set_bit()); pwm._2_gena.write(|w| w.actcmpau().zero().actcmpad().one()); // 528 cycles (264 up and down) = 4 loops per video line (2112 cycles) pwm._2_load.write(|w| unsafe { w.load().bits(263) }); pwm._2_cmpa.write(|w| unsafe { w.compa().bits(64) }); pwm.enable.write(|w| w.pwm4en().set_bit()); } ]]></content>
  </entry>
  
  <entry>
    <title>风河VxWorks搭档TI处理器助汽车AI/ML大放异彩</title>
    <url>/post/vxworks/wind-river-vxworks-and-ti-processors-help-automotive-ai-ml-shine.html</url>
    <categories><category>VxWorks</category>
    </categories>
    <tags>
      <tag>VxWorks</tag>
      <tag>AI/ML</tag>
      <tag>ADAS</tag>
    </tags>
    <content type="html"><![CDATA[在技术进步的大潮中， 人工智能(AI)  和机器学习(ML)已经成为一股革命性的力量，重新塑造着各个行业，同时拓展了我们曾经以为不可企及的新疆界。AI/ML与嵌入式系统的融合激发出了大量的商业机会，特别是在汽车和工业领域。嵌入式系统 开发人员可以从 VxWorks  实时操作系统(RTOS)所具备的高确定性和高性能中大为受益，这套操作系统还具有高效的AI/ML集成功能，并且支持在云计算环境中进行开发、测试和部署——由此极大地加快了企业产品进入市场的速度。运行在德州仪器TDA4VH-Q1处理器之上的VxWorks，可以为开发人员提供极有价值的板上功能，包括集成化图形能力、AI加速功能和视频协处理功能。这种强强组合可以为高级驾驶辅助系统(ADAS)提供智能边缘计算支持。
AI/ML在汽车嵌入式系统中的兴起 嵌入式系统以紧凑的外形尺寸和专属的功能为特征，传统上依赖于确定性算法来执行各类任务。然而，汽车中的摄像头、雷达、激光雷达和超声波等传感器数量激增，加上对实时感知和决策的需求，推动了AI/ML的集成和应用。这些应用包括ADAS和其他用于目标检测的视频处理，以及边缘到云的应用场景，例如预测性维护、量身定制车辆保险、自主导航和其他尚未明确定义的车辆传感器数据预处理应用。
Wind River VxWorks ——实时智能的基石 VxWorks可以推动AI/ML与嵌入式系统的无缝集成，它是部署最广泛且最值得信赖的RTOS，以其高度确定的性能、可扩展性、安全认证和设计质量而享誉业界，为在不同环境中部署关键任务应用提供了坚实的基础。它还可运行于Amazon云之上，与DevSecOps流水线集成，支持对Kubernetes编排的容器化部署。所有这些特色相结合，为嵌入式系统行业带来了一场极具划时代意义的变革，并使嵌入式系统开发团队能够显著缩短产品上市时间。
那么，VxWorks中都有哪些AI/ML支持呢?首先，集成了TensorFlow Lite开源深度学习框架，可以在资源受限环境中加速机器学习模型的实现。为了进行大规模数据处理，Pandas和NumPy等基于python的库可支持复杂的数学运算，以及复杂的数据操作和分析。
德州仪器TDA4VH-Q1 ——赋能智能边缘计算 半导体在现代汽车技术中发挥着至关重要的作用，为汽车的功能和安全性做出了多方面的贡献。其中包括：电子控制单元(ECU)——管理和控制各种系统;车辆域控制器——在汽车架构中处理多种ECU功能;被动安全系统——如防抱死制动，以及牵引力控制;信息娱乐和网络互联;高级驾驶辅助系统。
先进的高性能半导体，如德州仪器(TI) TDA4VH-Q1汽车片上系统(SoC)，具有集成化图形、人工智能加速和视频协处理等功能，可在ADAS应用中实现传感器融合和自主2/3层域控制。
TDA4VH-Q1是TI高性能处理器产品组合的组成部分，旨在满足汽车应用的严格要求，具有8个Arm® Cortex®-A72 CPU内核可用于应用程序处理，6个Arm® Cortex®-R5F协处理器可用于实时处理。集成化的深度学习加速功能可提供高效的机器学习推理,采用经过训练的机器学习模型,基于新的非可见数据进行预测/决策。此外，这款SoC还为边缘检测等计算机视觉任务提供了可配置的视觉加速部件，以及与传感器和外设无缝连接的高速接口。
在TDA4VH-Q1上运行的VxWorks ——推动汽车 AI/ML 创新 VxWorks的更新节奏可确保开发者每年都可看到好几个重要版本，定期把最先进的SOC添加到稳定支持硬件列表之中，同时提供持续创新的RTOS功能。最新版本的VxWorks已于2024年3月初发布，其中带有TI的深度学习(TIDL)库——增加了对深度学习应用程序的支持，这也是TI深度学习算法加速软件生态系统的组成部分。TIDL库支持用户使用卷积神经网络(CNN)在TI设备上对预训练模型进行推理——这是一类深度神经网络，主要用于视觉图像分析。TIOVX——这是一套OpenVX标准的视觉应用软件实现，同时支持视频平台、优化、节能加速。
将VxWorks引入汽车技术可以实现众多不同的应用场景，例如:
 复杂的算法——用于ADAS和自动驾驶。 先进的影像处理——面向高分辨率摄像头和环视系统，能够准确检测和识别车辆周围的物体和障碍物。 周边设备集成——例如CAN(控制器局域网)、以太网、PCIe(外围组件互连Express)、 USB等，为汽车系统提供网络互联选项。 汽车应用功能特性——确保系统的可靠性和安全性，包括功能安全机制、安全引导等。 汽车级的可靠性——满足汽车行业的严格要求，包括温度、可靠性和质量标准。   基于TDA4VH-Q1的VxWorks  使开发人员能够创建人工智能解决方案，并且具备高性能、高可靠性和高安全性。
]]></content>
  </entry>
  
  <entry>
    <title>高质量C语言的开发技巧</title>
    <url>/post/programming/high-quality-c-language-development-skills.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>C</tag>
      <tag>High quality</tag>
    </tags>
    <content type="html"><![CDATA[前面发布很多理论方面的文章，诸如4篇：基于RTOS的软件开发理论，嵌入式软件的设计模式（上），嵌入式软件的设计模式（下），嵌入式软件分层隔离的典范。这些都是具备一定基础后在架构上的描述，类似于气宗性质，这种比&gt;较抽象、见效慢。但高质量的软件开发，也是存在见效快的套路，针对有一定嵌入式C语言开发基础的，以剑宗之法进行描述，可重点关注if判断和内存管理相关的讲解，抛砖引玉。
剑宗气宗之争 《笑傲江湖》中华山派的剑宗和气宗之争，可谓异常激烈。那么问题就来了，既然有剑宗气宗之争，到底应该先练剑，还是先练气呢？引申到软件开发行业有没剑气之争呢？
文件结构   C 程序通常分为两类文件，一种是程序的声明称为头文件，以“.h”为后缀，另一种是程序的实现，以“.c”为后缀，一般每个c文件有个同名的h文件。
  软件的头文件数目比较多，应将头文件和定义文件分别保存于不同的目录，例如将头文件保存于 include或者inc 目录，将定义文件保存于 source 或src目录；如果某些头文件是私有的，它不会被用户的程序直接引用，则没有必要公开其“声明”。为了加强信息隐藏，这些私有的头文件可以和定义文件存放于同一个目录，即私有的h文件放在src目录。
  在文件头添加版权和版本的声明等信息，主要包括版权和功能，以及修改记录，必要时可以为整个功能文件夹单独新建readme说明文档。
  为了防止头文件被重复引用，必须用 ifndef/define/endif 结构产生预处理块。
  头文件中只存放“声明”而不存放“定义”，更别提放变量，这是严重的错误。
  用 #include &lt;filename.h&gt; 格式来引用标准库的头文件，用 #include “filename.h” 格式来引用非标准库的头文件（编译器将从用户的工作目录开始搜索）。
  文件可按层或者功能组件划分不同的文件夹，便于其他人阅读。
  程序版式 版式虽然不会影响程序的功能，但会影响可读性，程序的风格统一则是赏心悦目。
代码排版在编码时确实很难把握，但可以编码完成后统一用工具格式化，不管编码使用Keil/MDK、Qt等集成工具，或者纯粹的代码编辑工具Source Insight，一般都支持自定义运行可执行文件，如Astyle。可以客制化新菜单，一键执行Astyle，将代码一键格式化，排版统一、层次分明。
Astyle官网 http://astyle.sourceforge.net/   按要求下载安装，只需要AStyle.exe即可。关于其使用和参数，可以再进入Documentation。对代码基本风格，{}如何对齐、是否换行，switch-case如何排版，tab键占位宽度，运算符或变量前后的空格等等，基本上代码排版涉及的方方面面都有参数说明。个人选择的编码参数是
--style=allman -S -U -t -n -K -p -s4 -j -q -Y -xW -xV fileName 效果如下
int Foo(bool isBar) { if (isBar) { bar(); return 1; } else { return 0; } } 也可以参考 代码的保养 第3章。关于注释，重要函数或段落必不可少，修改代码同时修改相应的注释，以保证注释与代码的一致性。
命名规则 比较著名的命名规则当推 Microsoft 公司的“匈牙利”法，该命名规则的主要思想是“在变量和函数名中加入前缀以增进人们对程序的理解”。例如所有的字符变量均以 ch 为前缀，若是指针变量则追加前缀 p。但没有一种命名规则可以让所有的程序员满意，制定一种令大多数项目成员满意的命名规则，重点是在整个团队和项目中贯彻实施。
事实上开发大多数基于SDK，一般底层命名规则尽量与SDK风格保持一致，至于上层就按团队标准，个人比较倾向全部小写字母，用下划线分割的风格，例如 set_apn、timer_start。
不要出现标识符完全相同的局部变量和全局变量，尽管两者的作用域不同而不会发生语法错误，但会使人误解，全局变量也不要过于简短。
变量的名字应当使用“名词”或者“形容词＋名词”，函数的名字应当使用“动词”或者“动词＋名词”，用正确的反义词组命名具有互斥意义的变量或相反动作的函数等。
基本语句 表达式和语句都属于C 语法基础，看似简单，但使用时隐患比较多，提供一些建议。
if if 语句是 C 语言中最简单、最常用的语句，然而很多程序员却用隐含错误的方式，仅以不同类型的变量与零值比较为例，展开讨论。
1.布尔变量与零值比较
不可将布尔变量直接与 TRUE、FALSE 或者 1、0 进行比较。根据布尔类型的语义，零值为“假”（记为 FALSE），任何非零值都是“真”（记为TRUE）。TRUE 的值究竟是什么并没有统一的标准。
假设布尔变量名字为 flag，它与零值比较的标准 if 语句如下：
if (flag) // 表示 flag 为真 if (!flag) // 表示 flag 为假 其它的用法都属于不良风格，例如：
//错误范例  if (flag == TRUE) if (flag == 1 ) if (flag == FALSE) if (flag == 0) 整型变量与零值比较  整型变量用“==”或“！=”直接与 0 比较，假设整型变量的名字为 value，它与零值比较的标准 if 语句如下：
if (value == 0) if (value != 0) 不可模仿布尔变量的风格而写成
//错误范例 if (value) // 会让人误解 value 是布尔变量 if (!value) 浮点变量与零值比较  不可将浮点变量用“==”或“！=”与任何数字比较，无论是 float 还是 double 类型的变量，都有精度限制。不能将浮点变量用“==”或“！=”与数字比较，应该设法转化成“&gt;=”或“&lt;=”形式。假设浮点变量的名字为 x，应当将
if (x == 0.0) // 隐含错误的比较，错误 转化为
const float EPSINON = 0.00001 if ((x&gt;=-EPSINON) &amp;&amp; (x&lt;=EPSINON)) //其中 EPSINON 是允许的误差（即精度），即x无限趋近于0.0 指针变量与零值比较  指针变量用“==”或“！=”与 NULL 比较， 指针变量的零值是“空”（记为 NULL），尽管 NULL 的值与 0 相同，但是两者意义不同。假设指针变量的名字为 p，它与零值比较的标准 if 语句如下：
if (p == NULL) // p 与 NULL 显式比较，强调 p 是指针变量  if (p != NULL) 不要写成
if (p == 0) // 容易让人误解 p 是整型变量 if (p != 0) if (p) // 容易让人误解 p 是布尔变量 if (!p) for 在多重循环中，如果有可能，应当将最长的循环放在最内层，最短的循环放在最外层，以减少 CPU 切换循环层的次数。
//不良范例 for (row=0; row&lt;100; row++) { for ( col=0; col&lt;5; col++ ) { sum = sum + a[row][col]; } } for (col=0; col&lt;5; col++ ) { for (row=0; row&lt;100; row++) { sum = sum + a[row][col]; } } switch switch 是多分支选择语句，而 if 语句只有两个分支可供选择；虽然可以用嵌套的if 语句来实现多分支选择，但那样的程序冗长难读。这是 switch 语句存在的理由。
switch-case 即使不需要 default 处理，也应该保留语句 default : break; 这样做并非多此一举，而是为了防止别人误以为你忘了 default 处理。确实不需要break的case，务必加上注释标明。
goto 很多人建议禁止使用 goto 语句，但实事求是地说，错误是程序员自己造成的，不是 goto 的过错。goto 语句至少有一处可显神通，它能从多重循环体中一下子跳到外面，特殊场景下可以使用，在很多if嵌套的场景，比如都有同样的错误处理，或者成对操作的文件开关，或者内存申请释放，就比较适合goto统一处理。
//代码只是表意，可能无法编译 #include &lt;stdlib.h&gt; void test(void) { char *p1,*p2; p1=(char *)malloc(100); p1=(char *)malloc(200); if(0) { //do something  goto exit; } else if(0) { //do something  goto exit; } //do something  //... exit: free(p1); free(p2); } int main() { goto_test(); return 0; } 对于内存申请释放、文件打开关闭这种成对操作，或者各种异常处理的统一支持场景，就比较适合goto。类似的还有do-while(0)这种语句。
关于运算优先级，熟记运算符优先级是比较困难的，如果代码行中的运算符比较多，为了防止产生歧义并提高可读性，全部加括号明确表达式的操作顺序，虽然愚笨但是可靠。
常量 常量是一种标识符，它的值在运行期间恒定不变。C 语言用 #define 来定义常量（称为宏常量），但用 const 来定义常量（称为 const 常量）其实更佳。
#define MAX 100 const float PI = 3.14159; const 常量有数据类型，而宏常量没有数据类型。编译器可以对前者进行类型安全检查,而对后者只进行字符替换，没有类型安全检查，并且在字符替换可能会产生意料不到的错误，所以复杂参数宏必须为每个参数加上()限制。
但也有特例
const int SIZE = 100; int array[SIZE]; // 有的编译器认为是错误，这就必须用define了 需要对外公开的常量放在头文件中，不需要对外公开的常量放在定义文件的头部。为便于管理，可以把不同模块的常量集中存放在一个公共的头文件中。
函数 函数设计的细微缺点很容易导致该函数被错用，函数接口的两个要素是参数和返回值，C 语言中函数的参数和返回值的传递方式有值传递（pass by value）和指针传递（pass by pointer）两种。
参数的规则 参数的书写要完整，不要贪图省事只写参数的类型而省略参数名字，如果函数没有参数，则用 void 填充。
void set_size(int width, int height); // 良好的风格 void set_size(int, int); // 不良的风格 int get_size(void); // 良好的风格 int get_size(); // 不良的风格 参数命名要恰当，顺序要合理。例如字符串拷贝函数
char *strcpy(char* dest, const char *src); 从名字上就可以看出应该把 src 拷贝到 dest。还有一个问题，两个参数哪个该在前哪个该在后？参数的顺序要遵循程序员的习惯。一般地，应将目的参数放在前面，源参数放在后面。
这里也说明下const的意义，如果参数仅作输入用，则应在类型前加 const，以防止在函数体内被意外修改。
避免函数有太多的参数，参数个数尽量控制在 5 个以内，如果参数太多，在使用时容易将参数类型或顺序搞错，可以定为结构体指针，但尽量带上参数注释。
除了printf、sprintf标准库或基于这类的日志输出接口，尽量不要使用类型和数目不确定的参数。
返回值的规则 不要省略返回值的类型，默认不加类型说明的函数一律自动按整型处理。为了避免混乱，如果函数没有返回值，应声明为 void 类型。
不要将正常值和错误标志混在一起返回。正常值用输出参数获得，而错误标志用 return 语句返回。
函数内部实现的规则 不同功能的函数其内部实现各不相同，看起来似乎无法就“内部实现”达成一致的观点。但根据经验，我们可以在函数体的“入口处”和“出口处”从严把关，从而提高函数的质量。
在函数体的“入口处”，对参数的有效性进行检查，很多程序错误是由非法参数引起的，我们应该充分理解并正确使用“断言”（assert）来防止此类错误。
在函数体的“出口处”，对 return 语句的正确性和效率进行检查。如果函数有返回值，那么函数的“出口处”是 return 语句。调用处应该尽量关注返回值，对异常进行处理
关于return的值，不可返回指向“栈内存”的“指针，该内存在函数体结束时被自动销毁。例如
char * Func(void) { char str[] = “hello world”; // str 的内存位于栈上  … return str; // 将导致错误  } 尽量避免函数带有“记忆”功能，相同的输入应当产生相同的输出。带有“记忆”功能的函数，其行为可能是不可预测的，因为它的行为可能取决于某种“记忆状态”。这样的函数既不易理解又不利于测试和维护。在 C语言中，函数 的 static 局部变量是函数的“记忆”存储器。建议尽量少用 static 局部变量，除非必需。
断言 程序一般分为 Debug 版本和 Release 版本，Debug 版本用于内部调试，Release 版本发行给用户使用。断言 assert 是仅在 Debug 版本起作用的宏，它用于检查“不应该”发生的情况。在运行过程中，如果 assert 的参数为假，那么程序就会中止。
void *memcpy(void *pvTo, const void *pvFrom, size_t size) { assert((pvTo != NULL) &amp;&amp; (pvFrom != NULL)); // 【使用断言】  byte *pbTo = (byte *) pvTo; // 防止改变 pvTo 的地址  byte *pbFrom = (byte *) pvFrom; // 防止改变 pvFrom 的地址  while(size -- &gt; 0 ) *pbTo ++ = *pbFrom ++ ; return pvTo; } assert 不应该产生任何副作用。所以 assert 不是函数，而是宏。可以把assert 看成一个在任何系统状态下都可以安全使用的无害测试手段。如果程序在 assert处终止了，并不是说含有该 assert 的函数有错误，而是调用者出了差错，assert 有助于找到发生错误的原因。
软件有必要进行防错设计，如果“不可能发生”的事情的确发生了，则要使用断言进行报警。
内存管理 C语言的内存管理既是它的优势，也是劣势。理解它的原理了才能更好的管理内存。
内存分配方式 内存分配方式有三种：
  从静态存储区域分配。内存在程序编译的时候就已经分配好，这块内存在程序的整个运行期间都存在。例如全局变量，static 变量。
  在栈上创建。在执行函数时，函数内局部变量的存储单元都可以在栈上创建，函数执行结束时这些存储单元自动被释放。栈内存分配运算内置于处理器的指令集中，效率很高，但是分配的内存容量有限。
  从堆上分配，亦称动态内存分配。程序在运行的时候用 malloc 或 new 申请任意多少的内存，程序员自己负责在何时用 free 或 delete 释放内存。动态内存的生存期由我们决定，使用非常灵活，但风险也大。
  内存错误及其对策 发生内存错误是件非常麻烦的事情。编译器不能自动发现这些错误，通常是在程序运行时才能捕捉到，而这些错误大多没有明显的症状，时隐时现，增加了改错的难度。常见的内存错误及其对策如下：
 内存分配未成功，却使用了它  编程新手常犯这种错误，因为他们没有意识到内存分配会不成功。常用解决办法是，在使用内存之前检查指针是否为 NULL。如果指针 p 是函数的参数，可在函数的入口处用 assert(p!=NULL)进行检查，或者用 if(p==NULL) 或 if(p!=NULL)进行防错处理。
内存分配虽然成功，但是尚未初始化就引用它  犯这种错误主要有两个起因：一是没有初始化的观念；二是误以为内存的缺省初值全为零，导致引用初值错误。内存的缺省初值究竟是什么并没有统一的标准（尽管有些时候为零值），为了安全，对分配的内存都进行清零。
内存分配成功并且已经初始化，但操作越过了内存的边界  数组使用时经常会发生下标“多 1”或“少 1”的操作。特别是在 for 循环语句中，循环次数很容易搞错，导致数组操作越界。
忘记释放内存，造成内存泄露  含有这种错误的函数每被调用一次就丢失一块内存。刚开始时系统的内存充足，运行正常，但随着运行时间加长，程序突然死掉，内存耗尽。动态内存的申请与释放必须配对，程序中 malloc 与 free 的成对使用。
已经释放的内存却继续使用它  程序中的调用关系过于复杂，逻辑顺序错误，或者使用了指向“栈内存”的“临时指针，使用 free 或 delete 释放了内存后，务必将指针设置为 NULL，使用前判断是否为NULL。
关于指针的使用建议，用 malloc 申请内存之后，应该立即检查指针值是否为 NULL，非NULL的赋初值；使用结束后用 free 释放，且将指针设置为 NULL，防止误用“野指针”。对动态内存的一些防护性操作，可以参考微信公众号【嵌入式系统】的文章动态内存管理及防御性编程。
指针与数组的对比 C 程序中指针和数组在不少地方可以相互替换着用，让人产生一种错觉，以 为两者是等价的。
数组要么在静态存储区被创建（如全局数组），要么在栈上被创建。数组名对应着（而不是指向）一块内存，其地址与容量在生命期内保持不变，只有数组的内容可以改变。
指针可以随时指向任意类型的内存块，它的特征是“可变”，所以我们常用指针来操作动态内存。指针远比数组灵活，但也更危险。
下面以字符串为例比较指针与数组的特性。
 修改内容  字符数组 a 的容量是 6 个字符，其内容为 hello\0。a 的内容可以改变，如 a[0]= ‘X’。指针 p 指向常量字符串“world”（位于静态存储区，内容为 world\0），常量字符串的内容是不可以被修改的。从语法上看，编译器并不觉得语句 p[0]= ‘X’有什么不妥，但是该语句企图修改常量字符串的内容而导致运行错误。
char a[] = “hello”; a[0] = ‘X’; cout &lt;&lt; a &lt;&lt; endl; char *p = “world”; // 注意 p 指向常量字符串 p[0] = ‘X’; // 编译器不能发现该错误 cout &lt;&lt; p &lt;&lt; endl; 内容复制与比较  不能对数组名进行直接复制与比较，若想把数组 a 的内容复制给数组 b，不能用语句 b = a ，否则将产生编译错误。应该用标准库函数 strcpy 进行复制。同理，比较 b 和 a 的内容是否相同，不能用 if(b == a) 来判断，应该用标准库函数 strcmp进行比较。
语句 p = a 并不能把 a 的内容复制指针 p，而是把 a 的地址赋给了 p。要想复制 a的内容，可以先用库函数 malloc 为 p 申请一块容量为 strlen(a)+1 个字符的内存，再用 strcpy 进行字符串复制。同理，语句 if(p==a) 比较的不是内容而是地址，应该用库函数 strcmp 来比较。
// 数组  char a[] = &#34;hello&#34;; char b[10]; strcpy(b, a); // 不能用 b = a;  if(strcmp(b, a) == 0 ) // 不能用 if ( b == a)  // 指针  int len = strlen(a); char *p = (char *)malloc(sizeof(char)*(len+1)); strcpy(p,a); // 不要用 p = a;  if(strcmp(p, a) == 0) // 不要用 if (p == a) 计算内存容量  用运算符 sizeof 可以计算出数组的容量（字节数）。sizeof(a)的值是 12（注意别忘了’\0’）。指针 p 指向 a，但是 sizeof(p)的值却是 4。这是因为sizeof(p)得到的是一个指针变量的字节数，相当于 sizeof(char*)，而不是 p 所指的内存容量。/C 语言没有办法知道指针所指的内存容量，只能在申请内存时记住它。
char a[] = &#34;hello world&#34;; char *p = a; cout&lt;&lt; sizeof(a) &lt;&lt; endl; // 12 字节  cout&lt;&lt; sizeof(p) &lt;&lt; endl; // 4 字节 当数组作为函数的参数进行传递时，该数组自动退化为同类型的指针。不论数组 a 的容量是多少，sizeof(a)始终等于 sizeof(char *)。
void Func(char a[100]) { cout&lt;&lt; sizeof(a) &lt;&lt; endl; // 4 字节而不是 100 字节 } 指针参数是如何传递内存  如果函数的参数是一个指针，不要指望用该指针去申请动态内存。
void get_memory(char *p, int num) { p = (char *)malloc(sizeof(char) * num); } void test(void) { char *str = NULL; get_memory(str, 100); // str 仍然为 NULL  strcpy(str, &#34;hello&#34;); // 运行错误 } ```c test 函数的get_memory(str, 100) 并没有使 str 获得期望的内存，str 依旧是 NULL，为什么？ 问题出在函数 get_memory，编译器总是要为函数的每个参数制作临时副本，指针参数 p 的副本是 _p，编译器使 _p = p。如果函数体内的程序修改了_p 的内容，就导致参数 p 的内容作相应的修改。这就是指针可以用作输出参数的原因。而范例中_p 申请了新的内存，只是把_p 所指的内存地址改变了，但是 p 丝毫未变。所以函数 get_memory并不能输出任何东西。事实上，每执行一次 get_memory就会泄露一块内存，因为没有用free 释放内存。 如果非得要用指针参数去申请内存，那么应该改用“指向指针的指针”，正确范例如下： ```c void get_memory2(char **p, int num) { *p = (char *)malloc(sizeof(char) * num); } void test2(void) { char *str = NULL; get_memory2(&amp;str, 100); // 注意参数是 &amp;str，而不是 str  strcpy(str, &#34;hello&#34;); free(str); } 由于“指向指针的指针”这个概念不容易理解，可以用函数返回值来传递动态内存，这种方法更加简单。
char *get_memory3(int num) { char *p = (char *)malloc(sizeof(char) * num); return p; } void test3(void) { char *str = NULL; str = get_memory3(100); //建议增加str指针是否为NULL判断，并清零内容  strcpy(str, &#34;hello&#34;); free(str); } 用函数返回值来传递动态内存这种方法虽然好用，但是常常有人把 return 语句用错，不要用 return 语句返回指向“栈内存”的指针，因为该内存在函数结束时自动消亡，错误范例如下：
//错误范例 char *get_string(void) { char p[] = &#34;hello world&#34;; return p; // 编译器将提出警告 } void test4(void) { char *str = NULL; str = get_string(); // str 的内容是随机垃圾 } 执行str = get_string()后 str 不再是 NULL 指针，但是 str 的内容不是“hello world”而是垃圾。
char *get_string2(void) { char *p = &#34;hello world&#34;; return p; } void test5(void) { char *str = NULL; str = get_string2(); } 函数 test5 运行虽然不会出错，但是函数 get_string2的设计概念却是错误的。因为 get_string2内的“hello world”是常量字符串，位于静态存储区，它在程序生命期内恒定不变。无论什么时候调用 get_string2，它返回的始终是同一个“只读”的内存块，也就是test5是无法修改str的。
free 把指针怎么了  free 只是把指针所指的内存给释放掉，但并没有把指针本身干掉；指针 p 被 free 以后其地址仍然不变（非 NULL），只是该地址对应的内存是垃圾，p 成了“野指针”。如果此时不把 p 设置为 NULL，会让人误以为 p 是个合法的指针。
如果程序比较长，我们有时记不住 p 所指的内存是否已经被释放，在继续使用 p 之前，通常会用语句 if (p != NULL)进行防错处理。很遗憾，此时 if 语句起不到防错作用，此时 p 不是 NULL 指针，但它也不指向合法的内存块。
char *p = (char *) malloc(100); strcpy(p, “hello”); free(p); // p 所指的内存被释放，但是 p 所指的地址仍然不变  if(p != NULL) // 没有起到防错作用 { strcpy(p, “world”); // 出错 } 动态内存会被自动释放吗  函数体内的局部变量在函数结束时自动消亡。
void func(void) { char *p = (char *) malloc(100); // 动态内存会自动释放吗？ } 但是，变量p 是局部的指针变量，它消亡的时候并不会让它所指的动态内存一起完蛋。发现指针有一些“似是而非”的特征：
（1）指针消亡了，并不表示它所指的内存会被自动释放。
（2）内存被释放了，并不表示指针会消亡或者成了 NULL 指针。
杜绝“野指针”  “野指针”不是 NULL 指针，是指向“垃圾”内存的指针。人们一般不会错用 NULL指针，因为用 if 语句很容易判断；但是“野指针”是很危险的，if 语句对它不起作用。“野指针”的成因主要有三种：
（1）指针变量没有被初始化。任何指针变量刚被创建时不会自动成为 NULL 指针，它的缺省值是随机的，所以，指针变量在创建的同时应当被初始化。
（2）指针 p 被 free 或者 delete 之后，没有置为 NULL，让人误以为 p 是个合法的指针。
（3）指针操作超越了变量的作用范围。这种情况让人防不胜防。
内存耗尽怎么办  如果在申请动态内存时找不到足够大的内存块，malloc 将返回 NULL 指针， 宣告内存申请失败。判断指针是否为 NULL，如果是则马上用 return 语句终止本函数，或者用 exit(1)终止整个程序的运行。如果发生“内存耗尽”，一般说来应用程序已经无药可救，嵌入式设备只能重启了。
心得体会  很少有人能拍拍胸脯说通晓指针与内存管理，越是怕指针，就越要使用指针。不会正确使用指针，肯定算不上是合格的嵌入式程序员。
其它编程经验 使用 const 提高函数的健壮性 const 是 constant 的缩写，“恒定不变”的意思。被 const 修饰的东西都受到强制保护，可以预防意外的变动，能提高程序的健壮性。很多 C++程序设计书籍建议：“Use const whenever you need”。
 用 const 修饰函数的参数 如果参数作输出用，不论它是什么数据类型，都不能加 const 修饰，否则该参数将失去输出功能。const 只能修饰输入参数，如果输入参数采用“指针传递”，那么加 const 修饰可以防止意外地改动该指针，起到保护作用。例如 strcpy函数：  char *strcpy(char* dest, const char *src); 其中 src是输入参数，dest是输出参数。给 src加上 const修饰后，如果函数体内的语句试图改动 src 的内容，编译器将指出错误。
如果输入参数采用“值传递”，由于函数将自动产生临时变量用于复制该参数，该输入参数本来就无需保护，所以不要加 const 修饰。  void func1(int x) 写成 void func1(const int x) //const无意义 对于非内部数据类型的参数而言，如 void func(A a) 这样声明的函数注定效率比较低，其中 A 为用户自定义的数据类型，可以理解为大结构。  函数体内将产生 A 类型的临时对象用于复制参数 a，而临时对象的构造、 复制、析构过程都将消耗时间。为了提高效率，可以将函数声明改为:
void func(A &amp;a) 因为“引用传递”仅借用一下参数的别名而已，不需要产生临时对象。但是函数 存在一个缺点，“引用传递”有可能改变参数 a，这是我们不期望的。解决这个问题很容易，加 const修饰即可，因此函数最终成为
void func(const A &amp;a) 4. 用 const 修饰函数的返回值，如果给以“指针传递”方式的函数返回值加 const 修饰，那么函数返回值（即指针）的内容不能被修改，该返回值只能被赋给加 const 修饰的同类型指针。例如函数
const char * get_string(void); char *str = get_string(); //出现编译错误：  const char *str = get_string(); //正确的用法 提高程序的效率 程序的时间效率是指运行速度，空间效率是指程序占用内存或者外存的状况。
不要一味地追求程序的效率，应当在满足正确性、可靠性、健壮性、可读性等质量因素的前提下，设法提高程序的效率。
在优化程序的效率时，应当先找出限制效率的“瓶颈”，不要在无关紧要之处优化。有时候时间效率和空间效率可能对立，此时应当分析那个更重要，作出适当的折衷。例如多花费一些内存来提高性能。
关于其它C关键字用法，可以参考 C语言关键字应用技巧。
小结 不论剑宗、气宗优劣，先把功能跑通再反推代码原理和实现流程，还是先理清时序和原理再编码实现功能，短期内剑宗效率高，加工资快，但后期发展有限；气宗则面临前期可能被淘汰，尤其在势利的小公司，不注重新人培养，但前期积累，后期融会贯通，在技术方面成为权威。如果合二为一，项目紧急则拿来就用，空闲时专研总结，取长补短，则是高级程序员的素质。
]]></content>
  </entry>
  
  <entry>
    <title>C++函数返回值类型后置</title>
    <url>/post/programming/c-plus-plus-function-return-value-type-postfix.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>C++</tag>
    </tags>
    <content type="html"><![CDATA[在C++的语法中，函数的返回值类型一直是函数声明的重要组成部分。然而，随着C++11及后续版本的引入，函数返回值类型后置（Trailing Return Type）的语法为开发者提供了新的可能。这种语法虽然看似是语法上的细微调整，却在现代C++的函数设计和代码可读性方面带来了显著的影响。
C++ 函数返回值类型的传统声明 在传统的C++函数声明中，返回值类型通常位于函数名之前。例如：
int add(int a, int b) { return a + b; } 这种声明方式一直沿用至今，直观、简洁，并且与大多数开发者的编程习惯相吻合。然而，随着泛型编程和lambda表达式在C++中的应用，传统的返回值声明方式逐渐暴露出一些局限性。
函数返回值类型后置的语法 函数返回值类型后置是C++11引入的新语法，使用 -&gt; 符号将返回值类型放置在参数列表之后。其基本语法格式如下：
auto function_name(parameters) -&gt; return_type { // function body } 例如，上述 add 函数可以改写为：
auto add(int a, int b) -&gt; int { return a + b; } 虽然这样的语法在功能上与传统语法没有太大区别，但它为处理复杂的返回值类型提供了更大的灵活性。
返回值类型后置的应用场景  函数模板和复杂返回值  在使用函数模板时，返回值类型可能依赖于模板参数。在这种情况下，返回值类型后置语法显得尤为方便。例如：
template&lt;typename T1, typename T2&gt; auto multiply(T1 a, T2 b) -&gt; decltype(a * b) { return a * b; } 在这个例子中，返回值类型 decltype(a * b) 依赖于 a 和 b 的类型，后置的语法使得这种声明更加直观。
Lambda表达式与 auto 类型推导  在现代C++中，lambda表达式广泛应用，而 auto 关键字的引入更是简化了类型推导。当需要显式指定lambda的返回类型时，后置语法可以提供更清晰的代码结构。例如：
auto lambda = [](int x) -&gt; int { return x * 2; }; 与 decltype 和 std::declval 的结合  对于一些依赖于表达式结果的返回值类型， decltype 和 std::declval 常与返回值类型后置语法结合使用。例如：
template&lt;typename T&gt; auto getValue(T&amp; t) -&gt; decltype(t.getValue()) { return t.getValue(); } 这种方式不仅简化了类型推导，还使代码结构更加清晰。
返回值类型后置的优势  提高代码可读性  在函数返回值依赖于参数类型的情况下，使用后置返回值类型语法能够将函数的主要信息集中在函数名和参数列表上，使代码更具可读性。
增强模板的灵活性  对于模板函数，返回值类型后置语法避免了过早指定返回值类型的问题，允许编译器在更晚的阶段决定最终的返回类型。
与现代C++特性的良好兼容性  随着C++的演进，lambda表达式、 decltype 、 auto 等新特性广泛应用，返回值类型后置语法与这些特性无缝衔接，为开发者提供了更加灵活的语法选择。
返回值类型后置的局限性 尽管返回值类型后置有诸多优势，但它也并非适用于所有场景。对于简单的函数，传统的返回值类型声明依然是最佳选择。此外，过度使用后置返回值类型可能会增加代码的复杂度，尤其是在不熟悉该语法的团队中，可能会引起阅读障碍。
总结 C++函数返回值类型后置语法是现代C++的一项重要特性，它为函数设计和模板编程提供了更大的灵活性。在理解和掌握这种语法的基础上，开发者可以编写出更加清晰、简洁和灵活的代码。然而，在实际应用中，仍需根据具体场景选择合适的语法，以平衡代码的可读性和灵活性。
]]></content>
  </entry>
  
  <entry>
    <title>硬盘IOPS简介</title>
    <url>/post/storage/introduction-to-hard-drive-IOPS.html</url>
    <categories><category>Storage</category>
    </categories>
    <tags>
      <tag>Hard drive</tag>
      <tag>IOPS</tag>
    </tags>
    <content type="html"><![CDATA[IOPS (Input/Output Per Second)即每秒的输入输出量(或读写次数)，是衡量磁盘性能的主要指标之一。
IOPS是指单位时间内系统能处理的I/O请求数量，一般以每秒处理的I/O请求数量为单位，I/O请求通常为读或写数据操作请求。随机读写频繁的应用，如OLTP(Online Transaction Processing)，IOPS是关键衡量指标。
数据吞吐量(Throughput)，指单位时间内可以成功传输的数据数量。对于大量顺序读写的应用，如VOD(Video On Demand)，则更关注吞吐量指标。
旋转延迟Trotation是指盘片旋转将请求数据所在扇区移至读写磁头下方所需要的时间。旋转延迟取决于磁盘转速，通常使用磁盘旋转一周所需时间的1/2表示。比如，7200 rpm的磁盘平均旋转延迟大约为60*1000/7200/2 = 4.17ms，而转速为15000 rpm的磁盘其平均旋转延迟约为2ms。
数据传输时间Ttransfer是指完成传输所请求的数据所需要的时间，它取决于数据传输率，其值等于数据大小除以数据传输率。目前IDE/ATA能达到133MB/s，SATA II可达到300MB/s的接口数据传输率，数据传输时间通常远小于前两部分时间。
理论上可以计算出磁盘的最大IOPS，即IOPS = 1000 ms/ (Tseek + Troatation)，忽略数据传输时间。假设磁盘平均物理寻道时间为3ms, 磁盘转速为7200,10K,15K rpm，则磁盘IOPS理论最大值分别为
 IOPS = 1000 / (3 + 60000/7200/2) = 140 IOPS = 1000 / (3 + 60000/10000/2) = 167 IOPS = 1000 / (3 + 60000/15000/2) = 200  固态硬盘SSD是一种电子装置， 避免了传统磁盘在寻道和旋转上的时间花费，存储单元寻址开销大大降低，因此IOPS可以非常高，能够达到数万甚至数十万。
实际测量中，IOPS数值会受到很多因素的影响，包括I/O负载特征(读写比例，顺序和随机，工作线程数，队列深度，数据记录大小)、系统配置、操作系统、磁盘驱动等等。因此对比测量磁盘IOPS时，必须在同样的测试基准下进行，即便如何也会产生一定的随机不确定性。通常情况下，IOPS可细分为如下几个指标：
 Toatal IOPS，混合读写和顺序随机I/O负载情况下的磁盘IOPS，这个与实际I/O情况最为相符，大多数应用关注此指标。 Random Read IOPS，100%随机读负载情况下的IOPS。 Random Write IOPS，100%随机写负载情况下的IOPS。 Sequential Read IOPS，100%顺序负载读情况下的IOPS。 Sequential Write IOPS，100%顺序写负载情况下的IOPS。  IOPS的测试benchmark工具主要有Iometer, IoZone, FIO等，可以综合用于测试磁盘在不同情形下的IOPS。对于应用系统，需要首先确定数据的负载特征，然后选择合理的IOPS指标进行测量和对比分析，据此选择合适的存储介质和软件系统。
在计算具体的磁盘IOPS之前，需要对常见的RAID类型的读写比、不同硬盘类型的IOPS值、具体应用的IOPS需求等等有一些了解。
]]></content>
  </entry>
  
  <entry>
    <title>SSH 是如何工作的</title>
    <url>/post/linux/how-does-ssh-work.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>SSH</tag>
    </tags>
    <content type="html"><![CDATA[在计算机网络中，需要登录远程计算机。从历史上看，这是通过 Telnet（始于 20 世纪 60 年代末）和 rlogin（始于 20 世纪 80 年代）实现的。然而，这些早期的程序并不安全。SSH（Secure Shell）是为解决这些缺陷而创建的协议。它在不安全的网络上建立安全连接。
SSH 采用客户端-服务器架构。客户端是指试图连接远程服务器的机器。SSH 对客户机和服务器进行身份验证。它规定了客户端和服务器如何就共享秘密达成一致，以便随后对数据包进行加密。最后，它还提供数据完整性保护。
SSH 有两个版本：SSH-1 和 SSH-2。SSH-2 由 IETF 标准化。SSH 已被广泛采用。现代操作系统都内置了 SSH 客户端。OpenSSH 是一种流行的开源实现方式。也有商业实施方案。
下面将逐步解释 SSH 的三个主要层： 传输层、验证层和连接层。
传输层 传输层提供加密、完整性和数据保护，确保客户端和服务器之间的通信安全。
第 1 步 - 建立 TCP 连接 客户端启动与 SSH 服务器的 TCP 连接，通常是在 22 端口。
第 2 步 - 协议版本交换 客户端和服务器交换包含各自支持的 SSH 协议版本的标识字符串。
第 3 步 - 算法协商 客户端和服务器就加密、密钥交换、MAC（消息验证码）和压缩所使用的加密算法达成一致。
第 4 步 - 密钥交换 密钥交换算法（如 Diffie-Hellman）用于安全生成共享秘密。这一过程可确保双方在不直接传输的情况下获得相同的会话密钥。
第 5 步 - 会话密钥推导 会话密钥是从共享秘密和其他交换信息中推导出来的。这些密钥用于加密和解密通信。
第 6 步 - 密码初始化 双方使用导出的会话密钥初始化各自选择的加密和 MAC 算法。
第 7 步 - 数据完整性和加密 传输层为客户端和服务器之间的所有后续通信提供数据完整性检查和加密。
验证层 验证层验证客户端的身份，确保只有授权用户才能访问服务器。
第 1 步 - 服务请求 客户端请求 &ldquo;ssh-userauth &ldquo;服务。
第 2 步 - 身份验证方法广告 服务器公布可用的身份验证方法（如密码、公钥、键盘交互）。
第 3 步 - 客户端身份验证 客户端尝试使用一种或多种可用方法进行身份验证。常见方法包括：
 密码验证：客户端向服务器发送密码，由服务器进行验证。 公钥验证：客户端证明拥有与先前注册的公钥相对应的私钥。 键盘交互式身份验证：服务器向客户端发送提示，客户端回复所需的信息（如 OTP、安全问题）。  第 4 步 - 身份验证成功/失败 如果客户端的凭据有效，服务器将允许访问。否则，客户端可尝试其他认证方法或关闭连接。
连接层 连接层将加密和认证通信复用为多个逻辑通道。
第 1 步 - 创建通道 客户端请求为各种类型的通信（如 shell 会话、文件传输、端口转发）打开通道。
第 2 步 - 信道请求 每个通道请求包括所需服务类型和任何附加参数等详细信息。
第 3 步 - 信道数据传输 数据通过已建立的信道传输。每个通道独立运行，允许多个服务在一个 SSH 连接上同时运行。
第 4 步 - 关闭通道 通道可以相互独立关闭，而不会影响整个 SSH 连接。关闭所有通道后，客户端即可终止 SSH 会话。
第 5 步 - 全局请求 连接层还支持全局请求，全局请求会影响整个连接而不是单个通道（例如，重新加密钥会话）。
]]></content>
  </entry>
  
  <entry>
    <title>PCI SIG 在 fms 2024 上演示了 PCIe 6.0 的互操作性</title>
    <url>/post/hardware/pcisig-demonstrates-pcie-60-interoperability-at-fms-2024.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>PCIe</tag>
      <tag>PCIe Gen6</tag>
      <tag>PCIe Gen7</tag>
    </tags>
    <content type="html"><![CDATA[随着 PCIe 5.0 在数据中心和消费市场的部署不断升温，PCI-SIG 并没有闲着，而是已经开始着手让生态系统为 PCIe 规范的更新做好准备。
在 FMS 2024 上，一些供应商甚至在谈论具有 128 GT/s 功能的 PCIe 7.0  ，尽管 PCIe 6.0 甚至还没有开始出货。我们采访了 PCI-SIG，了解了其活动的最新进展，并讨论了 PCIe 生态系统的现状。
PCI-SIG 已向其成员提供了 PCIe 7.0 规范 (v 0.5)，并预计完整规范将于 2025 年某个时候正式发布。目标是使用 x16 链路实现 128 GT/s 数据速率和高达 512 GBps 的双向流量。与 PCIe 6.0 类似，此规范也将利用 PAM4 信号并保持向后兼容性。在起草过程中，还考虑了功率效率以及硅片面积。
与之前的 NRZ 方案相比，转向 PAM4 信号传输会带来更高的误码率。这使得 PCIe 6.0 必须采用不同的纠错方案 - PCIe 6.0 的流量控制单元 (FLIT) 编码不是对可变长度数据包进行操作，而是对固定大小数据包进行操作，以帮助进行前向纠错。PCIe 7.0 保留了这些方面。
PCIe 6.0 合规计划的集成商名单也有望在 2025 年公布，尽管初步测试已经在进行中。FMS 2024 演示中就体现了这一点，该演示涉及 Cadence 的 3nm 测试芯片（用于其 PCIe 6.0 IP 产品）以及 Teledyne Lecroy 的 PCIe 6.0 分析仪。这些时间表与前几代 PCIe 的规范完成日期和合规计划可用性保持一致。
我们还收到了有关光学工作组的最新消息 - 虽然该工作组与光学技术无关，但它还打算开发特定于技术的形式因素，包括可插拔光学收发器、板载光学器件、共封装光学器件和光学 I/O。PCIe 6.0 规范的逻辑层和电气层正在得到增强，以适应新的光学 PCIe 标准化，这一过程也将在 PCIe 7.0 中完成，以配合该标准明年的发布。
PCI-SIG 也有正在进行的布线计划。在消费者方面，我们看到 Thunderbolt 和外部 GPU 机箱的吸引力显著增加。然而，数据中心和企业系统也正在转向布线解决方案，因为很明显，将存储等组件与 CPU 和 GPU 分开更有利于热设计。此外，对于板载信号线来说，在较长的距离上保持信号完整性变得困难。计算系统内部的布线可以提供帮助。
OCuLink 成为了一个不错的候选方案，并被广泛用作服务器系统的内部链路。它甚至出现在一些中国制造商的迷你电脑中，以面向消费市场的外部化身出现，尽管吸引力有限。随着速度的提高，广泛采用的外部 PCIe 外围设备标准（甚至系统内的连接组件）将变得势在必行。
]]></content>
  </entry>
  
  <entry>
    <title>Intel 13/14代酷睿稳定性补丁Linux实测</title>
    <url>/post/soc/intel-13rd-14th-generation-core-stability-patch-linux-test.html</url>
    <categories><category>SOC</category>
    </categories>
    <tags>
      <tag>Intel</tag>
      <tag>Core 13/14</tag>
      <tag>Linux patch</tag>
    </tags>
    <content type="html"><![CDATA[Intel近期发布了0x129的微码补丁，旨在解决其13代和14代酷睿处理器的稳定性问题。
在经过一系列的更新后，0x129补丁在Windows上的基准测试已经显示出积极的结果，而根据Phoronix的测试，该补丁在Linux平台上的表现也同样令人满意。
Phoronix对0x129微码进行了188项基准测试，并与之前的0x125进行了对比。
3DMark Wild Life Extreme测试中，新微代码没有显示出性能下降的迹象，反而在帧率上有了微小但可忽略的提升。
7-Zip压缩、PyPerformance、PyBench、Cryptsetup和Perl基准测试等环境中，CPU的特定元素，无论是浮点单元(FPUs)还是指令集架构(ISA)，都得到了测试。
从这些基准测试可以看出，14代和13代CPU在更新0x129后，在Linux平台上性能也几乎没有损失。
然而，在WireGuard测试中却出现了性能下降11.8%，这种下降可能与新微码影响单线程性能有关，尽管影响并不大。
Intel此前官方声明中曾强调，基于默认设置的内部测试显示，打补丁前后的性能波动在正常误差范围内，3DMark Time Spy、WebXPRT 4、CineBench R24、Blender 4.2.0等常见的基准测试项目均是如此。
不过，少数测试子项会收到些许影响，比如WebXPRT Online Homework、PugetBench GPU Effects Score。
游戏性能也几乎没有变化，包括《赛博朋克2077》、《古墓丽影：暗影》、《全面战争：战锤III－疯狂之镜》等等，还有《杀手3：达特摩尔》的变化略大一些。
Intel还表示，产品发布时候承诺的睿频频率可以达到，但对比没有限制的超频性能，确实会有损失。
其中，全核满负载应用会有一定的性能损失，比如视频编辑渲染等，而单核应用、游戏等不会吃满全核的应用，基本没有影响。
为了获得最佳日常体验，Intel建议所有的13/14代酷睿用户使用推荐的默认设置。
Intel当前的分析发现，由于电压升高，受影响处理器的多个核心的最低运行电压(Vmin)都会显著增加，而且会随着时间的推移而累积，导致整个处理器的最低电压升高。
0x129版微代码更新后，会限制高于1.55V的电压请求，对于未出现不稳定现象的处理器可起到预防作用。
Intel还确认，基于广泛深入的验证，所有的未来处理器都不会再受此问题影响。
]]></content>
  </entry>
  
  <entry>
    <title>DDR SDRAM 的读写timing</title>
    <url>/post/hardware/ddr-sdram-read-and-write-timing.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>DDR</tag>
      <tag>SDRAM</tag>
    </tags>
    <content type="html"><![CDATA[本文简要讲述了DDR SDRAM 的读写timing。
Read burst 下图显示了CL = 2和BL = 4的read burst 的timing。
在READ突发期间，有效数据在READ命令后的CL之后可用。每个后续的数据在下一个CLK和CLK#的交叉处有效。DDR SDRAM与输出数据一起驱动DQS。DQS上的初始LOW状态被称为read preamble；与最后一个数据输出重合的LOW状态被称为read postamble。完成read burst后，假设没有启动其他命令，DQ将变为High-Z。
来自任何READ的数据都可以与后续READ命令的数据连起来。通过这种方式，可以保持数据的连续传输。新read的第一个数据紧跟着上一个read的最后一个数据。
Write burst 下图显示了BL = 4的WRITE的时序。DQ上出现的输入数据被写入内存阵列。DQS和DM信号现在由内存控制器与数据一起传输。如果DM信号为LOW，则相应的输入数据将写入内存。如果DM信号被为HIGH，则忽略相应的输入数据，并且不会向该列位置WRITE。
在WRITE期间，第一个有效的输入数据按照WRITE命令在DQS的第一个上升边沿寄存。后续数据在DQS的连续边缘上寄存。写入命令和第一个上升沿之间的DQS上的LOW状态被称为write preamble，最后一个输入数据元素之后的DQS上的LOW状态被称为write postamble。
]]></content>
  </entry>
  
  <entry>
    <title>使用Flatpak安装linux应用</title>
    <url>/post/linux/install-linux-applications-using-flatpak.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Flatpak</tag>
    </tags>
    <content type="html"><![CDATA[所有 Linux   桌面都可以使用 Flathub   获取和分发应用。Flathub 由 Flatpak 驱动，让 Flathub 应用在几乎所有的 Linux 发行版上都能运行。
安装Flatpak 各种Linux发行版安装Flatpak的设置说明：https://flathub.org/zh-Hans/setup
在Ubuntu 18.10 (Cosmic Cuttlefish) 或之后的新版本上，只需要运行:
sudo apt install flatpak 对于老点的版本需要先安装Flatpak的PPA源，再去安装
sudo add-apt-repository ppa:flatpak/stable sudo apt update sudo apt install flatpak Flathub：Linux 应用商店 安装Flathub的存储库，之后就可以安装flathub上的应用了。
flatpak remote-add --if-not-exists flathub https://dl.flathub.org/repo/flathub.flatpakrepo 安装Linux应用 比如上一篇文章说的Kodi，就可以通过以下命令来安装
flatpak install flathub tv.kodi.Kodi ]]></content>
  </entry>
  
  <entry>
    <title>嵌入式Linux程序如何开机自启动</title>
    <url>/post/linux/how-to-start-an-embedded-linux-program-automatically-at-boot-time.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Embedded Linux</tag>
    </tags>
    <content type="html"><![CDATA[在很多嵌入式系统中，由于可用资源较少，常常在系统启动后就直接让应用程序自动启动，以减少用户操作和节省资源。
如何让自己的应用程序自动启动呢？
自启动的三种方式 在Linux系统中，配置应用程序自动启动的方法有以下三种：
通过/Linuxrc脚本直接启动 Linux内核一旦开始执行，它将通过驱动程序来初始化所有硬件设备，这个初始化过程可以在启动时的PC显示器上看到，每个驱动程序都打印一些相关信息。
初始化完成后，通常调用的是init，通过loader调用init内的init=/app_program语句（通过loader向核心传入init=/program可以定制首先运行的程序）。
比如在桌面Linux系统中，init进程会读取/etc/inittab文件，来决定执行级别和哪些脚本和命令。
嵌入式应用开发中，可以根据实际情况决定是否使用标准的init执行方式，也许这个init是个静态程序，它能够完成我们的嵌入应用的特定任务，那完全不用考虑inittab了，在这里可以采用比较灵活的措施。
在/etc/init.d下添加启动脚本 主要用来启动一些系统服务，根据runlevel执行rcx.d, 而在rcx.d中都会在最后调用rc.local
一般情况下，大多数的Linux操作系统使用/etc/init.d/(或/etc/rc.d/init.d)下的脚本来配置应用程序的自动启动。
例如，在某些Linux系统中，
corn程序通过/etc/init.d/corn脚本启动，
Apache通过/etc/init.d/httpd启动，
syslogd通过/etc/init.d/syslogd启动，
而sshd则通过/etc/init.d/sshd脚本启动。
通常这些脚本通过来自特定rc.d目录的符号链接运行。
为了配置从哪个rc.d目录运行脚本，Linux系统提供了许多不同的工具，同时也可以手工进行配置。
Linux系统有一个包含所有实际启动脚本文件的目录。它可能是/etc/init.d，也可能是/etc/rc.d/rc.d。
同时对应每个运行级别（runlevel）又有一个另外的目录，它们可能是/etc/rc2.d，也可能是/etc/rc.d/rc2.d。这些目录中的文件通常是指向实际脚本文件的符号链接。
直接在/etc/rc.d/rc.local脚本中添加命令 在Linux系统中，有一个类似Windows系统中autoexec.bat的文件，它就是/etc/rc.d/rc.local，系统开机后自动运行用户的应用程序或启动系统服务的命令保存在开发板根文件系统的这个文件中。
因此可以编辑rc.local文件，将要执行的程序（命令）添加到该文件夹中。Linux系统在启动后还未登录前，将自动执行该程序（命令），达到开机自动运行用户的应用程序的目的。
运行级别 Linux系统有7个运行级别(runlevel)  运行级别0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动 运行级别1：单用户工作状态，root权限，用于系统维护，禁止远程登陆 运行级别2：多用户状态(没有NFS) 运行级别3：完全的多用户状态(有NFS)，登陆后进入控制台命令行模式 运行级别4：系统未使用，保留 运行级别5：X11控制台，登陆后进入图形GUI模式 运行级别6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动  运行级别的原理  在目录/etc/rc.d/init.d下有许多服务器脚本程序，一般称为服务(service)(注：自己常用的有/etc/init.d/network&hellip;) 在/etc/rc.d下有7个名为rcN.d的目录，对应系统的7个运行级别 rcN.d目录下都是一些符号链接文件，这些链接文件都指向init.d目录下的service脚本文件，命名规则为K+nn+服务名或S+nn+服务名，其中nn为两位数字。 系统会根据指定的运行级别进入对应的rcN.d目录，并按照文件名顺序检索目录下的链接文件   对于以K开头的文件，系统将终止对应的服务 对于以S开头的文件，系统将启动对应的服务   查看运行级别用：runlevel 进入其它运行级别用：init N 另外init0为关机，init 6为重启系统  linux系统开机流程  机器开机通电bios质检，质检通过后，bios会根据boot菜单，选择从硬盘启动。 读取硬盘中的MBR（主引导记录），加载引导程序（linux中是grub），grub程序会加载系统kernel和虚拟文件系统。 系统会运行init程序，该进程会读取/etc/inittab配置文件，决定运行级别，运行/etc/rc.d/rc.sysinit脚本对系统进行初始化，该脚本会设置系统时间，主机名，开启卷管理，以读写的方式重挂root文件系统，挂载本地文件系统，开启本地磁盘配额，开启交换空间。 运行该运行级别下的/etc/rc.d/rcN.d/下的脚本，开启系统服务进程。 读取/etc/rc.d/rc.local下的命令，并逐条运行。 启动虚拟终端，进入登录界面。 ]]></content>
  </entry>
  
  <entry>
    <title>接地电阻为什么一般不大于4Ω</title>
    <url>/post/hardware/grounding-resistance-selection-tips.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>Grounding Resistor</tag>
    </tags>
    <content type="html"><![CDATA[接地电阻就是电流由接地装置流入大地再经大地流向另一接地体或向远处扩散所遇到的电阻，它包括接地线和接地体本身的电阻、接地体与大地的电阻之间的接触电阻以及两接地体之间大地的电阻或接地体到无限远处的大地电阻。
接地电阻大小直接体现了电气装置与“地”接触的良好程度，也反映了接地网的规模。在单点接地系统、干扰性强等条件下，可以采用辅助地极的测量方式进行测量。
接地主要分以下四种  保护接地：电气设备的金属外壳，混凝土、电杆等，由于绝缘损坏有可能带电，为了防止这种情况危及人身安全而设的接地。 防静电接地：防止静电危险影响而将易燃油、天然气贮藏罐和管道、电子设备等的接地。 防雷接地：为了将雷电引入地下，将防雷设备（避雷针等）的接地端与大地相连，以消除雷电过电压对电气设备、人身财产的危害的接地，也称过电压保护接地。 工作接地：是指将电力系统的某点（如中性点）直接接大地，或经消弧线圈、电阻等与大地金属连接，如变压器、互感器中性点接地等。  接地电阻规范要求  独立的防雷保护接地电阻应小于等于10欧 独立的安全保护接地电阻应小于等于4欧 独立的交流工作接地电阻应小于等于4欧 独立的直流工作接地电阻应小于等于4欧 防静电接地电阻一般要求小于等于100欧 共用接地体（联合接地）应不大于接地电阻1欧 仪表携带、使用时须小心轻放，避免剧烈震动  注：避雷针的地线属于防雷保护接地，如果避雷针接地电阻和防静电接地电阻都是按要求设置的，那么就可以将防静电设备的地线与避雷针地线接在一起，因为避雷针的接地电阻比静电接地电阻小10倍，因此发生雷电事故时大部分雷电将从避雷针地泄放，经过防静电地的电流则可以忽略不计。
接地电阻为什么一般不大于4Ω？ 接地电阻是电流由接地装置流入大地再经大地流向另一接地体或向远处扩散所遇到的电阻，作用是向大地放电，以保证安全。
很多家用电器，我们在使用过程中，往往会接触到电器的壳体，比如冰箱开关门、洗衣机开关门等。但一旦电器发生故障，壳体会带上一定的电压，当人接触壳体时会发生电击，十分危险，因此通过接地电阻把电引向大地是非常有必要的。
对于接地电阻，我们希望越小越好。根据欧姆定律，电压一定时，电阻与电流成反比；因此，工作电压确定的情况下，接地电阻越小，能通过的电流就越大，电流泄放效果越好。如果漏电的话，电就全部从接地的地线上传到地下了。
接地电阻的最高限值为什么是4Ω 正常情况下，当电气系统设备假若发生故障时，故障电流正常不会大于10A，所以当接地电阻为4Ω时，流过接地电阻时产生的故障电压可由欧姆定律计算：4x10=40V。而经实验形成的标准规定，在正常和故障情况下，任何两导体间或任一导体与大地之间的电压均不得超过交流(50～500Hz)有效值50V。
这样小于50V的故障电压是较为安全的，几乎不可能引起人体电击事故，因此接地电阻的最高限值就规定在了4Ω。
重要原因 符合法律法规和行业标准是保持接地电阻相应阻值的重要原因。电力系统建设、运行和管理必须遵守一系列法律法规和行业标准，在电力行业这些规范规定，都是经过无数次的实践检验得来的经验教训，尤其是涉及安全的条款，无一不是电力先行者们经过血的教训而得来的实践经验和数据，其中就包括接地电阻的规定。
例如《电气安全规范》，规定了不同类型的电气设备对于接地电阻的要求。若接地电阻大于规定值，则会导致设备存在安全隐患，就违反相关标准和法规，更重要的是影响周边人员生命安全。
合适的接地方式 选用合适的接地方式也能保证接地电阻不大于4Ω。在不同的用电场所，需要选择适当的接地方式。例如，对于干式变压器，使用母线法接地，可以减小母线电感，提高设备的抗干扰能力，进而保持接地电阻不大于4Ω。
地面环境和接地材料的影响 通常，地面的土壤电阻率、潮湿程度、温度等因素会影响接地电阻。选择合适的接地材料和合理的接地方式，可以调整接地电阻的大小，保证接地电阻不大于4Ω。
接地电阻的应用场景 具体应用来看，接地分好多种场景，常见的接地包括：防雷接地（也就是常见的避雷针）：接地电阻越小，一旦遭受雷击时雷电通过接地线向大地放电就越快，也就是越安全。
电器设备安全地（如洗衣机机壳接地）： 很多家用电器尤其是大电器像冰箱、洗衣机、空调等，都需要接地使用（机壳接地）。当机壳漏电时，接地电阻越小，漏电就会越多通过接地线传入大地。但如果接地电阻太大（假如大过人体电阻），当人触摸机壳时，人体就成了接地线，电流就从人体流入大地，导致人体触电，相当危险。
还有电子设备内部的工作接地等等，都是接地电阻越小越好。有一些电子设备会产生静电，如果接入了接地电阻，能够防止因为静电而产生的危险。
]]></content>
  </entry>
  
  <entry>
    <title>嵌入式 C 语言的动态变长数组</title>
    <url>/post/programming/embedded-c-dynamic-variable-length-array.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>Embedded C</tag>
      <tag>Dynamic variable length</tag>
    </tags>
    <content type="html"><![CDATA[我们在编写 C 语言程序的时候，如果使用的编译器只支持 C89 标准，那么，在定义数组的时候，数组长度必须确定，例如：int arr[10]，因为数组是静态分配内存的，所以数组的长度必须要在编译时进行确定。
当然，我们也可以采用指针变量和动态内存分配的方式，来模拟动态数组的行为，可以使用 malloc 或者 calloc 相关的函数，在程序运行时进行动态内存分配。
示例代码如下：
#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt; int main() { int *arr = NULL; int n = 10; // 动态分配数组  arr = (int *)malloc(n * sizeof(int)); if (arr == NULL)return -1; // 使用数组  for (int i = 0; i &lt; n; i++) { arr[i] = i; } // 打印数组内容  for (int i = 0; i &lt; n; i++) { printf(&#34;%d \mn &#34;, arr[i]); } // 释放内存  if(NULL != arr)free(arr); return 0; } 然而，除了使用 malloc 来模拟实现动态数组的功能之外，在 C 语言的 C99 标准里面，还引入了可变长数组（VLA）的概念，也就是说，可以允许数组的长度在程序运行时进行确定。
示例代码如下：
#include &lt;stdio.h&gt; int main() { int n = 0; printf(&#34;Enter the number of elements: &#34;); scanf(&#34;%d&#34;, &amp;n); // 创建一个VLA  int arr[n]; // 使用VLA  for (int i = 0; i &lt; n; i++) { arr[i] = i * i; } // 打印数组内容  for (int i = 0; i &lt; n; i++) { printf(&#34;%d \mn&#34;, arr[i]); } return 0; } C99 标准里面的可变长数组特性，它允许数组在运行时才确定大小，尤其是在处理大小不确定的数据集时，可变长数组（VLA）为程序员提供了更大的灵活性。
使用可变长数组（VLA）的优势：
  动态分配大小：使用可变长数组，可以根据实际需要，在程序运行时分配数组的带下，在处理不确定大小的数据集时，非常有用。
  减少内存浪费：由于可变长数组是运行时才确定其大小的，因此可以避免在编译时分配固定内存，造成内存浪费。
  但是，可变长数组（VLA）这种特性有利也有弊，在使用的时候需要注意一些潜在的问题。
使用可变长数组的注意事项：
  编译器兼容问题：不是所有的编译器都支持 C99 标准里面的VLA特性，并且可能需要特定的编译器标志位来启用。
  程序可移植性：VLA特性在 C11 标准里面已经被进行标记，可能会在未来的 C 语言标准中移除，因此需要注意代码的可移植性。
  栈溢出风险：VLA在程序运行时分配数组空间大小，其位于内存的栈区域，使用不当可能会导致栈溢出。
  性能优化问题：编译器可能无法对VLA进行优化，其代码性能可能不如静态分配数组或使用malloc分配内存。
  总的来说，可变长数组（VLA）特性，在编译时不确定数组大小，而是在程序运行时才确定，在使用时需要权衡其优缺点，并且注意潜在的风险。在某些情况下，使用VLA特性与动态内存 malloc 分配，可能会是一个不错的选择。
]]></content>
  </entry>
  
  <entry>
    <title>华为下一代麒麟芯片或将采用统一内存架构</title>
    <url>/post/soc/huawei-next-generation-kirin-chip.html</url>
    <categories><category>SOC</category>
    </categories>
    <tags>
      <tag>Huawei</tag>
      <tag>Kirin</tag>
    </tags>
    <content type="html"><![CDATA[华为下一代麒麟芯片或将采用统一内存架构
华为或将借鉴苹果和英特尔的CPU策略，下一代麒麟CPU可能采用类似Apple M系列芯片和Intel Lunar Lake的封装技术。
华为的下一代麒麟芯片据称将采用统一内存架构（Unified Memory Architecture, UMA），类似于苹果M系列芯片和英特尔Core Ultra系列CPU所采用的技术。这意味着华为将在下一代麒麟处理器中内置RAM，并使其对GPU和CPU可用。
一位微博用户预测，这款麒麟CPU配备Taishan V130核心，其性能可与苹果M3相媲美，并指出：“这是专门为 AI终端产品  规划的芯片，其内存带宽是之前PC芯片的两倍。”在另一篇微博帖子中表示：“AI本地大型模型和推理速度由内存带宽决定，使用IC基板上的UMA统一内存-DRAM将比目前Windows系统中使用的芯片具有更强的协同性能。”
这可能意味着华为正在开发一款旨在与高通Snapdragon X处理器竞争的芯片，后者具有45 TOPs的神经处理单元（NPU）。即便华为成功生产出可以与苹果和高通SoC在性能上匹敌的芯片，我们仍然不确定它是否能够使用微软的人工智能功能，尤其是Copilot+ PC品牌。
]]></content>
  </entry>
  
  <entry>
    <title>谷歌不再支持 Golang</title>
    <url>/post/programming/google-no-longer-supports-golang.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>Google</tag>
      <tag>Golang</tag>
    </tags>
    <content type="html"><![CDATA[Golang，又称 Go 语言，自 2009 年由 Google 发布以来，凭借其简洁的语法、高效的并发性能和强大的标准库，迅速成为云计算、网络编程、微服务等领域备受青睐的编程语言。然而，近期 Google 对其 Python 核心团队和 Flutter/Dart 团队的裁员引发了开发者社区的广泛关注，也不禁让人思考：如果有一天 Google 不再支持 Golang，Golang 的未来将会如何？
Google 真的会放弃 Golang 吗？ 尽管 Google 近期的裁员行为引发了一些担忧，但从目前的情况来看，Google 放弃 Golang 的可能性微乎其微。Golang 已经成为 Google 内部许多关键基础设施的核心语言，例如 Kubernetes、Docker、gRPC 等，这些项目对 Google 的云计算战略至关重要。此外，Golang 拥有庞大而活跃的开发者社区，他们在推动语言发展、维护开源库和构建生态系统方面发挥着不可替代的作用。因此，Google 不太可能轻易放弃 Golang 这样一门成熟、高效且具有战略意义的编程语言。
如果 Google 不再支持 Golang，会发生什么？ 尽管 Google 放弃 Golang 的可能性很小，但我们不妨假设这种情况真的发生，探讨一下 Golang 未来的发展方向：
 社区主导开发：  和许多开源项目一样，Golang 的未来将更多地由社区主导。核心开发者团队可能会从 Google 内部转移到其他公司或组织，继续领导语言的设计和开发工作。社区也会更加积极地参与到语言的改进和推广中，例如组织开发者大会、提供学习资源、开发第三方库等。
出现分支和方言：  在没有 Google 的统一领导下，Golang 社区可能会出现不同的分支和方言。这些分支可能会专注于不同的应用场景或技术方向，例如 Web 开发、数据科学、嵌入式系统等。不同分支之间可能会出现一些兼容性问题，但也会促进语言的多样性和创新性。
发展速度放缓：  失去 Google 的资源支持，Golang 的发展速度可能会放缓。新特性的添加、Bug 的修复、性能的优化等都需要更多的时间和精力。社区需要更加努力地筹集资金、招募志愿者，才能保证语言的持续发展。
影响力下降：  没有 Google 的背书，Golang 的影响力可能会下降。一些企业或开发者可能会选择其他编程语言，导致 Golang 的市场份额下降。
Golang 的未来展望 尽管存在一些潜在的风险，但 Golang 凭借其自身的优势和社区的支持，仍然拥有光明的发展前景。以下是一些可能的发展趋势：
 泛型编程的进一步完善: 泛型编程是 Golang 社区一直以来呼声很高的特性，它可以提高代码的复用性和类型安全性。 并发模型的持续优化: Golang 的并发模型简单高效，未来将会继续优化，以更好地适应多核处理器和分布式系统的需求。 WebAssembly 的支持: WebAssembly 是一种新兴的 Web 技术，可以让 Golang 代码运行在浏览器中，拓展其应用场景。 与其他语言的互操作性: Golang 将会更加注重与其他语言的互操作性，例如 Python、Java、JavaScript 等，方便开发者构建混合语言的应用程序。  示例：社区主导的 Go 项目
为了更好地理解社区主导的 Go 项目，我们可以参考一些成功的案例，例如：
 etcd: etcd 是一个分布式键值存储系统，被广泛应用于 Kubernetes、CoreDNS 等项目中，它完全由社区维护和开发。 Hugo: Hugo 是一个快速、灵活的静态网站生成器，拥有庞大的用户群体和活跃的社区，它也是一个完全由社区驱动的项目。 Traefik: Traefik 是一个现代化的反向代理和负载均衡器，它支持 Docker、Kubernetes 等容器平台，并且拥有活跃的社区支持。  总结 总而言之，尽管 Google 对 Golang 的支持对语言的发展至关重要，但即使 Google 不再支持，Golang 也不会因此消亡。 庞大的社区、成熟的生态以及语言本身的优势，都将推动 Golang 继续向前发展。
]]></content>
  </entry>
  
  <entry>
    <title>RISC-V向量处理器设计概述</title>
    <url>/post/soc/risc-v-vector-processor-design-overview.html</url>
    <categories><category>SOC</category>
    </categories>
    <tags>
      <tag>RISC-V</tag>
    </tags>
    <content type="html"><![CDATA[RISC-V Vector扩展（以下简称RVV）定义了RISC-V的向量指令集。本文基于RVV 1.0标准，介绍向量处理器设计时需要注意的一些问题。
注：本文假定您已阅读过RVV 1.0标准。
Configuration-setting 指令 Vsetvli（以及vsetivli/vsetvl）指令用来设定接下来的向量指令的vl和vtype值。这里vl和vtype都是向量CSR值，因此vsetvli指令会进行写CSR的操作。一般来说，vsetvli指令在标量核里执行，因为它不访问向量寄存器。
对于顺序核来说，vsetvli先更新CSR，之后的向量指令再读取该CSR，没有任何问题。典型的顺序处理器的设计为：向量指令经过标量核的所有流水级后，再分发给向量核。这种架构下，vsetvli会在某一流水级更新vl和vtype CSR，这样之后的向量指令在同一流水级读取更新后的vl和vtype，然后被分发给向量核。这种顺序处理器的架构保证了每个向量指令都能读到正确的vl和vtype (但Fault-Only-First load除外，它可能会修改vl值)。
对于乱序处理器，情况要复杂一些。Vsetvli和之后的向量指令可能会改变执行顺序。具体来说，向量指令和其对应的vsetvli指令之间存在关于vtype/vl的RAW依赖，向量指令和其后面的vsetvli指令存在关于vtype/vl的WAR依赖。RAW是true dependence，无法消除。但WAR是false dependence，可以通过对vtype/vl的renaming来消除。Vtype/vl的renaming和经典的寄存器renaming的原理是一致的。当然，Vtype/vl的物理寄存器的数量没必要太多，因为vsetvli指令密度一般要比向量指令密度小很多。
EEW和EMUL SEW是vsetvli指令设定的元素位宽参考值，EEW指的是元素的实际位宽。一个向量指令可能包含两个源操作数和一个目的操作数，这三个操作数都有对应的EEW。例如，如果一个setvli指令设定了SEW值，它之后的vadd.vv指令的EEW等于SEW，它之后的widen指令vwadd.vv则包含两个源操作数EEW等于SEW，但目的EEW等于2*SEW。
EMUL和LMUL的关系可参考EEW和SEW的关系。他们之间满足EEW/SEW = EMUL/LMUL。
对于某些运算单元，需要考虑不同的SEW运算尽量复用同一个电路，以减少面积开销。例如乘法指令，一个64x64的乘法单元要能够处理2个32x32，4个16x16，以及8个8x8操作。
Mask/tail Mask/tail的agnostic情况允许写全1或者保持原值。这里，“全1”指的是每一个比特都为1。
对于顺序架构，agnostic保持原值是一种很自然的想法，只需在将运算结果vd写回寄存器时采用masked写回即可。注意，这种masked写回采用字节为基本单位即可，不需要以比特为基本单位。
但对于乱序架构，agnostic写全1是更为普遍的做法。因为乱序架构中的重命名机制会导致原值和新值为不同的物理寄存器。Tail写原值的话，需要读取原值的物理寄存器，这相当于引入了新的RAW依赖。
对于某些指令，其tail不受vta控制。例如integer compare指令，其tail总是采用agnostic。
特别的，除了mask load指令，其他会产生mask result的指令（例如compare）在对于tail的处理上可以认为vl=VLMAX。更进一步，对于mask-logical等操作mask的指令，在tail的处理上可以认为vl=VLEN。详见标准文档。
Vstart 对于运算（非load/store）指令，可以设定vstart始终为0，从而简化设计。
当load/store异常发生时，vstart可能会被重置，并且进入异常处理程序。对于顺序处理器来说，当一条load/store向量指令无法确定是否会发生异常时，后面的指令不能写回寄存器（或者直接等待，不执行）。当异常发生时，需要冲刷流水线。对于乱序处理器，在异常处理进行重定向时修改vstart。
对于vl值，Fault-Only-First load指令可能会修改vl，从而影响之后的指令。对于顺序处理器，当一条Fault-Only-First load指令无法确定是否会修改vl时，后面的向量指令需要等待。对于乱序处理器，一种可能的方案是当vl被修改后，重定向至受影响的指令，更新对应vl后重新执行。
Illegal instruction 对于标准中描述为“reserved”的配置，可以报或者不报illegal instruction异常。如果参考Spike模拟器的话，其对”reserved”的情况基本都报了illegal instruction异常。
]]></content>
  </entry>
  
  <entry>
    <title>SRAM/DRAM单元的简单介绍</title>
    <url>/post/hardware/a-brief-introduction-to-SRAM-DRAM-cells.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>SRAM</tag>
      <tag>DRAM</tag>
    </tags>
    <content type="html"><![CDATA[静态随机存取存储器( 静态RAM或SRAM  )是一种使用双稳态元件存储1比特信息的存储器 (RAM) 。这种类型的内存被用作大多数cache的组件，因为它比其他内存结构(特别是DRAM) 有优越的性能，SRAM比DRAM速度更快，也更昂贵;通常用于CPU cache，而DRAM用于计算机的主存储器。
典型的SRAM单元由六个MOS晶体管组成&ndash;形成两个交叉耦合的反相器(双稳态),两个接入nMOS作为开关，用于在读写操作期间控制双稳态元件的状态。下图显示了一个SRAM单元。
SRAM单元中的每个位都存储在由四个晶体管组成的双稳态元件中，这些晶体管构成两个交叉耦合的反相器。只要持续供电，这种交叉耦合连接可以一直存储单个数据。
SRAM cell还具有两条位线，第一位线 (BL) ,持有存储在cell中的相同值。第二位线 (BL)保持存储在cell中的值反相。
当未选择字线 (WL) (WL=0)时，cell处于待机模式。将字线设置为逻辑高可以访问nMOS晶体管。这将cell与两条位线连接，并允许读取或写入cell。通过拉高WL和检测位线BL和BL处的电压差来读取SRAM cell。同样，通过在位线BL和BL上设置内容并拉高字线来写入SRAM cell。
由于其一直存储信息的能力和SRAM cell的高速，它们被用于在微处理器中实现cache。此外，SRAM的主要优点是它使用与微处理器核心相同的制造工艺，简化了高速cache和CPU寄存器集成到 处理器芯片  上。另一方面，SRAM芯片的主要缺点是密度低和运行功耗高。这些缺点妨碍了主计算机存储器使用SRAM。
SRAM单元不用于构建主存储器，相比之下，DRAM通常使用不同的制程，这对于逻辑电路来说不是最优的工艺，这使得CPU逻辑和DRAM的集成比CPU逻辑和SRAM的集成更加困难。但是DRAM体积更小、更便宜、耗电更少，这使得它们成为实现主存储器的更好选择。
DRAM单元介绍 动态随机存取存储器(DRAM)基本上是用于所有芯片的主存储器。电脑为了在每个芯片上封装更多的存储比特，使用的DRAM cell仅由单个MOS晶体管(T) 和存储电容 (C)组成，如下图所示。
cell中的数据可以通过位线(BL)读取或写入。与SRAM相反，DRAM将其内容存储为电容器C上的电荷。这样，DRAM单元比SRAM单元小得多。晶体管T作为存储电容器和位线之间的开关。字线 (WL)用于开关晶体管T开/关。从DRAM单元读取使电容器放电，从而破坏信息。
即使我们不读取DRAM单元，电荷也会从电容中泄漏，因为cell晶体管并没有完全断开存储电容与位线之间的连接。即使晶体管被关闭，也有微小的电流从电容流到位线并放电电容。所以，充电动作(信息)必须每秒刷新几次。因此得名动态dynamic。
]]></content>
  </entry>
  
  <entry>
    <title>英特尔2025 年工艺路线图</title>
    <url>/post/soc/intel-2025-process-roadmap.html</url>
    <categories><category>SOC</category>
    </categories>
    <tags>
      <tag>Intel</tag>
      <tag>Process</tag>
      <tag>Roadmap</tag>
    </tags>
    <content type="html"><![CDATA[英特尔或在2025年夺回制程技术领先地位。
在英特尔的路线图中，该公司在向新制造工艺过渡方面取得了重大进展。
Intel 7和Intel 4已经完成，Intel 3、20A 和 18A 将在未来几年推出。 Intel 7  是该公司的 10nm 工艺，Intel 4是其 7nm 工艺。这些名称可能会产生误导，但芯片中的纳米测量现在大多是营销术语。
Intel 4 是近期的趋势，用于 Meteor Lake，它主要采用这种工艺制造。然而，它是第一个使用极紫外光刻技术的处理器，可以实现更高的产量和面积缩放，从而提高能效。Intel 3 是 Intel 4 的后续产品，旨在用于数据中心，预计每瓦性能将提高 18%。Intel 20A 将与 Arrow Lake 处理器一起首次亮相，采用 PowerVia 和 RibbonFET 技术，每瓦性能比 Intel 提高 15%。
Intel 18A 是最先进的节点，预计将于 2024 年下半年开始生产，每瓦性能将提升 10%。
英特尔去年在 Raptor Lake Refresh 发布会上推出了 Meteor Lake 笔记本电脑处理器，并再次更新了该公司于 2021 年首次发布的制程节点路线图。在那张路线图中，该公司表示希望在四年内实现五个节点，这是多年来其他公司从未实现过的。英特尔自己的路线图指出，它的目标是在 2025 年实现“工艺领先”。按照英特尔的标准，工艺领先意味着每瓦性能最高。
在笔者分析英特尔的路线图时发现，Lunar Lake 完全没有被涵盖。它不在路线图之内，原因很简单，Lunar Lake 不是采用英特尔的任何工艺生产的。Lunar Lake 由台积电生产，尽管它应该是第一款采用Intel 18A 生产的芯片。Lunar Lake 本质上是 Meteor Lake 的后续产品，混合了台积电 N3B 和台积电 N6。未来，英特尔将重新采用英特尔的制造工艺，但 Lunar Lake 今年已外包给台积电。
英特尔 2025 年前的路线图 在上述路线图中，英特尔已完成向Intel 7和Intel 4的过渡，Intel 3、20A 和 18A 将在未来几年内推出。作为参考，Intel 7是该公司对其 10nm 工艺的命名，Intel 4是其对其 7nm 工艺的命名。这些名称的来源（尽管有人可能会认为它们具有误导性），尽管Intel 7是基于 10nm 工艺制造的，但其晶体管密度与台积电的 7nm 非常相似。Intel 4也是如此，WikiChip 实际上得出的结论是，Intel 4的密度很可能略高于台积电的 5nm N5 工艺。
话虽如此，20A 和 18A 的情况就变得非常有趣了。据说 20A（该公司的 2nm 工艺）是英特尔实现“工艺平价”的阶段，并将在 Arrow Lake 上首次亮相，这也是该公司首次使用 PowerVia 和 RibbonFET，然后 18A 将是 1.8nm，同时使用 PowerVia 和 RibbonFET。有关更详细的细分，请查看下面制作的图表。
英特尔路线图
在平面 MOSFET 时代，纳米测量更为重要，因为它们是客观测量，但转向 3D FinFET 技术已将纳米测量变成了单纯的营销术语。
Intel 7 Intel 7 以前被称为 Intel 10nm Enhanced SuperFin（10 ESF），后来该公司将其更名为 Intel 7，本质上是为了与制造业其他领域的命名惯例保持一致。虽然有人可能会说这是误导，但芯片中的纳米测量目前只不过是一种营销手段，而且这种做法已经持续了很多年。
Intel 7 是英特尔使用深紫外光刻 (DUV) 的最后一项工艺。Intel 7 曾用于生产 Alder Lake、Raptor Lake 以及最近宣布的与 Meteor Lake 一起推出的 Raptor Lake Refresh。然而，Meteor Lake 是在 Intel 4 上生产的。
Raptor Lake Refresh 很可能是Intel 7的最后一款产品，英特尔承诺未来将转向新的工艺节点。由于 Meteor Lake 搭载在Intel 4上，我们不太可能看到任何在此制造节点上运行的新芯片。
Intel 4 Meteor Lake大部分都是基于 Intel 4 制造的。Meteor Lake 新 CPU 的计算机 Tile 是基于 Intel 4 制造的，但图形 Tile 是基于 TSMC N3 制造的。这两个 Tile（以及 SoC Tile 和 I/O Tile）使用英特尔的 Foveros 3D 封装技术集成。
然而，与Intel 4相比，一个重大变化是，它是英特尔首次利用极紫外光刻技术的制造工艺。这可以实现更高的产量和面积缩放，从而最大限度地提高能效。正如英特尔所说，与Intel 7相比，Intel 4的高性能逻辑库面积缩放是Intel 7的两倍。这是该公司的 7nm 工艺，再次类似于业内其他制造厂所称的 5nm 和 4nm 工艺的能力。
到目前为止，Intel 4看起来取得了成功，而 Core Ultra 是英特尔的一大变革……至少在Acer Swift Go 14中是如此。英特尔在这方面的进展将特别有趣，但笔者预计英特尔在 CPU 生产方面可能不再处于劣势。
Intel 3 Intel 3 是 Intel 4 的后续产品，但预计性能功耗比 Intel 4 提升 18%。它拥有更密集的高性能库，但目前仅针对数据中心使用，包括 Sierra Forest 和 Granite Rapids。目前你不会在任何消费级 CPU 中看到这个。笔者对这个节点了解不多，但考虑到它更注重企业，普通消费者不必太在意它。
Intel 20A 英特尔知道，在制造工艺方面，它在某种程度上落后于其他行业，并且它计划在 2024 年下半年推出并生产用于其 Arrow Lake 处理器的 Intel 20A。这也将首次推出该公司的 PowerVia 和 RibbonFET，其中 RibbonFET 只是栅极全场效应晶体管 (GAAFET) 的另一个名称（由英特尔起）。台积电正在将其 2nm N2 节点转向 GAAFET，而三星正在将其 3nm 3GAE 工艺节点转向 GAAFET。
PowerVia 的特别之处在于它允许在整个芯片中进行背面供电，其中信号线和电源线被分离并分别进行优化。使用正面供电（目前业界的标准）时，由于空间原因，存在很大的瓶颈，同时也可能引发电源完整性和信号干扰等问题。PowerVia 将信号线和电源线分开，理论上可以实现更好的供电。
背面供电并不是一个新概念，但多年来它一直是个难题。如果你考虑到 PowerVia 中的晶体管现在处于电源和信号之间的夹层中（晶体管是芯片中最难制造的部分，因为它们最有可能出现缺陷），那么在你已经为其他部分投入资源之后，你正在生产芯片最难的部分。再加上晶体管是 CPU 中产生大部分热量的地方，现在你需要通过一层电源或信号传输来冷却 CPU，你就会明白为什么技术很难做好。
据称，该节点的每瓦性能比Intel 3 提高了 15%。据报道，英特尔第 15 代 Arrow Lake 将采用这一工艺制造，这意味着PC电脑应该在今年首次体验到它。
英特尔18A 英特尔的 18A 是迄今为止最先进的节点，它将于 2024 年下半年开始生产。这将用于生产未来的消费级 Lake CPU 和未来的数据中心 CPU，每瓦性能提升高达 10%。目前还没有太多关于它的细节被分享，它在 RibbonFET 和 PowerVia 上的投入翻了一番。Panther Lake 将以这个工艺节点首次亮相，采用 Cougar Cove P-Cores。
自该节点首次亮相以来，唯一的变化是它最初应该使用高 NA EUV 光刻技术，但情况已不再如此。部分原因是英特尔的 18A 节点推出时间略早于最初预期，该公司将其推迟到 2024 年底而不是 2025 年。由于生产 EUV 光刻机的荷兰公司 ASML 仍在 2025 年推出其首款高 NA 扫描仪 (Twinscan EXE:5200)，这意味着英特尔必须在 2024 年跳过它。顺便说一句，对于任何 EUV，公司都必须求助于 ASML，所以没有其他选择。
英特尔仍有望在 2024 年下半年开始生产 18A。
英特尔的路线图雄心勃勃 现在您了解了英特尔今年和明年的路线图，可以说它绝对是雄心勃勃的。英特尔自己将其宣传为“四年五个节点”，因为他们知道这有多么令人印象深刻。虽然您可能预料到在此过程中可能会出现一些小问题，但自英特尔于 2021 年首次公布该计划以来，唯一的变化是将Intel 18A提前到更早的发布时间。其他一切都保持不变。
此后，该公司宣布将推出 18A-P，随后还将推出英特尔 14A 和 14A-E。其中，P 代表性能改进，E 代表功能扩展。这些都着眼于未来，直到 2027 年，但表明英特尔有宏伟的计划，不仅要赶上，还要主导其余的竞争对手。
英特尔是否会继续保持其渐进式的增加还有待观察，但该公司唯一需要做出的改变是比预期更早推出其最先进的节点，这是一个好兆头。虽然目前尚不清楚英特尔在更先进的工艺方面（尤其是当它达到 RibbonFET 时）是否会成为台积电和三星的强大竞争对手。Meteor Lake 是一个良好的开端，大家都迫不及待地想看看英特尔还有什么准备。
]]></content>
  </entry>
  
  <entry>
    <title>Containerd：下一代Linux容器技术</title>
    <url>/post/linux/containerd-the-next-generation-of-linux-container-technology.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Linux</tag>
      <tag>Containerd</tag>
      <tag>Container</tag>
    </tags>
    <content type="html"><![CDATA[在现代IT基础设施中，容器技术已经成为不可或缺的一部分。它不仅提高了应用的部署效率，还极大地改善了资源利用率。
引言 本文将深入探讨containerd，这个关键的容器运行时组件，帮助你理解其工作原理和如何使用它来管理容器。
容器技术背景 容器是一种轻量级的虚拟化技术，它允许多个独立的应用在同一个操作系统内核上运行。与传统的虚拟机不同，容器共享主机的操作系统内核，从而减少了资源开销。
容器技术的发展历程从初期的LXC（ Linux Containers  ）到后来风靡全球的Docker，再到现在的containerd，每一步都在解决不同的技术难题和应用需求。
什么是containerd Containerd是一个由云原生计算基金会（CNCF）管理的开源容器运行时。最初由Docker公司开发，后移交给CNCF管理。containerd的设计目标是提供一个简单、高效、安全的容器运行时，专注于核心功能，避免了不必要的复杂性。
containerd的特点  轻量级和高效：  Containerd 设计为一个轻量级、高效的容器运行时，只专注于容器的核心管理功能。
它以简洁和高效的方式处理容器生命周期管理、镜像管理和存储管理。
模块化设计：  Containerd 的架构采用模块化设计，各个组件（如 Snapshotter、Content Store 等）可以独立扩展和替换。
这种设计使得 Containerd 可以灵活地适应不同的场景和需求。
标准化和可扩展性：  Containerd 遵循 OCI（Open Container Initiative）标准，确保了容器镜像和运行时的一致性和互操作性。
支持多种插件和扩展机制，如自定义 Snapshotter 和 Runtime。
强大的 API 支持：  Containerd 提供了功能强大的 gRPC API，可以方便地集成到各种系统和工具中。
开发者可以通过这些 API 实现细粒度的容器管理操作。
广泛的社区支持：  Containerd 是一个开源项目，由 Cloud Native Computing Foundation (CNCF) 维护，得到了广泛的社区支持。
它有一个活跃的开发社区，持续推动项目的改进和创新。
与 Kubernetes 的紧密集成：  Containerd 可以作为 Kubernetes 的容器运行时（通过 CRI 插件），与 Kubernetes 紧密集成。
这种集成使得 Containerd 成为许多 Kubernetes 集群的首选运行时。
可靠性和稳定性：  Containerd 经过大规模生产环境验证，具有高度的可靠性和稳定性。
它被广泛应用于各大互联网公司和云服务提供商的生产系统中。
镜像管理和分发：  Containerd 提供了高效的镜像管理和分发机制，支持镜像的拉取、推送、存储和快照管理。
Content Store 组件负责管理和存储容器镜像及其层次结构。
命名空间和隔离：  支持命名空间机制，使得不同用户或应用可以在隔离的环境中运行容器，增强了安全性和资源管理的灵活性。
高性能的文件系统快照：  Containerd 的 Snapshotter 组件提供了高性能的文件系统快照功能，支持多种存储后端，如 OverlayFS、Btrfs 等。
技术细节 Containerd 的架构是高度模块化的，包括以下几个关键组件：
 Client:  Containerd 提供了一个客户端库，可以用来与 Containerd 守护进程进行通信。开发者可以使用这个客户端库来编写管理容器的工具和应用。
gRPC API:  Containerd 通过 gRPC 提供了一套功能丰富的 API，用于容器的生命周期管理、镜像管理和存储管理。
gRPC API 使得 Containerd 能够与其他系统和工具进行无缝集成。
Containerd Daemon (守护进程):  这是 Containerd 的核心守护进程，负责处理所有容器管理的请求。
守护进程在后台运行，管理容器的生命周期、镜像、存储和网络等资源。
Content Store:  Content Store 是一个内容寻址的存储系统，用于存储容器镜像的层（layers）和其他内容。
它支持高效的镜像拉取、推送和存储操作。
Snapshotter:  Snapshotter 是一个模块化的文件系统快照管理器，负责管理容器的文件系统层。
Containerd 支持多种 Snapshotter 实现，如 OverlayFS、Btrfs 和 ZFS 等。
Metadata Store:  Metadata Store 负责存储和管理与容器、镜像相关的元数据。
它确保了容器运行时的状态一致性和数据持久性。
Task and Execution Management:  Containerd 提供了一个任务管理系统，用于管理容器的运行时任务（如启动、停止、删除容器等）。
该系统与底层的容器运行时（如 runc）交互，执行具体的容器操作。
Event System:  Containerd 包含一个事件系统，用于发布和订阅容器生命周期中的各种事件。
开发者可以订阅这些事件，以便在容器状态发生变化时执行相应的操作。
CRI Plugin:  CRI (Container Runtime Interface) 插件是 Kubernetes 与容器运行时进行通信的接口。
Containerd 的 CRI 插件使得它可以作为 Kubernetes 的容器运行时，负责处理 Pod 和容器的生命周期管理。
Namespace:  Namespace 是一个逻辑分区，用于隔离不同用户或应用的容器资源。
不同的命名空间可以独立管理容器和镜像，增强了多租户环境的安全性和资源隔离。
Plugins:  Containerd 的架构支持插件机制，可以通过插件扩展其功能。
常见的插件包括 Snapshotter 插件、Content Store 插件、Runtime 插件等。
这些组件共同构成了 Containerd 的核心架构，提供了一个高效、灵活、可靠的容器运行时环境。
安装和配置 安装 你可以使用包管理工具（如apt或yum）来安装containerd，或者从源码编译。
# 使用 apt 安装 sudo apt update sudo apt install containerd # 使用 yum 安装 sudo yum install containerd Containerd的两种安装方式
https://www.gaitpu.com 配置(TODO) 安装完成后，配置文件通常位于/etc/containerd/config.toml。你可以编辑这个文件来配置containerd的行为。
sudo vim /etc/containerd/config.toml 启动服务 启动并使containerd在系统启动时自动启动。
sudo systemctl start container sudo systemctl enable containerd 入门教程 通过一个简单的示例，引导你如何使用containerd来运行和管理容器。
# 拉取镜像 sudo ctr images pull docker.m.daocloud.io/library/hello-world:latest # 运行容器 sudo ctr run --rm docker.m.daocloud.io/library/hello-world:latest hello-world 高级特性 网络配置：支持多种网络插件，允许自定义网络配置。
存储驱动：提供多种存储驱动，支持不同的存储需求。
镜像管理：高效的镜像传输和存储机制，支持镜像的增量更新。
社区和资源 GitHub 仓库：https://github.com/containerd/containerd
社区论坛：
CNCF Slack : https://communityinviter.com/apps/cloud-native/cncf  
未来展望 Containerd的开发团队正不断引入新的特性和优化。未来的版本可能会引入更高效的存储机制、更灵活的网络配置以及更多的安全特性。
结论 Containerd作为下一代的容器运行时，具有高性能、安全性和灵活性等优点，能够帮助开发者和运维人员更高效地管理容器化应用。
图：containerd的常见使用方式
参考文献 containerd 官方文档：https://containerd.io/ Kubernetes CRI 文档： https://kubernetes.io/docs/concepts/architecture/cri/  
附录：简单操作
# 检查containerd状态 sudo systemctl status containerd # 重启containerd服务 sudo systemctl restart containerd # 列出所有容器 sudo ctr containers list ]]></content>
  </entry>
  
  <entry>
    <title>英特尔宣布裁员15000人，股价暴跌</title>
    <url>/post/news/intel-announces-layoffs-15000-employees-shares-plummet.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Intel</tag>
      <tag>Layoff</tag>
    </tags>
    <content type="html"><![CDATA[英特尔日前公布了2024年第二财季的财报，数据显示，公司在本季度实现营收128亿美元，同比下降1%，而净亏损则达到了16亿美元。与去年同期的15亿美元净利润相比，英特尔在这一财季由盈转亏。此外，公司毛利润为45.47亿美元，同比有所下降。由于英特尔宣布停止派息，导致其股价暴跌了20%。
英特尔CEO帕特·基辛格（Pat Gelsinger）宣布了一项“重大成本削减措施”，计划在2025年前节省100亿美元的成本。此次措施将涉及裁减约15,000个职位，占员工总数的15%。基辛格表示：“这是我不得不分享的痛苦消息，我知道你们读到这些消息会更加难受。这一天对Intel来说，是非常艰难的一天，因为我们正在做出公司历史上一些最重要的变革。”
芯片与工艺的进展 英特尔在其第二季度财报电话会议上分享了芯片和工艺技术的最新进展。英特尔第一代Core Ultra CPU（代号Meteor Lake）产量遇到瓶颈，但其出货量已超过1,500万，预计年底前将达到4,000万。OEM厂商对Meteor Lake芯片的需求旺盛，而英特尔正在努力弥补早期阶段造成的毛利率损失。
接下是Lunar Lake，它是英特尔第二代Core Ultra产品。英特尔表示，Lunar Lake已经于7月实现量产，并将为20多家OEM厂商的80多款Copilot+ PC提供支持。这些PC预计将在本季度开始出货。英特尔还确认Lunar Lake CPU将于9月3日推出，并将在IFA 2024活动前进行发布。
与上一代产品相比，Lunar Lake功耗降低50%，图形性能提升50%，能效提高40%。Lunar Lake主要面向轻薄型笔记本电脑，其后续产品Arrow Lake则定位于台式机平台。英特尔计划在Arrow Lake中采用N3B或20A工艺技术，具体选择将视细分市场而定。
英特尔还提供了关于工艺技术进展的更新。18A PDK 1.0已于上个月发布，并计划于2024年底投入生产。此外，14A和10A节点的开发也在顺利进行中。
英特尔强调，公司在18A工艺节点上已签署了价值150亿美元的承诺交易，并有更多交易正在筹备中。尽管目前的财报表现不尽如人意，英特尔仍然坚定不移地推进下一代工艺和芯片的开发，力争重回“芯片巨头”的地位。
未来产品路线 英特尔计划在Lunar Lake和Arrow Lake之后推出Panther Lake CPU。该产品将使用英特尔18A工艺技术，并计划在2025年上半年开始生产，于2025年下半年推出。Panther Lake将把芯片制造带回美国，计划采用性能卓越且具有成本竞争力的工艺。
在台北国际电脑展上，英特尔宣布Panther Lake CPU已实现，目前正在运行Windows系统，表现“非常健康”。此外，这些芯片将率先采用RibbonFet、PowerVia等新技术以及其他先进封装技术。
在数据中心方面，英特尔宣布其下一代Xeon E-Core CPU系列Clearwater Forest将使用Foveros Direct和其他先进的封装技术。Clearwater Forest将基于Darkmont Core架构，配备多达288个内核。该系列计划于2025年推出，将成为Sierra Forest芯片的后续产品。
英特尔表示，Clearwater Forest也已实现开机，并计划于2025年推出。作为首款采用混合键合技术的英特尔18A服务器产品，Clearwater Forest将进一步巩固英特尔在数据中心领域的竞争力。
展望英特尔的未来 英特尔当前的状况反映了其在战略转型中的阵痛期。公司决定将代工制造业务和芯片设计业务分开，以更好地应对市场竞争和行业发展。
英特尔的这一举措需要大量的资本投入。在美国，英特尔已经在代工制造领域投入了巨额资金用于设备采购和生产设施的建设。这项业务的对标对象是台积电，而这一目标的实现并非一朝一夕之功。代工制造本质上是一项长期的、建设性的投资，需要持续不断的资本投入和技术研发支持。
英特尔的代工制造业务一旦成功，将会形成与台积电两足鼎立的市场格局。这不仅意味着英特尔可以为自家产品代工生产，也能为其他半导体设计公司提供代工服务。随着全球对制造产能需求的增加，英特尔有望在未来获得大量订单。
英特尔能否成功打破台积电在技术上的垄断，是实现其战略目标的关键所在。英特尔一直以来都在工艺技术方面进行积极的研发和创新，试图通过技术突破来提升竞争力。
从消费者的角度来看，市场的竞争格局越多元化，受益的将是整个行业及其消费者。假设英特尔倒了，未来消费市场中的AMD和制造业中的台积电保持一枝独秀，对于消费者来说并非好事。竞争可以激发企业不断创新、提升产品质量并降低成本，这最终会为消费者带来更具吸引力的产品和服务。
尽管过程艰难，但如果英特尔能够在技术上取得突破，并顺利实现代工业务的扩展，其未来的发展前景将非常光明。
]]></content>
  </entry>
  
  <entry>
    <title>嵌入式Linux日志log快速定位方法</title>
    <url>/post/linux/embedded-linux-log-fast-positioning-method.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>linux</tag>
      <tag>embedded</tag>
    </tags>
    <content type="html"><![CDATA[本文向大家介绍以下嵌入式 Linux  日志log快速定位的方法。
针对大量log日志快速定位错误地方 动态查看日志
tail -f catalina.ou 从头打开日志文件
cat catalina.ou 可以使用 &gt;vxworks.txt 输出某个新日志去查看
[root@vxworks]# cat -n catalina.out |grep 717892466 &gt;vxworks.txt tail/head简单命令使用：
[root@vxworks]# tail -n number catalina.out 查询日志尾部最后number行的日志 [root@vxworks]# tail -n +number catalina.out 查询number行之后的所有日志 [root@vxworks]# head -n number catalina.out 查询日志文件中的前number行日志 [root@vxworks]# head -n -number catalina.out 查询日志文件除了最后number行的其他所有日志 第一种方式（根据关键字查找出行号）：
用grep拿到的日志很少，我们需要查看附近的日志。我是这样做的，首先: cat -n test.log | grep“关键词” 得到关键日志的行号
[root@vxworks]# cat -n catalina.out |grep 717892466 13230539 [11:07 17:47:11] INFO nanjiang:Edit Old Article：717892466-2020-11-07 17:47:11 13230593 [11:07 17:47:15] INFO nanjiangSave Article ID IS：717892466 13230595 717892466 article.getDisplayTime()1 = 2020-11-07 16:25:11 13230596 717892466 article.getDisplayTime()2 = 2020-11-07 16:25:11 13230601 [11:07 17:47:15] INFO 南江 10.10.10.39 edit article 717892466 编辑文章 cat -n catalina.out|tail -n +13230539|head -n 10
 tail -n +13230539表示查询13230539行之后的日志 head -n 10则表示在前面的查询结果里再查前10条记录  [root@vxworks]# cat -n catalina.out |tail -n +13230539|head -n 10 13230539 [11:07 17:47:11] INFO nanjiang:Edit Old Article：717892466-2020-11-07 17:47:11 13230540 [11:07 17:47:11] INFO Takes:2 ms class com.tmg.cms.manager.dao.article.impl.ArticleContentDaoImpl getListByArticleId [NzE3ODkyNDY2] [int] 13230541 [11:07 17:47:11] INFO Takes:1 ms class com.tmg.cms.manager.dao.resourceImage.impl.ResourceImageDaoImpl load 13230542 [11:07 17:47:11] INFO Takes:0 ms class com.tmg.cms.manager.dao.resourceImage.impl.ResourceImageDaoImpl load 13230543 [11:07 17:47:11] INFO Takes:1 ms class com.tmg.cms.manager.dao.resourceImage.impl.ResourceImageDaoImpl load 13230544 [11:07 17:47:11] INFO article.getImage3：/uploadImages/2020/312/02/3NXCRK4U3589_2.jpg 13230545 [11:07 17:47:11] INFO Takes:0 ms class com.tmg.cms.manager.dao.resourceImage.impl.ResourceImageDaoImpl load 13230546 [11:07 17:47:11] INFO Takes:2 ms class com.tmg.cms.manager.dao.privilege.impl.UserDaoImpl getUserByid 13230547 [11:07 17:47:11] INFO Takes:57 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl selectSitemapWithoutAudit [MQ==] [int] 13230548 [11:07 17:47:11] INFO Takes:5 ms class com.tmg.cms.manager.dao.forbidword.impl.ForbidwordDaoImpl getForbidwordBysiteid [MjI=] [int] 第二种方式：查看指定时间段内的日志
首先要进行范围时间段内日志查询先查看是否在当前日之内存在，
grep &#39;11:07 18:29:20&#39; catalina.out grep &#39;11:07 18:31:11&#39; catalina.out 时间范围内的查询
sed -n &#39;/11:07 18:29:20/,/11:07 18:31:11/p&#39; catalina.out sed -n &#39;/11:07 18:29:/,/11:07 18:31:/p&#39; catalina.out 第三种方式：查看日志中特定字符的匹配数目
[root@vxworks]# grep &#39;1175109632&#39; catalina.out | wc -l 第四种方式：查询最后number行，并查找关键字“结果”
[root@vxworks]# tail -n 20 catalina.out | grep &#39;INFO Takes:1&#39; [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.config.impl.ConfigInfoDaoImpl load [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [NTkwOTQ5] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MzI0] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MzI3] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MzMw] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MzA5NA==] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [Mzc4Mg==] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [OTM1MA==] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MTE5MDMw] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [NTQ2MzQw] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [NTg2NzYy] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MzYyMjA=] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.configModule.impl.ConfigModuleDaoImpl getPersonMenuList 第五种方式：查询最后number行，并查找关键字“结果”并且对结果进行标红
[root@vxworks]# tail -n 20 catalina.out | grep &#39;INFO Takes:1&#39; --color [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.config.impl.ConfigInfoDaoImpl load [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [NTkwOTQ5] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MzI0] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MzI3] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MzMw] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MzA5NA==] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [Mzc4Mg==] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [OTM1MA==] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MTE5MDMw] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [NTQ2MzQw] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [NTg2NzYy] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MzYyMjA=] [int] 第六种方式：查询最后number行，并查找关键字“结果”并且对结果进行标红，上下扩展两行
[root@vxworks]# tail -n 20 catalina.out | grep &#39;INFO Takes:1&#39; --color -a2 [11:11 22:02:51] INFO Takes:0 ms class com.tmg.cms.manager.dao.article.impl.ArticleContentDaoImpl getArticlePageNum [NzE4MTM2ODky] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.config.impl.ConfigInfoDaoImpl load [com.tmg.cms.manager.model.config.ConfigInfo] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [NTkwOTQ5] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MzI0] [int] [11:11 22:02:51] INFO Takes:0 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MzI1] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MzI3] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MzMw] [int] [11:11 22:02:51] INFO Takes:0 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MzAzNg==] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MzA5NA==] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [Mzc4Mg==] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [OTM1MA==] [int] [11:11 22:02:51] INFO Takes:0 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MTE0MjQ4] [int] [11:11 22:02:51] INFO Takes:0 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MTE4MDc4] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MTE5MDMw] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [NTQ2MzQw] [int] [11:11 22:02:51] INFO Takes:0 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [NTQ3MTIw] [int] [11:11 22:02:51] INFO Takes:0 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [NTY4OTYx] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [NTg2NzYy] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.sitemap.impl.SitemapDaoImpl getSitemapTop [MzYyMjA=] [int] [11:11 22:02:51] INFO Takes:1 ms class com.tmg.cms.manager.dao.configModule.impl.ConfigModuleDaoImpl getPersonMenuList [com.tmg.cms.manager.model.config.ConfigPersonMenu] 第七种方式：分页查看，使用空格翻页(使用more/less)
[root@vxworks]# tail -n 2000 catalina.out | grep &#39;INFO Takes:1&#39; --color -a2 | more [root@vxworks]# tail -n 2000 catalina.out | grep &#39;INFO Takes:1&#39; --color -a2 | less 附加  全屏导航  ctrl + F - 向前移动一屏 ctrl + B - 向后移动一屏 ctrl + D - 向前移动半屏 ctrl + U - 向后移动半屏 单行导航  j - 向前移动一行 k - 向后移动一行 3.其它导航 G - 移动到最后一行 g - 移动到第一行 q / ZZ - 退出 less 命令 ]]></content>
  </entry>
  
  <entry>
    <title>ARM V2处理器微架构介绍</title>
    <url>/post/soc/introduction-to-ARM-V2-processor-microarchitecture.html</url>
    <categories><category>SOC</category>
    </categories>
    <tags>
      <tag>ARM V2</tag>
    </tags>
    <content type="html"><![CDATA[今年V3/N3已经发布，但考虑到没有公布太多的细节，我依据手册在 ARM发布新一代高性能处理器  一文中对微架构有阐述，本文主要简单分析ARM V2的一些微架构内容。
IFU   每个cycle预测两个分支，这个特性在服务器系列中是N2/V2刚有的，实现难度还是挺大的，复杂度相对比较高，需要平衡的东西比较多。
  实现了uOp Cache，对于RISC指令集而言，这个选择不是很常见，尽管在N2/V2系列刚实现这个特性，但实际移动端早在A77就开始采用这个微架构，今年发布的V3/N3放弃了uOp Cache这个设计。可能是考虑到功耗问题，再加上优化了ICache和其它IFU方面的特性提升比较可观，对比下uOp Cache收益没有牺牲的面积和功耗大，所以放弃了。
  增加了TAGE预测器以及BTB的容量，这就属于常规的升级了，基本是一些参数化的升级，更细节的算法优化不是很清楚。
  给间接指令设计了单独的预测器，这里有历史遗留问题，由于N和V系列实际也是A系列演变而来，出于经典的A76微架构，一开始移动端和服务器端区分不明显，所以之前间接预测器和移动端一样都是混合的。而服务器端由于间接指令占比相对较多，移动端有些微架构是确定是间接指令之后再查找非独立IBTB的设计方案（为了节约面积功耗）可能不那么适合服务器端了，即使是解耦设计，也不太容易覆盖住间接指令预测带来的延迟。
  取指队列也从原来的16entry升级到现在的32entry。
  Decode/Rename/Dispatch Decode/Rename/Dispatch细节没有更多的信息，decode宽度提升到6，由于uOp Cache的存在，命中uOp Cache可以低延迟的发出8 uOps。Decode Queueyou由16提升到32，增加了Rename Checkpoint以及优化了Rename Rebuild。
Issue/Execute 增加了2个单周期ALUs，增加Issue Queues，SX/MX从20增到22entries，VX从20增加到28entries等等。
LSU   增加DTLB数量至48entry。
  DCache将PLRU替换算法改为RRIP，ARM常用的替换算法，NRU/PLRU/RRIP，L1 Cache使用PLRU更多，更重视L1 Cache的时候会牺牲更多资源在替换算法上。现在论文常讲的更“细粒度”的替换算法，在实际工程中见的更频繁了。例如初始化区分历史，将数据或者指令视作不等价等。简单讲，有一种观点是不全部强调命中率，更强调整体的性能，举个简单的例子，有些数据不命中，对其miss系统损失的代价更高，即使依据频繁访问原则“它”应该被踢掉，但由于“它”地位更高，所以不将“它”替换掉。或者有观点，识别数据本身的特性以及访问频率等情况综合去考量替换问题，这无疑会消耗更多的资源，对于路数更多的L2可能使用类似“细粒度”的替换算法收益更高。但现在ARM L1 Cache也开始逐步使用相对复杂的替换算法。其它就是一些常规的参数级别的升级，例如2LS，1LD，一些buffer深度给出了升级。
  L2 8路，2MB，和1MB的延迟一致（比较前版本），替换算法使用6-state RRIP。单个bank每2cycle读或写64B，共计4bank。
总结  arm  的微架构给我的感觉是细节特别多，很多微小的特性都会抓取去优化，这是国内很多公司不具备的，国际一线的CPU公司，微架构方向的优化每年提升都放缓了，更多的是面向特定场景的优化，反而是工艺的提升以及SoC系统级微架构的提升对芯片系统的影响更大了。当然国内对CPU微架构的设计依然相对落后一些，即使在“参数上”追上了国际水平，并且抛开一些生态问题，实际“面积”“功耗”以及常规情况下的性能依旧有不少的提升空间（国内有些CPU性能出于宣传的角度，不少是在特定情况下测试的），当然以上的总结只是亦安个人的观点，很多是基于自己的感觉，大家见仁见智。
]]></content>
  </entry>
  
  <entry>
    <title>cache的物理结构</title>
    <url>/post/soc/the-physical-structure-of-the-cache.html</url>
    <categories><category>SOC</category>
    </categories>
    <tags>
      <tag>ARM</tag>
      <tag>Cache</tag>
    </tags>
    <content type="html"><![CDATA[教材上通常介绍的cache就是直接映射，组相联和全相联。
如下图所示
那么cache真实的物理层面物理结构是什么样的呢？
bank cache是由SRAM实现的，通过wordline选中，从bitline输出。
L3 cache可能需要在同一个时钟周期处理多个请求，于是需要增加多个读写端口。如果只是增加wordline和bitline，如下图所示，
那么SRAM的面积会随着端口的二次方增长。
随着SRAM面积的增加，SRAM的延迟也会增加，big and slow。
为了支持对cache的同时多个读请求，我们可以interleaved的形式将cache切分成多个bank。
注意上图中的index+bank替代了传统的index，在物理地址上连续的block地址，索引不同的bank。
这样如果cpu同时对连续的地址发出请求，就可以同时向不同的bank发出请求，返回响应。
timing 决定cache的延迟的因素：
 senseamp 和 comparator 延迟是常量，不随cache size 变化 driver delay 随着cache的增加成比例增加 decoder的延迟包括了将address route到center predecoder，将precoder route到 final stage decoder的路由延迟和逻辑延迟。decoder的延迟取决于cache size和sram subarray的数量。 wordline delay和bitline delay，sram subarray，wordline和bitline delay越大。  subarray-size越小，那么wordline&amp;bitline delay越小。但是因为此时subarray的number就要增加，于是route的delay也就增加。
在cache大小相同的情况下，也就是切的bank越多，访问bank内部的时间越少，但是花在路由上的时间越多，如上图所示。
subbank 如果同时有4个地址请求，那么在请求不发生bank conflict的条件下，可以同时访问4个不同的bank。同一个bank不允许同时处理多个请求。
如上图所示，bank内部分为subbank，subbank内包含多个mat。
一个mat包括4个subarray，subarray之间共享central predecoder。
如果需要响应cacheline 64byte的请求，那么是subbank内的所有mat同时响应，每个mat负责处理一部分。如下图所示，红色虚线方框即为一个mat，每个mat处理16byte，4个mat处理一个64byte。
mat中选中的那个subarray返回128bit。如下图所示：
nuca 随着cache size的增加，因为走线延迟的增加，越靠近读写端口的SRAM的延迟越小，越靠近远端的SRAM延迟越大。
如上图d）所示，最靠近读端口的sram只需要9个cycle，最远的则需要32个cycle。
不同的sram bank延迟不同的架构被称为non-uniform cache structure。
上图是一个4路组相联cache，一共32个bank。有三种映射方式：
simple mapping，简单，缺点是不同的set的延迟不同，越靠近读端口的那个set的延迟越小。同时，同一个set内，越靠近读端口的那一way的延迟越小。
fair mapping，不同的set的平均延迟相同
shared mapping，将最靠近读端口的sram平均分配给各个set。
文中插图出处：
[1] An Optimized 3D-Stacked Memory Architecture by Exploiting Excessive, High-Density TSV Bandwidth [2] CACTI 6.0: A Tool to Understand Large Caches [3] CACTI 3.0: An Integrated Cache Timing, Power, and Area Model [4] Flexicache: Highly Reliable and Low Power Cache [5] Best Memory ArchitectureExploration under Parameters Variations accelerated with Machine Learning [6] A Shared Polyhedral Cache for 3D Wide-I/O Multi-core Computing Platforms [7] 4-Port Unified Data/Instruction Cache Design with Distributed Crossbar and Interleaved Cache-Line Words [8] Unified Data/Instruction Cache with Bank-Based Multi-Port Architecture [9] The Effect of Interconnect Design on the Performance of Large L2 Caches [10] An Adaptive, Non-Uniform Cache Structure for Wire-Delay Dominated On-Chip Caches
]]></content>
  </entry>
  
  <entry>
    <title>数据中心高效液冷技术研究现状</title>
    <url>/post/datacenter/research-status-of-efficient-liquid-cooling-technology-in-data-centers.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Liquid Cooling</tag>
    </tags>
    <content type="html"><![CDATA[摘要：随着我国信息技术的发展和互联网络的普及，计算机已经成为了人们生活、工作不可或缺的重要工具，在应用计算机和互联网的过程中，会产生大量的信息及数据，要实现对海量信息数据的存储、整合、保管与利用，就需要进行数据中心机房的建设。随着科技的飞速发展，我国正在迈向数字化时代，这一时期的背景使得 数据中心  的运维管理工作取得了长足的进步。数据技术的广泛应用，为各行各业的生产和管理提供了极大的便利。在这种新的信息技术发展趋势下，如何充分利用数据中心高效液冷技术的优势，制定出适当的管理策略，将会对数据中心的未来发展起到至关重要的作用。
关键词：数据中心；高效液冷技术；研究现状
数字化浪潮正在席卷着千行百业。数据中心是数字经济时代的核心基础设施和国家战略资源。作为“新基建”的代表产业，数据中心为人工智能、移动互联网、物联网等产业的快速发展提供了重要支撑，是我国数字经济发展的重要引擎。数据中心的能耗主要发生在服务器散热和电力传输两个环节，在液冷方面，一方面，需要在保证相同处理能力的前提下，提高服务器等主要设备的电源效率，降低设备功耗；另一方面，还需要提高液冷效率，从而降低相关辅助液冷系统的功耗，在电力传输的前后环节，可以优化供配电系统设计，辅以新型节能变配电系统等。高效液冷技术的加持可以让数据中心提供更好的运行策略来降低能耗，成为节能技术的发展方向之一。基于此，本文针对数据中心高效液冷技术的应用进行重点分析。
数据中心总体架构设计 随着社会的快速发展，许多数据中心客户的方向在这个阶段发生了变化，全球化趋势成为影响客户选择的关键因素。许多国家过去一直处于孤立状态，但随着进入应对全球气候变化的关键时期，这种情况必须改变。能源、成本效率、新技术以及社区参与是至关重要的，通过寻找余热回收利用提供平衡的服务来实现能源转型。
随着越来越多的电动汽车、5G、人工智能和机器学习等的使用量大幅增长，对数据的需求也在持续上升。数据中心行业的增长也必须是可持续的，并对设施所在的社区产生积极影响，这绝对是我们现在要更加关注的事情。虽然数据中心的建设持续刷新历史最高水平，但它们也面临着多种批评。一方面，它们消耗大量电力和水资源；另一方面，它们的运营会排放温室气体，产生大量废弃物，往往需要填埋处理。在生产和日常生活中，无论是视频交流、网上购物、网上社交、沉浸式视频会议、工业流程和生产控制等，都离不开数据中心的输出。数字世界需要大量的处理和存储能力，这是数据中心指数级增长的驱动力[1]。
如何提高数据中心的能源效率，减少其对环境的影响，解决电力供应带来的挑战，已成为当前备受关注的话题。作为数据中心等关键应用领域基础设施建设和数字化服务的全球领导者，施耐德电气每年年初都会基于深刻的行业洞察和实践，发布《看得见的未来——数据中心市场的新趋势与新突破》，揭示新的一年数据中心行业将会发生哪些变化，如何影响未来的发展方向，以及这些变化和趋势对数据中心运营商产生的价值和意义。
现代数据中心需要在云计算和大数据技术的基础上，覆盖基础设施、数据资源、云计算平台、业务应用、运维管理、安全防护等不同的功能属性。大数据和云计算平台主要是对各个系统形成的数据进行汇总，并通过过滤、多维分析等方式进行处理。在此基础上，可以对平台上的数据进行深入地分析和挖掘，获得最终可用的数据。门户应用程序（如“Manage Cockpit”）可用于向最终用户显示最终可用数据。业务应用系统需要实现信息的交换和共享，这就要求数据中心使用统一的服务平台提供相应的信息服务。此外，数据中心还可以对相应的资源进行综合监控和管理，可以采用多样化的方式（如二维拓扑、三维仿真、动态环境系统等）实现综合管理服务。安全保护部分是保证整个系统安全的基础，可以保证系统的访问、连接和主机的安全。此外，基于大数据和云计算技术的数据中心是相对开放的平台，允许用户轻松扩展和开发内容，从而形成更多样化的服务和功能，并能够灵活配置各种资源（如计算资源、网络资源、存储资源等）。
数据中心高效液冷技术应用意义 高效液冷技术并不是一项新技术，从液冷历史来看，海外企业更具先发优势。早在1964年，IBM公司就研发出了世界上首款冷冻水冷却计算机System360，开创了液冷计算机先河。此后高效液冷技术突破性进展不大，直到2008年，IBM重回液冷，发布了液冷超级计算机Power575。2009年， Intel  推出了矿物油浸没散热系统。此时，全球气候变暖的环境下，节能减排形势日益严重。如“南水北调”一般，“东数西算”是一项巨大的工程，不能一蹴而就。工程启动至今时隔两年，一些问题也逐渐凸显。例如，大量数据中心服务器的CPU平均利用率仅为5%至10%，多数时间处于低效的“空转”状态；一些地方甚至尚未理清数据中心原理，就盲目扎堆上项目等一系列问题已经显现，加剧了数据中心的能耗高、碳排放高的困境[2]。
信息技术的快速发展，数据中心需要承担更多的处理任务，其算力和功率都在不断提高，这直接导致了支撑IT设备运行的能耗的增加，除了运行设备所需的电能之外，数据中心还需要大量的电力来散热。IT设备会将99%以上的电能转化为热能，如果这些热量不能高效降温，将对服务器产生巨大的负面影响，液冷散热为行业发展的必然趋势。
液冷技术在国内的发展比国际晚了仅50年。这也与中国互联网起步和发展晚于国际密切相关。中科曙光从2011年率先开始了服务器高效液冷技术的探索与研究，并于2013年完成了首台冷板式液冷服务器原理机和首台浸没式液冷原理验证。随后，华为、浪潮信息、联想、阿里巴巴等一众国内企业先后实现了液冷服务器大规模商业应用项目的探索与落地。实际上，高效液冷技术虽然早已诞生，但由于传统服务器对使用寿命和安全性要求高，一直未曾实现大面积的应用。高效液冷技术节能减排特点显著，与传统风冷技术相比，可降低数据中心约30%-50%的能耗。此外，高效液冷技术还具有高散热密度、低噪音和占地面积小等优点。在总结高效液冷技术应用策略之前，需要对数据中心进行分类。数据中心依据自身功能、规模、负载类型等因素可以分为不同的类型，如智能化数据中心、高性能计算数据中心、云计算数据中心等。每一种类型的数据中心在使用高效液冷技术过程中都会面临不同的问题和需求。因此，需要针对不同类型的数据中心制定不同的应用策略[3]。
数据中心高效液冷技术分析 浸没式液冷技术 浸没式液体冷却技术将服务器浸入特殊的矿物油或氟碳化合物中进行冷却。该系统对矿物油和碳氟化合物的化学成分、密封和净化条件要求较高。一般单机功率超过12kW的机房建议采用液冷方式。与风冷相比，液冷可以有更高的效率。但所有液冷服务器都需要定制，初期投资高，对建筑承载能力有一定要求，后期维护投资大。例如华为外挂式水冷板式液冷服务器，需要使用华为公司专门提供的蒸馏水。其实，这也反映了液冷技术的一些弊端，与传统的风冷服务器相比，专用的液冷服务器会给机房日常的运维工作带来一定的困难。
冷板式液冷技术 冷板液冷是一种紧密耦合（间接接触）的液冷技术，主要通过冷板、支管和冷却分配装置与设备的更近距离接触。冷板相当于冷却系统中的“皮肤”。对于高密度散热的服务器，服务器内部的散热空间非常小，这意味着冷板需要离服务器内部的发热部件更近，才能实现高效地换热。支管负责输送分布在服务器两端的冷却介质，并与冷板相连，相当于人体内连接心脏和人体组织的“血管”。因此，冷却分配装置相当于冷却系统中的“心脏”，保证冷却介质在冷却系统中的循环效率。理想情况下，液冷技术在精确制冷过程中PUE可达1.01-1.20，显著降低数据中心的能耗。目前，行业内广泛使用的液冷技术多为间接液冷（板式），也有采用直接液冷方式，如阿里采用的单向浸入式液冷技术。由此可见，在可预见的未来，液冷技术将在数据中心制冷系统中占据重要地位。
数据中心高效液冷技术应用策略 优化数据中心的布局和空间利用 智能化数据中心作为目前最常见的数据中心类型，其规模庞大、负载密集，并且对能耗和性能要求较高。针对智能化数据中心的特点，高效液冷技术应用策略应包括以下三个方面。
第一，规模扩大的策略。随着智能化数据中心规模的不断扩大，高效液冷技术的应用在规模扩大方面将发挥重要的作用。包括采用更大规模的液冷技术、优化数据中心的布局和空间利用率等。 第二，能效提升的策略。智能化数据中心对能耗要求较高，因此高效液冷技术的应用策略应包括提高能效的措施，如采用高效的液冷技术、优化能源利用等。 第三，智能化管理的策略。随着智能化数据中心规模的不断扩大，其管理和维护成本也会逐渐增加。高效液冷技术的应用策略应该包括智能化管理的措施，如采用智能化的液冷控制系统、实时监测和管理等。
利用高效液冷技术提升散热效率 随着智能化和大数据时代的到来，数据中心规模不断扩大，对能源消耗和散热管理提出了更高的要求。传统的空气冷却系统已经无法满足大规模数据中心的散热需求。因此，高效液冷技术在解决数据中心规模扩大带来的散热问题方面具有巨大的潜力。高效液冷技术可以通过直接将冷却介质（如水）接触到热源，以更高的散热效率来降低数据中心的温度。云计算数据中心由于其分布式的特点，所处环境复杂多变，对高效液冷技术的应用提出了不同的需求。针对云计算数据中心的特点，高效液冷技术应用策略应包括三个方面。
第一，灵活性和可扩展性的策略。云计算数据中心由于分布较广，所处环境差异较大，要求高效液冷技术具有灵活性和可扩展性。策略可以包括采用模块化的液冷技术、可调控的液冷设备和系统等。 第二，适应环境变化的策略。云计算数据中心所处环境复杂多变，高效液冷技术的应用策略应包括适应环境变化的措施，如应用适应不同环境温度和湿度的液冷技术、优化冷却介质选择等。 第三，能耗控制的策略。云计算数据中心由于分布较广，能源和供电成本相对较高。因此，高效液冷技术应用策略应包括能耗控制的措施，如采用能效较高的液冷技术、优化能源利用等。
加强数据中心液冷安全建设 液冷安全是数据中心机房安全建设的重要保障。液冷安全主要是指机房内部物理环境，具体包括机房内的各类消防、减灾、供电等的设施。首先，从供电设施上来看，电力是机房安全和稳定运行的保障，要提升数据中心机房的安全性能，就要尽可能将外部的风险问题发生几率降到最低，优化机房内的供电线路设计，可以引入双路供电系统，采用双UPS，一旦机房的电力负荷过载，出现断电等问题，UPS至少可以维持两小时的供电，这样就为数据备份等提供了时间。其次，从数据中心机房的消防减灾系统设置上来看，为避免自然灾害或者消防事故对机房信息系统及数据的破坏，机房内必要配置高性能的防雷、抗震以及环境监测设备，同时设计火灾自动报警系统，及时发现安全隐患。再次，还要注意数据中心机房的环境安全，将机房的温度和湿度都控制在科学的范围之内，建立专门的通风和除湿系统，以免由于环境湿度过大影响数据中心的安全运行。最后，数据中心机房的液冷安全还要从机房进出这方面加以控制，尤其是有关人员进入机房的核心区域，必须有严格的登记审批程序，控制进出时间，保留监控记录，降低机房人为安全问题发生。
增强数据中心信息安全建设 信息安全是数据中心机房安全建设的核心，信息安全主要是指通过采用一定的技术手段，保证数据中心存储、处理和传输的信息的保密性、完整性和真实性。在数据中心的信息安全建设过程中，数据中心应该遵循一定的原则。不同的信息数据具有不同的安全保护级别。通过对信息的组织和分类，建立分层次的信息安全体系，实现分层次的信息安全管理。它是一个技术原则，要实现信息安全建设，必须依靠安全防御技术的支撑。通过技术架构有效识别和防范信息风险，构建信息安全技术体系，提出相对应的技术措施。一是身份识别与认证，这需要对被访问的主体和客体进行必要的身份识别；二是访问管理，即对整个访问过程进行加密和监控，通过防止恶意代码等技术手段保证访问过程的安全性；三是进行完整的访问记录和审计跟踪，一旦出现访问安全问题，可以通过访问记录来识别安全问题和安全隐患；四是及时对机房内的信息数据进行备份和处理，提高整体保护效果。数据中心的硬件配置需求较高，每台分布式服务器要保证CPU为双路24核Intel 处理器以上，内存至少在512GB 。配置2块480GB SSD硬盘或2块600GB10GKSAS 硬盘，采用RAID1 格式，确保数据中心运行的稳定性。
结语 随着国家节能减排政策越来越严格，以及5G、大数据、人工智能、云服务应用所需的资源越来越多，保障数据中心运营效率和节能工作成为重点难题，通过优化数据中心布局、建立高效液冷手段等措施，保证在数据中心全生命周期内做到能耗控制和管理，为数据中心在数字经济中发挥的作用做好保障。 参考文献：
[1] 杨辰，李俊山. 基于数据中心高效液冷技术分析[J].信息技术与标准化，2023，（12）：55-58. [2] 庄泽岩，孙聪，张琦，等. 数据中心高效液冷技术与应用现状分析[J]. 通信管理与技术，2023，（03）：27-30. [3] 冯帅，王国岩，何嘉俊，等. 浸没式交换机液冷技术仿真与实验[J]. 制冷学报，2023，42（03）：135-144.
]]></content>
  </entry>
  
  <entry>
    <title>英特尔G-Flow可满足千瓦芯片的散热需求</title>
    <url>/post/datacenter/intel-G-Flow-can-meet-the-cooling-needs-of-kilowatt-chips.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Intel</tag>
    </tags>
    <content type="html"><![CDATA[由于受 生成式AI  技术的影响，数据中心的算力需求呈现爆发式增长，随之而来的是功耗和散热的巨大挑战。
为应对这一挑战，英特尔推出了G-Flow浸没式液冷技术，在几乎没有新增成本的情况下。
G-Flow将散热能力最多提高两倍，满足千瓦级功耗芯片的散热需求，为数据中心的散热难题提供了一个突破性解决方案。
从风冷转向液冷是大势所趋 当前，数据中心散热从风冷向液冷转变是一个明显的行业趋势。这不仅仅是因为液冷在降低数据中心PUE（电源使用效率）方面更有效，还因为从服务器运行的角度来看，液冷相比风冷在高效散热方面更具优势。
风冷可以提高风扇转速来提高散热效果，然而，这并非可持续发展。首先，提高转速带来的能耗增加是非线性的，而且，随着风速的提高，散热收益不断收窄。同时，风扇转速提高增加的噪音和震动，还会影响服务器的可靠性。
相比之下，液冷散热通过直接接触散热片的液体带走热量，通常具备更好的散热效果，能够压制更大的散热功耗。同时，液冷的方案不仅省去了风扇的噪音和震动，还能让芯片稳定维持在较低的温度区间，避免了瞬时温度升高，从而降低故障率，延长设备的使用寿命。
现阶段，第五代英特尔至强的TDP已经来到了350瓦，风扇需要在较高的转速区间才能满足这一级别处理器的散热需求。未来，随着更高TDP处理器的出现，而且考虑到还有包括GPU在内的各种AI加速器，单台服务器的功耗只会越来越高，由此可见数据中心急需由风冷向液冷的转化。
同时，绿色云图CEO 胡世轩还提到了业内对液冷市场的预测报告，报告指出，2023年是液冷的技术验证阶段，而后新建数据中心会有较大比例会采用液冷技术。
总之，在可见的未来，数据中心从风冷转向液冷是大势所趋。
常见的数据中心液冷技术方案 简单了解一下液冷。数据中心液冷技术主要分为浸没式和冷板式，每种又包含单相和双相两种。
先来看浸没式液冷。单相浸没式液冷可以使用合成油或氟碳化合物液体。其中，氟碳化合物散热效果更好，但易挥发的特性容易对环境造成影响，而双相浸没式液冷目前只能使用氟碳化合物。
冷板式液冷方案中，单相方案以水为导热介质，散热效果好且环境友好，但存在液体泄漏的风险。双相冷板方案也用了氟碳化合物，虽然散热能力更出色，但技术更新颖、更复杂，产业链还有待完善。
英特尔作为行业产业链纽带，素来与生态保持着密切沟通，如今也正以实际行动推动着液冷技术的落地和发展。在技术选型上，英特尔支持环境保护，希望减少或是禁止使用含氟化合物的方案。
埃克森美孚不仅是一家石油和天然气公司，也能提供散热用的冷却液。埃克森美孚产品解决方案业务部门亚太区产品总监王欣指出，合成油冷却液在安全性、性能和成本效益方面有显著优势，而氟化液有不易降解的有害物质，一些地区已经限制甚至禁止使用氟化液，所以，合成油冷却液是更好的选择。
考虑到环保需求，含有氟化液的双相散热方案，就远不如单相冷板和单相浸没式方案更具优势。如果同时又考虑到安全性，想要避免漏液风险，最好的选择就是采用合成油的单相浸没式液冷方案。
绿色云图CEO 胡世轩介绍称，2023年6月，三大运营商发布的《电信运营商液冷技术白皮书》明确了液冷技术的范围，只纳入单相浸没和冷板技术，排除了喷淋和相变等其他液冷技术。
事实上，单相浸没式液冷技术具备很多优势。最主要的是，它的技术相对简单，即将服务器完全浸没在冷却液中，服务器工作时产生的热量通过泵驱动冷却液循环，将热量带走，从而实现散热。
提高单相浸没式液冷的散热表现 从实际部署来看，单相浸没式液冷技术因其简单、可靠和成本较低而受到广泛关注和欢迎。然而，这一方案最大的问题是散热能力上限不高，而且，如果单纯通过提高液体流量来增强散热，效果并不明显。
英特尔资深技术专家表示研究后发现，即使提高液体流速，大部分的冷却液并不会流经散热器，无法带走更多热量，导致芯片温度只能降低1-2度。
这也主要归因于流经散热器的液体较少，因此，为提高流经散热器的液体流量，英特尔推出G-Flow浸没式液冷技术。通过在散热器和机柜液体出口之间建立一个连通的管道，利用重力或泵的吸力来加速流体通过散热器。
在这套方案中，冷却液从机柜底部泵入腔体，当冷却液超过散热器表面时，会沿着管道流下出口。这个过程主要依靠重力驱动，简单高效，流经散热片的液体也会增多。
英特尔资深技术专家表示，在液面高度差为1米的情况下，理论上可以将液体流速加速至4.4米/秒。而传统自然对流趋动下，液体流速仅为几毫米/秒，这套方案将流速提高了几百倍。
考虑到流体黏度和流动损失，即使达不到理论数值，也依然可以显著提高流速，提高流经散热器的液体流量，因为所有被泵入的液体都要经过散热器，最终大大提高了散热效率。
为了让这一技术真正快速用起来，英特尔考虑了设计要保持简单，成本要较低，客户实施的风险要低，同时，还要通过模块化设计提高系统的灵活性和适应性。
事实上，这套新的散热方案对服务器、对机柜的改动都非常小，可以在无需引入新技术的前提下完成。它与现有的冷板散热方案有些相似之处，但它只有出水管，没有进水管，也完全不用担心泄露导致短路。
对服务器而言，只需兼容新的散热器，底部支持新的转接板即可。机柜要做的就是增加固定转接板的构造。中间的转接板可根据服务器的具体形状和接口位置定制，确保在更换服务器时无需改动机柜结构。
英特尔资深技术专家表示，未来如果这一技术得到广泛认可，会进一步简化改造过程。
与产业伙伴合作来推广新技术 任何新技术都离不开产业生态各方的支持，为了验证和推广这项新技术，英特尔联合服务器合作伙伴，液冷技术提供商以及液冷产品解决方案供应商做了很多验证工作。
据了解，英特尔已经与新华三和中兴通讯两家服务器厂商合作，对服务器进行内部改造。秉承开放的态度，未来英特尔希望与更多服务器领域的合作伙伴展开合作。
为了确保冷却液的兼容性，英特尔还与埃克森美孚合作开展了一系列实验。埃克森美孚和英特尔在浸没式液冷方面有长期的合作关系，英特尔对其产品的各方面表现都非常有信心，其研发实力可以帮助解决在可靠性、兼容性等方面的问题。
从王欣的介绍中了解到，埃克森美孚有专门的团队负责液冷解决方案，埃克森美孚与包括英特尔在内的芯片厂商、液冷产业链厂商以及超大规模数据中心都有合作。针对浸没式液冷场景，埃克森美孚推出了EMDC系列产品来推动行业实现更高的散热表现。
此外，英特尔还与液冷产品解决方案供应商绿色云图和立讯技术开展了合作，这部分合作伙伴负责设计机柜和冷却分配单元（CDU），期间英特尔也与合作伙伴携手解决了很多在实践中遇到的问题。
在沟通过程中，绿色云图CEO 胡世轩表示看好液冷市场的未来发展，尽管现阶段冷板技术的应用比例略高，但浸没式液冷技术的需求增长速度也非常快。
今年4月，英特尔与生态伙伴合作推出了首台样机。目前，这台样机在实验室里已经顺利运行了两个多月，而且测试结果符合此前预期。
英特尔资深技术专家表示，这一方案可以把基于合成油的单相浸没式液冷的散热能力，其上限提高1倍甚至是2倍，可以满足千瓦级芯片散热需求。
]]></content>
  </entry>
  
  <entry>
    <title>DRAM 灵敏放大器的基本操作</title>
    <url>/post/hardware/basic-operation-of-DRAM-sensitive-plattoa.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>DRAM</tag>
    </tags>
    <content type="html"><![CDATA[DRAM 灵敏放大器是由两个交叉耦合的CMOS反相器组成的简单电路，因此它是一个SRAM单元。
位线(BL) 最初被预充电到Vdd/2。在读取期间，位线的电压变化很小。如果位线的电压高于Vdd/2 (图a)，则n2 nMOS晶体管开始导通，并将预充电线拉低到“0”，这反过来又导致p1 pMOS晶体管导通。在一个小的延迟之后,BL被拉高，并且OUT=1。
另一方面，如果位线的电压低于Vdd/2 (图b) ,(p2)pMOS晶体管开始导通，并将预充电线拉到“1”。这反过来又导致n1 nMOS晶体管导通。在一个小的延迟之后，BL被拉到“0”，并且OUT=0。
因此，交叉连接的反相器产生的反馈放大了BL与预充电输入基准之间的小电压差,直到位线完全处于最低电压或最高电压。
 DRAM   灵敏放大器的主要功能是当晶体管被接通，感应位线上微小电压摆动。放大器的第二个功能是在感应到位线上的电压后，恢复电容的值。
打开晶体管允许存储电容与位线共享其存储的电荷。然而，共享存储电容电荷的过程使该DRAM cell放电，因此DRAM cell中的信息丢失，无法再次读取。但这些信息存储在感应放大器中，因为感应放大器的双稳态电路由两个交叉耦合的反相器组成。因此，只要电源电压存在，它就可以存储信息。因此，在感应之后，使用感应放大器将位值写回存储单元。这种操作称为预充电precharge。
]]></content>
  </entry>
  
  <entry>
    <title>摩尔线程开发MTLink挑战英伟达NVLink技术</title>
    <url>/post/hardware/moore-threads-develop-MTLink-to-challenge-Nvidia-NVLink.html</url>
    <categories><category>AI</category>
    </categories>
    <tags>
      <tag>Moore Thread</tag>
      <tag>MTLink</tag>
      <tag>NVIDIA</tag>
      <tag>NVLINK</tag>
    </tags>
    <content type="html"><![CDATA[在当今科技高速发展的时代， 数据中心  和 人工智能  （AI）运算的需求不断增加，推动了GPU技术的快速进步。英伟达（NVIDIA）作为GPU行业的领军企业，以其强大的硬件和软件解决方案在全球数据中心中占据了主导地位。然而，中国的GPU厂商摩尔线程（Moores Threads）正在迎头赶上，通过自主研发和技术创新，力图在全球市场中争得一席之地。
英伟达的市场领先地位 英伟达在数据中心和AI运算领域的成功，得益于其高效的CUDA架构和NVLink互连技术。这些技术不仅提升了GPU的计算性能，还实现了大规模集群的横向扩展，使得英伟达的解决方案在高性能计算（HPC）和人工智能训练中表现卓越。通过不断优化硬件和软件，英伟达为数据中心提供了强大的计算能力，满足了日益增长的市场需求。
摩尔线程的崛起 面对英伟达的强大竞争，我国的GPU厂商摩尔线程不甘示弱，积极投入研发，以应对市场需求。摩尔线程近期升级了其AI KUAE数据中心服务器，通过集成八个MTT S4000 GPU，并采用MTLink网络技术，实现了类似英伟达NVLink的互连能力。摩尔线程的GPU基于MUSA架构，拥有128个张量核心和48GB的GDDR6内存，带宽高达768GB/s。尽管与英伟达的顶级GPU相比，摩尔线程的产品在性能上仍有差距，但其在大规模集群中的潜力不小。
技术创新与挑战 摩尔线程的技术创新不仅体现在硬件上，还在网络技术上有所突破。MTLink网络技术的推出，使得单个数据中心集群可以连接高达一万颗GPU，极大地提升了计算能力。然而，实际性能表现如何，仍取决于许多因素，包括硬件的优化程度、软件的兼容性以及数据中心的整体架构。
摩尔线程的崛起并非一帆风顺。由于受到美国商务部的制裁，摩尔线程被列入实体清单，无法获得先进的制程技术。然而，这并未阻止摩尔线程在人工智能和数据中心领域的探索。尽管面临技术和市场的双重挑战，摩尔线程依然坚持自主研发，不断推出新的产品和解决方案。
战略合作与市场拓展 为了进一步提升技术水平和市场竞争力，摩尔线程与移动、联通等主要电信商，以及中国能源建设集团、大数据科技有限公司建立了战略合作伙伴关系。通过这些合作，摩尔线程开发了三个新的运算集群，致力于提升我国在人工智能领域的技术能力。
与此同时，摩尔线程近期完成了一轮总额达25亿人民币的融资。这笔资金将用于支持公司的扩张计划和技术研发，帮助摩尔线程在激烈的市场竞争中站稳脚跟。
未来展望 尽管摩尔线程在技术和市场上面临诸多挑战，但其不断创新和坚持自主研发的精神，值得肯定。在全球科技竞争日益激烈的今天，摩尔线程的努力不仅是为了提升自身的市场地位，更是为了推动中国在高性能计算和人工智能领域的技术进步。
未来，摩尔线程需要继续加强与产业链上下游的合作，充分利用自身的技术优势，提升产品的性能和可靠性。此外，摩尔线程还需要积极应对国际市场的竞争，通过不断优化产品和服务，赢得更多客户的认可和信任。
摩尔线程在面对技术和市场的双重挑战时，展现出了顽强的毅力和创新的精神。通过不断的技术突破和市场拓展，摩尔线程有望在未来的全球GPU市场中占据一席之地，为推动人工智能和高性能计算的发展做出更大的贡献。
]]></content>
  </entry>
  
  <entry>
    <title>网络编程：解密 libcurl库</title>
    <url>/post/programming/network-programming-decrypting-the-libcurl-library.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>Network programming</tag>
      <tag>libcurl</tag>
    </tags>
    <content type="html"><![CDATA[在这个信息化的时代，网络编程已经渗透到各行各业，从网页爬虫到远程服务器的交互，网络通信无处不在。对于 C++ 开发者来说，libcurl 无疑是一个强大的工具，它提供了一个简洁而灵活的接口来处理各种网络协议。
什么是 libcurl？ libcurl 是一个开源的、跨平台的网络传输库，支持 HTTP、HTTPS、FTP、SMTP 等多种协议。它不仅可以用于简单的文件下载和上传，还能处理复杂的网络请求、响应解析等操作。libcurl 的设计初衷是为了让开发者能够轻松进行网络通信，而不必关心底层实现细节。
为什么选择 libcurl？ 跨平台支持：libcurl 支持几乎所有主流操作系统，包括 Windows、macOS、Linux 等。你只需编写一次代码，就可以在多个平台上运行。
丰富的协议支持：除了常见的 HTTP 和 HTTPS，libcurl 还支持 FTP、FTPS、SFTP、SMTP 等多种协议，满足不同场景的需求。
高性能：libcurl 采用了高效的底层实现，能够处理高并发的网络请求。
易用性：libcurl 提供了简单易用的 API，开发者可以快速上手，进行各种网络操作。
libcurl 的基本使用 在实际使用 libcurl 进行网络编程时，我们通常会按照以下几个步骤进行：
初始化：在使用 libcurl 之前，需要进行全局初始化。
创建句柄：每次进行网络操作时，需要创建一个 CURL 句柄。
设置选项：通过 curl_easy_setopt 函数设置 URL、请求类型、回调函数等选项。
执行请求：调用 curl_easy_perform 函数执行请求，并获取响应结果。
清理资源：完成网络操作后，释放资源，进行全局清理。
以下是一个简单的示例代码，演示了如何使用 libcurl 发送 HTTP GET 请求并获取响应数据。
#include &lt;iostream&gt;#include &lt;curl/curl.h&gt; // 回调函数，用于处理响应数据 size_t WriteCallback(void* contents, size_t size, size_t nmemb, void* userp) { ((std::string*)userp)-&gt;append((char*)contents, size * nmemb); return size * nmemb; } int main() { CURL* curl; CURLcode res; std::string readBuffer; curl_global_init(CURL_GLOBAL_DEFAULT); curl = curl_easy_init(); if(curl) { curl_easy_setopt(curl, CURLOPT_URL, &#34;http://www.example.com&#34;); curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, WriteCallback); curl_easy_setopt(curl, CURLOPT_WRITEDATA, &amp;readBuffer); res = curl_easy_perform(curl); if(res != CURLE_OK) { fprintf(stderr, &#34;curl_easy_perform() failed: %s\n&#34;, curl_easy_strerror(res)); } else { std::cout &lt;&lt; readBuffer &lt;&lt; std::endl; } curl_easy_cleanup(curl); } curl_global_cleanup(); return 0; } 深入解析 HTTP 响应 在实际项目中，我们不仅需要发送请求，还需要处理服务器返回的响应，例如获取响应码、解析响应头等。libcurl 提供了多种方法来获取这些信息，使得我们可以轻松地进行后续处理。
为了获取 HTTP 响应码和响应头，我们可以设置额外的选项和回调函数。例如，通过 curl_easy_getinfo 函数获取响应码，通过设置 CURLOPT_HEADERFUNCTION 回调函数处理响应头。以下代码展示了如何实现这些功能：
#include &lt;iostream&gt;#include &lt;curl/curl.h&gt; size_t WriteCallback(void* contents, size_t size, size_t nmemb, void* userp) { ((std::string*)userp)-&gt;append((char*)contents, size * nmemb); return size * nmemb; } size_t HeaderCallback(char* buffer, size_t size, size_t nitems, void* userdata) { std::string header(buffer, size * nitems); std::cout &lt;&lt; &#34;Header: &#34; &lt;&lt; header &lt;&lt; std::endl; return size * nitems; } int main() { CURL* curl; CURLcode res; std::string readBuffer; curl_global_init(CURL_GLOBAL_DEFAULT); curl = curl_easy_init(); if(curl) { curl_easy_setopt(curl, CURLOPT_URL, &#34;http://www.example.com&#34;); curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, WriteCallback); curl_easy_setopt(curl, CURLOPT_WRITEDATA, &amp;readBuffer); curl_easy_setopt(curl, CURLOPT_HEADERFUNCTION, HeaderCallback); res = curl_easy_perform(curl); if(res != CURLE_OK) { fprintf(stderr, &#34;curl_easy_perform() failed: %s\n&#34;, curl_easy_strerror(res)); } else { long response_code; curl_easy_getinfo(curl, CURLINFO_RESPONSE_CODE, &amp;response_code); std::cout &lt;&lt; &#34;HTTP Response Code: &#34; &lt;&lt; response_code &lt;&lt; std::endl; std::cout &lt;&lt; &#34;Response Data: &#34; &lt;&lt; readBuffer &lt;&lt; std::endl; } curl_easy_cleanup(curl); } curl_global_cleanup(); return 0; } 处理 POST 请求 除了 GET 请求，POST 请求也是非常常见的。在 POST 请求中，我们需要向服务器发送数据，libcurl 支持多种数据格式，例如 JSON、表单数据等。通过设置 CURLOPT_POSTFIELDS 选项，我们可以轻松发送 POST 数据。
以下代码演示了如何使用 libcurl 发送一个简单的 POST 请求，并处理响应数据：
#include &lt;iostream&gt;#include &lt;curl/curl.h&gt; size_t WriteCallback(void* contents, size_t size, size_t nmemb, void* userp) { ((std::string*)userp)-&gt;append((char*)contents, size * nmemb); return size * nmemb; } int main() { CURL* curl; CURLcode res; std::string readBuffer; curl_global_init(CURL_GLOBAL_DEFAULT); curl = curl_easy_init(); if(curl) { curl_easy_setopt(curl, CURLOPT_URL, &#34;http://www.example.com/post&#34;); curl_easy_setopt(curl, CURLOPT_POST, 1L); curl_easy_setopt(curl, CURLOPT_POSTFIELDS, &#34;name=test&amp;project=curl&#34;); curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, WriteCallback); curl_easy_setopt(curl, CURLOPT_WRITEDATA, &amp;readBuffer); res = curl_easy_perform(curl); if(res != CURLE_OK) { fprintf(stderr, &#34;curl_easy_perform() failed: %s\n&#34;, curl_easy_strerror(res)); } else { std::cout &lt;&lt; readBuffer &lt;&lt; std::endl; } curl_easy_cleanup(curl); } curl_global_cleanup(); return 0; } libcurl 的高级特性 libcurl 提供了许多高级特性，可以满足复杂的网络通信需求。以下是几个常见的高级特性：
SSL/TLS 加密：通过设置 CURLOPT_USE_SSL 和 CURLOPT_CAINFO 选项，可以启用 SSL/TLS 加密通信，确保数据传输的安全性。
处理 Cookie：libcurl 支持 Cookie 的处理，可以通过设置 CURLOPT_COOKIE 和 CURLOPT_COOKIEFILE 选项来管理 Cookie。
自定义请求头：通过设置 CURLOPT_HTTPHEADER 选项，可以添加自定义请求头，例如 User-Agent、Content-Type 等。
多线程支持：libcurl 提供了 easy 和 multi 两种接口，multi 接口支持多线程并发请求，提高了网络操作的效率。
实际案例应用 为了更好地理解 libcurl 的应用场景，我们来看一个实际的案例：通过 libcurl 实现一个简单的网页爬虫。
需求分析 假设我们需要从一个新闻网站抓取最新的新闻标题，并将这些标题保存到本地文件中。这个任务包含以下几个步骤：
发送 HTTP GET 请求获取网页内容。
解析网页内容，提取新闻标题。
将新闻标题保存到本地文件。
实现步骤 发送 HTTP GET 请求：使用 libcurl 发送 GET 请求，获取网页内容。
解析网页内容：使用正则表达式或其他解析工具提取新闻标题。
保存数据：将提取的新闻标题保存到本地文件。
实现代码 #include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;regex&gt;#include &lt;curl/curl.h&gt; size_t WriteCallback(void* contents, size_t size, size_t nmemb, void* userp) { ((std::string*)userp)-&gt;append((char*)contents, size * nmemb); return size * nmemb; } int main() { CURL* curl; CURLcode res; std::string readBuffer; std::ofstream outFile(&#34;news_titles.txt&#34;); curl_global_init(CURL_GLOBAL_DEFAULT); curl = curl_easy_init(); if(curl) { curl_easy_setopt(curl, CURLOPT_URL, &#34;http://www.newswebsite.com&#34;); curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, WriteCallback); curl_easy_setopt(curl, CURLOPT_WRITEDATA, &amp;readBuffer); res = curl_easy_perform(curl); if(res != CURLE_OK) { fprintf(stderr, &#34;curl_easy_perform() failed: %s\n&#34;, curl_easy_strerror(res)); } else { std::regex title_regex(&#34;&lt;h1 class=\&#34;title\&#34;&gt;(.*?)&lt;/h1&gt;&#34;); std::smatch matches; std::string::const_iterator searchStart(readBuffer.cbegin()); while (std::regex_search(searchStart, readBuffer.cend(), matches, title_regex)) { outFile &lt;&lt; matches[1] &lt;&lt; std::endl; searchStart = matches.suffix().first; } } curl_easy_cleanup(curl); } curl_global_cleanup(); outFile.close(); return 0; } 在这个示例中，我们使用 libcurl 获取网页内容，然后通过正则表达式提取新闻标题，并将其保存到本地文件中。这是 libcurl 在实际项目中的一个典型应用场景，展示了其强大的网络请求处理能力。
总结 libcurl 是一个功能强大且灵活的网络通信库，适用于各种网络编程需求。从简单的 HTTP 请求到复杂的多协议支持，libcurl 都能轻松应对。在本文中，我们从 libcurl 的基础使用方法开始，逐步介绍了处理 HTTP 响应、发送 POST 请求和使用高级特性等内容，并通过实际案例展示了 libcurl 的应用场景。libcurl 的世界远不止于此，期待你在实际应用中发现更多的惊喜和可能性！
]]></content>
  </entry>
  
  <entry>
    <title>48.3亿！AMD宣布收购 Silo AI</title>
    <url>/post/hardware/AMD-acquires-Silo-AI.html</url>
    <categories><category>AI</category>
    </categories>
    <tags>
      <tag>AMD</tag>
      <tag>Silo AI</tag>
    </tags>
    <content type="html"><![CDATA[AMD公司宣布以大约6.65亿美元（约合人民币48.3亿元）的现金交易，收购了欧洲最大的私人AI实验室Silo AI。这一举措标志着AMD在 人工智能  领域的进一步拓展，并可能加强其在快速增长的AI硬件市场上与英伟达的竞争地位。
Silo AI总部位于芬兰赫尔辛基，业务遍及欧美，拥有300多名AI科学家与工程师，专注于端到端 AI驱动解决方案，可以帮助客户在其产品、服务和运营中快速集成AI，安联、飞利浦、劳斯莱斯、联合利华等大型企业都是其客户。此次收购将使AMD获得Silo AI的主要产品SiloGen模型平台，以及与AMD有着良好合作的Poro、Viking等先进的开源大语言模型。
AMD表示，Silo AI的加入将有助于推动其在人工智能领域的发展，并与英伟达展开更激烈的竞争。过去一年来，AMD已经连续投资了十几家AI公司，总额超过1.25亿美元，还收购了Mipsology、Nod.ai。预计这笔收购将在2024年下半年完成，届时Silo AI将成为AMD AI集团的一部分，而其首席执行官、联合创始人Peter Sarlin将继续领导团队。
Peter Sarlin在一份声明中表示：“在Silo AI，我们的使命从一开始就是打造一家人工智能旗舰公司。”“今天的声明是我们与AMD联手塑造人工智能计算未来的合乎逻辑的下一步。”
]]></content>
  </entry>
  
  <entry>
    <title>什么是MD5</title>
    <url>/post/programming/introduction-to-message-digest-algorithm-5.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>Programming</tag>
      <tag>MD5</tag>
    </tags>
    <content type="html"><![CDATA[MD5，全称为Message-Digest Algorithm 5，是一种广泛使用的加密哈希函数，可以生成一个128位（16字节）的哈希值（杂凑值），用于确保信息传输完整一致。
MD5是哈希算法中的一种，它能够将任意长度的数据输入通过一系列复杂的运算，生成一个定长的输出——这就是我们常说的哈希值。
MD5是由美国计算机科学家罗纳德·李维斯特（Ronald Rivest）在1991年设计并发布的。这是一种改进和增强后的版本，因为在此之前的MD2、MD3和MD4在安全性和效率上存在一定的问题。MD5的设计初衷是为了提供一种能够快速计算且具有较高安全性的哈希算法。
MD5的正式发布是在1991年，详细的技术描述被发布在一篇名为《The MD5 Message-Digest Algorithm》的论文中，这篇论文详细描述了MD5算法的设计原理和实现细节。MD5在发布后很快被广泛应用于密码学、数据完整性校验和数字签名等领域。
MD5的设计者和发布年份 罗纳德·李维斯特是麻省理工学院（MIT）的一名教授，同时也是RSA公钥加密算法的发明人之一。作为密码学领域的重要人物，他在设计MD5算法时，综合了之前版本的优缺点，并在安全性和计算效率上做出了平衡。
MD5算法在1991年发布，此时互联网刚刚起步，各类网络协议和数据传输方式尚不完善。MD5在此时的发布，为数据传输的完整性和安全性提供了一种有效的解决方案。
MD5的工作原理 哈希函数 哈希函数是一种数学算法，它接收任意长度的数据输入，通过一系列运算生成固定长度的输出，这个输出被称为哈希值（或摘要）。
哈希函数的核心特性包括：
 固定长度输出：无论输入数据的长度如何，哈希值的长度是固定的。 不可逆性：从哈希值无法反推出原始数据。 唯一性：不同的输入数据生成不同的哈希值（理想情况下）。  哈希函数在计算机科学和密码学中有广泛应用，包括数据校验、数据索引、密码加密、数字签名等。哈希函数的效率和安全性直接影响这些应用的可靠性。
MD5算法的步骤 MD5算法的第一步是对输入数据进行填充，使其长度满足特定条件。
填充的规则如下：
 首先在数据末尾添加一个比特1（即0x80）。 然后添加足够多的比特0，使得填充后的数据长度对512取模等于448（即比512的倍数少64位）。 最后，使用64位表示数据的原始长度，并将其附加到填充后的数据末尾。  例如，对于一个长度为448位的数据，填充后的长度为512位；对于一个长度为500位的数据，填充后的长度为1024位。
MD5算法使用四个32位的寄存器（A、B、C、D）作为初始状态。
这四个寄存器被初始化为以下常数：
 A = 0x67452301 B = 0xEFCDAB89 C = 0x98BADCFE D = 0x10325476  填充后的数据被分成多个512位的数据块，每个数据块被进一步分成16个32位的子块。
MD5算法对每个512位的数据块进行处理，具体步骤如下：
主循环：主循环分为四轮，每轮包含16次操作，每次操作使用不同的非线性函数（F、G、H、I）和常数值。 四轮运算： 第一轮：使用非线性函数F，并且每次操作中，寄存器的内容会循环左移。 第二轮：使用非线性函数G，类似地，寄存器的内容会循环左移。 第三轮：使用非线性函数H，进行类似的操作。 第四轮：使用非线性函数I，进行最后一轮操作。
每个数据块处理完后，算法将更新寄存器的值，这些寄存器的值会被用于处理下一个数据块。最终的哈希值由这些寄存器的最终值连接而成。
MD5的内部结构 MD5算法使用四个非线性函数（F、G、H、I）来处理数据。这些函数将当前的寄存器值与数据块进行复杂的混合，以确保哈希值的唯一性和不可逆性。
 函数F：F(B, C, D) = (B &amp; C) | (~B &amp; D) 函数G：G(B, C, D) = (B &amp; D) | (C &amp; ~D) 函数H：H(B, C, D) = B ^ C ^ D 函数I：I(B, C, D) = C ^ (B | ~D)  MD5算法使用一组64个常数值（T[i]），这些常数值是在算法设计时预先计算的，定义为： [ T[i] = \lfloor 2^{32} \times |\sin(i)| \rfloor ] 其中，i取值范围为1到64。
MD5算法的整个数据处理流程可以总结为以下几个步骤：
 初始化：设置寄存器A、B、C、D的初始值。 填充数据：根据规则对输入数据进行填充。 分块处理：将填充后的数据分成多个512位的数据块。 主循环运算：对每个数据块执行四轮非线性函数运算和常数值混合运算。 更新寄存器：更新寄存器的值以处理下一个数据块。 输出哈希值：连接最终寄存器的值，生成128位的MD5哈希值。  MD5的优缺点 MD5的优点 计算速度快 MD5算法的一个显著优点是其计算速度非常快。由于MD5算法的设计较为简洁，数据处理效率高，这使得它在需要快速生成哈希值的场景中非常适用。例如，在文件完整性校验和数据传输过程中，MD5能够快速计算哈希值，从而提高整体效率。
实现简单 MD5算法相对简单且易于实现。这种简洁性使得它可以在多种编程语言和平台上实现，无论是C、C++、Java，还是Python、JavaScript，都有成熟的MD5实现库。开发人员可以方便地在各种应用中集成MD5功能。
广泛应用 由于MD5发布较早，并且在很长一段时间内被认为是安全的，它在众多应用中得到了广泛使用。例如，MD5常用于校验文件完整性、生成数字签名、保护密码等。即使在今天，尽管已知其安全性问题，MD5依然在一些非安全关键的应用场景中被使用。
MD5的缺点 碰撞漏洞 MD5的一个主要缺点是存在碰撞漏洞。碰撞指的是两个不同的输入数据生成相同的哈希值。由于MD5生成的哈希值长度有限（128位），理论上存在无限个不同的输入可以生成相同的哈希值。2004年，研究人员成功发现了实际的MD5碰撞攻击方法，证明MD5无法抵御这种攻击。
预映像攻击 预映像攻击是指给定一个哈希值，找到一个输入数据使其生成这个哈希值。虽然MD5在设计时考虑了抗预映像攻击，但随着计算能力的提升，找到预映像变得更加可行。尤其是在密码学和数字签名等安全关键应用中，这种攻击对MD5的安全性构成了严重威胁。
彩虹表攻击 彩虹表是一种预计算哈希表，通过预先计算并存储大量可能的哈希值及其对应的输入数据，攻击者可以快速查找哈希值的对应输入。由于MD5的哈希值长度较短，生成和存储彩虹表相对容易，使得MD5更易受到彩虹表攻击。
 尽管MD5在早期广泛用于密码加密和数字签名，但随着安全性问题的暴露，它在这些领域逐渐被更安全的哈希算法取代。尤其是在处理敏感信息和安全关键应用中，MD5已不再适用。 虽然MD5常用于数据完整性校验，但其碰撞漏洞意味着它不能完全保证数据的唯一性和安全性。对于需要高安全性的场景，推荐使用更强的哈希算法，如SHA-256或SHA-3。
 MD5的应用 数据完整性验证 在文件下载和传输过程中，确保文件未被篡改和损坏是非常重要的。MD5常用于生成文件的校验码（checksum），用户下载文件后可以计算本地文件的MD5值，并与服务器提供的MD5值进行比对，从而验证文件的完整性。例如，许多开源软件和大文件下载网站会提供文件的MD5校验码供用户验证。
在网络数据传输中，数据包可能会由于各种原因（如网络干扰）而发生错误。通过在传输前后对数据包计算MD5哈希值并进行比对，可以确保数据包在传输过程中未被篡改和损坏。这在数据备份和分布式系统中尤为重要。
数字签名与证书 数字签名是对数字信息进行加密的哈希值，用于验证信息的真实性和完整性。MD5曾广泛用于生成数字签名，因为它可以快速生成定长哈希值，便于签名和验证。然而，随着MD5安全问题的暴露，越来越多的系统开始转向更安全的哈希算法（如SHA-256）。
在公共密钥基础设施（PKI）中，数字证书用于验证公钥的所有权。MD5曾被用于生成证书的哈希值，确保证书的唯一性和完整性。然而，出于安全性考虑，现代PKI系统已逐步弃用MD5，转而使用更安全的哈希算法。
密码加密 在用户密码管理中，存储密码的明文是非常不安全的。通过对密码进行MD5哈希处理，存储哈希值而不是明文，可以增加密码的安全性。即使数据库被泄露，攻击者也难以直接获得明文密码。然而，由于MD5的安全性问题，现代系统通常使用更安全的哈希算法（如bcrypt、scrypt、Argon2）来存储密码。
在用户登录时，系统会对输入的密码进行哈希处理，并将其与数据库中存储的哈希值进行比对。如果哈希值匹配，说明用户输入了正确的密码。这种方式可以避免在传输过程中暴露用户的明文密码。
软件开发中的版本控制 在软件开发过程中，管理和追踪文件的变化是非常重要的。通过对文件生成MD5哈希值，开发者可以快速确定文件是否发生了变化。这对于版本控制系统（如Git）和配置管理工具（如Ansible）非常有用，可以有效管理代码和配置文件的变更。
在大型软件项目中，重复文件会浪费存储空间并增加管理复杂度。通过计算文件的MD5哈希值，可以快速检测和删除重复文件，优化存储空间和资源利用。
MD5的安全性问题 碰撞攻击 碰撞攻击指的是找到两个不同的输入数据，使它们生成相同的哈希值。由于MD5生成的哈希值长度有限，理论上存在无限个不同的输入数据可以生成相同的哈希值。2004年，研究人员成功发现了实际的MD5碰撞攻击方法，这对MD5的安全性构成了严重威胁。
碰撞攻击的存在意味着攻击者可以伪造哈希值相同的不同数据，从而绕过数据完整性校验或数字签名验证。这在安全关键应用中是不可接受的，因为它破坏了哈希函数的唯一性和不可篡改性。
一个实际的例子是2008年，研究人员通过碰撞攻击成功伪造了一个有效的SSL证书，使其看起来是由一个可信的证书颁发机构（CA）签发的。这种攻击证明了MD5在公共密钥基础设施中的安全性不足。
预映像攻击 预映像攻击指的是给定一个哈希值，找到一个输入数据使其生成这个哈希值。尽管MD5在设计时考虑了抗预映像攻击，但随着计算能力的提升，找到预映像变得更加可行。
预映像攻击使得攻击者可以伪造任意数据，使其哈希值与目标哈希值相同。
在密码学应用和数字签名中，预映像攻击会对安全性产生严重影响。例如，如果一个数字签名系统依赖于MD5哈希值来验证数据的完整性和真实性，攻击者可以通过找到与目标哈希值匹配的任意数据，伪造合法的签名。这种攻击可能导致伪造的交易、合同或其他重要文件被接受为真实和合法的。
彩虹表攻击 彩虹表是一种预计算哈希表，包含大量可能的哈希值及其对应的输入数据。通过预先计算并存储这些哈希值，攻击者可以在极短的时间内查找并匹配目标哈希值，找到对应的明文数据。
彩虹表攻击使得破解哈希值变得更加容易和快速，尤其对于MD5这样哈希长度较短的算法。攻击者可以利用彩虹表迅速找到与哈希值对应的原始数据，从而破解存储在数据库中的密码或其他敏感信息。
为了抵御彩虹表攻击，可以使用“加盐”技术。加盐是指在原始数据（如密码）中添加一个随机值（盐），然后再计算哈希值。这样，即使相同的原始数据在不同情况下生成的哈希值也不同，增加了破解的难度。
MD5的过时问题 随着计算能力的提升和密码学研究的深入，MD5的安全性问题逐渐暴露。碰撞攻击和预映像攻击的有效性使得MD5在许多安全关键应用中不再适用。
由于MD5的安全性问题，许多系统和应用已经逐渐转向使用更安全的哈希算法，例如SHA-256和SHA-3。这些算法在设计上更加复杂，哈希值长度更长，抵御攻击的能力更强。
MD5与其他哈希算法的比较 MD5与SHA-1 SHA-1（Secure Hash Algorithm 1）是由美国国家安全局（NSA）设计，并由美国国家标准与技术研究院（NIST）发布的另一种广泛使用的哈希算法。SHA-1生成160位的哈希值，比MD5的128位更长，理论上更安全。
尽管SHA-1在设计上比MD5更安全，但随着计算能力的提升，SHA-1也逐渐暴露出安全性问题。2017年，谷歌和荷兰信息安全研究所（CWI）成功进行了SHA-1的碰撞攻击，进一步证明了其不安全性。
SHA-1曾被广泛应用于数字签名、证书生成和数据完整性验证等领域。然而，随着SHA-1安全性问题的暴露，许多系统和应用已经逐渐转向使用SHA-256和SHA-3等更安全的哈希算法。
MD5与SHA-256 SHA-256（Secure Hash Algorithm 256）是SHA-2家族的一部分，生成256位的哈希值，提供了更高的安全性。SHA-256在设计上更加复杂，哈希值更长，使其在抵御碰撞攻击和预映像攻击方面更为有效。
与MD5和SHA-1相比，SHA-256在安全性上有显著提升。SHA-256在设计上考虑了许多现代密码学攻击方法，迄今为止，尚未发现有效的碰撞攻击或预映像攻击。
SHA-256广泛应用于比特币和其他加密货币、TLS/SSL证书、数字签名和数据完整性验证等领域。由于其高安全性，SHA-256成为许多现代安全协议和系统的首选哈希算法。
MD5与SHA-3 SHA-3（Secure Hash Algorithm 3）是最新的哈希算法标准，由NIST在2015年发布。SHA-3基于Keccak算法，设计上与SHA-2完全不同，提供了更高的安全性和灵活性。
SHA-3在设计上吸取了SHA-1和SHA-2的经验教训，具有更强的抵御碰撞攻击和预映像攻击的能力。SHA-3的内部结构更加复杂，使其在面对现代密码学攻击时表现出色。
SHA-3适用于需要高安全性的场景，如数字签名、密码学协议和数据完整性验证。尽管SHA-3的应用尚未像SHA-2那样广泛，但随着时间推移，越来越多的系统和应用将逐步采用SHA-3。
替代MD5的方案 SHA-256的应用 SHA-256可以用于文件完整性校验，与MD5类似，但提供了更高的安全性。通过计算文件的SHA-256哈希值，用户可以验证文件在传输过程中未被篡改或损坏。例如，许多开源项目和软件发布网站已经开始提供SHA-256校验码供用户验证下载的文件。
SHA-256在数字签名和证书生成中广泛应用。由于其更高的安全性，SHA-256能够有效抵御碰撞攻击和预映像攻击，确保数字签名的真实性和完整性。现代TLS/SSL证书大多使用SHA-256生成哈希值，以提高通信安全性。
HMAC的使用 HMAC（Hash-based Message Authentication Code）是一种基于哈希函数的消息认证码，用于验证数据的完整性和真实性。HMAC结合了哈希函数和一个密钥，通过对消息和密钥进行多次哈希运算，生成一个认证码。
尽管MD5本身存在安全性问题，HMAC-MD5通过引入密钥，提高了抵御攻击的能力。然而，出于安全考虑，HMAC-SHA256逐渐成为更常用的选择。HMAC-SHA256结合了SHA-256的高安全性和HMAC的认证特性，广泛应用于安全通信和数据保护中。
HMAC常用于安全协议（如TLS、IPsec）、API认证、数据完整性校验等场景。通过使用HMAC，系统能够有效验证数据的来源和完整性，防止数据篡改和伪造。
新型哈希算法 Argon2是一种现代密码哈希函数，设计用于密码哈希和密钥派生。Argon2在设计上考虑了高并行性和抗侧信道攻击，提供了强大的安全性。Argon2有三个版本：Argon2d、Argon2i和Argon2id，分别侧重于抗GPU攻击、抗时间攻击和混合攻击。
Bcrypt是一种基于Blowfish加密算法的密码哈希函数，广泛用于密码存储。Bcrypt通过引入盐值和可配置的工作因子（cost factor），增加了哈希计算的复杂性和时间，使其在抵御彩虹表攻击和暴力破解方面非常有效。
写在最后 最后给大家来个总结：
MD5（Message-Digest Algorithm 5）是一种广泛使用的密码散列函数，能够将任意长度的输入数据转换成一个固定长度的128位（16字节）散列值。这个散列值通常表示为32个十六进制数字的形式。
MD5的主要用途是用于数据完整性的校验，例如在网络传输或存储数据时，可以通过计算数据的MD5散列值来检测数据是否被篡改或损坏。然而，由于MD5算法已经被证明存在一些安全漏洞，如碰撞攻击和预映像攻击等，因此它不再被视为一种安全的密码散列函数，不建议在需要高度安全性的场合中使用。在这些情况下，更安全的散列函数，如SHA-256或SHA-3，应该被优先考虑。
]]></content>
  </entry>
  
  <entry>
    <title>二十张图看懂VPN</title>
    <url>/post/linux/twenty-pictures-to-understand-vpn.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>VPN</tag>
    </tags>
    <content type="html"><![CDATA[近年来，随着家庭办公的普及，越来越多的公司发现了远程办公的各种优势，于是乎各大公司纷纷推行远程办公。
在家远程办公的时候，不可避免地需要访问公司内网的数据资源。这个时候就不得不提今天的主角——VPN了。
VPN是个啥 那什么是VPN？我们为什么要通过VPN来访问公司内网呢？
别着急，今天文档君就来跟大家聊一聊VPN是如何为远程办公的信息安全保驾护航的。
在讲VPN之前，我们先简单了解一下公司内网。
所谓公司内网，就是公司内部建立的一个局域网。这个网络是封闭的，公司外面的黑客无法访问这个网络，这样公司内部的资料数据就是安全的。
如果说因特网是一片广阔的天地，那公司内网就是被四面墙围起来的一个小房子，只有处在这个房子里的人才能够使用它的资源。
那在家远程办公，处于因特网中的你，要如何使用房子里的资源，又不泄露房子里的秘密信息呢？
那就要悄咪咪地建立一条只有你知公司知的“秘密隧道”了。
一开始大家想到的是专线，在总部和分部拉条专线，只传输自己的业务，但是这个专线的费用高昂，不是一般公司能够承受的，而且维护也很困难。
那么有没有物廉价美，更具有性价比的方案呢？
有，那就是VPN。
VPN：（Virtual Private Network），学名虚拟专用网络，便可以帮你和公司之间搭建这么一条“秘密隧道”。当你通过VPN访问公司内网时，数据传递全部通过“秘密隧道”悄悄进行，这样就保证了你和公司之间的网络通讯是安全私密的。
但是这条隧道并不是真实存在的，而是通过数据加密技术封装出来的一条虚拟数据通信隧道，实际上它借用的还是互联网上的公共链路。
VPN会对你和公司之间传递的数据进行加密处理，加密后的数据会在一条专用的数据链路上进行安全传输，如同架设了一个专用网络一样。所以VPN称为虚拟专用网络。
VPN咋工作 那VPN具体是如何工作的呢？
当开启VPN后，你访问公司内网的办公网站时，不再直接访问公司内网的服务器，而是去访问VPN服务器，并给VPN服务器发一条指令“我要访问办公网站”。
VPN服务器接到指令后，代替你去访问办公网站，收到公司办公网站的内容后，再通过“秘密隧道”将内容回传给你，这样你就通过VPN成功访问到你需要的内网资源啦。
VPN关键技术 隧道技术 隧道技术其实就是对传输的报文进行封装，利用公网的建立专用的数据传输通道，从而完成数据的安全可靠性传输。
隧道通过隧道协议实现。如GRE（Generic Routing Encapsulation）、L2TP（Layer 2 Tunneling Protocol）等。
身份认证 VPN网关对接入VPN的用户进行身份认证，保证接入的用户都是合法合规用户。
数据加密 将明文通过加密技术成密文，哪怕信息被获取了，也无法识别。
数据验证 通过数据验证技术验证报文的完整性和真伪进行检查，防止数据被篡改。
VPN好在哪 了解了VPN的工作原理和关键技术，是时候总结一下它的优点了。
安全：通过数据加密，VPN可以防止数据在互联网传输中被窃听，被篡改。一般数据在互联网中是明文传输的，而数据加密技术则提高了数据的安全性，降低了公司机密信息在远程通信中被泄露的风险。
成本低：VPN好用不贵！采用VPN可以达到与租用专线相同的效果，但所花费用要比租用专线低40%～60%。
支持移动业务：支持出差时使用，VPN用户可在任何时间、任何地点的移动接入，能够满足不断增长的移动业务需求。
可扩展性：VPN应用灵活，具有良好的可扩展性。如果企业想扩大VPN的容量和覆盖范围，只需改变一些配置，或增加几台设备、扩大服务范围。
VPN分几类 根据VPN建设单位不同进行划分。
 租用运营商VPN专线搭建企业网络  运营商的专线网络大多数都是使用的MPLS VPN，企业通过购买运营商提供的VPN专线服务实现总部和分部间的通信需求。VPN网关为运营商所有。
 企业自建VPN网络  企业自己基于Internet自建vpn网络，常见的如IPsec VPN、GRE VPN、L2TP VPN。
根据工作网络层次进行划分
VPN可以按照工作层次进行划分：
 应用层：SSL VPN 网络层：IPSEC VPN 、GRE VPN 数据链路层：L2TP VPN、PPTP VPN  工作在网络层和数据链路层的VPN又被称为三层VPN和二层VPN。
IPSec VPN IPSec（IP Security） VPN一般部署在企业出口设备之间，通过加密与验证等方式，实现了数据来源验证、数据加密、数据完整性保证和抗重放等功能。
 数据来源验证：接收方验证发送方身份是否合法。 数据加密：发送方对数据进行加密，以密文的形式在Internet上传送，接收方对接收的加密数据进行解密后处理或直接转发。 数据完整性：接收方对接收的数据进行验证，以判定报文是否被篡改。 抗重放：接收方拒绝旧的或重复的数据包，防止恶意用户通过重复发送捕获到的数据包所进行的攻击。  GRE VPN 通用路由封装协议（General Routing Encapsulation，GRE）是一种三层VPN封装技术。
GRE可以对某些网络层协议（如IPX、IPv4、IPv6等）的报文进行封装，使封装后的报文能够在另一种网络中（如IPv4）传输，从而解决了跨越异种网络的报文传输问题。
GRE还具备封装组播报文的能力。由于动态路由协议中会使用组播报文，因此更多时候GRE会在需要传递组播路由数据的场景中被用到，这也是GRE被称为通用路由封装协议的原因。
L2TP VPN L2TP是虚拟私有拨号网VPDN（Virtual Private Dial-up Network）隧道协议的一种，它扩展了点到点协议PPP的应用，是一种在远程办公场景中为出差员工或企业分支远程访问企业内网资源提供接入服务的VPN。
L2TP组网架构中包括LAC（L2TP Access Concentrator，L2TP访问集中器）和LNS（L2TP Network Server，L2TP网络服务器）。
LAC是网络上具有PPP和L2TP协议处理能力的设备。LAC负责和LNS建立L2TP隧道连接。
MPLS VPN MPLS是一种利用标签（Label）进行转发的技术，最初为了提高IP报文转发速率而被提出，现主要应用于VPN和流量工程、QoS等场景。
MPLS VPN网络一般由运营商搭建，VPN用户购买VPN服务来实现用户网络之间的路由传递、数据互通等。
基本的MPLS VPN网络架构由CE（Customer Edge）、PE（Provider Edge）和P（Provider）三部分组成。
各VPN的隧道身份认证、数据加密、验证参见下表。
VPN用何处 以上三大亮点使VPN在企业中广受青睐。针对不同的需求，VPN还能提供更有针对性的应用场景。比如：
 远程接入VPN：用于异地办公的员工访问公司内网。 内联网VPN：将企业总部和外地分公司通过虚拟专用网络连接在一起。 外联网VPN：将一个公司与另一个公司的资源进行连接，与合作伙伴企业网构成外联网。  VPN就像是连接公司内网和外部因特网的一个窗口，扩大了公司内网的边界，使远程办公的你我他也能安全放心地访问公司内网资源。这样的VPN谁能不爱呢！
有VPN为我们的远程办公安全保驾护航，相信未来我们人人都可以实现在家办公呢，到时候边撸猫边办公，想一想就美滋滋！
]]></content>
  </entry>
  
  <entry>
    <title>盘一盘Linux技术灵魂人物</title>
    <url>/post/linux/let-us-talk-about-linux-technical-soul.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Technical Soul</tag>
    </tags>
    <content type="html"><![CDATA[在软件世界的宏伟教堂中， Linux  以其开放源代码、强大效能和灵活性，赢得了一席之地。它不仅是技术的革新，更是一种文化的象征，背后，是一群令人敬仰的技术灵魂人物，他们用执着的信仰和创新的精神，推动了这个自由软件运动的进步。
今天，我们就来盘点那些在Linux技术领域留下深刻印迹的灵魂人物。
Linux Torvalds - Linux内核创始人 林纳斯·托瓦兹（Linus Torvalds），生于1969年12月28日，是一位芬兰-美国计算机科学家，以创造和初始开发Linux内核而闻名于世。Linux内核是现代最流行的操作系统之一，是无数Linux发行版和Android操作系统的核心。
林纳斯·托瓦兹出生于芬兰赫尔辛基，在一个讲瑞典语的家庭中长大。他的爷爷是统计学家和计算机科学家Leo Törnqvist，这可能对他从小对计算机的兴趣产生了影响。托瓦兹在赫尔辛基大学获得了计算机科学方面的硕士学位，他的硕士论文题目是“Linux: A Portable Operating System”。
1991年，Linus Torvalds在大学学习期间开始了Linux内核的初始开发工作，最初他的目的只是创建一个基于UNIX的操作系统，供个人使用，并且可以在他的个人电脑上运行。Linux的名字源于Linus的名字和Minix操作系统的组合。
之后， Linux内核在互联网上迅速传播，吸引了全球大量开发者的关注和贡献。托瓦兹作为项目的领导者，使用GPL（GNU General Public License）许可证发布了Linux内核，从而保证了Linux的开源性质，并允许任何个人或组织自由地使用和修改。
托瓦兹坚持开放源代码的信念，并对微软等对手的FUD战略大为不满。例如，在一封回应微软资深副总裁克瑞格·蒙迪批评开放源代码运动破坏了知识产权的电子邮件中，托瓦兹写道：“我不知道蒙迪是否听说过艾萨克·牛顿爵士，他不仅因为创立了经典物理学而出名，也还因为说过这样一句话而闻名于世：‘我之所以能够看得更远，是因为我站在巨人肩膀上的缘故。’”托瓦兹又说道：“我宁愿听牛顿的也不愿听蒙迪的。他（牛顿）虽然死了快300年了，却也没有让房间这样的臭气熏天。”
林纳斯在网上邮件列表中也以火暴的脾气著称。例如，有一次与人争论Git为何不使用C++开发时与对方用“bullshit”。他更曾以“一群自慰的猴子”（原文为“OpenBSD crowd is a bunch of masturbating monkeys”）来称呼OpenBSD团队，因为林纳斯认为软件一般性的错误比安全漏洞来得要多，而信息安全人士因为找到漏洞而成为英雄，而忽略了一般性软件错误的修补，并认为OpenBSD团队过度重视安全性忽略其他部分。
2012年6月14日，托瓦兹在出席芬兰的阿尔托大学所主办的一次活动时称Nvidia是他所接触过的“the worst company”和 “the worst trouble spot”，因为Nvidia一直没有针对Linux平台发布任何官方的Optimus支持，随后托瓦兹当众对着镜头竖起了中指，说“So, Nvidia, fuck you!”。
林纳斯·托瓦兹的这些行为反映了他作为一个领导者，对于开源理念和合作精神有着非常强烈的信仰与保护之心。同时，他的这种不加掩饰的情感表达，也展现了他作为一个人平易近人和情感真实的一面。
Richard Stallman - 自由软件运动创始人 理查德·马修·斯托曼（Richard Stallman，1953年3月16日—），美国程序员，自由软件活动家。他发起自由软件运动，倡导软件用户能够对软件自由进行使用、学习、共享和修改，确保了这些软件被称作自由软件。斯托曼发起了GNU项目，并成立了自由软件基金会。他开发了GCC、GDB、GNU Emacs，同时编写了GNU通用公共许可协议。
理查德·斯托曼和Linux的关系起源于斯托曼早在Linux出现之前就开始的开源运动。斯托曼是自由软件运动的创始人，他于1983年创建了GNU项目，旨在开发一个完全自由的UNIX式操作系统。该项目生产了很多类UNIX系统所需要的组件，包括编译器、编辑器和各种工具，但直到90年代初，GNU项目并没有完成一个完整的操作系统内核。这个内核被命名为GNU Hurd，但其开发进度较慢。
另一边，林纳斯·托瓦兹在1991年开始开发Linux内核，并在1992年采用了GNU通用公共许可证（GPL）。Linux内核的出现和采用GPL是两个项目相结合的关键。由于GNU项目已经有了大量可以配合内核使用的工具和库，Linux内核很快就得以与GNU的组件结合使用，形成了一个完整的操作系统。这个结合起来的操作系统通常被呼为“GNU/Linux”，尽管在日常语言中，人们通常简称为“Linux”。
理查德·斯托曼并没有直接参与Linux内核的开发，但他为自由软件所设立的理念和标准，尤其是GPL许可证的设计，对于Linux的发展有着深远的影响。GPL保证了用户的自由，包括自由地使用软件、查看源代码、修改代码以及重新分发修改后的版本，这些都是Linux成功的重要因素。
因此，尽管理查德·斯托曼个人与Linux内核的开发关系不大，但是他通过GNU项目和GPL许可证对Linux操作系统的发展起到了关键性的推动作用，并且他积极地维护GNU/Linux系统的命名和自由软件的哲学。斯托曼强调，许多现代Linux发行版实际上是GNU/Linux系统，因为它们结合了Linux内核和大量的GNU软件。
Ian Murdock - Debian创始人 伊恩·阿什利·默多克（Ian Ashley Murdock，1973年4月28日 - 2015年12月28日）是一位重要的美国计算机软件工程师，最著名的成就是他创建了Debian项目及其对应的Linux发行版。Debian是一个广泛使用的自由操作系统，以其稳定性、安全性和开放源代码的承诺而闻名。此外，Debian的包管理系统和政策对许多其他Linux发行版，包括Ubuntu，产生了深远的影响。
默多克在普渡大学攻读计算机科学本科学位时开始了Debian项目。1993年，他发布了名为“The Debian Manifesto”的宣言，阐明了Debian项目的目标和原则。Debian承诺提供一个完全由自由软件构成的操作系统，并强调社群协作以及软件质量。Debian社区遵循的哲学，包括其开发者的社区指导原则、自由软件准则和Debian社区指南，都在《Debian宪章》中有所体现，后者被认为是透明、民主的开源项目治理的典范。
除了创立Debian外，默多克亦在多个知名科技公司担任要职。他的职业生涯涵盖了在Sun Microsystems担任技术架构师，在软件公司ExactTarget负责平台和开发者社区，以及在Docker Inc.担任首席技术官和副总裁。
2015年12月28日，默多克以42岁年龄去世，他的离世对开源社区产生了重大影响，引发了广泛的哀悼。作为Debian项目和整个开源运动的一个关键人物，他的贡献被广泛记忆和尊敬，而Debian项目本身继续作为一个重要的自由软件生态系统得到维护和发展。
Mark Shuttleworth - Ubuntu Linux创始人 马克·沙特尔沃斯（Mark Shuttleworth），生于1973年9月18日，是一位南非企业家、风险投资家，以及航天员。他最为人所知的成就之一是创立了Canonical Ltd公司，这家公司负责开发Ubuntu操作系统，Ubuntu是基于Debian的Linux发行版之一，致力于提供用户友好的桌面和服务器操作系统。
在涉足操作系统开发之前，沙特尔沃斯成功创立了互联网安全公司Thawte，该公司专注于数字证书和网络安全，最终被VeriSign收购，使他赚得了一笔巨款。此后，他成为世界上第二个自费的太空游客，2002年搭乘俄罗斯的联盟TM-34飞船前往国际空间站进行了一次为期八天的飞行。
2004年，沙特尔沃斯创立了Canonical Ltd，并随后推出了Ubuntu。Ubuntu的目标是创建一个简单易用、定期更新和完全免费的Linux操作系统。Ubuntu以其一致性的发行周期、优秀的用户界面和对新手友好的特性而获得了广泛的赞誉，并迅速成为世界上最受欢迎的Linux发行版之一。
沙特尔沃斯致力于开源软件的发展，Canonical下的Ubuntu项目推行了很多创新的策略，包括易于使用的图形安装程序、实时的USB试用和安装、在线软件中心等，都显著降低了Linux系统的使用门槛。此外，Canonical也致力于推广Ubuntu在云计算和IoT（物联网）领域的应用。
作为Ubuntu项目的主要资助者和领导人，沙特尔沃斯对开源社区的贡献还包括赞助其他自由软件项目，他还积极参与到软件设计、产品战略和宣传活动中。他经常出席各种技术大会和开源相关的活动，分享他关于技术和开源文化的见解。
除了Canonical和Ubuntu，沙特尔沃斯还展现了对于科学和探险的热情，不仅通过他的太空飞行，还包括他参与推动的其他科技和创新项目，希望建立一个开放和共享知识的世界。
David Miller 大卫·史提芬·米勒（英语：David Stephen Miller，1974年11月26日—），网络昵称为 davem330，生于美国新泽西州新布朗斯维克，著名程序员与骇客，是Linux内核的高级开发者，对网络子系统作出了重大贡献。他也参与其他开源软件的开发，是GCC督导委员会的成员之一。
根据2013年8月的统计，米勒是Linux核心源代码第二大贡献者，自2005年开始，已经提交过4989个patch。
]]></content>
  </entry>
  
  <entry>
    <title>CXL Type 3内存扩展卡市场现状和趋势</title>
    <url>/post/hardware/cxl-memory-expansion-industry-trend.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>CXL</tag>
    </tags>
    <content type="html"><![CDATA[今天和大家简要介绍一下CXL Type 3内存扩展卡形态、主机连接和扩展柜、CXL协议解码等相关知识。
CXL Type 3内存扩展卡当前市场现状 CXL（Compute Express Link）Type 3内存扩展卡是近年来发展迅速的技术，旨在提供高性能和低延迟的内存扩展解决方案。以下是全球范围内CXL Type 3内存扩展卡的现状：
  技术成熟度：CXL技术本身已经取得显著进展，特别是在CXL 2.0和即将推出的CXL 3.0规范中，这些规范进一步优化了内存池化和共享的能力。CXL Type 3内存扩展卡作为这一技术的具体应用，已经进入了试验和部署阶段。
  市场接受度：目前，主要的服务器和数据中心厂商，如英特尔（Intel）、美光（Micron）、三星（Samsung）等，正在积极研发和推出基于CXL技术的内存扩展产品。大多数领先的数据中心和云服务提供商也在评估和测试这些产品，以提升其基础设施的灵活性和性能。
  应用场景：CXL Type 3内存扩展卡主要应用于需要大容量内存和高带宽的场景，例如人工智能和机器学习、大数据处理、实时分析和虚拟化环境。这些应用对内存容量和性能的需求非常高，而CXL Type 3技术能够有效满足这些需求。
  标准化和生态系统：CXL联盟（CXL Consortium）在推动CXL标准的制定和普及方面发挥了关键作用。越来越多的硬件和软件厂商加入了CXL联盟，共同推动CXL生态系统的发展。这种协作有助于加速CXL Type 3内存扩展卡的市场成熟度和普及度。
  挑战与展望：尽管CXL Type 3内存扩展卡前景广阔，但也面临一些挑战，包括兼容性问题、成本因素以及市场推广的速度。随着技术的进一步成熟和行业的逐步接受，这些挑战有望得到解决。
  总体而言，CXL Type 3内存扩展卡在全球范围内正处于快速发展和推广的阶段。各大厂商和数据中心都在积极测试和部署这项技术，以期在未来获得更高的计算和存储性能。
截至2024年中，CXL（Compute Express Link）Type 3内存扩展卡市场呈现出显著的增长和创新趋势。这些扩展卡旨在提高数据中心的内存容量和带宽，以应对AI、高性能计算（HPC）和数据分析日益增长的需求。
以下是当前市场的一些关键发展：
 三星的创新：  三星推出了CXL内存模块 - Box（CMM-B），可容纳多达8个CXL内存模块 - DRAM（CMM-D）设备，提供高达2TB的容量，带宽达到60GB/s，延迟极低。这一解决方案主要针对需要高内存容量的应用，如AI和内存数据库
此外，三星与Supermicro和VMware等公司合作，开发可扩展和可组合的内存解决方案，并展示了如CXL内存模块 - Hybrid（CMM-H）等用于分层内存管理的新产品
SMART Modular Technologies：  SMART Modular Technologies发布了CXL插入卡（CXA-4F1W和CXA-8F2W），支持DDR5 RDIMMs，容量高达4TB。这些卡旨在消除内存带宽瓶颈，并为服务器内存配置提供灵活性。
市场增长：  CXL内存组件市场预计将快速增长，预计到2026年市场规模将超过20亿美元。CXL技术被认为是实现内存分解和共享的重要步骤，这将显著提高数据中心内存部署的效率。
总体来看，CXL Type 3内存扩展卡市场由于数据密集型应用对更大内存容量和更高效内存利用的需求，正处于快速增长之中。像三星和SMART Modular Technologies这样的公司处于这一创新的前沿，提供先进的解决方案以满足现代计算环境不断发展的需求。
CXL内存扩展卡形态 &lsquo;插卡&rsquo;：这是绝大多数厂家都支持的形态，一般采用PCIe Gen5 x16插卡。
&lsquo;EDSFF&rsquo;：这个一般采用PCIe Gen5 x8 E3.S形态，Samsung, SK Hynix, Micron都提供这类的产品。
CXL内存扩展卡如何连接主机 &lsquo;直接插入主机&rsquo;：插卡直接插入插槽，或者E3.S直接插入服务器前面板的E3.S背板（如果提供的话）。
如果服务器不提供E3.S背板，那么需要将E3.S转接成插卡直接插入插槽，这一般会使用Serial Cables公司的PCIe Gen5 x8 E3.S/AIC转接卡。
&lsquo;插入扩展柜后，然后通过线缆连接到主机&rsquo;：2024年下半年Q3/Q4会有支持插入CXL type 3内存扩展卡（10槽位）或者E3.S内存扩展卡（24槽位）的扩展柜，内部通过CXL switch下行连接所有的插卡/E3.S内存扩展卡，上行则通过Gen5 x16线缆连接到主机端的PCIe Gen5 switch卡或者Retimer卡。
]]></content>
  </entry>
  
  <entry>
    <title>C/C++宏中使用do-while包裹的优势</title>
    <url>/post/programming/advantages-of-using-do-while-wrapper-in-CC++-macros.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>C</tag>
      <tag>C++</tag>
      <tag>do while</tag>
    </tags>
    <content type="html"><![CDATA[在 C 和 C++ 编程中，宏（macro）是一种非常强大的工具。它们允许开发者编写简洁、高效的代码，减少重复。然而，使用宏时也可能会引发一些意料之外的问题。为了防止这些问题，开发者经常使用 do-while 包裹宏。
宏的基本概念 在 C 和 C++ 中，宏是一种预处理器指令，通常使用 #define 来定义。宏可以使代码更简洁，提供条件编译、代码片段重用等功能。例如，简单的宏可以是如下定义：
#define SQUARE(x) (x * x) 这个宏定义了一个计算平方的简单公式。在代码中使用时，SQUARE(5) 将被替换为 5 * 5。
然而，宏并不是没有缺点。它们不像函数那样具有严格的类型检查和作用域控制，这就可能引发一些意外的行为。例如，上面的 SQUARE 宏在处理带有副作用的表达式时可能会出问题：
int a = 5; int result = SQUARE(a++); 这个表达式将展开为 a++ * a++，而不是预期的 (a + 1) * (a + 1)。
为什么使用 do-while 包裹宏 为了使宏更安全并减少意外行为，开发者常常使用 do { &hellip; } while (0) 结构来包裹宏定义。这种做法有几个显著的好处：
 保证单一的执行块  使用 do-while 包裹宏可以确保宏内的代码作为一个单独的执行块。这在处理多条语句时尤其重要。例如，考虑以下宏：
#define SWAP(a, b) \ int temp = a; \ a = b; \ b = temp; 如果直接在代码中使用 SWAP(x, y);，可能会导致意料之外的行为，特别是在条件语句中：
if (condition) SWAP(x, y); else // 其他操作 上面的代码将展开为：
if (condition) int temp = x; x = y; y = temp; else // 其他操作 这将导致编译错误，因为 else 部分并没有与 if 关联。为了避免这种问题，可以使用 do-while 包裹：
#define SWAP(a, b) \ do { \ int temp = a; \ a = b; \ b = temp; \ } while (0) 这样展开后，代码将变为：
if (condition) do { int temp = x; x = y; y = temp; } while (0); else // 其他操作 这样就保证了 if 和 else 结构的完整性，不会出现编译错误。
提高代码的可读性和可维护性  使用 do-while 包裹宏还可以提高代码的可读性和可维护性。通过明确地将宏包裹在一个单独的块中，开发者可以更容易地理解和维护代码。特别是在处理复杂的宏时，这种结构可以使代码更具逻辑性和条理性。
防止潜在的副作用  另一个重要的好处是，do-while 结构可以防止潜在的副作用。例如，在没有 do-while 包裹的宏中，如果宏内包含多条语句，这些语句可能会在意外的情况下被部分执行，而不是全部执行。这种情况可能会引发难以调试的错误。通过使用 do-while 包裹，开发者可以确保宏内的所有语句都被完整执行，或者都不执行。
避免作用域问题  使用 do-while 结构还可以避免作用域问题。在宏中定义的变量通常是局部变量，使用 do-while 包裹可以确保这些变量的作用域限制在宏的执行块内，而不会泄露到外部。例如：
#define MAX(a, b) \ do { \ int _a = (a); \ int _b = (b); \ (void)((_a &gt; _b) ? _a : _b); \ } while (0) 这里 _a 和 _b 仅在 do-while 块内有效，防止了变量名冲突。
实际应用示例 为了更好地理解 do-while 包裹宏的好处，让我们看看一个实际的示例。假设我们需要编写一个宏来打印调试信息：
#define DEBUG_PRINT(msg) \ do { \ printf(&#34;DEBUG: %s\n&#34;, msg); \ } while (0) 在调试模式下，我们可以安全地使用这个宏：
if (debugMode) DEBUG_PRINT(&#34;Debugging information&#34;); else printf(&#34;Normal operation\n&#34;); 由于使用了 do-while 包裹，DEBUG_PRINT 宏将安全地作为一个单独的块执行，不会影响 if-else 结构。
总结 在 C 和 C++ 编程中，宏是一种强大的工具，但也可能带来一些潜在的问题。通过使用 do-while 包裹宏，开发者可以显著提高代码的安全性、可读性和可维护性。do-while 结构确保了宏作为一个单独的执行块，防止了意外的行为和副作用，避免了作用域问题。
]]></content>
  </entry>
  
  <entry>
    <title>Nvidia HGX B200主板上的NVLink Switch变化</title>
    <url>/post/ai/ingrasys-shows-big-nvidia-nvlink-switch-chips-change-to-the-hgx-b200-b100.html</url>
    <categories><category>AI</category>
    </categories>
    <tags>
      <tag>NVLink</tag>
      <tag>Switch</tag>
      <tag>HGX B200</tag>
      <tag>H100</tag>
    </tags>
    <content type="html"><![CDATA[在Computex 2024展览上，富士康的一个部门Ingrasys的摊位上展示了一块没有安装散热片的Nvidia HGX B200的主板。我们可以看到NVLink Switch Chip的数量，从 H100  的4个减少到2个，并且在主板上的位置也发生了变化。
第一代NVSwitch 第一代NVSwitch是随着Nvidia的DGX-2计算机的推出而引入的。第一代NVSwitch是一个拥有20亿晶体管、18个端口的NVLink switch。每个端口以50 GB/s的速度运行，Switch的总带宽为900 GB/s。该交换机允许最多九个设备中的任何一个被路由到其他九个设备中的任何一个。除了NVLink端口，交换机还有额外的控制和管理的I/O接口，包括x4 PCIe Gen 2管理端口、I²C和GPIO。
DGX-2使用每块基板6个NVLink来创建GPU的全连接网络。
DGX-2有两个基板，以fully connect所有16个V100 GPU。每个Switch有2个未使用的端口。这些端口可能会在涉及POWER9微处理器的配置中使用，因为这些微处理器原生支持NVLink 2.0。
下图是Inspur NF5488M5 HGX 2 ，我们可以很清楚地看到NVSwitch的6个散热片。
第二代NVSwitch 进入NVIDIA A100时代，NVSwitch的散热器变得更大，但整个HGX A100平台是由NVIDIA构建的，并预先组装后发送给供应商。
下图是Inspur NF5488A5 NVIDIA HGX A100，我们可以看到6个NVSwitch散热片。
第三代NVSwitch H100这一代，4个NVSwitch在基板的一侧。
这是ASUS ESC N8A E12 NVIDIA HGX H100 。
下面是Nvidia HGX H200，看上去与HGX H100的设计非常相似。
第四代NVLink Switch 带着散热片的HGX B100
发布会上的HGX B100图片
注意观察，可以发现，在边缘侧的连接器附近是PCIe retimer，而不是NVSwitch了。
从没有装散热片的HGX B200主板，可以看到NVLink Switch 芯片的安装位置。经Nvidia工作人员确认HGX B100的主板上的NVLink Switch芯片安装与B200的相同。
在这里，我们可以看到PCIe Retimer从散热器中暴露出来。这些Retimer通常配备较小的散热器，因为它们的TDP大约在10-15W之间，具体取决于你讨论的是使用此尺寸的Astera Labs、Broadcom还是Marvell版本。
引人注意的是，NVLink Switch位于主板中央，而不是边缘侧，并且只有2个，而不是上一代的4个。
结论
现在的NVLink Switch芯片体积更大，将它们移动到主板中央可能有助于减少迹线长度。这对于进行高速信号传输是有益的。此外，NVIDIA B200 GPUs的四个GPU集合在两侧，这样做，进一步减少了GPU到NVLink Switches的迹线长度。
]]></content>
  </entry>
  
  <entry>
    <title>精讲网络速率测试工具iperf3使用</title>
    <url>/post/linux/detailed-introduction-to-iperf3-network-speed-test-tool.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Iperf3</tag>
    </tags>
    <content type="html"><![CDATA[iPerf3是用于主动测试IP网络上最大可用带宽的工具。
iperf3简介 iPerf3支持时序、缓冲区、协议（TCP，UDP，SCTP与IPv4和IPv6）有关的各种参数。对于每次测试，它都会详细的带宽报告，延迟抖动和数据包丢失。
它与原始 iPerf 不共享任何代码，也不向后兼容。
它是一个C/S架构的测试工具，需要在同时运行在服务器端和客户端。
支持的平台：Windows, Linux  , Android, MacOS X, FreeBSD, OpenBSD, iPhone/iPad等
iperf 主要测试网络以下三个方面：吞吐量、稳定性、可靠性。
iperf 的参数比较多，主要从三个对象进行罗列，分别是客户端、服务端、公共使用
执行：iperf3 -h 可以查看所有的命令信息，以下是中文信息， iPerf 3.1.2 支持的所有参数：
iperf3安装 Windows 下载链接：https://iperf.fr/iperf-download.php
官网直接下载Android/iPhone/iPad安装包，下载完成后解压，使用时进入 cmd 终端，cd 到解压路径，可以找到 iperf3.exe 程序即可正常使用。
Linux 在 Ubuntu 系统中可以直接使用以下命令进行安装，或者 官网下载离线安装包：
sudo apt install -y iperf3 iperf3命令说明 用法：
iperf3 [-s|-c host] [options] 公共使用参数 -f, --format [kmKM] 报告格式: Kbits, Mbits, KBytes, MBytes。 -i, --interval # 设置每次报告之间的时间间隔，单位为秒。如果设置为非零值，就会按照此时间间隔输出测试报告。默认值为零。 -l, --len #[KM] 设置读写缓冲区的长度。TCP方式默认为8KB，UDP方式默认为1470字节。 -m, --print_mss 输出TCP MSS值（通过TCP_MAXSEG支持）。MSS值一般比MTU值小40字节。 -o, --output &lt;filename&gt; 将报告或错误消息输出到此指定文件。 -p, --port # 设置端口，与服务器端的监听端口一致。默认是5001端口，与ttcp的一样。 -u, --udp 使用UDP而不是TCP，默认是TCP -w, --window #[KM] 设置套接字缓冲区为指定大小。对于TCP方式，此设置为TCP窗口大小。对于UDP方式，此设置为接受UDP数据包的缓冲区大小，限制可以接受数据包的最大值。 -B, --bind &lt;host&gt; bind to &lt;host&gt;, an interface or multicast address -C, --compatibility 与旧版本一起使用不会发送额外的MSG -M, --mss # 通过TCP_MAXSEG选项尝试设置TCP最大信息段的值。MSS值的大小通常是TCP/IP头减去40字节。在以太网中，MSS值 为1460字节（MTU1500字节）。许多操作系统不支持此选项。 -N, --nodelay 设置TCP无延迟选项，禁用Nagle&#39;s运算法则。通常情况此选项对于交互程序，例如telnet，是禁用的。 -V, --IPv6Version 绑定一个IPv6地址。服务端：$ iperf -s –V客户端：$ iperf -c &lt;Server IPv6 Address&gt; -V注意：在1.6.3或更高版本中，指定IPv6地址不需要使用-B参数绑定，在1.6之前的版本则需要。在大多数操作系统中，将响应IPv4客户端映射的IPv4地址。 服务端参数 -s, --server 在服务器模式下运行。 -U, --single_udp 在单线程UDP模式下运行。 -D, --daemon 将服务器作为守护进程运行。 客户端参数 -b, --bandwidth #[KM] UDP模式使用的带宽，单位bits/sec。此选项与-u选项相关。默认值是1 Mbit/sec。 -c, --client &lt;host&gt; 运行Iperf的客户端模式，连接到指定的 Iperf 服务器端。 -d, --dualtest 运行双测试模式。这将使服务器端反向连接到客户端，使用-L 参数中指定的端口（或默认使用客户端连接到服务器端的端口）。这些在操作的同时就立即完成了。如果你想要一个交互的测试，请尝试-r参数。 -n, --num #[KM] 指定传输的字节数，eg:iperf -c 222.35.11.23 -n 100000。 -r, --tradeoff 往复测试模式。当客户端到服务器端的测试结束时，服务器端通过-l选项指定的端口（或默认为客户端连接到服务器端的端口），反向连接至客户端。当客户端连接终止时，反向连接随即开始。如果需要同时进行双向测试，请尝试-d参数。 -t, --time # 设置传输的总时间。Iperf在指定的时间内，重复的发送指定长度的数据包。默认是10秒钟。参考-l与-n选项。 -F, --fileinput &lt;name&gt; 使用特定的数据流测量带宽，例如指定的文件。$ iperf -c &lt;server address&gt; -F &lt;file-name&gt; -I, --stdin 与-F一样，由标准输入输出文件输入数据。 -L, --listenport # 指定服务端反向连接到客户端时使用的端口。默认使用客户端连接至服务端的端口。 -P, --parallel # 线程数。指定客户端与服务端之间使用的线程数。默认是1线程。需要客户端与服务器端同时使用此参数。 -T, --ttl # 出栈多播数据包的TTL值。这本质上就是数据通过路由器的跳数。默认是1，链接本地。 -Z, --linux-congestion &lt;algo&gt; 设置TCP拥塞控制算法（仅限Linux）。 iperf3操作实例解析 运行iperf3在客户端，使用UDP协议，并设置使用的测试带宽 iperf3 -c serverIP -b 1000M -t 60 -d -c 为客户端运行并要指定服务端的IP地址 -b 表示使用的测试带宽 -t 表示以时间为测试结束条件进行测试，默认为 10 秒; -d 打印出更详细的debug调试信息 iperf3 -c 192.168.1.1 -b 1000M -t 60 -d 运行iperf3在服务器端，默认端口号为5201 iperf3 -s 运行iperf3在服务器端，如果需要指定的端口号加上-p port iperf3 -s -p 【这里写指定的端口号】
iperf3 -s -p 8888 运行iperf3在客户端，开始带宽测试 iperf3 -c [serverIP，这些写服务器的IP地址]
iperf3 -c 192.168.1.1 运行iperf3在客户端，设置多个并行数据流-P iperf3 -c serverIP -P streams iperf3 -c 192.168.1.1 -P 3 运行iperf3在客户端，反向测试（服务器端发送数据到客户端） iperf3 -c serverIP -R iperf3 -c 192.168.1.1 -R 常见遇到的打流问题  server端报出端口busy  解决办法：server和client设置到其他端口在进行打流
client端报出iperf3：error - unable to read form stream socket：Resource temporarily unavailable  解决办法：server和client两端都是加上-B 192.168.132.xx（本地IP）
]]></content>
  </entry>
  
  <entry>
    <title>SSD 固态硬盘介绍</title>
    <url>/post/hardware/introduction-to-solid-state-drive.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>SSD</tag>
    </tags>
    <content type="html"><![CDATA[固态硬盘（Solid State Drive，简称 SSD  ）是一种数据存储设备，使用闪存存储数据，与传统机械硬盘（Hard Disk Drive，简称HDD）相比，具有更快的读写速度、更低的功耗和更高的可靠性。
 SSD  已经广泛应用于各种计算设备，从个人计算机、 服务器  到移动设备等。本文将详细介绍SSD的结构、工作原理、类型、优缺点以及应用场景。
SSD的结构 SSD的主要组件包括：   闪存（NAND Flash）：这是SSD的核心部分，用于存储数据。根据制造工艺和存储单元的排列方式，闪存可以分为SLC（单层单元）、MLC（多层单元）、TLC（三层单元）和QLC（四层单元）等类型。SLC的速度最快、寿命最长，但成本也最高；QLC则存储密度最高、成本最低，但性能和寿命较差。
  控制器（Controller）：控制器负责管理数据的读写操作、执行错误校正、优化性能和延长闪存寿命等任务。它相当于SSD的大脑，不同品牌和型号的SSD通常会使用不同的控制器，这也是影响SSD性能的重要因素之一。
  缓存（Cache）：为了提高数据读写速度，SSD通常会配备一定容量的缓存。缓存可以是DRAM（动态随机存取存储器），也可以是闪存本身的一部分。缓存的存在可以有效减少数据访问的延迟。
  接口（Interface）：SSD通过接口与计算机系统进行数据传输。常见的接口有SATA（串行ATA）、PCIe（Peripheral Component Interconnect Express）和M.2等。不同接口的传输速率和性能差异较大。
  SSD的工作原理 SSD的工作原理主要基于闪存的存储机制。闪存是一种非易失性存储介质，即使在断电的情况下也能保持数据。其基本存储单元是浮栅晶体管，通过控制电压在浮栅中存储电荷来表示二进制信息（0或1）。
在SSD中，数据的读写操作通过控制器来完成。读操作相对简单，控制器从指定地址读取数据并传输给主机。写操作则复杂得多，因为闪存的写入速度较慢，且在写入数据之前需要先擦除原有数据。
闪存的擦除操作通常以“块”为单位进行，而写入操作则以“页”为单位。由于每次写入都需要先擦除相应的块，这使得SSD的写入性能较HDD更为复杂。为了解决这一问题，控制器会使用“垃圾回收”（Garbage Collection）和“磨损均衡”（Wear Leveling）等技术来优化性能和延长闪存寿命。
  垃圾回收：当SSD中有部分页面的数据变得无效（例如文件被删除或修改），控制器会定期进行垃圾回收，将有效数据重新整理并写入新的块，然后擦除旧块，以释放空间。
  磨损均衡：由于闪存的写入寿命有限，控制器会使用磨损均衡技术，确保所有块的写入次数大致相同，防止某些块过早损坏。
  SSD的类型 根据接口和用途的不同，SSD可以分为以下几种类型：
 SATA SSD：SATA是最常见的SSD接口，主要用于消费级市场。SATA SSD的传输速率一般为6Gbps，性能相比HDD有显著提升，但相对于其他接口的SSD（如PCIe SSD）则略显逊色。   PCIe SSD：PCIe接口的SSD具有更高的传输速率，通常用于高性能计算和数据中心等领域。PCIe SSD可以提供几GBps的传输速率，是SATA SSD的数倍。
  M.2 SSD：M.2是一个小型化的接口标准，广泛应用于超薄笔记本、台式机和服务器等设备。M.2接口可以支持SATA和PCIe两种模式，PCIe模式的M.2 SSD性能更为强劲。
  NVMe SSD：NVMe（Non-Volatile Memory Express）是一种针对SSD优化的协议，专为PCIe接口设计。NVMe SSD的性能比传统的AHCI协议更为出色，延迟更低，读写速度更快。  SSD的优缺点 优点：   速度快：SSD的读写速度远高于HDD，尤其是在随机读写性能方面更是显著领先。这使得系统启动、程序加载和文件传输等操作更加迅速。
  功耗低：SSD没有机械部件，功耗相比HDD更低，尤其适用于笔记本和移动设备，有助于延长电池续航时间。
  可靠性高：SSD没有活动部件，不容易受到震动和冲击的影响，数据丢失风险较低。此外，SSD的故障模式相对可预测，可以提前预警。
  噪音低：由于没有机械部件，SSD在工作时几乎没有噪音，非常适合需要安静环境的应用场景。
  缺点：   成本较高：尽管SSD的价格近年来有所下降，但与HDD相比，单位存储成本仍然较高，特别是对于大容量SSD而言。
  寿命有限：闪存单元的写入次数有限，尽管现代SSD通过各种技术延长寿命，但长时间大量写入操作仍可能导致性能下降甚至损坏。
  容量有限：目前市场上的SSD容量通常在几十GB到几TB之间，虽然已经能够满足大多数需求，但在极大容量存储方面仍不及HDD。
  SSD的应用场景 SSD因其卓越的性能和可靠性，被广泛应用于各个领域。
  个人计算机：SSD已经成为个人计算机中不可或缺的存储设备。无论是台式机还是笔记本电脑，SSD都能显著提升系统响应速度，优化用户体验。
  服务器和数据中心：在服务器和数据中心领域，SSD凭借其高性能和低延迟特点，大大提升了数据处理和访问效率，尤其在高频交易、数据库管理和虚拟化等应用中表现突出。
  消费电子：从智能手机、平板电脑到游戏机和智能家居设备，SSD被广泛应用于各种消费电子产品中，提供快速的数据存储和访问能力。
  企业级存储：企业级SSD通常具备更高的耐用性和可靠性，适用于大规模存储和高性能计算环境，如科学研究、影视制作和金融分析等领域。
  嵌入式系统：在嵌入式系统中，SSD以其体积小、抗震能力强和高可靠性特点，被广泛应用于工业控制、医疗设备和汽车电子等领域。
  知名的 SSD 厂商  三星（SAMSUNG）：全球知名的大型跨国企业集团，在固态硬盘领域处于领先地位，其产品性能强悍，品质可靠。 西部数据（WD）：全球较大的数据存储和硬盘制造商，旗下的 SSD 产品在市场上也有较高的声誉。 铠侠（KIOXIA）：由东芝存储器集团更名而来，是全球知名存储解决方案供应商，其闪存产品受到广泛认可。 金士顿（Kingston）：全球大型独立内存产品制造商，提供包括 SSD 在内的多种存储产品，内存模组和闪存卡可终身保修。 致态（ZHITAI）：长江存储旗下的消费级固态硬盘品牌，长江存储是一家专注于3D NAND 闪存设计制造一体化的 IDM 集成电路企业。 Solidigm：由海力士收购英特尔 NAND 固态硬盘业务后成立的全新品牌，能提供多种闪存技术解决方案。 威刚（ADATA）：其业务涵盖内存、闪存盘、闪存卡、SSD 固态硬盘及移动硬盘等领域。 雷克沙（Lexar）：全球知名的闪存存储品牌，产品覆盖专业影像存储、移动存储、个人系统存储等领域。 光威（Gloway）：嘉合劲威集团旗下知名内存品牌，致力于推进电脑存储产品的国产化进程。 英睿达（Crucial）：美光集团旗下品牌，采用美光原厂颗粒。 浦科特（PLEXTOR）：在 SSD 市场也有一定的知名度和用户群体。 闪迪（SanDisk）：被西部数据收购，其产品和西部数据的产品可以认为是同一家的。 金泰克（Kimtigo）：专注存储领域，SSD 产品销售以中国大陆市场为主。 影驰（GALAXY）：于1994年在中国香港成立，2011年进军存储领域，在显卡、SSD 等领域均有产品推出。 七彩虹（Colorful）：国内一家 DIY 硬件厂商，涵盖显卡、主板、存储等多种产品，2015年正式推出旗下首款七彩虹 SSD 产品。 联想（Lenovo）：著名的硬件厂商，其 SSD 产品主要集成于 PC 产品中提供给用户。  市场上还有其他许多厂商也在生产 SSD 产品。在选择 SSD 时，除了考虑厂商品牌外，还应关注产品的性能、容量、价格、可靠性等因素。同时，不同厂商的 SSD 在性能、特点和适用场景上可能会有所差异，可以根据自己的需求和预算进行选择。
SSD 寿命 SSD 的寿命主要取决于其闪存类型、写入量、主控算法以及使用环境等因素。
一般来说，采用 TLC 闪存的 SSD，在正常使用的情况下，其寿命可达 3 - 5 年甚至更久。而采用 MLC 闪存的 SSD 寿命通常会更长。
闪存的写入次数是影响 SSD 寿命的关键指标之一。例如，一款 256GB 的 TLC SSD 闪存颗粒的写入寿命可能在 150 - 300TBW（Terabytes Written，写入的总字节数）左右。如果您每天写入 20GB 的数据，那么大致可以使用 20 - 40 年。但实际使用中，很少有人能达到这样高的写入量。
此外，良好的使用习惯，如避免在 SSD 剩余空间过少时持续写入大量数据、避免频繁进行大量小文件的写入操作等，都有助于延长 SSD 的使用寿命。同时，合适的工作温度和湿度等环境条件也对 SSD 的寿命有积极影响。
总结 固态硬盘（Solid State Drive，简称 SSD）是一种使用闪存芯片来存储数据的存储设备。
与传统的机械硬盘（HDD）相比，固态硬盘具有多项显著优势。首先，它没有机械部件，如旋转的磁盘和移动的读写头，因此数据访问速度更快，能够大幅缩短系统启动时间、程序加载时间和文件传输时间。其次，固态硬盘更耐冲击和震动，运行时更安静，且功耗较低，产生的热量也相对较少。此外，固态硬盘的体积通常更小，更便于在各种设备中安装和使用。
在工作原理上，数据以电子方式存储在闪存芯片中，通过控制器进行管理和协调读写操作。
由于其出色的性能，固态硬盘在现代计算机系统中得到了广泛应用，尤其是在对速度和响应要求较高的场景，如高端笔记本电脑、台式电脑、服务器等。
]]></content>
  </entry>
  
  <entry>
    <title>Nessus 工具介绍与使用教程</title>
    <url>/post/linux/Nessus-tool-introduction-and-use-tutorial.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Boot</tag>
      <tag>Nessus</tag>
      <tag>Hacker</tag>
    </tags>
    <content type="html"><![CDATA[ Nessus   是一款广泛使用的网络漏洞扫描工具，由 Tenable Network Security 开发。
工具介绍 Nessus能够帮助网络管理员和安全专业人员识别网络中的安全漏洞、配置错误和潜在的安全威胁。Nessus 提供全面的漏洞检测功能，包括操作系统漏洞、应用程序漏洞、配置审计和合规性检查等。
功能与特点  广泛的漏洞库：Nessus 拥有一个庞大的漏洞数据库，能够检测数千种已知漏洞。 多平台支持：支持 Windows、Linux、macOS 等多种操作系统。 用户友好的界面：提供图形化界面，易于使用和理解。 定期更新：Nessus 的漏洞库和插件定期更新，以确保检测的及时性和准确性。 详细的报告：生成详细的扫描报告，帮助用户快速定位和修复漏洞。  使用教程 以下是使用 Nessus 进行漏洞扫描的详细教程。
步骤一：安装 Nessus 在 Linux 上安装  下载 Nessus 安装包：  访问 Tenable 官网，选择适合您操作系统的版本并下载。
安装 Nessus：  打开终端，导航到下载目录，运行以下命令进行安装：
sudo dpkg -i Nessus-&lt;version&gt;-debian6_amd64.deb 启动 Nessus 服务：  sudo systemctl start nessusd.service 启用 Nessus 服务开机启动：  sudo systemctl enable nessusd.service 在 Windows 上安装 下载 Nessus 安装包： 访问 Tenable 官网，选择适合您操作系统的版本并下载。
运行安装程序： 双击下载的安装程序，按照安装向导完成安装。
启动 Nessus： 安装完成后，Nessus 会自动启动，并在浏览器中打开配置页面。
步骤二：配置 Nessus 访问 Nessus 配置页面： 在浏览器中输入 https://localhost:8834 访问 Nessus 配置页面
创建账户： 根据提示创建 Nessus 管理员账户。
输入激活码： 输入从 Tenable 网站获取的激活码以激活 Nessus。
插件更新： 激活后，Nessus 会自动下载和更新插件。等待更新完成。
步骤三：创建扫描任务 登录 Nessus： 使用刚刚创建的账户登录 Nessus。
创建新的扫描任务： 在控制面板中，点击左侧的“扫描”选项卡，然后点击“新建扫描”按钮。
选择扫描模板： 根据需要选择合适的扫描模板。例如，选择“基本网络扫描”。
配置扫描参数： 在弹出的配置页面中，输入扫描任务的名称、描述和目标 IP 地址。其他参数可以根据需要进行调整。
保存并启动扫描： 配置完成后，点击“保存”按钮，然后在扫描任务列表中点击“启动”按钮开始扫描。
步骤四：查看扫描结果 查看扫描进度： 在扫描任务列表中，可以查看扫描任务的进度和状态。
查看扫描报告： 扫描完成后，点击扫描任务名称查看详细的扫描报告。报告中包含了发现的所有漏洞及其详细信息。
导出扫描报告： 可以将扫描报告导出为多种格式（如 PDF、CSV 等），以便于共享和分析。
总结 Nessus 是一款功能强大的网络漏洞扫描工具，通过其广泛的漏洞库和强大的扫描功能，能够帮助用户有效地识别和修复网络中的安全漏洞。通过本文的详细教程，您可以系统地学习如何安装、配置和使用 Nessus 进行漏洞扫描。在使用过程中，请务必遵守相关法律法规，仅在合法授权的情况下使用该工具。
]]></content>
  </entry>
  
  <entry>
    <title>英伟达核心壁垒: CUDA</title>
    <url>/post/hardware/cuda-nvidia-core-barrier.html</url>
    <categories><category>AI</category>
    </categories>
    <tags>
      <tag>GenAI</tag>
      <tag>Nvidia</tag>
      <tag>GPU</tag>
      <tag>CUDA</tag>
    </tags>
    <content type="html"><![CDATA[CUDA（Compute Unified Device Architecture），统一计算设备架构，英伟达推出的基于其 GPU  的通用高性能计算平台和编程模型。
什么是CUDA 借助CUDA，开发者可以充分利用英伟达GPU的强大计算能力加速各种计算任务。
&lsquo;软件生态的基石&rsquo;：CUDA构成了英伟达软件生态的基础，诸多前沿技术均基于CUDA构建。
例如，TensorRT、Triton和Deepstream等，这些技术解决方案都是基于CUDA平台开发的，展示了CUDA在推动软件创新方面的强大能力。
&lsquo;软硬件的桥梁&rsquo;：英伟达的硬件性能卓越，但要发挥其最大潜力，离不开与之相匹配的软件支持。
CUDA正是这样一个桥梁，它提供了强大的接口，使得开发者能够充分利用GPU硬件进行高性能计算加速。
就像驾驶一辆高性能汽车，CUDA就像是一位熟练的驾驶员，能够确保硬件性能得到充分发挥。
&lsquo;深度学习框架的加速器&rsquo;：CUDA不仅在构建英伟达自身的软件生态中扮演关键角色，在推动第三方软件生态发展方面也功不可没。
特别是在深度学习领域，CUDA为众多深度学习框架提供了强大的加速支持。
例如，在Pytorch、TensorFlow等流行框架中，CUDA加速功能成为标配。
开发者只需简单设置，即可利用GPU进行高效的训练和推理任务，从而大幅提升计算性能。
CPU＋GPU异构计算 &lsquo;CPU&rsquo;：中央处理器（Central Processing Unit）作为计算机系统的运算和控制核心，是信息处理、程序运行的最终执行单元。
运算核心较少，但是其可以实现复杂的逻辑运算，因此其适合控制密集型任务，CPU更擅长数据缓存和流程控制——（少量的复杂计算）
&lsquo;GPU&rsquo;：图形处理器（Graphics Processing Unit），常被称为显卡，GPU最早主要是进行图形处理的。
如今深度学习大火，GPU高效的并行计算能力充分被发掘，GPU在AI应用上大放异彩。
GPU拥有更多的运算核心，其特别适合数据并行的计算密集型任务，如大型矩阵运算——（大量的简单运算）
一个典型的 CPU 拥有少数几个快速的计算核心，而一个典型的 GPU 拥有几百到几千个不那么快速的计算核心。
CPU的晶体管设计更多地侧重于数据缓存和复杂的流程控制，而GPU则将大量晶体管投入到算术逻辑单元中，以实现并行处理能力。
因此，GPU正是通过其众多的计算核心集群来实现其相对较高的计算性能。
使用CUDA编程，开发者可以精确地指定数据如何被分配到GPU的各个核心上，并控制这些核心如何协同工作来解决问题。
GPU不能单独进行工作，GPU相当于CPU的协处理器，由CPU进行调度，CPU+GPU组成异构计算架构。
在由 CPU 和 GPU 构成的异构计算平台中，通常将起控制作用的 CPU 称为主机（host），将起加速作用的 GPU 称为设备（device）。
主机和设备之间内存访问一般通过PCle总线链接。
计算生态 开发工具链 NVIDIA driver  显卡驱动是连接操作系统和显卡硬件之间的桥梁，确保显卡能够正常工作并发挥最佳性能。 显卡驱动包含硬件设备的信息，使得操作系统能够识别并与显卡硬件进行通信。 显卡驱动对于启用显卡的全部功能、性能优化、游戏和应用程序兼容性以及修复问题和安全更新都至关重要  CUDA Toolkit  CUDA Toolkit是一个由NVIDIA开发的软件开发工具包，它为NVIDIA GPU提供了一组API和工具，使得开发人员可以利用GPU的并行计算能力来加速计算密集型应用程序。 CUDA Toolkit包括CUDA编译器（NVCC）、CUDA运行时库、CUDA驱动程序等组件，它们协同工作，使得开发人员可以使用C或C++编写GPU加速的代码。  CUDA API  CUDA API是CUDA编程的接口集合，它允许开发者使用CUDA进行高性能计算。 CUDA API包括CUDA Runtime API和CUDA Driver API，它们提供了用于管理设备、内存、执行等功能的函数。 开发者可以通过CUDA API来编写CUDA程序，以利用GPU的并行计算能力。  NVCC  NVCC是CUDA的编译器，属于CUDA Toolkit的一部分，位于运行时层。 NVCC是一种编译器驱动程序，用于简化编译C++或PTX代码。它提供简单且熟悉的命令行选项，并通过调用实现不同编译阶段的工具集合来执行它们。 开发者在编写CUDA程序时，需要使用NVCC来编译包含CUDA核心语言扩展的源文件。  NVIDIA driver是确保显卡正常工作的基础，而CUDA Toolkit则是利用GPU进行高性能计算的软件开发工具包。
CUDA API是CUDA编程的接口，而NVCC则是CUDA的编译器，用于将CUDA程序编译成可在GPU上执行的代码。
应用框架与库支持 CUDA广泛支持各类科学计算、工程、数据分析、人工智能等领域的应用框架和库。
例如，在深度学习领域，TensorFlow、PyTorch、CUDA Deep Neural Network Library (cuDNN) 等工具均深度整合了CUDA，使得开发者可以轻松利用GPU加速神经网络训练和推理过程。
深度学习框架  TensorFlow：TensorFlow是Google开发的开源机器学习框架，它支持分布式计算，并且可以高效地使用GPU进行数值计算。TensorFlow在底层使用了CUDA和cuDNN等NVIDIA的库来加速深度学习模型的训练和推理过程。 PyTorch：PyTorch是Facebook人工智能研究院（FAIR）开发的深度学习框架。PyTorch也支持CUDA，并且提供了丰富的API来让开发者轻松地使用GPU进行深度学习模型的训练和推理。PyTorch和CUDA的版本之间存在一定的兼容性关系，需要确保PyTorch的版本与CUDA的版本兼容。  CUDA库  cuBLAS：用于线性代数运算的库，如矩阵乘法、前缀求和等，常用于科学和工程计算。 cuDNN：NVIDIA CUDA深度神经网络库（cuDNN）是一个用于深度学习的GPU加速库，提供了一系列深度学习算法的高效实现。 cuSPARSE：针对稀疏矩阵的线性代数库。 cuFFT：快速傅里叶变换库，用于执行高效的FFT（快速傅里叶变换）操作。 cuRAND：随机数生成库，允许开发者在GPU上生成随机数。  这些库为开发者提供了丰富的计算资源，使他们能够更高效地开发GPU加速的应用程序。
CUDA编程语言 C、C++、Fortran、Python 和 MATLAB
]]></content>
  </entry>
  
  <entry>
    <title>VXLAN: 彻底改变网络虚拟化</title>
    <url>/post/linux/vxlan-revolutionizing-network-virtualization.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Linux</tag>
      <tag>VXLAN</tag>
    </tags>
    <content type="html"><![CDATA[网络虚拟化技术已经成为数据中心和云计算环境的关键组件，从最早的虚拟局域网（VLAN）到如今的虚拟扩展局域网（VXLAN），每一步技术进步都在推动着网络基础设施的变革和创新。VXLAN作为一种先进的网络虚拟化技术，通过解决传统VLAN的局限性，实现了更高效、更灵活的网络管理，彻底改变了 数据中心  和云计算环境中的网络架构。
网络虚拟化技术的初衷是为了提高网络资源的利用率，简化网络管理，并增强网络的灵活性和可扩展性。在早期，VLAN技术的引入使得网络管理员可以在物理网络基础上创建多个虚拟网络，从而实现网络资源的隔离和分配。然而，随着云计算、大数据等技术的迅猛发展，传统的VLAN技术逐渐暴露出了一些局限性，特别是在大规模数据中心环境中，VLAN技术面临着可扩展性差、网络隔离不彻底、跨域通信复杂等问题。
为了解决上述问题，虚拟扩展局域网（VXLAN）应运而生。VXLAN由IETF（互联网工程任务组）提出，是一种基于隧道技术的网络虚拟化方案。与传统的VLAN不同，VXLAN利用UDP（用户数据报协议）进行数据封装，通过在现有的IP网络上建立虚拟隧道，将二层网络扩展到更大的范围，实现跨越多个物理网络的虚拟网络连接。
VXLAN的核心理念是通过引入24位的VNI（VXLAN Network Identifier，VXLAN网络标识符），可以支持多达1600万个虚拟网络，远远超过了VLAN的4096个ID限制。这样一来，VXLAN不仅解决了VLAN的可扩展性问题，还提供了更细粒度的网络隔离和更高效的跨域通信能力。
VXLAN的引入对于现代数据中心和云计算环境具有重要意义。首先，VXLAN的高可扩展性使得数据中心可以容纳更多的虚拟网络和租户，满足了大规模环境下的网络需求。其次，VXLAN利用现有的IP网络进行传输，无需对底层网络进行大规模改造，降低了部署成本和复杂性。最后，VXLAN通过VNI实现了有效的网络隔离，确保了多租户环境下的安全性和资源隔离。
VXLAN不仅在技术层面上带来了革新，还推动了网络管理模式的变革。通过结合SDN（软件定义网络）等技术，VXLAN实现了网络资源的动态配置和管理，提升了网络的灵活性和可管理性。这使得网络管理员可以更加高效地管理和优化网络资源，满足不断变化的业务需求。
VXLAN概述 VXLAN（Virtual Extensible LAN） 是一种网络虚拟化技术，由IETF（互联网工程任务组）提出，用于在大规模数据中心环境中实现虚拟网络的扩展和隔离。VXLAN利用隧道技术，通过在现有IP网络上建立虚拟隧道，将二层网络扩展到更大的范围，从而实现跨越多个物理网络的虚拟网络连接。
随着云计算、大数据和物联网等技术的发展，数据中心的规模和复杂性不断增加。传统的网络架构和虚拟化技术在面对这种大规模环境时，逐渐暴露出了一些问题：
 VLAN ID限制：传统的VLAN技术最多只能支持4096个VLAN ID，无法满足大规模数据中心中大量虚拟网络的需求。 网络隔离：在多租户环境中，需要更有效的网络隔离机制以确保安全性和资源隔离。传统VLAN的隔离方式在面对大规模租户时显得捉襟见肘。 跨域通信：随着数据中心的扩展，网络需要支持跨越多个物理域的虚拟网络连接，传统VLAN在实现这一点时复杂度较高且效率较低。 可扩展性：传统的二层网络在面对大规模扩展时，存在广播域过大、MAC地址表过载等问题，影响网络性能和稳定性。  VXLAN的诞生是为了应对上述问题，并提供一种更灵活、高效的网络虚拟化解决方案。
VXLAN通过以下方式解决了传统VLAN的局限性：
 大规模可扩展性：VXLAN引入了24位的VNI（VXLAN Network Identifier），可以支持多达1600万个虚拟网络，远远超过了VLAN的4096个ID限制。这使得VXLAN可以满足大规模数据中心中大量虚拟网络的需求。 有效的网络隔离：VXLAN通过VNI实现了更细粒度的网络隔离，确保多租户环境下的安全性和资源隔离。每个虚拟网络都有一个唯一的VNI，不同VNI之间的流量是隔离的。 跨域通信的简化：VXLAN利用现有的IP网络进行数据传输，通过在IP网络上建立虚拟隧道，实现了跨越多个物理网络的虚拟网络连接。这大大简化了跨域通信的复杂性，提高了网络的灵活性。 优化的二层网络扩展：VXLAN通过隧道技术，将二层网络封装在UDP报文中，通过IP网络传输。这不仅解决了传统二层网络的广播域过大和MAC地址表过载问题，还提高了网络的可扩展性和稳定性。  VXLAN的技术特点  基于隧道的封装技术：VXLAN利用UDP进行封装，通过在IP网络上建立虚拟隧道，实现二层网络的扩展和隔离。VXLAN报文由外部IP头、UDP头、VXLAN头和以太网帧组成，通过IP网络进行传输。 高效的跨域通信：VXLAN通过在现有IP网络上建立虚拟隧道，实现了跨越多个物理网络的虚拟网络连接。这使得数据中心可以在不同物理位置之间灵活地进行虚拟网络的部署和管理。 支持多租户环境：VXLAN通过VNI实现了有效的网络隔离，确保多租户环境下的安全性和资源隔离。每个租户可以拥有独立的虚拟网络，互不干扰。 兼容现有网络设备：VXLAN可以在现有的IP网络基础上运行，无需对底层网络进行大规模改造。大多数现代网络设备都支持VXLAN，通过简单的配置即可实现VXLAN功能。  VXLAN的核心组件   VXLAN隧道终结点（VTEP）：VTEP是VXLAN的关键设备，用于终结和建立VXLAN隧道。每个VTEP都有一个唯一的IP地址，负责将本地二层帧封装为VXLAN报文，并通过IP网络传输到远端VTEP。VTEP在VXLAN网络中扮演着重要角色，负责报文的封装和解封装。
  VXLAN网络标识符（VNI）：VNI是VXLAN网络的标识符，每个虚拟网络都分配一个唯一的VNI，用于区分不同的虚拟网络。VNI的长度为24位，可以支持多达1600万个虚拟网络。
  VXLAN的工作原理 VXLAN（虚拟扩展局域网）通过在现有IP网络上建立虚拟隧道，将二层网络扩展到更大的范围，实现跨越多个物理网络的虚拟网络连接。理解VXLAN的工作原理对于其部署和管理至关重要。
VXLAN架构 VXLAN的核心组件包括VXLAN隧道终结点（VTEP）和VXLAN网络标识符（VNI）。这些组件共同构建了VXLAN网络的基础架构。
VXLAN隧道终结点（VTEP） VTEP是VXLAN的关键设备，用于终结和建立VXLAN隧道。每个VTEP都有一个唯一的IP地址，负责将本地二层帧封装为VXLAN报文，并通过IP网络传输到远端VTEP。VTEP在VXLAN网络中扮演着以下重要角色：
  封装和解封装：VTEP将本地网络中的二层以太网帧封装为VXLAN报文，并在接收时解封装VXLAN报文，还原原始的以太网帧。
  隧道建立：VTEP通过IP网络建立VXLAN隧道，确保端到端的数据传输。
  VXLAN网络标识符（VNI） VNI是VXLAN网络的标识符，每个虚拟网络都分配一个唯一的VNI，用于区分不同的虚拟网络。VNI长度为24位，可以支持多达1600万个虚拟网络，这远远超过了传统VLAN的4096个ID限制。
VXLAN隧道与封装 VXLAN的工作原理基于隧道技术，将二层以太网帧封装在UDP报文中，通过IP网络进行传输。
VXLAN报文的结构如下：
 外部IP头：用于在IP网络中传输VXLAN报文。外部IP头包含源IP地址和目的IP地址，分别对应源VTEP和目的VTEP的IP地址。 UDP头：VXLAN使用UDP协议进行封装，默认使用端口4789。UDP头包含源端口和目的端口信息。 VXLAN头：VXLAN头包含VNI，用于标识虚拟网络。VXLAN头结构如下：   Flags：标志字段，指示VXLAN报文的属性。 VNI：VXLAN网络标识符，用于标识虚拟网络。  以太网帧：原始的二层以太网帧，包括目标MAC地址、源MAC地址和以太网负载。  以下是VXLAN封装和解封装的工作流程：
VXLAN封装流程  数据帧发送：源设备发送一个二层以太网帧到本地网络。 VTEP封装：本地VTEP接收到以太网帧，将其封装为VXLAN报文。封装过程包括添加VXLAN头、UDP头和外部IP头。 报文传输：封装后的VXLAN报文通过IP网络传输到目的VTEP。IP网络可以是任意支持IP协议的网络，包括局域网、广域网和互联网。  VXLAN解封装流程  报文接收：目的VTEP接收到VXLAN报文，检查报文的外部IP头和UDP头。 VTEP解封装：目的VTEP根据VXLAN头中的VNI，将VXLAN报文解封装，提取出原始的二层以太网帧。 数据帧转发：解封装后的以太网帧被转发到目的网络中的目标设备，实现端到端的二层通信。  VXLAN的数据平面和控制平面 VXLAN的工作原理可以分为数据平面和控制平面两个方面。
数据平面 数据平面负责实际的数据传输，包括VXLAN报文的封装、传输和解封装。在数据平面上，VTEP设备执行以下任务：
 封装以太网帧：将本地二层以太网帧封装为VXLAN报文。 传输VXLAN报文：通过IP网络传输封装后的VXLAN报文。 解封装以太网帧：接收并解封装VXLAN报文，提取出原始的以太网帧。  控制平面 控制平面负责VXLAN网络的管理和配置，包括VTEP设备之间的隧道建立、VNI的分配和维护、以及网络拓扑的管理。在控制平面上，常见的协议和技术包括：
  BGP EVPN（边界网关协议虚拟私有网络）：BGP EVPN是一种常用的控制平面协议，用于在VXLAN网络中分发MAC地址和VNI信息。通过BGP EVPN，VTEP设备可以相互学习网络拓扑和虚拟网络的配置信息，实现自动化和动态化的网络管理。
  SDN（软件定义网络）控制器：SDN控制器可以用于集中管理VXLAN网络，通过南向接口与VTEP设备通信，实现集中化的网络配置和监控。
  VXLAN的优势 VXLAN作为一种先进的网络虚拟化技术，具有以下主要优势：
 高可扩展性：VXLAN通过引入24位的VNI，可以支持多达1600万个虚拟网络，显著提高了网络的可扩展性。 灵活性和兼容性：VXLAN可以在现有的IP网络上运行，无需对底层网络进行大规模改造，降低了部署成本和复杂性。 有效的网络隔离：VXLAN通过VNI实现细粒度的网络隔离，确保多租户环境的安全性和资源隔离。 跨域通信的简化：VXLAN利用IP网络进行数据传输，通过在IP网络上建立虚拟隧道，实现了跨越多个物理网络的虚拟网络连接，简化了跨域通信的复杂性。  VXLAN与传统网络虚拟化技术的比较 VXLAN（虚拟扩展局域网）作为一种新兴的网络虚拟化技术，解决了传统网络虚拟化技术中的许多局限性。为了更好地理解VXLAN的优势和应用场景，下面将详细对比VXLAN与传统的网络虚拟化技术，特别是VLAN（虚拟局域网）。
VLAN的概述 VLAN是一种通过网络设备（如交换机）将一个物理网络划分为多个逻辑网络的技术。这种技术允许在同一物理网络中创建多个独立的虚拟网络，实现网络资源的隔离和管理。
VLAN通过在以太网帧中添加一个VLAN标识符（VLAN ID）来实现网络的划分。VLAN ID是12位的字段，可以支持最多4096个VLAN。每个VLAN代表一个独立的广播域，VLAN之间的通信需要通过三层设备（如路由器）进行。
VLAN的优点  网络隔离：VLAN可以将同一物理网络中的不同部门或应用隔离开来，提高网络的安全性和管理性。 简化网络管理：通过VLAN可以简化网络配置和管理，减少广播域的大小，提高网络性能。 灵活性：VLAN可以灵活地添加、删除或修改网络配置，适应不同的网络需求。  VLAN的局限性  可扩展性差：VLAN ID只有12位，最多只能支持4096个VLAN，这在大规模数据中心和云计算环境中是远远不够的。 跨域通信复杂：VLAN的广播域限制了跨越多个物理网络的虚拟网络连接，跨域通信需要复杂的配置和管理。 网络隔离不彻底：在多租户环境中，VLAN的隔离机制不足以提供高效的网络隔离和安全性。  VXLAN与VLAN的对比 VXLAN在设计上克服了VLAN的许多局限性，提供了更高效、更灵活的网络虚拟化解决方案。
可扩展性 VLAN：最多支持4096个VLAN，无法满足大规模数据中心的需求。VXLAN：引入24位的VNI，可以支持多达1600万个虚拟网络，显著提高了网络的可扩展性。
网络隔离 VLAN：基于VLAN ID进行网络隔离，隔离粒度较粗，无法满足多租户环境下的高效隔离需求。VXLAN：通过VNI实现细粒度的网络隔离，确保多租户环境的安全性和资源隔离。
跨域通信 VLAN：跨域通信需要复杂的配置和管理，无法高效地实现跨越多个物理网络的虚拟网络连接。VXLAN：利用IP网络进行数据传输，通过建立虚拟隧道实现跨域通信，简化了配置和管理。
部署和兼容性 VLAN：需要对底层网络设备进行支持和配置，部署和扩展较为复杂。VXLAN：可以在现有IP网络上运行，无需对底层网络进行大规模改造，大多数现代网络设备都支持VXLAN，部署更加灵活和简便。
性能和效率 VLAN：在大规模环境中，广播域过大和MAC地址表过载会影响网络性能和稳定性。VXLAN：通过隧道技术和UDP封装，将二层网络流量封装在IP报文中传输，减少了广播域的大小，提高了网络性能和稳定性。
适用场景 VLAN：适用于中小型网络或需要简单网络隔离的环境，如企业局域网、校园网等。VXLAN：适用于大规模数据中心、云计算环境和多租户环境，特别是需要高可扩展性和高效隔离的场景。
VXLAN的部署方法 VXLAN（虚拟扩展局域网）的部署涉及多个步骤和组件，包括网络设计、VTEP配置、控制平面协议的选择等。
在部署VXLAN之前，需要进行详细的规划和准备工作，以确保部署过程顺利且高效。准备工作包括以下几个方面：
 网络拓扑设计：规划数据中心的网络拓扑结构，确定VTEP的位置和数量，以及IP网络的设计。 设备支持检查：确保现有网络设备（如交换机、路由器、服务器）支持VXLAN功能，并且具备相应的硬件和软件版本。 控制平面协议选择：选择适合的数据中心环境的控制平面协议，如BGP EVPN（边界网关协议虚拟私有网络）或SDN（软件定义网络）控制器。 地址规划：规划VNI（VXLAN网络标识符）和IP地址分配方案，确保网络资源的合理利用和管理。  VTEP是VXLAN的核心组件，负责VXLAN报文的封装和解封装。VTEP的配置主要包括以下步骤：
 创建VTEP接口：在支持VXLAN的设备上创建VTEP接口，并分配唯一的IP地址。 配置VXLAN隧道：在VTEP设备上配置VXLAN隧道，指定远端VTEP的IP地址和对应的VNI。 配置VNI与VLAN的映射：将VNI映射到本地的VLAN，确保本地网络流量能够正确封装为VXLAN报文。  以下是一个典型的VTEP配置示例：
interface VTEP1 ip address 192.168.1.1/24 vxlan encapsulation vxlan vni 10001 remote vtep 192.168.1.2 vlan 10 在这个示例中，VTEP1接口的IP地址为192.168.1.1，使用VNI 10001与远端VTEP（192.168.1.2）建立VXLAN隧道，并将本地的VLAN 10映射到VNI 10001。
控制平面协议的配置 控制平面协议负责VXLAN网络的管理和配置，包括MAC地址的学习和分发、VNI的分配等。常用的控制平面协议包括BGP EVPN和SDN控制器。
BGP EVPN是一种常用的VXLAN控制平面协议，通过BGP（边界网关协议）分发VXLAN网络中的MAC地址和VNI信息。以下是BGP EVPN的配置步骤：
 启用BGP：在VTEP设备上启用BGP，并配置BGP邻居。 配置EVPN地址族：在BGP配置中启用EVPN地址族，指定VNI和对应的VTEP。 分发MAC地址和VNI信息：通过BGP EVPN分发VXLAN网络中的MAC地址和VNI信息，实现VTEP之间的网络拓扑学习。  以下是一个BGP EVPN配置示例：
router bgp 65000 neighbor 192.168.1.2 remote-as 65000 address-family l2vpn evpn advertise-all-vni advertise-macip exit-address-family exit 在这个示例中，配置了BGP邻居192.168.1.2，启用了EVPN地址族，并分发所有VNI和MAC地址信息。
SDN控制器是一种集中化的网络管理工具，通过南向接口与VTEP设备通信，实现集中化的VXLAN网络配置和管理。以下是SDN控制器的配置步骤：
 部署SDN控制器：在数据中心中部署SDN控制器，确保其能够与所有VTEP设备通信。 配置南向接口：在SDN控制器上配置南向接口，与VTEP设备建立通信连接。 集中化管理和配置：通过SDN控制器实现VXLAN网络的集中化管理和配置，包括VNI分配、VXLAN隧道建立和MAC地址学习等。  验证和测试 在完成VXLAN的配置后，需要进行验证和测试，以确保网络的正常运行和性能。验证和测试包括以下几个方面：
 连通性测试：验证VTEP之间的连通性，确保VXLAN隧道的正常建立和数据传输。 VNI映射验证：验证VNI与VLAN的映射，确保本地网络流量能够正确封装和解封装为VXLAN报文。 性能测试：进行网络性能测试，评估VXLAN网络的延迟、带宽和可靠性。 故障排除：在部署过程中遇到问题时，通过日志、监控工具和故障排除方法进行排查和解决。  VXLAN的应用场景 VXLAN（虚拟扩展局域网）作为一种先进的网络虚拟化技术，具有高可扩展性、灵活性和高效的网络隔离特性，因此在现代数据中心、云计算环境和企业网络中得到了广泛应用。
数据中心的网络虚拟化 在现代数据中心中，虚拟化技术已经成为提高资源利用率和灵活性的重要手段。VXLAN通过在IP网络上建立虚拟隧道，将二层网络扩展到更大的范围，实现了跨越多个物理网络的数据中心网络虚拟化。
 多租户环境：在云服务提供商的数据中心，多个租户共享同一物理基础设施。VXLAN通过VNI实现租户间的网络隔离，确保不同租户的网络流量互不干扰，提高安全性。 虚拟机迁移：在数据中心中，虚拟机的动态迁移是常见需求。VXLAN通过扩展二层网络，使得虚拟机可以在不同物理服务器之间自由迁移，而无需改变其IP地址和网络配置。 大规模数据中心：传统VLAN的ID数量有限，无法满足大规模数据中心的需求。VXLAN通过24位的VNI支持多达1600万个虚拟网络，适用于大规模数据中心的网络虚拟化。  云计算环境中的应用 随着云计算的快速发展，数据中心网络面临着越来越高的扩展性和灵活性需求。VXLAN在云计算环境中的应用，主要体现在以下几个方面：
 弹性扩展：云计算平台需要根据需求动态分配计算和存储资源。VXLAN通过虚拟网络的弹性扩展，使得资源可以灵活调度和分配，提高资源利用率。 混合云和多云部署：企业越来越多地采用混合云和多云策略。VXLAN通过在不同云环境之间建立虚拟隧道，实现了跨云的网络连接和数据流动，简化了混合云和多云部署的网络管理。 自动化网络配置：云计算平台通常使用自动化工具进行资源管理和配置。VXLAN的配置和管理可以通过API和控制平面协议（如BGP EVPN）实现自动化，提高网络管理的效率和灵活性。  企业网络的应用 除了数据中心和云计算环境，VXLAN在企业网络中的应用也越来越广泛，主要体现在以下几个方面：
 分支机构互联：企业通常在不同地理位置有多个分支机构。VXLAN通过在IP网络上建立虚拟隧道，实现了分支机构之间的虚拟网络连接，确保数据的安全传输和网络的统一管理。 远程办公支持：随着远程办公的普及，企业需要为远程员工提供安全可靠的网络连接。VXLAN通过在远程办公设备和企业内部网络之间建立虚拟隧道，实现了远程办公的无缝接入和安全通信。 安全隔离：企业网络中不同部门和应用需要进行网络隔离，以确保安全性和资源独立性。VXLAN通过VNI实现细粒度的网络隔离，满足企业对网络安全和管理的要求。  物联网（IoT）环境中的应用 物联网设备的快速增长给网络带来了新的挑战，包括设备数量庞大、网络流量复杂以及安全性要求高等。VXLAN在物联网环境中的应用，主要体现在以下几个方面：
 大规模设备连接：物联网设备数量庞大，传统网络虚拟化技术难以满足其扩展性需求。VXLAN通过24位的VNI支持多达1600万个虚拟网络，可以满足大规模物联网设备的连接需求。 设备隔离和安全：物联网设备通常分布在不同的物理位置，并且有不同的安全需求。VXLAN通过VNI实现设备之间的隔离，确保不同设备组的网络流量互不干扰，提高网络安全性。 动态网络配置：物联网设备的网络需求动态变化，传统静态配置难以适应。VXLAN的配置和管理可以通过自动化工具和控制平面协议实现动态调整，满足物联网环境的灵活性需求。  高性能计算（HPC）环境中的应用 高性能计算环境通常需要高速网络连接和大规模数据传输。VXLAN在高性能计算环境中的应用，主要体现在以下几个方面：
 高速网络连接：高性能计算节点之间需要高速、低延迟的网络连接。VXLAN通过在IP网络上建立虚拟隧道，实现了高速的二层网络扩展，满足高性能计算的网络需求。 数据传输优化：高性能计算环境中的大规模数据传输对网络性能要求极高。VXLAN通过隧道技术和UDP封装，提高了数据传输的效率和稳定性。 灵活的网络拓扑：高性能计算环境需要根据计算任务的需求灵活调整网络拓扑。VXLAN通过虚拟网络的灵活配置，实现了高性能计算环境中网络拓扑的动态调整和优化。  总结 VXLAN（Virtual eXtensible Local Area Network）之所以被视作彻底改变了网络虚拟化，主要是因为它解决了传统网络虚拟化技术如VLAN的一些根本性限制，并引入了新的网络架构和操作模式，从而极大地增强了网络的灵活性、可扩展性和效率。
传统VLAN技术最多支持4094个VLAN标识符（VLAN ID），这在小型网络中可能足够，但在大规模数据中心或云环境中则显得捉襟见肘。VXLAN通过使用24位的VXLAN Network Identifier (VNI)，可以支持高达16777216个逻辑网络，极大地提升了网络的可扩展性。
VLAN受限于二层网络，当虚拟机跨越不同物理网络或子网时，传统的VLAN无法直接支持它们之间的通信。VXLAN通过在网络层（三层）之上封装二层帧，实现了跨三层网络的虚拟机通信，允许虚拟机在不同的物理位置之间自由迁移而不必更改其IP地址或MAC地址。
由于VXLAN可以在现有的三层网络基础设施上运行，它减少了对专用硬件的需求，简化了网络设计，并且不需要对每个VLAN进行单独的路由配置。这降低了网络的复杂性，同时也减轻了网络管理员的负担。
每个VXLAN网络都可以视为独立的二层隔离域，这意味着即使在共享的物理网络上，不同的VXLAN网络也能保持完全的隔离。这为租户提供了更好的安全性和隐私保护。
通过VXLAN，数据中心可以更高效地利用物理网络资源，减少对昂贵的网络设备升级的需求，同时支持更多的虚拟网络，从而提高了整体的资源利用率和成本效益。
VXLAN网络通常与软件定义网络（SDN）控制器协同工作，这使得网络配置和虚拟网络的创建、修改或删除可以自动化完成，进一步提高了网络的敏捷性和响应速度。
]]></content>
  </entry>
  
  <entry>
    <title>MOS管烧了的原因</title>
    <url>/post/hardware/the-reason-why-the-MOS-tube-burned-out.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>PCB</tag>
      <tag>Layout</tag>
    </tags>
    <content type="html"><![CDATA[MOS 管可能会遭受与其他功率器件相同的故障，例如过电压（半导体的雪崩击穿）、过电流（键合线或者衬底熔化）、过热（半导体材料由于高温而分解）。
更具体的故障包括栅极和管芯其余部分之前的极薄氧化物击穿，这可能发生在相对于漏极或者源极的任何过量栅极电压中，可能是在低至10V-15V 时发生，电路设计必须将其限制在安全水平。
还有可能是功率过载，超过绝对最大额定值和散热不足，都会导致MOS管发生故障。
接下来就来看看所有可能导致失效的原因。
过电压 MOS管对过压的耐受性非常小，即使超出额定电压仅几纳秒，也可能导致设备损坏。
MOS管的额定电压应保守地考虑预期的电压水平，并应特别注意抑制任何电压尖峰或振铃。
长时间电流过载 由于导通电阻相对较高，高平均电流会在MOS管中引起相当大的热耗散。
如果电流非常高且散热不良，则MOS管可能会因温升过高而损坏。
MOS管可以直接并联以共享高负载电流。
瞬态电流过载 持续时间短、大电流过载会导致MOS管器件逐渐损坏，但是在故障发生前MOS管的温度几乎没有明显升高，不太能察觉出来。（也可以看下面分析的直通和反向恢复部分）
击穿（交叉传导） 如果两个相对MOS管的控制信号重叠，则可能会出现两个MOS管同时导通的情况，这会使电源短路，也就是击穿条件。
如果发生这种情况，每次发生开关转换时，电源去耦电容都会通过两个器件快速放电，这会导致通过两个开关设备的电流脉冲非常短但非常强。
通过允许开关转换之间的死区时间（在此期间两个MOS管均不导通），可以最大限度地减少发生击穿的机会，这允许一个MOS管在另一个MOS管打开之前关闭。
没有续流电流路径 当通过任何电感负载（例如特斯拉线圈）切换电流时，电流关闭时会产生反电动势。在两个开关设备都没有承载负载电流时，必须为此电流提供续流路径。
该电流通常通过与每个开关器件反并联连接的续流二极管安全地引导回电源轨道。
当MOS管用作开关器件时，工程师可以简单获得MOS管固有体二极管形式的续流二极管，这解决了一个问题，但创造了一个全新的问题&hellip;&hellip;
MOS管体二极管的缓慢反向恢复 诸如特斯拉线圈之类的高 Q 谐振电路能够在其电感和自电容中存储大量能量。
在某些调谐条件下，当一个MOS管关闭而另一个器件打开时，这会导致电流“续流”通过 MOS管的内部体二极管。
这个原本不是什么问题，但当对面的MOS管试图开启时，内部体二极管的缓慢关断（或反向恢复）就会出现问题。
与MOS管 自身的性能相比，MOS管 体二极管通常具有较长的反向恢复时间。如果一个 MOS管的体二极管在对立器件开启时导通，则类似于上述击穿情况发生“短路”。
这个问题通常可以通过在每个MOS管周围添加两个二极管来缓解。
首先，肖特基二极管与MOS管源极串联，肖特基二极管可防止MOS管体二极管被续流电流正向偏置。其次，高速（快速恢复）二极管并联到MOS管/肖特基对，以便续流电流完全绕过MOS管和肖特基二极管。
这确保了MOS管体二极管永远不会被驱动导通，续流电流由快恢复二极管处理，快恢复二极管较少出现“击穿”问题。
过度的栅极驱动 如果用太高的电压驱动MOS管栅极，则栅极氧化物绝缘层可能会被击穿，从而导致MOS管无法使用。
超过 +/- 15 V的栅极-源极电压可能会损坏栅极绝缘并导致故障，应注意确保栅极驱动信号没有任何可能超过最大允许栅极电压的窄电压尖峰。
栅极驱动不足（不完全开启） MOS管只能切换大量功率，因为它们被设计为在开启时消耗最少的功率。工程师应该确保MOS管硬开启，以最大限度地减少传导期间的耗散。
如果MOS管未完全开启，则设备在传导过程中将具有高电阻，并且会以热量的形式消耗大量功率，10到15伏之间的栅极电压可确保大多数MOS管完全开启。
缓慢的开关转换 在稳定的开启和关闭状态期间耗散的能量很少，但在过渡期间耗散了大量的能量。因此，应该尽可能快地在状态之间切换以最小化切换期间的功耗。由于MOS管栅极呈现电容性，因此需要相当大的电流脉冲才能在几十纳秒内对栅极进行充电和放电，峰值栅极电流可以高达一个安培。
杂散振荡 MOS管 能够在极短的时间内切换大量电流，输入也具有相对较高的阻抗，这会导致稳定性问题。在某些条件下，由于周围电路中的杂散电感和电容，高压MOS管会以非常高的频率振荡。（频率通常在低 MHz），但这样是非常不受欢迎的，因为它是由于线性操作而发生的，并且代表了高耗散条件。
这种情况可以通过最小化MOS管周围的杂散电感和电容来防止杂散振荡，还应使用低阻抗栅极驱动电路来防止杂散信号耦合到器件的栅极。
“米勒”效应 MOS管在其栅极和漏极端子之间具有相当大的“米勒电容”。在低压或慢速开关应用中，这种栅漏电容很少引起关注，但是当高压快速开关时，它可能会引起问题。
当底部器件的漏极电压由于顶部MOS管的导通而迅速上升时，就会出现潜在问题。
这种高电压上升率通过米勒电容电容耦合到MOS管的栅极，会导致底部MOS管的栅极电压上升，从而导致MOS管也开启，就会存在击穿情况，即使不是立即发生，也可以肯定MOS管故障。
米勒效应可以通过使用低阻抗栅极驱动器来最小化，该驱动器在关闭状态时将栅极电压钳位到 0 伏，这减少了从漏极耦合的任何尖峰的影响。在关断状态下向栅极施加负电压可以获得进一步的保护。例如，向栅极施加 -10 V电压将需要超过12V的噪声，以冒开启本应关闭的MOS管的风险。
对控制器的辐射干扰 想象一下，将 1pF 的电容从你的火花特斯拉线圈的顶部连接到固态控制器中的每个敏感点的效果，存在的数百千伏射频可以毫无问题地驱动大量电流通过微型电容器直接进入控制电路。
如果控制器没有放置在屏蔽外壳中，这就是实际会发生的情况。
控制电路的高阻抗点几乎不需要杂散电容即可导致异常操作，但运行不正常的控制器可能会尝试同时打开两个相反的MOS管 ，控制电子设备的有效射频屏蔽至关重要。
分离电源和控制电路也是非常理想的，电源开关电路中存在的快速变化的电流和电压仍然具有辐射显着干扰的能力。
对控制器的传导干扰 大电流的快速切换会导致电源轨上的电压骤降和瞬态尖峰。如果电源和控制电子设备共用一个或多个电源轨，则可能会对控制电路产生干扰。
良好的去耦和中性点接地是应该用来减少传导干扰影响的技术。作用于驱动MOS管的变压器耦合在防止电噪声传导回控制器方面非常有效。
静电损坏 安装MOS管或IGBT器件时，应采取防静电处理措施，以防止栅氧化层损坏。
高驻波比 这里要着重说一下，来自一位专业射频工程师的解释。
在脉冲系统中，VSWR不像在CW系统中那么大，但仍然是一个问题。
在CW系统中，典型的发射器设计用于50欧姆的电阻输出阻抗。工程师通过某种传输线连接到负载，希望负载和线路也是50欧姆，并且电力沿电线很好地流动。
但如果负载阻抗不是50欧姆，那么一定量的功率会从阻抗不连续处反射回来。但反射功率会导致几个潜在问题：
 发射器看起来像一个负载并吸收了所有的功率，这不是一个好的现象。  例如，你的放大器效率为80%，你输入的功率1KW，通常情况下，设备的功耗为200W，最终的功耗为800W，如果所有800W的功耗都被反射回来，忽然之间，这些设备就需要消耗全部的功耗。
前向波和反射波的组合会在传输线中产生驻波，在相距1/2波长的点处会变得非常高，从而导致击穿或者其他不良情况，这本质上是表现负载阻抗不是预期的结果。  如果你有一个射频电源在几十兆赫兹，你可以装配一个开放的平行线传输线，在脉冲系统中，你可能会遇到沿线路传播的脉脉冲、阻抗不连续性、反射回以及与发送的下一个脉冲相加的问题。
反射脉冲是相同极性还是不同极性取决于距离和相对阻抗。
如果你有几个不匹配，可能会得到很多来回移动的脉冲，这些脉冲会加强或者取消。这个对于商业配电来说是一个真正的大问题，因为沿线路的传播时间是线路频率周期的很大一部分，当断路器打开和关闭以及雷击时会引起问题。
以上就是关于MOS管烧毁的原因分析。
]]></content>
  </entry>
  
  <entry>
    <title>电容触摸的基本知识与原理</title>
    <url>/post/hardware/basic-knowledge-and-principles-of-capacitive-touch.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>PCB</tag>
      <tag>Layout</tag>
    </tags>
    <content type="html"><![CDATA[电容式触摸感应，是一种通过电容的变化来检测手指接近或触及触摸表面的技术。
概述 通过电容式感应，机械开关和旋钮可替换为外观雅致的按钮、滑条和滚轮，以解决：
 长时间使用后磨损和可靠性降低 前面板与按键之间存在缝隙，容易被水分渗透，而引起不良 需施加力度才能触发 前面板开孔会一定程度上增加成本 按键形状较为固定  基本原理 常见的电容触摸传感器如图2-1所示，一般以PCB上的覆铜作为电极。结构上，顶层会覆盖非导电性的防护层，如玻璃或塑料，利用胶水和PCB粘连。另外传感器周围会覆有网格地。
基于所检测电容的类型，电容触摸可分为自感型电容检测（检测单电极和地之间的电容值），互感型电容检测（检测双电极之间的电容值）。
自感型电容检测 以最简单的单按键为例，自感型电容的检测示意图如图 2-2 所示，检测模型如图 2-3 所示。自感型电容利用覆铜形成的单电极（接收电极Rx），来检测电极对地的电容变化。按键对地的初始电容为Cp，当人手触摸时，会给整个环路引入 Ct，Ch 与 Cg，从而使按键的对地电容增大。
说明：实线表示实际走线，虚线表示非实际走线。灰色元器件表示等效电容或电阻。
Rh: 人体电阻。 Rs: 串联电阻，推荐值为 470Ω。 Cp: 按键与所连导线的对电源地寄生电容。 Cg: 电源地与大地之间的电容。对于电池应用，大约为 1pF。对于接地应用为短路。 Ch: 人体与大地之间的串联电容。 Ct: 电级与人指尖形成的电容，类似于平板电容器结构。 Cd: 人手与电源地形成的电容。
为便于分析，忽略 Rh，Rs 的影响。按键对地的等效电容如公式 1-1 所示。灵敏度可以表征为触摸产生的电容变化与基础电容之间的比，如公式 1-1 所示。其中由于 Ch 较于 Cg 和 Ct 较大，因此可忽略。在地平面较稀时，𝐶𝑑较小，因此𝐶𝑔 + 𝐶𝑑可约等于𝐶𝑔。
A: 手指与传感器垫片覆盖层的接触面积。 d: 覆盖层的厚度。 ε0: 空气介电常数。 εr: 覆盖层的介电常数。
由公式 1-2 和 1-3 可知，提高灵敏度的方法有：
1）减小盖板的厚度，提高盖板的𝜀𝑟，从而提高𝐶𝑡； 2）减小网格地的密度，或增加 PCB 的厚度，从而降低𝐶𝑝； 3）由于𝐶𝑡与𝐶𝑔数量级相同，合理的将电源地与大地相连从而增加𝐶𝑔； 4）合理的增大电极的面积，通过提高手指与传感器垫片覆盖层的接触面积 A 来提高 Ct。
要注意，无法通过无限增大电极的方式来增加灵敏度。主要因为平行板电容 Ct 的最大有效面积与手指触摸面积相同，另外过大的电极面积无法增加触摸信号强度，反而会增加𝐶𝑝，导致灵敏度降低。
互感型电容检测 如图 2-4 所示，互感型电容利用覆铜形成的双电极（接收电极 Rx，发送电极 Tx）来检测两电极之间电容的变化。互感型电容检测的最大特点是可以忽略按键对电源地的寄生电容 Cp 的影响。以最简单的单按键为例，互感型电容的检测模型如图 2-5 所示。当人手触摸时，CRT 变成两个 2CRT，同时引入 CRTt，Ct，Ch 与 Cg。最终使双电极间的电容减小。
说明：实线表示实际走线，虚线表示非实际走线。灰色元器件表示等效电容或电阻。
CRTt: 手指触摸引入的Rx和Tx电极之间的并联电容。 CRT: Rx和Tx电极之间的电容，当人手触摸时，等效分为两个容值2CRT的电容。 Tx 与 Rx 之间的等效电容如公式1-4所示。灵敏度可以表征为触摸产生的电容变化与基础电容之间的比，如公式 1-5 所示。
对于互感触摸，提高灵敏度的主要方式为：
1）降低覆层的厚度； 2）增大 Tx 和 Rx 之间的间距。要注意虽然增大 Tx 和 Rx 之间的间距能够减小𝐶𝑅𝑇，提高检测距离，变相地提高灵敏度，但如果手指无法同时覆盖 Tx与 Rx，灵敏度反而会减小。
一般来说，对于自感与互感型电容检测，手指触摸产生的电容变化均在 1pF 左右。但自感的 base 电容（触摸前的电容值）一般会高于互感的 base 电容。因此相对来说互感的灵敏度更高，但也更易受噪声的影响。
从应用的角度来看，自感型方案由于结构简单使用的更广，而互感型方案更多的用于矩阵按键，以使支持的按键数远超过电容触摸的 IO 口数（自感型按键数）。两种方案之间的比较如表 2-1 所示。
TI 的电容触摸感应技术 CapTIvate™以电荷转移采集为基础。该原理包括：
1）将传感器电容𝐶𝑒𝑞𝑢𝑎𝑙充电； 2）将累积电荷转移至内部采样电容𝐶𝑠𝑎𝑚𝑝𝑙𝑒 ，这两部分。
此过程将不断重复，直至 𝐶𝑠𝑎𝑚𝑝𝑙𝑒 两侧电压达到内部比较器的触发电压𝑉𝑡𝑟𝑖𝑝。达到阈值所需的电荷转移次数直接表征𝐶𝑒𝑞𝑢𝑎𝑙的大小。当电容传感器被人手触摸时，𝐶𝑒𝑞𝑢𝑎𝑙发生改变，这意味着 𝐶𝑠𝑎𝑚𝑝𝑙𝑒 电压达到 𝑉𝑡𝑟𝑖𝑝所需的电荷转移次数发生了变化，MCU 通过比较前后电荷转移次数的差异，来感知触摸事件的发生。MSP430 内部采用电流镜来控制𝐶𝑠𝑎𝑚𝑝𝑙𝑒 的输入电流和𝐶𝑒𝑞𝑢𝑎𝑙的放电电流之间的比例关系，以此来等效放大𝐶𝑠𝑎𝑚𝑝𝑙𝑒 ，从而拥有较大的量程。
对于自感检测，传感器电容𝐶𝑒𝑞𝑢𝑎𝑙等于 Tx I/O 口的对地电容，通过对地的充放电，将𝐶𝑒𝑞𝑢𝑎𝑙中的电荷转移到内部的𝐶𝑠𝑎𝑚𝑝𝑙𝑒 中，如图 2-6 所示。互感检测的传感器电容𝐶𝑒𝑞𝑢𝑎𝑙等于 Tx 与 Rx 两 I/O 之间的互电容。互感检 测的电路结构相比自感检测更为复杂，但实际上也是对地充放电。通过保持充放电前后对地电容两端电压不变的方式，来实现仅对互电容的电荷转移。
]]></content>
  </entry>
  
  <entry>
    <title>OpenBMC: coredump自动发现及调试实践</title>
    <url>/post/programming/OpenBMC-coredump-automatic-discovery-and-debugging-practice.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>OpenBMC</tag>
    </tags>
    <content type="html"><![CDATA[BMC（Baseboard Management Controller）是一种服务器上的独立微控制器，BMC 在提供 7*24 的硬件监控和管理功能的同时，还承担着诸多关键职责，为整个系统的可靠性、管理性提供了基础支持。
背景 BMC 介绍 关键职责和功能：
 硬件监控与诊断：BMC 负责监控计算机硬件的状态，包括处理器、内存、存储、风扇、温度传感器等关键组件。通过实时监测，提供各组件的实时状态信息。 散热管理：BMC 负责监控所有的部件板卡的部件温度， 动态的调整服务器系统的风扇转速。 远程管理与控制：BMC 具备远程管理功能，允许运维人员无需物理接触硬件即可对系统进行监控和管理。通过 KVM ，查看和控制 server， 运维人员可以执行如开关机、重启、甚至是 OS 装机。 事件日志与报警：BMC 记录系统事件日志，其中包括硬件错误、警告和其他重要事件。运维人员可以通过检查事件日志及时发现潜在问题。 固件更新：BMC 提供服务器上大多数固件的远程更新， 包含 BIOS ， 多种 CPLD ， 电源， Retimer ， Box等&hellip;  OpenBMC 简单介绍 OpenBMC 项目的目标是一个开放、灵活、可定制的开源BMC解决方案, 主要基于 Yocto 作为构建组件及 Systemd 和 D-Bus 作用应用层的基础组件。
痛点 在 7*24 小时监控的环境中，我们提供多种服务并应对各种硬件配置的挑战，在安全且稳定的基础上提供高效服务。在这个复杂的情境中，我们不可避免地面临着在线上极低概率下产生 coredump 的可能性，通常这些 coredump 会带来一些不可预测的后果。同时还会遇到一些难以复现的问题，这些问题会消耗大量的人力和时间成本，而且这些问题可能发生在无法直接访问的业务机器上。
coredump 自动发现及调试实践 面对故障无法及时发现、日志无法及时收集以及复现环境成本可能过高等痛点，STE 团队提出了一套集成化流程，即 coredump 感知、收集、上报以及自动化分析。这一流程的实施解决了上面提到的问题。通过该流程，我们能够更快速地检测故障，实时收集相关日志，并在需要时重现环境，大大降低了故障排查的难度和成本。整体流程的图示如下：
感知   systemd-coredump 是一个 systemd 提供的一个 daemon，用于配置 linux coredump 和监听 coredump 的生成。它通过感知异常终止的进程并收集相关的 coredump 文件。
  内部一个 daemon（debug-collector） ， 用于 watch coredump 产生， 主动触发 coredump 收集流程。
  收集 coredump 在 BMC 固件内部， 监听到 coredump 事件产生之后， 会主动发起日志收集流程。主要包含工作包含：
日志收集  coredump 文件：这是最关键的信息，包含了异常终止进程的内存快照。 Journal 日志：包括核心转储进程的 PID 以及与其相关的所有系统日志。这有助于全面了解故障上下文。 os-release 信息：指示固件版本，为问题溯源提供环境信息。 其他日志：包括系统和应用程序的其他关键日志，为问题定位提供更多上下文。  日志上传 等待所有日志收集完成， 打包所有日志并推送指定的服务器， 并标记日志为 coredump。
日志告警 服务器接收到日志， 如果日志为 coredump， 将日志信息推送到告警平台。
如何 debug （Offline） 准备 rootfs Legacy Solution：
 git clone 代码并且切换 coredump 发生的版本上。 每次可能需要现场编译， 每次固件可能需要较长时间也可能因为编译环境的缺失/版本不兼容原因耗费一部分时间。 根据 coredump 异常的组件， 查找依赖， 手动拷贝相关的 debug 依赖的文件。  Current Solution：
 IPK 包（ipk）是一种用于嵌入式 Linux 系统的软件包格式，通常用于嵌入式设备。这种软件包格式是用于轻量级 Linux 发行版的一种标准，具有简单、紧凑、易于管理的特点。 Ipk/opkg 之于 debian 相当于 deb/apt 的关系。 可以用 opkg 命令进行安装  将所有的包（包含 debug/src 包）准备为独立的 ipk 包:
https://subscription.packtpub.com/book/iot-and-hardware/9781785281952/5/ch05lvl2sec44/ipk-packages Yocto 提供一种能力， 使得我们可以将所有的包（包含 debug/src 包）准备为独立的 ipk 包。并且可能根据编译依赖， 生成正确的依赖分析， 使得安装时能够自动安装依赖包。
$ bitbake package-index 随后可以正确的在 Yocto 目录 $BUILDDIR/tmp/deploy/ipk下找到对应的 ipk 包。
 每次编译将所有 ipk 包 推送至远程文件服务器中， 并持久化存储。 将上述工作集成中 CI 中。  启动 debug 使用 GDB + 准备好的 rootfs + core file。
自动化 基于以上方案， 我们已经可以在本地环境， 使用一个集成的 debug_dump 的脚本集成 debug 工具。
 收到告警信息后， 包含 coredump 日志 所在的 日志 URI， 可以下载指定的日志地址。 根据 coredump 日志包， 获取指定版本，待调试的 core 文件。 根据 coredump 产生所属的 binary 文件， 分析所来源的 ipk 包。 通过 opkg 命令， Install 指定的 ipk 包， 生成待调试的 rootfs 启动 gdb 并调试  Example:
~ ./debug_dump.py -u https://&lt;path/to/your/ipk/source&gt;/bmc_dump/obmcdump_coredump_22_67.tar.gz INFO:debug_dump:Found core execfn /lib/systemd/systemd-journald INFO:debug_dump:Downloading from https://&lt;path/to/your/ipk/source&gt;/bmc_ipk/ipks.tar to /tmp/ipkdbg_n_zlxpd3/ipks.tar ... Installing systemd (250.3) on root. Downloading file:///tmp/ipkdbg_n_zlxpd3/./ipk/arm1176jzs/systemd_250.3-r0_arm1176jzs.ipk. ... Core was generated by `/lib/systemd/systemd-journald&#39;. Program terminated with signal SIGABRT, Aborted. #0 __pthread_kill_internal (threadid=&lt;optimized out&gt;, signo=6) at pthread_kill.c:45 45 pthread_kill.c: No such file or directory. #0 __pthread_kill_internal (threadid=&lt;optimized out&gt;, signo=6) at pthread_kill.c:45 #1 0x76be7fd0 in __GI_raise (sig=sig@entry=6) at ../sysdeps/posix/raise.c:26 #2 0x76bd2428 in __GI_abort () at abort.c:79 .... 接下来是进一步提高自动化的一些方法：将告警 bot 和 debug_dump 联动起来。
 Dump 分析服务：此服务接受 coredump 的日志链接， 接受到链接之后会执行 debug_dump 脚本进行分析。 告警 Bot: 生成 Dump 分析服务的一键到达的链接， 展示在相关的卡片中， 用户点击相关链接即可一键分析。 WebUI: 用户可点击一键分析链接之后，即可在浏览器中打开WebUI，自动执行debug_dump，并可在webshell中直接进行gdb分析。  总结 通过这套流程，我们显著提升了 coredump 处理效率，减少了人力和时间成本，大幅提高了故障排查的速度和准确性。未来，我们将继续优化这一流程，确保在复杂的线上环境中提供高效稳定的服务, 我们未来的改进方向包括以下:
 智能化分析：引入机器学习算法，对历史 coredump 数据进行分析，提前预测潜在问题，进一步提升系统的稳定性和预防性维护能力。 扩展自动化工具集：开发更完善的自动化调试工具，支持更多类型的故障和环境。 实时监控与响应：提升实时监控能力，确保在第一时间发现并处理coredump事件，减少系统宕机时间，保障业务连续性。 社区合作与反馈：积极参与 OpenBMC 社区，与其他开发者共享我们的经验和改进建议，吸收社区反馈，持续优化我们的解决方案。  通过这些改进，我们相信能够进一步提高 OpenBMC 固件的稳定性和可靠性，为用户提供更优质的服务体验。同时，我们也期待与业界同行深入交流合作，共同推动 BMC 技术的发展与创新。
]]></content>
  </entry>
  
  <entry>
    <title>大厂加速自研AI芯片：Nvidia主导地位受到挑战</title>
    <url>/post/hardware/nvidias-dominance-in-ai-chips-challenged-by-big-tech-companies.html</url>
    <categories><category>AI</category>
    </categories>
    <tags>
      <tag>GenAI</tag>
      <tag>Nvidia</tag>
      <tag>GPU</tag>
      <tag>OpenAI</tag>
    </tags>
    <content type="html"><![CDATA[随着生成式 人工智能  （GenAI）热潮的兴起，大型科技公司对全球领先的高端图形处理器（GPU）制造商Nvidia的依赖性日益凸显。人工智能解决方案对专用芯片的需求激增助长了这种依赖。然而，这种依赖局面即将被打破。
科技巨头如亚马逊、微软、谷歌和Meta正逐渐通过开发自家的AI芯片来降低对Nvidia的依赖。2023年9月，亚马逊宣布对人工智能研究公司Anthropic投资40亿美元，旨在加速其未来基础模型的研发，并推动AWS客户广泛应用这些模型。
据悉，Anthropic已同意在其人工智能解决方案中采用亚马逊设计的AI芯片，这也是其获得这项巨额投资的关键因素之一。这一举措明确表明，亚马逊已在不断升级的人工智能竞赛中站稳脚跟。
Meta Platforms，即Facebook的母公司，也正计划打造一款定制芯片，以推动其AI技术的发展，这家社交媒体领域的巨头正不断加强其计算资源，以支撑耗电且性能要求高的GenAI产品。
随着Meta将其AI技术更广泛地整合到其社交平台中，对专用芯片的需求也随之增长。通过成功部署定制芯片，Meta有望在AI芯片采购和能源消耗方面节省数亿美元的开支。
与此同时，其他科技巨头也在积极发展自身的计算技术。微软已经宣布推出其首款专为模型训练设计的定制AI芯片。谷歌的芯片设计团队也在利用谷歌服务器上运行的DeepMind AI来开发AI处理器。
根据彭博社的报道，OpenAI的首席执行官Sam Altman正致力于筹集数十亿美元资金用于建立人工智能芯片工厂，Altman一直在与全球投资者以及某顶级但未公开的芯片制造商进行接洽，以制造专门用于AI的芯片。
根据市场研究机构Gartner的预测，到2027年，全球人工智能芯片市场的规模将达到1400亿美元，这些芯片是聊天机器人和其他AI系统的核心需求，Nvidia正是凭借其在设计和制造专用芯片方面的专业技术，从而在AI芯片市场中占据了领导地位。尽管大型科技公司对Nvidia进行了大量投资，但由于这家芯片制造商并没有跟上市场需求，这也是推动这些科技巨头自主研发AI芯片的主要原因。
Nvidia的H100等备受欢迎的人工智能芯片目前非常抢手且价格昂贵。Nvidia不仅承受着来自科技巨头的激烈竞争，而且还承受着必须不断提升AI芯片性能和效率的压力，以保持领先于其行业竞争者AMD和英特尔。为了巩固其市场领导地位，Nvidia推出了新一代的GH200 Grace Hopper芯片。据Nvidia宣称，这款新芯片的内存容量将是其广受欢迎的H100 GPU的三倍。
值得注意的是，一些正在自主研发AI芯片的大型科技公司同时也是Nvidia的重要战略伙伴。尽管这些公司仍将依赖Nvidia的芯片来驱动它们大部分的AI系统，但它们的目标是减少对Nvidia的依赖。对于所有利益相关者而言，实现这种平衡十分具有挑战性。
截至3月底，Nvidia的股价在今年已经上涨了近30%，去年Nvidia的市值曾一度飙升至1万亿美元，创下历史新高（5月最新市值2.2万亿美元）。然而，随着其主要客户开始自主研发AI芯片，这为Nvidia的长期收入增长带来了不确定性。随着GenAI用例的需求持续增长，AI芯片必将成为争夺AI领域主导地位的关键战场。
]]></content>
  </entry>
  
  <entry>
    <title>无惧抖动与串扰 还原信号完整性</title>
    <url>/post/hardware/no-worries-about-jitter-and-crosstalk-restore-signal-integrity.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>Oscilloscope</tag>
      <tag>Signal Integrity</tag>
    </tags>
    <content type="html"><![CDATA[信号链芯片的验证离不开抖动和眼图测量。
背景 抖动考虑的是时钟或数据过零点的时刻的不确定性，眼图则更加直观，以参考时钟的边沿为刀，将数据波形切割成无数的小段，每段波形只有1个 bit。然后将这些小段波形堆叠到一起，形成的眼睛形状的图片，称之为眼图。
抖动和眼图是高速串行信号的必测项目。抖动可以评估时钟或信号的稳定性，眼图可以综合评估信号的抖动，幅度，反射，串扰等信号完整性问题。如果再套上一个眼图模板，通过眼图是否触碰模板，就可以轻松评判信号质量的优劣。
图1. 眼图是由所有bits堆叠而形成的图样，包含所有bits的信号完整性信息
图2. 眼图及其模板
挑战 测试工程师测出了眼图和抖动的结果，给出测试Pass或Fail的测量报告，工作就算完成了。但怎样才能解决抖动和眼图Fail的问题呢？基本思路就是抓大放小，对症下药！
图3. 抖动的分类
经典理论下，信号的总体抖动（Tj, Total Jitter）可以分成以下几类。通过示波器的抖动软件，测量出每种抖动成分的大小，想办法将最严重的抖动成分降低，总体抖动就能降下来了。
其中，随机抖动（Rj, Random Jitter）是由布朗运动 / 热噪声引起的。通过改良电源的噪声性能，更换随机抖动较大的有源器件，可以降低系统的随机噪声。与随机抖动对应的，是确定性抖动（Dj, Deterministic Jitter），最重要的两种Dj是周期性抖动（PJ, Periodic Jitter）和数据相关性抖动（DDJ, Data-Dependent Jitter）。
周期性变化的抖动，称之为周期性抖动。一般由高速 SerDes 的参考时钟带入，或者旁路高速信号的串扰所引起。周期性抖动不仅要看抖动的峰峰值，还要关注 Pj 的频率。检查时钟，查找电路板上相关的干扰源，可以降低周期性抖动。
数据相关性抖动（DDj, Data Dependent Jitter），也叫码间干扰（ISI, Inter-Symbol Interference），当数据速率升高时，DDj会变得愈发重要。ISI不仅会影响到抖动，也会让眼图恶化。下图是一个高速信号，从发送端，到传输路径中间的测试点，再到接收端的眼图变化。随着传输线距离的增加，ISI越来越大，眼图的眼宽和眼高都明显收窄，甚至到最后眼图完全闭合了。降低传输线衰减，或者使用适当的Tx均衡和Rx均衡，可以有效降低DDj。
图4. 传输线损耗所引起的ISI抖动会导致眼图恶化
测试解决方案 泰克公司的MSO6B系列示波器最高带宽可达10GHZ， 采样率高达50GS/s， 底噪低至51.5uV，是测量信号链芯片抖动和眼图的最佳选择。
图5. 泰克MSO6B系列示波器
泰克公司的DJA抖动和眼图分析软件，可以精确并快速地测量信号的抖动和眼图，给出抖动和眼图的各项测量结果。DJA帮助工程师将总体抖动分解成各种抖动成分，还提供各种高级分析窗口，比如抖动频谱图，可以看到周期性抖动的频谱，帮助工程师快速确定干扰源；抖动直方图可以直观地判断是RJ或DJ占主导地位，确定调试方向。
图6. 泰克公司的DJA抖动和眼图分析软件丰富的测量项和分析窗口
总结 抖动和眼图是信号链芯片的重要测量项。泰克的MSO6B系列示波器和DJA软件，不仅可以帮助工程师精确地测量抖动和眼图结果，还可以通过抖动分解，和丰富的分析窗口，为工程师提供解决抖动和眼图问题的线索，帮助工程师快速解决问题。
使用1GHz~10GHz带宽对高速设计进行故障排除和验证，阅读原文，了解泰克6系列B MSO混合信号示波器更多高速设计。
]]></content>
  </entry>
  
  <entry>
    <title>SoC设计中的晶振是什么</title>
    <url>/post/soc/what-is-the-crystal-oscillator-in-soc-design.html</url>
    <categories><category>SOC</category>
    </categories>
    <tags>
      <tag>SoC</tag>
    </tags>
    <content type="html"><![CDATA[晶振是电路板数字电路的＂心跳＂，在电路板中随处可见,只要用得到处理器的地方就必定有晶振的存在,即使没有外部晶振，芯片内部也有晶振。
引言 SoC广泛应用于各种电子设备中，如智能手机、平板电脑、数字电视等。在这些设备中，晶振（Oscillator）是一个关键组成部分，它为SoC提供时钟信号，确保各个模块能够同步工作。本文将详细介绍晶振在SoC设计中的作用、类型和应用。
晶振在SoC设计中的作用 晶振在SoC设计中的作用是为整个系统提供稳定的时钟信号。时钟信号是数字电路工作的基础，它决定了数字信号的传输速率和处理速度。在SoC中，各个模块（如 CPU  、GPU、DSP、内存控制器等）需要协同工作，这就要求它们有一个共同的时钟信号作为同步参考。晶振产生的时钟信号经过分频、倍频和整形等处理，为各个模块提供所需的时钟频率。
晶振的类型 根据制造材料和结构的不同，晶振可以分为以下几种类型：
石英晶体振荡器（Quartz Crystal Oscillator） 石英晶体振荡器是应用最广泛的晶振类型，它使用石英晶体作为振荡元件。石英晶体具有优异的频率稳定性和温度特性，能够在较宽的温度范围内保持稳定的振荡频率。石英晶体振荡器通常用于要求高精度时钟的应用场景。
温度补偿型晶体振荡器（TCXO） 温度补偿型晶体振荡器（TCXO）是对石英晶体振荡器进行温度补偿的一种改进型晶振。它通过内置的温度传感器和补偿电路，对晶体的频率进行实时调整，从而提高晶振的温度稳定性。
压控型晶体振荡器（VCXO） 压控型晶体振荡器（VCXO）是一种可以通过外部电压控制振荡频率的晶振。它通常用于需要频率调制或扫描的应用场景，如无线通信系统中的频率合成器。
晶振在SoC设计中的应用 晶振在SoC设计中的应用非常广泛，以下是一些典型的应用场景：
时钟生成与分配 晶振为SoC提供参考时钟信号，经过时钟生成电路的处理，生成系统所需的时钟频率。这些时钟信号通过时钟分配网络分配给各个模块，确保它们能够同步工作。
数据传输与同步 在SoC的数据传输过程中，时钟信号用于同步发送端和接收端的数据传输。例如，在串行通信接口（如USB、PCIe等）中，时钟信号用于同步数据包的发送和接收。
时序控制 在数字电路中，时序控制是确保数据正确传输和处理的关键。晶振提供的时钟信号用于控制各个模块的时序，确保数据在正确的时间被处理和传输。
低功耗设计 在低功耗设计中，晶振的功耗是一个重要考虑因素。为了降低功耗，SoC设计者可以选择低功耗晶振，或者在不需要高精度时钟时关闭晶振。
结论 晶振是SoC设计中的关键组成部分，它为整个系统提供稳定的时钟信号，确保各个模块能够同步工作。根据不同的应用场景和需求，可以选择不同类型的晶振。随着集成电路技术的不断发展，晶振的设计和应用也在不断优化和创新，以满足更高性能、更低功耗的需求。
]]></content>
  </entry>
  
  <entry>
    <title>办公桌旁的大内存 Nvidia GH200：比您想象的更近</title>
    <url>/post/hardware/a-big-memory-nvidia-gh200-next-to-your-desk-closer-than-you-think.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>Nvidia GH200</tag>
    </tags>
    <content type="html"><![CDATA[研究微处理器的可能还记得，最初的 8086/8088 处理器没有浮点运算单元。主板上通常有一个额外的插座，用于选配 8087 数学协处理器。如今，CPU 已经没有可选的数学协处理器了。
不过，SIMD 处理器（又称 GPU）还是有可选项的。众所周知，GPU 的数学处理速度（如矩阵运算）远远超过 CPU 主机。
随着 Nvidia GH-200 处理器和 AMD MI300A APU 的推出，市场正在见证 &ldquo;8087 时刻&rdquo;&ndash;即 CPU 吸收外部性能硬件。Nvidia 和 AMD 都将 GPU 集成到了处理器中，其结果是 HPC 性能大幅跃升，为未来的发展埋下了伏笔。
再见 PCI AMD 和 Nvidia 的 GPU 都依赖 PCI 总线与 CPU 通信。CPU 和 GPU 有两个不同的内存域，数据必须通过 PCI 接口从 CPU 域传输到 GPU 域（然后再返回）。
使用 5 代 PCIe 总线全部 16 个通道的 GPU 的最大带宽约为 63GB/s。这一瓶颈将限制内存在 CPU 和 GPU 之间的传输。
Nvidia GH200 通过 900GB/s 双向 NVLink-C2C 连接 Grace CPU 和 Hooper GPU。这样算下来，速度大约提高了 14 倍。此外，GH200 还带来了单个共享 CPU-GPU 内存域的优势。无需在 CPU 和 GPU 之间通过 PCI 总线移动数据。如图 1 所示，CPU 和 GPU 对所有内存都有一致的看法。CPU 内存高达 480GB LPDDR5X（带 ECC），GPU 有 96GB HBM3 或 144GB HBM3e。总的一致性（单域）内存为 576GB 至 624GB。
图1. NVIDIA GH200 Grace Hopper超级芯片的逻辑概览 目前的 AMD Instinct MI300A APU 采用单内存域，CPU 和 GPU 之间使用 Infinity Fabric 相干共享 128GB HBM3 内存，包上峰值吞吐量为 5.3 TB/s。虽然 MI300A 目前不像 GH200 那样支持额外的 DDR 内存扩展，但 CXL 是未来需要记住的一个词。
对于 GH200 和 MI300A，最突出的关键是 &ldquo;呈现单一内存域&rdquo;。在传统的 CPU-PCIe-GPU 组合中，GPU 内存量往往小于 CPU 内存量，数据必须在 PCIe 接口上分流。这两种新设计消除了这一瓶颈。单个大内存域对 HPC 一直具有吸引力，而 GenAI 的发展则加速了这一需求（即在内存中加载大型模型并使用 GPU 运行这些模型的能力）。对于传统的 GPU，GPU 内存的数量限制了模型的大小，因此需要采用分布式 GPU 方法。(注：GH200 可通过外部 NVLinks 连接，创建一个庞大的统一内存；例如，Nvidia-AWS NLV32 可提供高达 20 TB 的统一内存）。
离个人办公桌并不遥远 技术发展的一个明确趋势是从昂贵的新市场向低成本的商品市场转变。高性能计算也不例外。随着市场的需求，从多核到高级内存，一切都已从高端市场转向 &ldquo;手机 &ldquo;市场。向单一内存域的转变就是这些变化之一。
最近，在 Linux 基准网站 Phoronix 上，特级测试员 Michael Larabel 在 GH200 工作站上运行了 HPC 基准测试。该系统由德国 GPTshop.ai 提供。
该系统塔式机箱如图 2 所示，配备了 GH200 Grace Hopper 超级芯片（内存总容量为 576G）、双 2000+ W 电源、QCT 主板以及 SSD 和 NVIDIA Bluefield/Connect-X 适配器等多种配置选项。一个有趣而实用的功能是，TDP 可以从 450W 编程到 1000W（CPU + GPU + 内存），这在非数据中心环境中应该很有用。此外，据报道，默认风冷噪音为 25 分贝。液冷也是一种选择。
不过，台式超级工作站的价格并不便宜。目前上市的 GH200 576GB 型号起价为 47500 欧元（根据 Phoronix 的说法，由于欧盟以外地区不征收 19% 的增值税，因此这一价格相当于 4.1 万美元）。
图2. GPTShop Nvidia GH200工作站内部视图。(源GPTshop.ai) 这个价格看似很高，但考虑到配备 80GB HBM2e 内存的 Nvidia H100 PCIe GPU 目前的市场价格在 3 万至 3.5 万美元之间。这还不包括为 GPU 供电和运行的主机系统。此外，用户还受到 80GB GPU 内存的限制，该内存通过 PCIe 总线与主内存域分离。
GPTshop 工作站提供 576GB 的单域内存。HPC 和 GenAI 用户会发现这半 TB 的 CPU-GPU 内存很有吸引力。
初步基准测试 由于 GPTshop 的帮助，Phoronix 可以远程运行几个基准测试。这些基准测试应被视为初步测试，而非最终性能测试。尤其是，这些基准测试仅使用 CPU，并未使用 Hopper A100 GPU。因此，基准测试结果并不完整。Phoronix 计划今后测试基于 GPU 的应用程序。
据 Phoronix 称，Ubuntu 23.10 和 Linux 6.5 使用的是 GCC-13 作为编译器。类似的环境还用于测试同类处理器，包括英特尔至强可扩展处理器、AMD EPYC 和 Ampere Altra Max 处理器。完整列表可在 Phoronix 网站上找到。
此外，没有基准运行的功耗数据。据 Phoronix 称，英伟达™（NVIDIA®）GH200 目前似乎没有在 Linux 下公开任何 RAPL/PowerCap/HWMON 接口，仅用于读取 GH200 功耗/能耗。系统上的 BMC 确实通过 Web 界面显示了整个系统的功耗，但没有通过 IPMI 显示功耗数据。
尽管存在这些限制，但在 Nvidia 之外，我们还是首次在 GH200 上运行了一些重要的基准测试。
优秀的 HPCG Phoronix 报告的第一个测试是标准 HPCG 内存带宽基准，如图 3 所示。
图3. Nvidia GH200运行HPCG基准测试的结果(来源：Phoronix)  可以看出，GH200 Arm 的性能达到了可观的 42 GFLOPS，仅次于 Xeon Platinum 8380 2P（40 GFLOPS），略低于 EPYC 9654 Genoa 2P（44 GFLOPS）。值得注意的还有 72 核 Arm Grace CPU，其性能几乎是 Ampere Altra Max 128 核 Arm 处理器的两倍。
GH200 在其他基准测试中表现出色。最令人印象深刻的结果如图 4 所示。使用 72 核 Arm GH200 运行的 NWChem（C240-Bucky Ball）以 1404 秒完成，仅次于领先的 128 核 Epyc 9554（2p）的 1323 秒。
图4. Nvidia GH200运行NWChem基准测试的结果(来源：Phoronix) 未来发展 Nvidia GH200 和 AMD MI300A 引入了新的处理器架构。与集成8087 数学协处理器类似，高端 CPU 也开始集成 GPU（或 SIMD 处理单元）。不过，这种想法并非创新性的。AMD 自 2011 年起就在台式机/笔记本 APU 处理器中集成了适度的 GPU。虽然这些高端处理器可能被认为是 &ldquo;专业 &ldquo;的，因此价格昂贵，但随着时间的推移，人们对 GenAI 的巨大兴趣可能会推动这些设计进入商品价位。随着更多基准的出现，这一情况还将继续发展。
此外，在办公桌旁配备一台个人高性能工作站，其内存足以运行一些最大的 LLM，也是一个重要的里程碑。更不用说运行许多大内存 GPU 优化 HPC 应用程序的能力了。数据中心和云计算仍将是当今的工作主力，但 &ldquo;拥有重启按钮 &ldquo;也是必须的。
]]></content>
  </entry>
  
  <entry>
    <title>为什么要使用LVDS或JESD204B标准</title>
    <url>/post/hardware/why-use-lvds-or-jesd204b-standards.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>LVDS</tag>
      <tag>JESD2048</tag>
    </tags>
    <content type="html"><![CDATA[信号链是连接真实世界和数字世界的桥梁。随着ADC采样率和采样精度的提升，接口芯片的信号传输速度也越来越快，高速信号传输的各种挑战慢慢浮现出来了。
引言 相比传统的CMOS传输技术，在信号链中引入LVDS或JESD204B，可以实现更高的信号传输速率，更低的功耗，具备更好的抗干扰性 (信噪比更佳)，而且线束数量会大幅降低。
什么是LVDS和JES204B? LVDS（Low-Voltage Differential Signaling ,低电压差分信号）是美国国家半导体（National Semiconductor, NS，现TI）于1994年提出的一种信号传输模式的电平标准，它采用极低的电压摆幅传输高速差分数据，可以实现点对点或一点对多点的连接，具有低功耗、低误码率、低串扰等优点，已经被广泛应用于串行高速数据通讯的各个场合，比较广为人知的有笔记本电脑的液晶显示，数据转换器(ADC/DAC)的高速数字信号传输，汽车电子的视频码流传输等。
JESD204是标准化组织JEDEC，针对数据转换器（ADC和DAC）和逻辑器件(FGPA)之间进行数据传输，而制定的高速串行接口。JESD204采用CML (Current-Mode Logic)技术来传输信号，该标准的 B 修订版支持高达 12.5 Gbps串行数据速率，并可确保 JESD204 链路具有可重复的确定性延迟。随着转换器的速度和分辨率不断提升，以及FPGA芯片对JESD204B标准的广泛支持，JESD204在高速转换器和集成RF收发器的应用中也变得更为常见。
高速信号传输的实际应用 LVDS是一种电流驱动的高速信号，在发送端施加一个3.5mA的恒定电流源。控制开关管的通断，就可以使得发送端流向接收端的电流，在正向和反向之间不断变化，从而在接收端的100欧姆差分负载上实现+/-350mV的差分电压变化，最高可实现3.125Gbps的高速数据传输。LVDS采用差分线的传输方式，会带来几个显著的优势：
 a. 允许发送端和接收端之间存在共模电压差异（0-2.4V范围内） b. 优秀的抗干扰能力，信噪比极佳 c. 极低的电压摆幅，功耗极低  图2. LVDS的工作方式 传统的LVDS采用同步时钟的方式，使用一对差分时钟，为最多三对数据信号提供时钟参考。每个时钟周期内，每对数据传输7 bits信息。需要用到SerDes芯片，在发送时，将并行信号通过并/串转换，变成高速串行信号；在接收到高速串行信号时，使用串/并转换，还原并行信号。
图3. LVDS 同步时钟为数据提供参考 现在使用的LVDS也支持8b/10b SerDes来实现更高效的信号传输。这种传输方式不再需要用到时钟信号，只需要传输Data信号就可以了，节省了一对差分线。通过8b/10b编码，将8bit有效数据映射成10bit编码数据，这个过程中虽然增加了25%的开销，但可以确保数据里有足够频繁的信号跳变。
在收到信号后，通过锁相环(PLL)从数据里恢复出时钟。这种传输架构称之为嵌入式时钟(Embeded Clock)。8b/10b编码还可以让传输信号实现直流平衡(DC Balance)，即1的个数和0的个数基本维持相等。直流平衡的传输链路可以串联隔直电容，提升链路的噪声和抖动性能。嵌入式时钟和8b/10b被广泛用于工业高速传输标准，比如PCIe，SATA, USB3等，也包括JESD204 (CML)。
图4. LVDS内嵌时钟的工作方式（图片来源TI） 不同于LVDS的是, CML(Current-Mode Logic)采用电压驱动的方式，在源端施加一个恒定的电压Vcc。通过控制开关管的通断，接收端就可以得到变化的差分电压。CML使用嵌入式时钟和8b/10b编码，工作电压比LVDS更高，同时在发送和接收芯片里使用均衡技术，以确保高速、长距离传输时仍具有很优秀的误码率。使用CML技术的JESD204B可支持高达12.5Gbps的data rate，其最新的C版本甚至可以支持高达32Gbps data rate。
图5. CML信号传输方式 那么我们在设计高速接口芯片时，到底应该使用LVDS还是CML(JESD204)呢？简单的原则是，CML速率更高，而LVDS则功耗更低。
图6. LVDS和CML的选择 当Data Rate低于2Gbps时，LVDS的应用更为广泛，其功耗更低，抗干扰强，较宽的共模电压范围让互连的要求变得很低。LVDS还有支持多点互连的M-LVDS和B-LVDS标准，可以多节点互连，应用场景非常丰富。当Data rate高于3.125Gbps就必须要使用CML了。当Data Rate在2G到3.125Gbps之间时，要综合考虑功能性，性能，和功耗的平衡。比如说传输距离较长，但信号品质要求又很高的时候，考虑用CML；传输距离较短，要求长续航，低功耗的时候，考虑用LVDS。
]]></content>
  </entry>
  
  <entry>
    <title>8款非常好用的漏洞扫描工具</title>
    <url>/post/linux/8-very-useful-vulnerability-scanning-tools.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Linux</tag>
      <tag>Vulnerability</tag>
      <tag>scanning</tag>
    </tags>
    <content type="html"><![CDATA[OpenVAS：OpenVAS是一种全面的漏洞分析工具，用于扫描服务器和网络设备。它可以检测开放端口、错误配置和漏洞，并生成详细的报告。OpenVAS基于Nessus的开源版本，具有强大的功能和活跃的社区支持。
OpenVAS OpenVAS：OpenVAS是一种全面的漏洞分析工具，用于扫描服务器和网络设备。它可以检测开放端口、错误配置和漏洞，并生成详细的报告。OpenVAS基于Nessus的开源版本，具有强大的功能和活跃的社区支持。
 全面的漏洞分析工具，用于扫描服务器和网络设备。 检测开放端口、错误配置和漏洞。 生成详细的报告。 基于Nessus的开源版本，具有强大的功能和活跃的社区支持。  Nessus Nessus：Nessus是一款面向安全专业人士的工具，用于解决补丁、软件问题、恶意软件和广告软件删除。它支持多种操作系统和应用程序上的错误配置。Nessus具有广泛的漏洞库，可以识别各种漏洞类型。
 面向安全专业人士的工具。 解决补丁、软件问题、恶意软件和广告软件删除。 支持多种操作系统和应用程序上的错误配置。 广泛的漏洞库，可以识别各种漏洞类型。  Acunetix Acunetix：Acunetix是一款Web应用程序安全测试工具，用于扫描Web站点和应用程序中的漏洞。它能够检测SQL注入、跨站脚本等常见漏洞。Acunetix还提供了自动化扫描、漏洞修复建议和报告生成功能。
 Web应用程序安全测试工具。 扫描Web站点和应用程序中的漏洞。 检测SQL注入、跨站脚本等常见漏洞。 提供自动化扫描、漏洞修复建议和报告。  Skipfish Skipfish：Skipfish是一款主动的Web应用程序安全侦察工具，用于创建交互式站点地图并进行安全扫描。它具有高度定制的配置选项，可以根据需求进行调整。
 主动的Web应用程序安全侦察工具。 创建交互式站点地图并进行安全扫描。 高度定制的配置选项。  root@kali:~# skipfish -o 202 http://192.168.1.202/wordpress skipfish version 2.10b by lcamtuf@google.com - 192.168.1.202 - Scan statistics: Scan time : 0:00:05.849 HTTP requests : 2841 (485.6/s), 1601 kB in, 563 kB out (370.2 kB/s) Compression : 802 kB in, 1255 kB out (22.0% gain) HTTP faults : 0 net errors, 0 proto errors, 0 retried, 0 drops TCP handshakes : 46 total (61.8 req/conn) TCP faults : 0 failures, 0 timeouts, 16 purged External links : 512 skipped Reqs pending : 0 Database statistics: Pivots : 13 total, 12 done (92.31%) In progress : 0 pending, 0 init, 0 attacks, 1 dict Missing nodes : 0 spotted Node types : 1 serv, 4 dir, 6 file, 0 pinfo, 0 unkn, 2 par, 0 val Issues found : 10 info, 0 warn, 0 low, 8 medium, 0 high impact Dict size : 20 words (20 new), 1 extensions, 202 candidates Signatures : 77 total [+] Copying static resources... [+] Sorting and annotating crawl nodes: 13 [+] Looking for duplicate entries: 13 [+] Counting unique nodes: 11 [+] Saving pivot data for third-party tools... [+] Writing scan description... [+] Writing crawl tree: 13 [+] Generating summary views... [+] Report saved to &#39;202/index.html&#39; [0x7054c49d]. [+] This was a great day for science! root@kali:~# skipfish -h skipfish web application scanner - version 2.10b Usage: skipfish [ options ... ] -W wordlist -o output_dir start_url [ start_url2 ... ] Authentication and access options: -A user:pass - use specified HTTP authentication credentials -F host=IP - pretend that &#39;host&#39; resolves to &#39;IP&#39; -C name=val - append a custom cookie to all requests -H name=val - append a custom HTTP header to all requests -b (i|f|p) - use headers consistent with MSIE / Firefox / iPhone -N - do not accept any new cookies --auth-form url - form authentication URL --auth-user user - form authentication user --auth-pass pass - form authentication password --auth-verify-url - URL for in-session detection Crawl scope options: -d max_depth - maximum crawl tree depth (16) -c max_child - maximum children to index per node (512) -x max_desc - maximum descendants to index per branch (8192) -r r_limit - max total number of requests to send (100000000) -p crawl% - node and link crawl probability (100%) -q hex - repeat probabilistic scan with given seed -I string - only follow URLs matching &#39;string&#39; -X string - exclude URLs matching &#39;string&#39; -K string - do not fuzz parameters named &#39;string&#39; -D domain - crawl cross-site links to another domain -B domain - trust, but do not crawl, another domain -Z - do not descend into 5xx locations -O - do not submit any forms -P - do not parse HTML, etc, to find new links Reporting options: -o dir - write output to specified directory (required) -M - log warnings about mixed content / non-SSL passwords -E - log all HTTP/1.0 / HTTP/1.1 caching intent mismatches -U - log all external URLs and e-mails seen -Q - completely suppress duplicate nodes in reports -u - be quiet, disable realtime progress stats -v - enable runtime logging (to stderr) Dictionary management options: -W wordlist - use a specified read-write wordlist (required) -S wordlist - load a supplemental read-only wordlist -L - do not auto-learn new keywords for the site -Y - do not fuzz extensions in directory brute-force -R age - purge words hit more than &#39;age&#39; scans ago -T name=val - add new form auto-fill rule -G max_guess - maximum number of keyword guesses to keep (256) -z sigfile - load signatures from this file Performance settings: -g max_conn - max simultaneous TCP connections, global (40) -m host_conn - max simultaneous connections, per target IP (10) -f max_fail - max number of consecutive HTTP errors (100) -t req_tmout - total request response timeout (20 s) -w rw_tmout - individual network I/O timeout (10 s) -i idle_tmout - timeout on idle HTTP connections (10 s) -s s_limit - response size limit (400000 B) -e - do not keep binary responses for reporting Other settings: -l max_req - max requests per second (0.000000) -k duration - stop scanning after the given duration hⓜ️s --config file - load the specified configuration file Send comments and complaints to &lt;heinenn@google.com&gt;. Goby Goby：Goby是一款新的网络安全测试工具，支持资产探测和漏洞扫描。它能够帮助安全入门者熟悉攻防，也适用于渗透测试。Goby的特点包括易于使用、自动化扫描和丰富的漏洞库。
 新的网络安全测试工具。 支持资产探测和漏洞扫描。 帮助安全入门者熟悉攻防，也适用于渗透测试。 易于使用、自动化扫描和丰富的漏洞库。  Retina Retina：Retina是一款基于Web的开源漏洞扫描工具，用于漏洞管理、修补、合规性和报告。它支持多种操作系统和应用程序，具有用户友好的界面和灵活的配置选项。
 基于Web的开源漏洞扫描工具。 用于漏洞管理、修补、合规性和报告。 支持多种操作系统和应用程序。 用户友好的界面和灵活的配置选项。  Nikto Nikto：Nikto是一个免费的在线漏洞扫描器，用于识别服务器功能、检查版本以及扫描不同的协议。它特别适用于Web服务器和Web应用程序的安全评估。
 免费的在线漏洞扫描器。 识别服务器功能、检查版本以及扫描不同的协议。 特别适用于Web服务器和Web应用程序的安全评估。  Vulnerability Manager Plus Vulnerability Manager Plus：Vulnerability Manager Plus是一款新的解决方案，用于自动扫描、影响评估、软件风险评估、安全性配置错误等。它具有直观的用户界面和丰富的报告功能。
 自动扫描、影响评估、软件风险评估、安全性配置错误等。 直观的用户界面和丰富的报告功能。  ]]></content>
  </entry>
  
  <entry>
    <title>C++中的临时对象</title>
    <url>/post/programming/temporary-object-in-c-plus-plus.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>C++</tag>
      <tag>Temporary Object</tag>
    </tags>
    <content type="html"><![CDATA[Temporary object，临时对象。一听名字就明白，这个对象的意义不大，只是临时中转一下或者存在一下，有的可能连个存在感都刷不到就消失了。但不要小看这种临时对象，对C/C++这种以效率严苛为前提的编程环境下，它就是效率低下的某种代名词。
临时对象 Temporary object，临时对象。一听名字就明白，这个对象的意义不大，只是临时中转一下或者存在一下，有的可能连个存在感都刷不到就消失了。但不要小看这种临时对象，对C/C++这种以效率严苛为前提的编程环境下，它就是效率低下的某种代名词。 但是，临时对象又无法完全避免，所以，怎么控制并减少临时对象的产生就是一个技术活儿了。
产生的时机 那么，在什么情况下会产生临时对象呢？在不同的编译器和不同的标准下，可能都有所不同，下面就分析一下产生临时对象的具体场景：
参数传递 一般值传递对象都会产生临时的对象：
#include &lt;iostream&gt; class Teacher { public: Teacher() { std::cout &lt;&lt; &#34;call Teacher construct func,old_ default is:&#34; &lt;&lt; old_ &lt;&lt; std::endl; } Teacher(int old) : old_(old) { std::cout &lt;&lt; &#34;call Teacher construct func,set old_ is:&#34; &lt;&lt; old_ &lt;&lt; std::endl; } Teacher(const Teacher &amp;t) { std::cout &lt;&lt; &#34;call Teacher copy construct func&#34; &lt;&lt; std::endl; } ~Teacher() { std::cout &lt;&lt; &#34;call Teacher deconstruct func!&#34; &lt;&lt; std::endl; } public: int old_ = 10; }; void TestPars(Teacher t) { std::cout &lt;&lt; &#34;Teacher old is:&#34; &lt;&lt; t.old_ &lt;&lt; std::endl; } int main() { TestPars(11); return 0; } 注意，编译时如果版本太高需要进行限制，比如此工程的运行环境是C++17,需要使用禁用优化并限定版本编译：
g++ -std=c++11 -fno-elide-constructors -g -o test main.cpp 运行结果：
call Teacher construct func,set old_ is:11 call Teacher copy construct func Teacher old is:10 call Teacher deconstruct func! call Teacher deconstruct func! 返回值 产生临时变量最容易就是在返回对象时：
#include &lt;iostream&gt; class Teacher { public: Teacher() { std::cout &lt;&lt; &#34;call Teacher construct func,old_ default is:&#34; &lt;&lt; old_ &lt;&lt; std::endl; } Teacher(int old) : old_(old) { std::cout &lt;&lt; &#34;call Teacher construct func,set old_ is:&#34; &lt;&lt; old_ &lt;&lt; std::endl; } Teacher(const Teacher &amp;t) { std::cout &lt;&lt; &#34;call Teacher copy construct func&#34; &lt;&lt; std::endl; } ~Teacher() { std::cout &lt;&lt; &#34;call Teacher deconstruct func!&#34; &lt;&lt; std::endl; } public: int old_ = 10; }; Teacher GetTeacher() { Teacher t; return t; } int main() { //Teacher t1;  //GetTeacher() = t1;//OK，可以测试并想一下  Teacher t = GetTeacher();//注意：此处t不声明照样产生临时变量  return 0; } 运行结果：
//注意：使用C++17编译器则始终是两个析构函数
tempObject$ g++ -std=c++11 -fno-elide-constructors -g -o test main.cpp tempObject$ ./test call Teacher construct func,old_ default is:10 call Teacher deconstruct func! call Teacher deconstruct func! call Teacher deconstruct func! ###类型转换
不同类型之间的隐式转换一般也可能产生临时对象 ：
int main() { //Teacher tmp;  //tmp = 88;  Teacher tmp = 88; return 0; } 运行结果：
call Teacher construct func,set old_ is:88 call Teacher copy construct func call Teacher deconstruct func! call Teacher deconstruct func! 这种情况和环境有比较大的关系，一般推荐是先声明对象变量，然后再用这个变量设置值，可在实际采用非优化的情况下，二者一致。
中间计算结果 #include &lt;iostream&gt; class Teacher { public: Teacher() { std::cout &lt;&lt; &#34;call Teacher construct func,old_ default is:&#34; &lt;&lt; old_ &lt;&lt; std::endl; } Teacher(int old) : old_(old) { std::cout &lt;&lt; &#34;call Teacher construct func,set old_ is:&#34; &lt;&lt; old_ &lt;&lt; std::endl; } Teacher(const Teacher &amp;t) { std::cout &lt;&lt; &#34;call Teacher copy construct func&#34; &lt;&lt; std::endl; } Teacher operator+(const Teacher &amp;t1) { Teacher t; t.old_ = old_ + t1.old_; return t; // return Teacher(old_ + t1.old_);  } ~Teacher() { std::cout &lt;&lt; &#34;call Teacher deconstruct func!&#34; &lt;&lt; std::endl; } public: int old_ = 10; }; void GetSum() { Teacher t1, t2, t3; Teacher t = t1 + t2 + t3; std::cout &lt;&lt; &#34;sum is:&#34; &lt;&lt; t.old_ &lt;&lt; std::endl; } int main() { GetSum(); std::cout &lt;&lt; &#34;cacl temp create!&#34; &lt;&lt; std::endl; return 0; } 运行结果：
call Teacher construct func,old_ default is:10 call Teacher construct func,old_ default is:10 call Teacher construct func,old_ default is:10 call Teacher construct func,old_ default is:10 call Teacher construct func,old_ default is:10 call Teacher deconstruct func! sum is:30 call Teacher deconstruct func! call Teacher deconstruct func! call Teacher deconstruct func! call Teacher deconstruct func! cacl temp create! 函数const引用参数 这个也是隐式转换：
void TestConstTmp(const std::string &amp;s) { std::cout &lt;&lt; &#34;tmp value:&#34; &lt;&lt; s &lt;&lt; std::endl; } int main() { char buf[] = &#34;this is test!&#34;; TestConstTmp(buf); return 0; } //或者
void TestConstTmp(const Teacher &amp;t) { std::cout &lt;&lt; &#34;tmp value:&#34; &lt;&lt; t.old_ &lt;&lt; std::endl; } int main() { TestConstTmp1(88); return 0; } 不过这种在实际模拟中未能模拟出临时对象的现象，所以待验证。而且需要注意的是，必须是const &amp;方可。
优化 优化的最基本方式 在上面提到过，优化最简单的方法便是使用更高标准的编译器，或者开启更高级别的优化选项。
对于类型转换产生临时对象的优化可以声明显示构造和重载operator=来解决 对于返回值只要按RVO和NRVO（C++11）的约定即可优化 对于传参产生的临时对象，可以采用引用传参来优化 对于中间值计算及其它，可以在代码层面避免，比如不采用多于两上的连加，不使用隐式类型转换等等。 在STL库容器增加元素增加了emplace系列，用它来替换insert,push等，减少临时对象的产生。这也是一种标准进步后的库的进步。
临时对象销毁 主要分两种情况，一种是做为引用的临时对象，其在绑定对象销毁时销毁；其它的非上述情况，则在作用域范围结束后销毁（指表达式的语句或者if,while等控制表达式的末尾）。
总结 如果工程对效率要求不高，其实对临时对象的处理也不用费什么心思。赶哪的集用哪的斗，不用刻意非得怎么着才行。主打一个兼顾效率和开发的平衡，优化未必时时都是一个必选项。
]]></content>
  </entry>
  
  <entry>
    <title>15 个最佳免费开源 Linux 文件管理器</title>
    <url>/post/linux/15-best-free-open-source-linux-file-managers.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>File Manager</tag>
    </tags>
    <content type="html"><![CDATA[在 Linux   系统中，文件管理器是用户管理文件和目录的主要工具之一。而在众多的文件管理器中，开源社区提供了许多优秀的选择，它们免费、灵活，并且可以根据用户的需求进行定制。
本文将介绍其中的五个最佳免费开源 Linux 文件管理器：Krusader、nnn、Dolphin、Midnight Commander 和 Double Commander。每个文件管理器都有其独特的特点和功能，满足了不同用户的需求。
Krusader krusader.org Krusader 是一个功能强大的双面板文件管理器，使用了 KDE 开发工具包（KDE toolkit）。它提供了类似于 Norton Commander 的界面，允许用户在两个面板之间轻松地拖放文件和目录。Krusader 还具有强大的文件操作功能，例如复制、粘贴、重命名、打包和解包等。
主要特点：
 双面板界面，便于文件操作和管理。 支持多种归档格式，包括 ZIP、TAR、RAR 等。 集成了强大的文件比较和同步工具。 支持通过插件扩展功能。  Krusader 是一个功能齐全的文件管理器，特别适合需要在不同目录之间频繁操作的用户。
nnn github.com/jarun/nnn nnn 是一个轻量级、快速的终端文件管理器，适用于那些喜欢使用命令行界面的用户。尽管它的界面简单，但它提供了丰富的功能和快捷键，使用户能够快速浏览、复制、移动和管理文件。
主要特点：
 使用 C 语言编写，运行速度快。 支持多种文件操作，包括复制、移动、重命名、删除等。 内置了强大的文件筛选和搜索功能。 可通过插件扩展功能，例如预览文件、集成版本控制等。  nnn 是一款简单但功能强大的文件管理器，特别适合喜欢在终端中快速操作文件的用户。
Dolphin userbase.kde.org/Dolphin Dolphin 是 KDE 桌面环境的默认文件管理器，它提供了直观的用户界面和丰富的功能，使用户能够轻松地管理文件和目录。Dolphin 支持标签页、书签、扩展和脚本等功能，使其成为一个强大而灵活的文件管理器。
主要特点：
 集成了 Dolphin 连接器，可以轻松地访问远程文件和目录。 支持标签页和书签，方便用户快速切换和定位。 内置了预览面板，支持预览文档、图片和视频等。 可通过插件和脚本扩展功能，满足不同用户的需求。  Dolphin 是一款功能丰富、易于使用的文件管理器，适用于各种用户，尤其是 KDE 桌面环境的用户。
Midnight Commander midnight-commander.org Midnight Commander 是一个经典的文本模式文件管理器，适用于那些喜欢在终端中工作的用户。它提供了双面板界面、快捷键操作和丰富的功能，使用户能够快速而高效地管理文件。
主要特点：
 双面板界面，支持在两个目录之间进行文件操作。 支持快捷键操作，方便用户快速执行命令。 内置了文件查看器、编辑器和压缩工具等。 支持自定义布局和主题，满足用户的个性化需求。  Midnight Commander 是一款经典而强大的终端文件管理器，适用于喜欢在命令行界面中工作的用户。
Double Commander doublecmd.sourceforge.io Double Commander 是一个基于两个面板的文件管理器，类似于 Total Commander。它提供了丰富的功能和可定制的界面，使用户能够方便地浏览、管理和操作文件。
主要特点：
 双面板界面，支持在两个目录之间进行文件操作。 支持多标签页和多窗口，方便用户同时处理多个任务。 内置了文件比较和同步工具，帮助用户快速找到差异和同步文件。 可通过插件和脚本扩展功能，满足用户的个性化需求。  Double Commander 是一款强大而灵活的文件管理器，适用于需要同时处理多个任务的用户。
Ranger ranger.github.io Ranger 是一个以 Vim 风格的终端文件管理器，它使用 Python 编写，提供了类似于 Vim 的键盘快捷键和命令模式，使用户能够快速而高效地管理文件。
主要特点：
 使用 Vim 风格的界面和键盘快捷键，提高了用户的操作效率。 支持预览文件、图片和视频等。 内置了文件搜索和标签功能，方便用户快速定位和管理文件。 可通过插件扩展功能，例如版本控制、远程访问等。  Ranger 是一款适用于终端环境的灵活而强大的文件管理器，特别适合喜欢使用命令行界面的用户。
PCManFM-Qt github.com/lxqt/pcmanfm-qt PCManFM-Qt 是 LXQt 桌面环境的默认文件管理器，它提供了简洁的用户界面和丰富的功能，使用户能够轻松地管理文件和目录。
主要特点：
 简洁直观的用户界面，易于使用。 支持标签页和书签，方便用户快速定位和管理文件。 内置了文件搜索和过滤功能，帮助用户快速找到所需文件。 支持自定义布局和主题，满足用户的个性化需求。  PCManFM-Qt 是一款轻量级而功能丰富的文件管理器，适用于 LXQt 桌面环境和其他轻量级桌面环境。
PCManFM wiki.lxde.org/en/PCManFM PCManFM 是 LXDE 桌面环境的默认文件管理器，它提供了简单而实用的功能，使用户能够轻松管理文件和目录。
主要特点：
 简单直观的用户界面，易于上手。 支持多标签页和书签，方便用户快速定位和管理文件。 内置了文件搜索和过滤功能，帮助用户快速找到所需文件。 支持自定义快捷键和菜单，满足用户的个性化需求。  PCManFM 是一款简单实用的文件管理器，适用于需要轻量级桌面环境的用户。
Files (Nautilus) github.com/elementary/files Files，也称为 Nautilus，是 GNOME 桌面环境的默认文件管理器，它提供了直观的用户界面和丰富的功能，使用户能够轻松地管理文件和目录。
主要特点：
 直观的用户界面，支持拖放、缩放和预览文件。 支持标签页和书签，方便用户快速定位和管理文件。 内置了文件搜索和过滤功能，帮助用户快速找到所需文件。 支持扩展和脚本，可以通过插件增强功能。  Files 是一款功能丰富、易于使用的文件管理器，适用于 GNOME 桌面环境和其他相关环境。
GNOME Files (Nemo) wiki.gnome.org/action/show/Apps/Files GNOME Files，也称为 Nemo，是 Linux Mint 操作系统的默认文件管理器，它基于 Nautilus 开发，并提供了额外的功能和改进。
主要特点：
 支持双面板和单面板视图，方便用户选择和切换。 内置了文件预览和元数据显示，帮助用户快速了解文件信息。 支持自定义布局和快捷键，满足用户的个性化需求。 可通过插件扩展功能，例如版本控制、压缩和解压等。  GNOME Files 是一款功能丰富、灵活的文件管理器，适用于 Linux Mint 和其他使用 Cinnamon 桌面环境的用户。
lf godoc.org/github.com/gokcehan/lf lf 是一个轻量级的终端文件管理器，它的设计受到了 Unix 哲学的影响，旨在提供简单、高效的文件管理功能。
主要特点：
 使用 Go 语言编写，运行速度快。 基于文本界面，简洁清晰。 支持 Vim 风格的键盘快捷键，提高了用户的操作效率。 可通过插件扩展功能，满足不同用户的需求。  lf 是一款轻量级而高效的终端文件管理器，适用于喜欢使用命令行界面的用户。
GNOME Commander gcmd.github.io GNOME Commander 是一个基于 GTK+ 的双面板文件管理器，提供了直观的用户界面和丰富的功能，使用户能够轻松地管理文件和目录。
主要特点：
 双面板界面，支持在两个目录之间进行文件操作。 支持多标签页和书签，方便用户快速定位和管理文件。 内置了文件搜索和过滤功能，帮助用户快速找到所需文件。 支持自定义快捷键和主题，满足用户的个性化需求。  GNOME Commander 是一款功能丰富、易于使用的文件管理器，适用于 GNOME 桌面环境和其他相关环境。
Spacedrive sudo dpkg -i Spacedrive-linux-x86_64.deb Spacedrive 是一个基于 Web 的文件管理器，它提供了云存储服务和在线文件编辑功能，使用户能够轻松地管理和共享文件。
主要特点：
 提供了云存储服务，支持多种云存储提供商。 内置了文件预览和编辑器，支持在线编辑文档、图片和视频等。 支持文件分享和协作，方便用户与他人共享文件和目录。 可通过插件扩展功能，例如版本控制、加密和解密等。  Spacedrive 是一款功能丰富、易于使用的基于 Web 的文件管理器，适用于需要在线协作和文件编辑的用户。
CliFM github.com/leo-arch/clifm/wiki CliFM 是一个轻量级的终端文件管理器，它的设计简洁明了，提供了基本的文件操作和浏览功能，适用于简单的文件管理任务。
主要特点：
 使用 C 语言编写，运行速度快。 基于文本界面，简单直观。 支持基本的文件操作，包括复制、移动、重命名、删除等。 可通过配置文件自定义快捷键和外观。  CliFM 是一款轻量级而简单的终端文件管理器，适用于简单的文件管理任务和喜欢使用命令行界面的用户。
Xfe roland65.free.fr/xfe/ Xfe 是一个轻量级的文件管理器，它使用 FOX 工具包（FOX toolkit）开发，提供了简单而实用的功能，适用于各种 Linux 桌面环境。
主要特点：
 使用 FOX 工具包开发，界面简洁美观。 提供了双面板界面，支持在两个目录之间进行文件操作。 支持多标签页和书签，方便用户快速定位和管理文件。 支持自定义布局和快捷键，满足用户的个性化需求。  Xfe 是一款简单实用的文件管理器，适用于各种 Linux 桌面环境和用户。
]]></content>
  </entry>
  
  <entry>
    <title>虚拟化技术：VCPU与物理CPU的关系</title>
    <url>/post/hardware/the-relationship-between-virtual-cpu-and-physical-cpu.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>VCPU</tag>
      <tag>PCPU</tag>
    </tags>
    <content type="html"><![CDATA[在数字化浪潮席卷的今天，虚拟化技术已成为数据中心、云计算以及IT领域不可或缺的一环。它以其出色的灵活性和资源利用率，为现代计算体系注入了新的活力。而在虚拟化技术中，vCPU（虚拟中央处理单元）则扮演着重要的角色。本篇文章将深入剖析vCPU与物理CPU的关系，帮助大家更好地了解虚拟化这一技术的核心。
什么是vCPU vCPU，全称为虚拟中央处理器（Virtual Central Processing Unit），是虚拟化环境中分配给虚拟机实例的重要计算资源。它是物理CPU的虚拟化产物，能够在逻辑上被虚拟机独立使用。每个vCPU都具备执行指令和处理数据的能力，使得虚拟机能够在没有物理CPU核心的情况下运行多线程应用程序。
vCPU的出现，极大地提高了物理CPU的利用率。通过将物理CPU分割成多个vCPU，我们可以在同一时间运行多个任务，从而实现资源的最大化利用。此外，vCPU还赋予了虚拟化环境更大的灵活性。我们可以根据虚拟机的实际需求，动态调整vCPU的数量，无需更换物理硬件即可满足不同的计算需求。 然而，正如任何技术都有其两面性一样，vCPU也不例外。虚拟化技术会带来一定的性能开销，尽管这一开销在大多数情况下并不显著，但在某些对性能要求极高的场景中，它可能成为影响系统性能的关键因素。此外，过度分配vCPU也可能导致CPU资源的浪费，从而降低整体的资源利用率。
vCPU VS CPU vCPU和CPU是两个在计算机体系结构中具有不同概念的术语：物理CPU，被称为处理器或中央处理器，它是计算机系统的核心组件，负责执行指令、进行运算和控制计算机的各个部分。而vCPU则是在虚拟化环境中对物理CPU的模拟和分配。它们之间的关系，可以说是共生共荣的伙伴关系。
一台物理计算机可以有多个CPU，每个CPU可以包含多个核心，每个核心可以处理独立的任务。同时，每个核心可以支持多个线程，通过超线程或多核心技术，实现并行处理。在虚拟化环境中，一台物理计算机可以同时运行多个独立的虚拟机，每个虚拟机都可以拥有一个或多个vCPU。这些vCPU在逻辑上是相互独立的，它们通过虚拟化层的调度和资源管理，实现了与物理CPU之间的资源隔离。这种隔离性使得不同虚拟机之间的计算任务可以互不干扰地运行，提高了系统的稳定性和安全性。
同时，vCPU与物理CPU之间的协作也至关重要。虚拟化层负责将虚拟机的指令转换为物理处理器可以执行的指令，从而实现了vCPU与物理CPU之间的无缝对接。当虚拟机需要执行计算任务时，它会将任务发送给vCPU，vCPU再将任务转化为物理CPU可以执行的指令。物理CPU则负责执行这些指令，完成计算任务。
vCPU的计算 在计算vCPU前需要了解一些术语：
Socket：插槽是一个阵列的引脚，用于固定处理器并将主板连接到可用的处理能力，插槽的数量由主板的容量决定。
Thread：线程是进程内的执行路径。一个进程可以包含一个或多个线程，线程与进程的主要区别在于，同一进程内的线程在共享内存空间中运行，而进程在独立的内存空间中运行。
Physical Core：物理核心，也被称为处理单元，位于CPU内。一个物理核心可能对应一个或多个逻辑核心。
Logical Core：逻辑核心使得单个物理核心能够同时执行两个或更多的操作。
系统可用的VCPU总数计算 在计算vCPU的数量时，通常需要考虑以下几个因素：
物理 CPU 的数量：这是服务器上实际插槽中的CPU个数；
每颗物理 CPU 的核心数：这是一块CPU上面能处理数据的芯片组的数量；
每个核心的超线程数：如果物理CPU支持超线程技术，那么每个CPU核心可以支持两个vCPU。
vCPU数量是通过芯片组为每个核心提供的处理线程数乘以占用的插槽数来计算的，即：
vCPU 数量=（线程 x 核心）x 物理 CPU
假设有一款物理CPU，该CPU具有8个核心和16个线程：（16线程x8核心x1物理CPU=128 vCPU），这意味着在这个物理CPU上，可以分配128个vCPU，这是通过将每个核心的线程数相乘，然后乘以物理CPU的数量得出的。
具体的 vCPU 分配取决于工作负载和应用程序的需求。例如，在一个大型数据库服务器上，我们可以选择将更多的 vCPU 分配给每个虚拟机，以确保足够的计算资源。而在轻负载的情况下，可以分配较少的 vCPU。假设总共有128个vCPU，然后根据每个虚拟机所需的vCPU数量来计算虚拟机的数量：如果每个虚拟机需要4个vCPU，则虚拟机数量=128vCPU/4vCPU=32：如果每个虚拟机需要2个vCPU，则虚拟机数量=128vCPU/2vCPU=64：这个计算过程根据实际需求进行调整。
虚拟机VCPU的分配与调度 我们可以从下图中看到虚拟机系统的结构与功能划分，客户操作系统与虚拟机监视器共同构建了一个精巧的两级调度框架，这一框架在多核环境下尤为显著。客户操作系统负责第二级调度，主要是线程或进程在vCPU上的调度。虚拟机监视器负责第一级调度, 即vCPU在物理处理单元上的调度。
值得注意的是，这两级调度的调度策略和机制是相互独立的，它们之间不存在依赖关系，这使得虚拟机系统能够根据不同的需求和应用场景进行优化。
此外，vCPU的调度方式也十分灵活。它可以在一个或多个物理处理单元上执行，实现分时复用或空间复用物理处理单元的效果。同时，它也可以与物理处理单元建立一对一固定的映射关系，限制访问指定的物理处理单元。这种灵活的调度方式使得虚拟机系统能够更好地适应各种复杂的计算场景。
当需要为云服务器的计算能力设置上限、下限以及云服务器占用资源的优先级时，可以选择启用 CPU QoS，即对CPU做QoS（Quality of Service）：
CPU预留：分配给该VM的最少CPU资源，即云服务器获得的最低计算能力。配置值为vCPU在竞争物理CPU资源时，至少占有物理CPU主频的大小，单位为MHz。例如，该规格对应的vCPU个数为2，若预留值配置为2000，则云服务器获得的计算能力不低于4000MHz。
CPU上限：分配虚拟机占用CPU资源的上限，即云服务器获得的最大计算能力。配置值为单个vCPU在竞争资源时最多占有单个物理CPU的比例。例如，配置为0.5时，表示该云服务器单个vCPU最多可占用单个物理CPU 50%的资源。
CPU份额：多个虚拟机在竞争CPU资源时，根据比例分配资源，按优先级有低、中、高三种。CPU份额只在各虚拟机竞争计算资源时发挥作用，如果没有竞争，有需求的虚拟机可以独占主机的物理CPU资源。例如，云服务器A的优先级为“中”，云服务器B的优先级为“低”，则在资源竞争时，云服务器A竞争获得的CPU资源约为云服务器B的两倍。
以一台24线程（24GHz）的单核物理机为例，按照vCPU数量计算公式，可分配的vCPU数量为1124=24vCPU，如果我们负载三台虚拟机，在没有资源竞争的情况下，每台虚拟机使用的资源都是8Ghz（平均分配）；如果资源发生抢占，每台虚拟机都分配了24个VCPU（可以分配给单台VM所在的物理CPU虚拟出来的最大VCPU个数，这时真实的物理CPU资源则不是根据VCPU个数来确定对应的使用资源,而是根据份额来分配），即按份额分配获得CPU资源。
总的来说，计算 vCPU 的关键在于了解底层硬件的规格，并根据工作负载和性能需求进行合理的分配。
vCPU 的应用场景 vCPU在多个领域有广泛的应用场景。以下是一些主要的应用场景：
虚拟化环境：虚拟化软件可以将物理CPU的计算能力划分为多个vCPU，并为每个虚拟机分配一个或多个vCPU，从而实现多个虚拟机同时运行的功能。在虚拟化环境下，通过使用vCPU，可以有效地管理和利用物理服务器的资源，提高服务器的利用率，降低了IT成本。
多租户系统：在多租户系统中，vCPU 是计算资源的主要计量单位。用户可以根据自己的需求，选择具有不同 vCPU 数量的云服务器。每个用户都可以根据业务需求灵活调整计算资源，以满足其应用程序的灵活计算需求，同时保证资源的隔离性和安全性。
弹性扩展：随着业务的发展，当应用负载增加时，可以动态调整vCPU的数量来保证应用的可用性和性能。这种弹性伸缩的能力使得vCPU能够根据实际需求合理分配计算资源。
云计算和数据中心管理：vCPU在云计算和数据中心管理中发挥着关键作用。通过合理地分配vCPU资源，可以实现高效的资源利用和成本控制。
此外，vCPU的应用还受到应用类型的影响。对于需要大量计算资源的应用，如科学计算、数据分析等，更高的vCPU数量可以提供更好的计算性能，将计算任务分配到多个vCPU 上，加快任务的执行速度。而对于对计算资源要求不高的应用，如网站托管、数据库等，较低的vCPU数量也能满足基本的需求。
总结 本文介绍了vCPU的定义、计算方式与应用场景，其中虚拟CPU（vCPU）与物理CPU（pCPU）之间的关系是虚拟化技术的核心。vCPU作为虚拟机中的虚拟处理器，通过虚拟化层与物理服务器上的pCPU进行交互，实现了硬件资源的共享与高效利用。虚拟化技术不仅提高了服务器的利用率，还降低了运营成本，为云计算、大数据等现代信息技术的快速发展提供了有力支撑。随着技术的不断进步，虚拟化技术将在未来继续发挥重要作用，推动计算机领域不断创新与发展。
]]></content>
  </entry>
  
  <entry>
    <title>程序员的十大优秀编码习惯</title>
    <url>/post/programming/top-10-best-coding-habits-for-programmers.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>C++</tag>
      <tag>Programmer</tag>
      <tag>Habit</tag>
    </tags>
    <content type="html"><![CDATA[在C++编程的世界中，良好的编码习惯是提高代码质量、可维护性和可读性的关键。作为一名C++程序员，我们应该时刻关注并遵循这些优秀的编码习惯，让我们的代码更加清晰、高效。
 注释清晰明了  在代码中添加详细的注释是十分重要的。注释应该解释代码的目的、实现方式以及特殊考虑。良好的注释能够使得代码更易于理解和维护，避免他人和自己在未来阅读代码时产生困惑。
命名规范  使用有意义的变量名、函数名和类名，遵循命名规范。这样可以使得代码更易于理解，并且增强代码的可读性和可维护性。避免使用单个字符或不具有明确含义的命名，让代码更加清晰明了。
避免使用全局变量  全局变量会增加代码的耦合度，降低代码的可维护性和可测试性。因此，尽量减少全局变量的使用，将变量的作用域限制在局部范围内。
常量和枚举  使用常量和枚举来代替魔法数字和硬编码的值。这样可以增强代码的可读性，并且便于修改和维护。尽量避免在代码中直接使用数字或字符串，提高代码的可维护性。
空指针检查  在使用指针前，始终进行空指针检查。这样可以防止空指针引起的程序崩溃和未定义行为，提高程序的健壮性。可以使用智能指针来管理内存，减少空指针的风险。
异常处理  在可能出现异常的代码块中加入适当的异常处理机制。这样可以保证程序的健壮性和稳定性，避免程序崩溃或数据丢失。同时，避免在程序中出现未捕获的异常。
内存管理  谨慎管理内存，避免内存泄漏和悬空指针。可以使用智能指针来自动管理内存，减少手动内存管理带来的错误和不便。这样可以使得代码更加健壮和可靠。
代码复用  提倡代码复用，避免重复编写相似功能的代码。使用函数、类和模板来封装通用功能，提高代码的可重用性和扩展性。这样可以减少代码量，提高编码效率。
单一职责原则  遵循单一职责原则，保持函数和类的简洁和高内聚性。每个函数和类应该只负责完成一个明确的任务，避免功能过于复杂和耦合度过高。这样可以使得代码更加清晰和易于维护。
持续学习和改进  持续学习新的编程技术和最佳实践，不断改进自己的编码习惯和技能水平。参与开源项目、阅读优秀的代码和书籍，与其他程序员交流经验和见解。这样可以不断提升自己的编程能力，保持在技术领域的竞争力。
总的来说，遵循这些优秀的编码习惯可以帮助C++程序员编写出高质量、可维护和可靠的代码，提高工作效率和代码质量。
]]></content>
  </entry>
  
  <entry>
    <title>生产环境中使用的Ubuntu哪个版本比较稳定</title>
    <url>/post/linux/which-version-of-ubuntu-is-more-stable-in-production-environment.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Linux</tag>
      <tag>Ubuntu</tag>
      <tag>LTS</tag>
    </tags>
    <content type="html"><![CDATA[ 服务器操作系统  的稳定性直接关系到企业业务的连续性和数据的安全性。Ubuntu作为一款开源且广泛应用的Linux发行版，其不同版本在性能、安全性和稳定性方面均有所差异。那么，在生产环境中，究竟哪个版本的Ubuntu表现最为稳定呢？本文将结合实际应用和社区反馈，为您深入分析并得出结论。
首先，我们需要明确“稳定性”这一概念。在生产环境中，稳定性通常指的是操作系统在长时间运行过程中，能够保持较高的可用性、较少的故障率以及优秀的性能表现。此外，系统的安全性、兼容性以及维护便利性也是评估稳定性的重要指标。
通过对Ubuntu各版本的比较，我们可以发现，长期支持版（LTS）在生产环境中的稳定性表现普遍优于非LTS版本。LTS版本经过严格的测试和验证，旨在为企业和组织提供长期稳定的运行环境。同时，LTS版本的更新周期较长，通常为两年一次，这有助于减少因频繁更新而带来的潜在风险。
在Ubuntu的LTS版本中，Ubuntu 18.04和Ubuntu 20.04是目前较为流行的两个版本。Ubuntu 18.04于2018年发布，至今已经得到了广泛的应用和验证。它具有良好的兼容性和稳定性，能够满足大部分生产环境的需求。而Ubuntu 20.04则在Ubuntu 18.04的基础上进行了优化和升级，引入了新的特性和技术，进一步提升了系统的性能和安全性。
然而，这并不意味着Ubuntu 18.04和Ubuntu 20.04在稳定性方面没有差异。实际上，由于Ubuntu 20.04采用了更新的内核和组件，它在某些方面可能具有更高的性能表现。但与此同时，新版本也可能带来一些未知的风险和兼容性问题。因此，在选择Ubuntu版本时，需要根据实际需求和系统环境进行权衡。
除了LTS版本外，Ubuntu还提供了非LTS版本，如Ubuntu 22.04等。这些版本通常具有更多的新特性和技术，但也可能存在更多的潜在问题和风险。因此，在生产环境中使用非LTS版本需要更加谨慎，并进行充分的测试和验证。
综合以上分析，我们可以得出以下结论：在生产环境中，Ubuntu的LTS版本通常具有更高的稳定性表现。其中，Ubuntu 18.04和Ubuntu 20.04是两个较为优秀的选择。具体选择哪个版本，需要根据实际需求和系统环境进行权衡。对于追求稳定性和兼容性的用户，Ubuntu 18.04可能是一个更为稳妥的选择；而对于希望体验新特性和技术的用户，Ubuntu 20.04则可能更具吸引力。
当然，无论选择哪个版本的Ubuntu，都需要注意以下几点：首先，确保系统得到及时的更新和维护，以修复潜在的安全漏洞和性能问题；其次，合理规划系统升级和迁移策略，避免在业务高峰期进行重大变更；最后，加强系统监控和日志分析，及时发现并解决潜在问题。
总之，在选择生产环境中使用的Ubuntu版本时，应综合考虑稳定性、安全性、兼容性和性能等因素，并根据实际需求和系统环境做出明智的选择。通过合理的配置和管理，我们可以确保Ubuntu在生产环境中发挥出最佳的性能和稳定性。
原文地址： 生产环境中使用的Ubuntu哪个版本比较稳定  
]]></content>
  </entry>
  
  <entry>
    <title>SoC的发展趋势</title>
    <url>/post/soc/SoC-development-trends.html</url>
    <categories><category>SOC</category>
    </categories>
    <tags>
      <tag>SoC</tag>
    </tags>
    <content type="html"><![CDATA[技术进步不断地加快，不断重塑当今科技的轮廓。
硅基科技的核心就是电子元件的演变，特别是它们如何集成和互连。这些技术创新中最具突破性的创新之一是System-on-a-Chip（SoC）。要充分掌握SoC发展的意义，必须探索其起源、演变及其对当今科技格局的影响。
片上系统的兴起 在电子系统演进的初期阶段，电路由分立元件组成，包括焊接在电路板上的单个晶体管、电阻器和电容器。随着技术的进步，这些组件被小型化并集成到称为集成电路 （IC） 的单芯片中。
一切并没有就此止步。
在摩尔定律的推动下，摩尔定律预测集成电路中的晶体管大约每两年翻一番，各路技术大牛努力地将越来越多的功能封装到这些IC中。
SoC 是这一进步的巅峰之作，它不仅集成了晶体管，还集成了整个功能系统，包括处理器 （CPU）、内存、输入/输出系统，有时甚至是完整的网络接口。这种集成类似于将整个计算机系统压缩到指甲盖大小的芯片上。这个想法不仅涉及小型化，还涉及提高效率、降低功耗和提高电子系统的整体性能。
##SoC是当今电子技术环境中的核心
当今的技术领域由互联设备主导，从智能手机和平板电脑到智能家居设备和可穿戴设备。其中许多设备的核心是 SoC。
 便携性和外形尺寸：设备一直朝着更轻、更薄、更紧凑的方向发展。SoC 在实现这一趋势方面发挥着至关重要的作用，因为它们将多个组件集成到一个芯片上，从而减少了设备中电子设备的整体占用空间。 电源效率：随着设备变得越来越便携，电池寿命变得至关重要。SoC 凭借其集成设计，通常比执行相同任务的分立元件集合消耗更少的功耗。 成本效益：通过将多个组件合并到一个芯片中，可以显着降低制造成本。这允许市场上更实惠的设备和更广泛的产品。 性能：将组件紧密封装在 SoC 中，数据传输速率更快。这种紧密集成可以显著提高设备的性能，使实时处理和多任务处理更加顺畅。 创新和多功能性：SoC 促进了从医疗电子到汽车系统等各行各业的大量设备的发展。它们的多功能性为以前被认为不切实际或不可能的创新铺平了道路。  逻辑和SoC技术的演变 要追溯逻辑和芯片技术的发展轨迹，了解从传统集成电路 （IC） 到更先进的片上系统 （SoC） 的潜在转变至关重要。这种转变不仅仅是将更多的晶体管塞进硅上，而是涉及我们如何设计和集成的范式转变。
传统集成电路与片上系统  规模和复杂性：传统IC从集成几个晶体管开始执行基本功能，称为小规模集成（SSI）。随着技术的进步，我们实现了中等规模集成（MSI），然后是大规模集成（LSI），它可以容纳数千个晶体管。SoC 代表超大规模集成 （VLSI），数百万到数十亿个晶体管在单个芯片上包含整个系统。 功能：IC 最初是为特定功能而设计的，无论是放大信号、开关还是基本逻辑操作。SoC 集成了多种功能：CPU、GPU、RAM、存储和其他专用组件，所有这些都在同一个硅芯片中。它们代表了一个完整的电子系统。 定制：虽然IC 在很大程度上是标准化的，但 SoC 可以针对特定应用或设备进行定制。这种定制可满足特定行业（无论是智能手机、医疗设备还是汽车应用）的特定功耗、性能和功能需求。  SoC 技术趋势 与任何处于创新前沿的技术一样，SoC 正在经历一系列演进步骤，每个步骤都增强了其功能并为科技行业设定了新标准。
小型化：突破 SoC 设计的极限  制造工艺：制造商正在生产基于5nm和3nm精细工艺的SoC，将更多的晶体管塞进芯片上以实现更高的性能和效率。 3D 堆叠：增加水平空间越来越困难，该行业正在垂直方向探索。3D 堆叠涉及将硅晶圆或芯片彼此叠加放置，使用硅通孔 （TSV） 连接。这不仅节省了空间，还可以提高性能。  基于 SoC 的能效和绿色计算  自适应电压调节：通过允许 SoC 根据计算需求动态调整其电压，可以显著降低功耗。 异构计算：整合针对特定任务优化的不同类型的处理器内核，确保只有所需的内核处于活动状态，从而节省能源。  AI 和机器学习核心的集成  专用神经处理单元 （NPU）：现代 SoC，尤其是智能手机和数据中心的 SoC，现在通常包括 NPU，以更有效地处理 AI 和 ML 任务。 边缘计算：通过在 SoC 中嵌入 AI 内核，设备可以在本地（在“边缘”上）处理数据，而不是将其发送到中央服务器。这减少了延迟和带宽使用，还可以提高隐私和安全性。  增强连接：5G、Wi-Fi 6 及更高  调制解调器：将高级调制解调器直接集成到 SoC 上，确保设备为最新的通信标准做好准备，无论是 5G 蜂窝网络还是 Wi-Fi 6 和 6E，从而提高速度和连接性。 物联网及其他领域：随着物联网 （IoT） 的不断扩展，对具有多种连接选项的 SoC 的需求不断增长，为真正的互联世界铺平了道路。  制造工艺和材料的转变  替代半导体材料：硅作为芯片的传统材料，正在面临竞争。氮化镓 （GaN） 和碳化硅 （SiC） 等材料因其在性能和效率方面的潜在优势而被探索。 EUV光刻：极紫外（EUV）光刻是一种尖端的芯片制造技术，允许在芯片上蚀刻更精细的细节，从而促进上述纳米级工艺。  除了主 CPU 内核之外，典型的 SoC 还包含大量针对特定功能量身定制的组件：
 GPU（图形处理单元）：GPU 主要用于渲染图像和处理图形，在并行数据处理任务中也找到了应用场景。 DSP（数字信号处理器）：DSP 针对数学运算和算法进行了优化，对于音频处理或蜂窝通信等任务至关重要。 内存控制器：这些组件处理 SoC 处理单元和 RAM 之间的通信，确保高效的数据流。 I/O 端口：USB、HDMI 等集成接口允许芯片与外部设备通信。 网络：Wi-Fi、蓝牙和蜂窝调制解调器现在通常直接嵌入到 SoC 中，从而促进无线通信。  逻辑门及其在 SoC 功能中的作用 逻辑门，包括 AND、OR、NOT、XOR 等，决定了系统的计算逻辑。它们根据一组二进制输入解释并生成二进制输出。通过以复杂的布置连接这些门，可以形成更大的电路，如多路复用器、算术逻辑单元和存储单元，从而驱动 SoC 的功能。
工艺节点：从微米到纳米 从较大（如90nm）到较小节点（如5nm）的过渡带来了显著的优势：
 密度：节点越小，同一空间内的晶体管就越多，处理能力就越强。 电源效率：在较小的规模下，晶体管需要更少的能量来切换，从而降低整体功耗。 性能：电子传播距离越短，开关速度越快，性能就越快。  然而，随着我们在这条道路上走得更远，量子效应、漏电流和其他问题变得更加突出，工程师还需要不断创新。
SoC 中的存储器层次结构和集成 SoC 中的内存层次结构对于弥合快速处理单元和较慢主内存之间的速度差距至关重要：
 缓存（L1、L2、L3）：这些是更小、更快的易失性内存类型，用于存储经常访问的数据。L1 缓存是最小但最快的，通常直接嵌入到 CPU 内核中，其次是较大的 L2 和 L3 缓存。 RAM：这是存储活动使用的应用程序和数据的主要易失性存储器。 非易失性存储：闪存甚至 SSD 控制器可以在一些 SoC 中找到，从而加快启动时间和即时数据访问。  解决现代 SoC 中的散热和电源挑战 现代 SoC 是工程领域的壮举，但并非没有挑战：
 散热：先进的散热解决方案，如散热器、液体冷却和改进的散热材料，用于有效散热。 动态电压和频率调节 （DVFS）：通过根据实时需求调整电压和频率，SoC 可以在不需要全部性能时降低功耗和热输出。 电源门控：这涉及关闭未使用的芯片部分，从而有效降低功耗和相关发热。  SoC 的最新研究与研究 工艺进步：预计 SoC 将变得更加强大和高效。半导体工艺的进步，包括更小的工艺节点和改进的电源管理，将提高SoC的性能。
AI 集成：人工智能 （AI） 功能正在集成到 SoC 中。这使设备能够在本地执行与 AI 相关的任务，从而减少对云处理的需求并增强实时决策。
物联网和边缘计算：SoC 在物联网 （IoT） 和边缘计算中发挥着至关重要的作用。它们旨在处理连接设备的处理需求，并支持边缘计算，以实现更快的数据分析并减少延迟。
安全功能：随着对网络安全的日益关注，SoC 正在整合增强的安全功能。这包括基于硬件的加密、安全启动机制和高级安全协议，以保护数据和设备。
定制：SoC 的可定制性越来越强，以满足特定的应用要求。这一趋势使制造商能够设计适合其设备的芯片，从而优化性能和功耗。
能源效率：高能效 SoC 是一个重点，尤其是在移动设备和物联网应用中。低功耗设计和改进的电源管理技术对于延长电池寿命至关重要。
5G 集成：随着 5G 网络的扩展，正在开发支持 5G 连接的 SoC。这对于移动设备和物联网生态系统中的高速数据传输和低延迟至关重要。
异构计算：SoC 正在整合异构计算架构，将 CPU、GPU 和加速器相结合，以更高效地处理各种工作负载。
环境可持续性：人们越来越重视使 SoC 更加环保，重点是减少电子垃圾和使用对环境影响较小的材料。
逻辑和SoC的未来 进一步小型化：随着摩尔定律继续影响半导体制造，我们可以期待进一步的小型化，可能达到1纳米甚至亚纳米工艺。
新技术的集成：量子元件、基于光子学的组件和仿生电路可能会进入传统 SoC。
柔性和可穿戴电子器件：SoC将在柔性电子器件的发展中发挥关键作用，为可穿戴和植入式设备提供新的外形尺寸。
3D 和 4D 集成：随着我们在 2D 平面小型化方面达到物理极限，3D 堆叠将变得更加普遍。此外，可以探索考虑基于时间的动力学的 4D 集成，以实现更高效的实时计算。
潜在挑战和需要改进的领域  热管理：随着芯片封装的晶体管和组件越来越多，高效散热将变得越来越具有挑战性。创新的冷却解决方案至关重要。 电源限制：随着对更多便携式和远程设备的推动，尤其是在物联网中，电源效率仍将至关重要。 制造复杂性：先进的工艺和各种组件的集成将对制造技术提出挑战，需要在光刻和材料方面进行创新。 安全性：随着 SoC 在关键领域找到应用，确保芯片级安全性免受物理和数字攻击至关重要。  哪些行业可以使用 SoC 医疗保健：随着远程医疗和远程诊断的兴起，SoC 可以驱动可穿戴健康监测器、智能植入物和个性化药物输送系统。
汽车：未来的自动驾驶汽车将严重依赖先进的SoC进行实时数据处理、传感器集成和决策。
航空航天和国防：SoC 可以为卫星、无人机和先进国防设备提供紧凑、强大的机载系统。
农业：从智能灌溉系统到基于无人机的作物监测，SoC 可以彻底改变精准农业和可持续农业实践。
娱乐和游戏：增强现实 （AR） 和虚拟现实 （VR） 将继续增长，需要具有高图形能力和低功耗的 SoC。
结论 SoC 诞生于实现更高效率、小型化和集成的愿望，已经从单纯的概念转变为为大量现代设备提供动力的无处不在的组件。从智能手机和可穿戴设备到自动驾驶汽车和智能家居系统，SoC 是推动当今技术奇迹的无声主力。
SoC 开发的最新趋势，包括量子计算元素的结合、对电源效率的追求以及 SoC 在边缘计算中的作用，凸显了该技术的多功能性和适应性。该行业不断推动更小的制造工艺、增强的性能指标以及与不同行业的整合，有望进一步塑造未来电子产品的轮廓。
]]></content>
  </entry>
  
  <entry>
    <title>机器学习如何帮助芯片设计</title>
    <url>/post/soc/how-machine-learning-can-help-chip-design.html</url>
    <categories><category>SOC</category>
    </categories>
    <tags>
      <tag>Machine Learning</tag>
      <tag>Chip</tag>
    </tags>
    <content type="html"><![CDATA[在过去的 20 年里，集成电路 （IC） 行业的发展是难以想象的，我们现在正在见证下一阶段的变化。为了利用半导体工艺的进步带来的机遇，需要不断开发新的工具和方法，以便使用这些技术的设计工程师尽可能地提高生产力。
机器学习（ML）也已经发展到现在似乎每个人都在使用的工具之一，但是ML对芯片设计意味着什么？是否ML确实是芯片设计的未来？
生产力挑战 在数字设计的萌芽阶段，工程师们采用了完全定制的电路布局方法，手动放置和连接每个晶体管，这是一项艰巨且耗时的任务。设计人员很快意识到，通过使用标准单元和原理图网表来实现数字逻辑设计，可以加快设计任务。
但是，创建原理图网表也是手动的。桌面 Unix 工作站的出现使寄存器传输逻辑 （RTL） 综合成为可能，允许工程师使用高级硬件描述语言（如 VHDL 和 Verilog）创建数字逻辑，这些语言可以快速综合具有数千个逻辑门的网表。
虽然这有助于克服设计问题，但它无意中创造了另一个问题——如何布局大量标准单元。反过来，通过开发自动化布局布线来克服这一点，这两个系统的综合效果是大大提高了数字设计流程的生产力，使设计人员能够专注于优化功耗、性能和面积 （power/performance /area PPA）。
然而，设计的挑战与设计的规模成比例，标准单元已从数千个迅速增长到数百万个。虽然IC的规模不断扩大，但可用的IC设计工程师的数量却跟不上，导致设计挑战不断加剧。随着晶圆代工工艺尺寸的缩小，晶体管密度不可避免地增加。为了让行业跟上这种不断提高的复杂性，设计工程师必须提高工作效率。
机器学习是帮助实现这一目标的理想之选。
EDA 中的 ML 1959年，计算机游戏和人工智能的先驱亚瑟·塞缪尔（Arthur Samuel）将ML定义为“使计算机能够在没有明确编程的情况下学习的研究领域”。芯片设计过程的每个元素都很难实现自动化，因为它在很大程度上依赖于单个工程师的经验。
传统上，该行业通过将大型芯片设计项目分解为更小的任务来解决这些问题，这包括timing closure、placement constraints、floor-planning 、了解EM以及DRC。需要许多芯片专家的投入，几乎每家芯片设计公司，无论大小，都有在timing、APR或功耗方面具有专业知识的常驻“专家”。
ML之所以非常适合应用到设计自动化，是因为大部分设计过程都是手动的，需要对可预测的场景进行迭代评估。机器学习推理的强大之处在于可以对其进行训练，以在比手动方法更短的时间内提供改进的结果。
ML 的概念在 1959 年只不过是一种理论，但计算机技术的巨大进步，导致多个强大的 GPU 并行运行，以及在云中执行复杂计算的能力，使计算机科学家在 ML 领域取得了巨大进步。近年来，它已经成功地应用于设计流程的各个任务（例如自动布局布线），但只有将其应用于更高层次，才能充分发挥ML的全部功能。
设计流程优化 虽然 ML 已经改进了设计流程的各个组件，但下一步是使用它来加速整个设计流程，目前这始终需要设计的手动交互。
使用AI获取这些专业知识对生产力可以产生更大的影响。
在当前的手动和迭代流程开发过程中，设计人员创建最初的RTL，综合设计，生成结果。然后，经验丰富的工程师根据输出结果调整流程，然后重新综合设计以生成新结果。此迭代过程一直持续到达到所需的 PPA 达到可以接受可用结果为止。
这需要大量的工程工作，并且对计算资源的使用效率低下。
在团队中增加更多的工程师并不一定能转化为 PPA 的改进。ImageImageImage
现在，有一种革命性的、机器学习驱动的芯片设计流程优化方法。这种新方法允许工程师指定 PPA 目标，然后以全自动方式优化数字 RTL 到 GDS 流程的各个方面，以比手动流程更快地实现这些目标。
今天的工程师可以在现有的 ML 架构上开发RTL，并利用现在可用的海量计算能力。ML仅使用实时设计数据样本，使其能够“即时”做出优化决策。这意味着它可以立即停止无法收敛于更好的 PPA 结果的RTL开发，从而将计算资源重新分配给其他更需要的任务。这种方法比手动流程调整更有效，因为在手动流程调整中，仅在每次综合结束时查看结果。
在流程优化过程中，学习引擎会分析大量设计数据。随着强化学习过程的进行，将创建一个机器学习模型，捕获设计数据分析。然后，通过在项目之间重用数据，可以将其用作未来设计流程优化的起点，从而节省大量计算资源并更快地交付改进的 PPA。
结论 半导体行业的持续增长将要求芯片设计工程师提高工作效率。利用现在可用的支持云的并行和分布式计算资源，ML工具将进一步改善PPA，使工程团队能够达到应对更大和日益复杂的芯片设计所带来的挑战所需的生产力水平。过去，EDA工具提高了工程师的工作效率。从现在开始，ML将提高EDA工具的生产力，从而提高使用它们的工程师的生产力。
]]></content>
  </entry>
  
  <entry>
    <title>各个Linux发行版的特点和优势</title>
    <url>/post/linux/features-and-advantages-of-each-linux-distribution.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Linux</tag>
      <tag>CentOS</tag>
      <tag>Ubuntu</tag>
      <tag>Debian</tag>
      <tag>Fedora</tag>
    </tags>
    <content type="html"><![CDATA[在信息化快速发展的今天，服务器操作系统的选择对于企业的业务运行至关重要。在众多操作系统中，CentOS、Ubuntu、Debian和Fedora因其独特的优势和特点而备受关注。
本文将对这四个系统进行详细解析，帮助读者了解其特点和优势，以便在选择时能够做出明智的决策。
CentOS的特点和优势 CentOS（Community ENTerprise Operating System，社区企业操作系统）是一个基于Red Hat Enterprise Linux（RHEL）源代码构建的开源操作系统。它继承了RHEL的稳定性和可靠性，为企业提供了一个成熟且安全的平台。CentOS的特点和优势主要体现在以下几个方面：
 稳定性与可靠性：CentOS经过严格的测试和验证，确保在各种环境下都能稳定运行。其强大的错误修复能力和完善的安全机制，使得企业可以信赖其作为关键业务的支撑平台。 广泛的软件支持：CentOS拥有庞大的软件仓库，涵盖了众多开源软件和商业软件。企业可以轻松安装和配置所需的应用程序，满足多样化的业务需求。 低成本：CentOS是免费的，企业无需支付高昂的许可费用。这使得CentOS成为许多预算有限企业的首选操作系统。 良好的兼容性：CentOS与RHEL具有高度的兼容性，使得企业可以在CentOS上无缝迁移RHEL的应用程序和数据。同时，CentOS还支持多种硬件架构，为企业的灵活部署提供了便利。  Ubuntu的特点和优势 Ubuntu是一款基于Debian的开源操作系统，以其友好的界面、强大的功能和广泛的应用支持而备受赞誉。Ubuntu的特点和优势主要体现在以下几个方面：
 易用性：Ubuntu拥有简洁明了的界面设计，使得初学者也能轻松上手。同时，其丰富的文档和社区支持，使得用户在使用过程中能够迅速解决问题。 强大的软件包管理：Ubuntu采用了先进的APT软件包管理系统，使得软件的安装、更新和卸载变得异常简单。用户只需通过简单的命令或图形界面，就能轻松管理系统的软件包。 广泛的应用支持：Ubuntu支持众多开源和商业软件，涵盖了办公、开发、多媒体等多个领域。这使得Ubuntu成为了一款功能强大的操作系统，能够满足企业的多样化需求。 活跃的社区和开源文化：Ubuntu拥有庞大的开源社区，为用户提供了丰富的资源和支持。同时，Ubuntu还积极倡导开源文化，推动开源软件的发展和创新。  Debian的特点和优势 Debian是一个由社区开发的开源操作系统，以其稳定性、可靠性和强大的软件包管理而著称。Debian的特点和优势主要体现在以下几个方面：
 卓越的稳定性：Debian在发布新版本之前会进行严格的测试和验证，确保系统的稳定性和可靠性。这使得Debian成为了一款适合长期运行的服务器操作系统。 强大的软件包管理：Debian采用了先进的DPKG软件包管理系统，为用户提供了强大的软件包管理功能。用户可以通过DPKG轻松安装、更新和卸载软件包，实现系统的灵活配置。 广泛的软件支持：Debian拥有庞大的软件仓库，涵盖了众多开源软件和商业软件。这使得企业可以根据自身需求选择合适的软件，实现业务的快速发展。 高度的自定义性：Debian允许用户根据自己的需求定制系统，包括选择软件包、配置系统参数等。这使得Debian成为了一款高度灵活和可定制的操作系统。  Fedora的特点和优势 Fedora是一个由Red Hat公司赞助的开源操作系统，以其创新性和前沿性而备受关注。Fedora的特点和优势主要体现在以下几个方面：
 创新性强：Fedora致力于推动开源技术的发展和创新，为用户提供了众多前沿的技术和功能。这使得Fedora成为了一款适合开发人员和技术爱好者的操作系统。 快速的更新周期：Fedora采用了较短的更新周期，使得用户能够及时获得最新的软件包和功能。同时，这也促进了Fedora社区的不断发展和进步。 广泛的应用支持：Fedora支持众多开源和商业软件，为用户提供了丰富的应用程序选择。这使得Fedora能够满足用户在各个领域的需求。 强大的社区支持：Fedora拥有活跃的社区和丰富的资源，为用户提供了解决问题的途径和支持。这使得Fedora成为了一款易于使用和学习的操作系统。  综上所述，CentOS、Ubuntu、Debian和Fedora各有其特点和优势。企业在选择服务器操作系统时，应根据自身需求、预算和技术实力进行综合考虑。无论选择哪个系统，都需要确保能够满足企业的业务需求，并为企业的发展提供稳定的支持。
原文地址： 各个Linux发行版的特点和优势  
]]></content>
  </entry>
  
  <entry>
    <title>Fedora 41引入了全新 DNF5 软件包管理</title>
    <url>/post/linux/fedora-41-introduces-new-dnf5-package-management.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Fedora 41</tag>
      <tag>DNF5</tag>
    </tags>
    <content type="html"><![CDATA[Fedora是一个各方面都经过精心打磨的发行版，在众多Linux发行版中名列前茅。然而，它的软件包管理器是其软肋，与当今标准相比显得过时。
使用它可能会考验用户的耐心，因为其性能明显落后于Debian快速的APT。与Arch的Pacman相比更是毫无竞争力。然而，在Fedora 41中，这一切都可能会发生改变。
提案 Fedora项目宣布了一项提案，计划在即将发布的Fedora 41中将默认软件包管理器从DNF更换为DNF5，预计在十月中旬发布。
由Red Hat的Jan Kolarik和Jaroslav Mracek领导的这项提议目前正在公开审查阶段，等待Fedora工程指导委员会（FESCo）的批准。
DNF5：功能和改进 DNF5承诺带来显著的改进。它旨在通过提供更快的查询处理速度和更小的占用空间来提升用户体验。
这意味着它不需要Python依赖项，并且将通过在dnf5和新的dnf5daemon之间共享元数据来帮助消除元数据的冗余。以下是可以期待的主要好处。
 增强性能：DNF5承诺加快仓库元数据处理速度和改进软件包查询操作，旨在节省用户在软件包管理任务中宝贵的时间。 减少系统占用：通过消除Python依赖项并合并DNF和MicroDNF的功能，DNF5提供了一个显著较小的安装大小，减少了元数据的冗余。 统一体验：Fedora旨在为所有平台提供一致的软件包管理体验，DNF5将成为服务器、工作站和容器的唯一软件包管理器。  展望未来 DNF团队已经为那些担心过渡的人制定了详细的计划，以确保顺利的升级路径。在系统升级或安装DNF5时，DNF5软件包将自动替换现有的DNF软件包。
此外，为了保持用户体验的连续性，还包括了对yum命令的向后兼容性和一个新的守护进程服务dnf5daemon。
然而，为了实现这一切，Fedora工程指导委员会（FESCo）必须首先批准该提议。目前，它正在进行第二次迭代，收集了来自初始尝试的广泛社区反馈。
总之，值得注意的是，关于DNF5成为默认软件包管理器的讨论始于大约一年半前，目的是将其引入Fedora 39中。
显然，那次过渡没有发生，甚至即将到来的Fedora 40版本，预计下个月发布，也避免了实施这一重大改变。
尽管如此，人们对此次DNF5提案能够获得批准充满了乐观情绪，为其在Fedora 41中的预期首次亮相铺平了道路。可以在此处查看提案的全部内容。
原文地址： Fedora 41引入了全新 DNF5 软件包管理  
]]></content>
  </entry>
  
  <entry>
    <title>AMD又发布一新品，还是Zen3架构的</title>
    <url>/post/soc/amd-launch-Ryzen-5000XT-CPU-based-on-zen3.html</url>
    <categories><category>SOC</category>
    </categories>
    <tags>
      <tag>AMD</tag>
      <tag>CPU</tag>
      <tag>AM4</tag>
      <tag>AM5</tag>
    </tags>
    <content type="html"><![CDATA[近期，AMD在北京举办的AI PC峰会上除了发布Zen5的新品，还表示对AM4平台的继续支持——宣布正在开发基于Zen 3架构的Ryzen 5000XT CPU，以更新其在中端市场的产品线。
这一系列产品将继承并提升自2019年推出的Ryzen 3000XT系列，这些处理器相比原有的-X型号，提升了最大加速频率。当时该系列为了与Intel的Comet Lake处理器竞争而推出，目的在于夺回AMD当时正在丧失的市场份额。
尽管AMD尚未正式公布新XT处理器的详细规格，但基于过去的情况，我们可以预期5000XT系列将在频率上相较非XT版本有所提升，而核心配置方面估计变动不大。
AMD针对不同价格区间准备了多款SKU。值得注意的是，除了5000XT系列之外，还透露了中国特供版本Ryzen 8000 APU，包括Ryzen 7 8700F和Ryzen 5 8400F，详情请见《AMD发布两款新U，专供中国市场》。
此外，AMD还证实华硕的X370主板可以通过刷华擎 B450的BIOS来支持Zen 3架构。这说明AMD还不想放弃AM4插槽，尽管AMD在2022年推出了AM5平台，但AM4平台凭借其较低的入门成本，仍然受到了追求性价比的玩家的欢迎。事实上，AMD在今年还推出了多款AM4平台的处理器，包括锐龙5 5700X3D和锐龙5 5600GT等，进一步丰富了AM4平台的产品线，满足了不同用户的需求。
AM4和AM5是AMD的两代处理器插座，AM4插座采用了PGA设计，拥有1331个针脚，支持从Ryzen 1000系列到Ryzen 5000系列的处理器，覆盖了Zen到Zen 3的各种架构。这一插座设计主要面向入门用户以及希望在现有硬件基础上升级的用户，支持DDR4内存和最高PCIe 4.0的标准，为大多数日常使用和游戏需求提供了充足的支持。
AM5转而采用LGA设计，提供1718个触点，这种设计不仅提升了处理器与主板间的联系稳定性，还增加了电路的复杂度，从而支持更高性能的处理器和技术。AM5插座原生支持基于Zen 4架构的处理器，如Ryzen 7000系列，并且首次将DDR5内存引入AMD平台，同时支持PCIe 5.0标准。
]]></content>
  </entry>
  
  <entry>
    <title>三星推出Mach-1 AI芯片，挑战英伟达</title>
    <url>/post/news/samsung-launches-Mach-1-AI-chip-to-challenge-Nvidia.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Samsung</tag>
      <tag>AI</tag>
      <tag>Nvidia</tag>
    </tags>
    <content type="html"><![CDATA[近日，作为全球领先的半导体科技巨头，三星近期在人工智能领域动作连连。首先是宣布即将推出其全新的 Mach-1 AI  芯片，准备在挑战NVIDIA的市场地位。然后是准备在高带宽存储器（HBM）领域加大投入，准备与其主要竞争对手SK hynix展开激烈竞争。接着宣布将成立一家人工通用智能（AGI）计算实验室，加速其在人工通用智能研发工作的进展。
庆桂显强调，目前的AI系统由于内存瓶颈问题，面临性能下降和功耗问题。三星通过建立AGI计算实验室和实现AI架构创新，希望能够解决这一问题。Mach-1芯片是三星研发的成果，该芯片作为AI加速器，通过减少GPU和内存板上存在的“瓶颈”，显著提高性能，从而打破NVIDIA在AI领域的主导地位。
三星计划利用多种算法，将内存和GPU芯片间发生的瓶颈现象减少到当前的八分之一，并将功耗效率提高八倍。Mach-1还将使得在低功耗内存而非高功耗HBM的情况下，也能进行大型语言模型（LLM）的推断。
庆桂显透露，三星计划在今年年底前制造Mach-1芯片的原型，并预计明年初开始看到使用最新AI芯片的AI系统。据报告，三星已经与韩国AI公司Naver达成协议，后者正逐步减少对NVIDIA芯片的依赖。这项协议包括价值7.52亿美元的Mach-1芯片，预计还会有更多订单。
从全局角度看，三星要想在AI领域全面超越英伟达，基本就是Mission Impossible。英伟达在AI和深度学习领域的领先地位，并不仅仅建立在其硬件产品的性能上，还依托于其软件生态系统、开发工具和广泛的合作伙伴网络。这些因素共同构成了英伟达的竞争壁垒，不是短期内就能轻易突破的。
三星虽然在半导体制造领域具有强大的实力，并可能在如内存瓶颈等特定技术问题上取得突破，但是在AI领域想要实现从硬件到软件生态系统的全面领先，需要时间、巨大的资源投入，以及持续的创新。即便是在某些技术或市场细分领域取得领先，也不等同于在整个AI领域实现全面胜利。
另外，AI领域的竞争是多方位的，不仅有英伟达，还有包括AMD、Intel、谷歌、亚马逊等多家技术巨头在内的激烈竞争。这些公司在不同的AI子领域有着深入的布局和独到的优势。三星要在这样一个高度竞争的环境中全面胜出，大家觉得有可能吗？
]]></content>
  </entry>
  
  <entry>
    <title>RGB颜色对照表</title>
    <url>/post/linux/rgb-color-value-in-hex.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>rgb</tag>
    </tags>
    <content type="html"><![CDATA[RGB色彩模式是工业界的一种颜色标准，是通过对红(R)、绿(G)、蓝(B)三个颜色通道的变化以及它们相互之间的叠加来得到各式各样的颜色的，RGB即是代表红、绿、蓝三个通道的颜色，这个标准几乎包括了人类视力所能感知的所有颜色，是目前运用最广的颜色系统之一。
]]></content>
  </entry>
  
  <entry>
    <title>不理解EMC，画不好PCB</title>
    <url>/post/hardware/understand-emc-deeply-will-help-pcb-layout.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>EMC</tag>
      <tag>Layout</tag>
      <tag>PCB</tag>
    </tags>
    <content type="html"><![CDATA[本文将从PCB的分层策略、布局技巧和布线规则三个方面，介绍EMC的PCB设计技术。
除了元器件的选择和电路设计之外，良好的印制电路板（PCB）设计在电磁兼容性中也是一个非常重要的因素。PCB EMC设计的关键，是尽可能减小回流面积，让回流路径按照设计的方向流动。最常见返回电流问题来自于参考平面的裂缝、变换参考平面层、以及流经连接器的信号。跨接电容器或是去耦合电容器可能可以解决一些问题，但是必需要考虑到电容器、过孔、焊盘以及布线的总体阻抗。
PCB分层策略 电路板设计中厚度、过孔制程和电路板的层数不是解决问题的关键，优良的分层堆叠是保证电源汇流排的旁路和去耦、使电源层或接地层上的瞬态电压最小并将信号和电源的电磁场屏蔽起来的关键。从信号走线来看，好的分层策略应该是把所有的信号走线放在一层或若干层，这些层紧挨著电源层或接地层。对於电源，好的分层策略应该是电源层与接地层相邻，且电源层与接地层的距离尽可能小，这就是我们所讲的“分层”策略。下面我们将具体谈谈优良的PCB分层策略。
 布线层的投影平面应该在其回流平面层区域内。布线层如果不在其回流平面层地投影区域内，在布线时将会有信号线在投影区域外，导致“边缘辐射”问题，并且还会导致信号回路面积地增大，导致差模辐射增大。 尽量避免布线层相邻的设置。因为相邻布线层上的平行信号走线会导致信号串扰，所以如果无法避免布线层相邻，应该适当拉大两布线层之间的层间距，缩小布线层与其信号回路之间的层间距。 相邻平面层应避免其投影平面重叠。因为投影重叠时，层与层之间的耦合电容会导致各层之间的噪声互相耦合。  多层板设计 时钟频率超过5MHz，或信号上升时间小于5ns时，为了使信号回路面积能够得到很好的控制，一般需要使用多层板设计。在设计多层板时应注意如下几点原则：
 关键布线层（时钟线、总线、接口信号线、射频线、复位信号线、片选信号线以及各种控制信号线等所在层）应与完整地平面相邻，优选两地平面之间，如图1所示。关键信号线一般都是强辐射或极其敏感的信号线，靠近地平面布线能够使其信号回路面积减小，减小其辐射强度或提高抗干扰能力。  图1 关键布线层在两地平面之间
电源平面应相对于其相邻地平面内缩（建议值5H～20H）。电源平面相对于其回流地平面内缩可以有效抑制“边缘辐射”问题，如图2所示。  图2电源平面应相对于其相邻地平面内缩
此外，单板主工作电源平面（使用最广泛的电源平面）应与其地平面紧邻，以有效地减小电源电流的回路面积，如图3所示。
图3 电源平面应与其地平面紧邻
单板TOP、BOTTOM层是否无≥50MHz的信号线。如有，最好将高频信号走在两个平面层之间，以抑制其对空间的辐射。  单层板和双层板设计 对于单层板和双层板的设计，主要应注意关键信号线和电源线的设计。电源走线附近必须有地线与其紧邻、平行走线，以减小电源电流回路面积。
单层板的关键信号线两侧应该布“Guide Ground Line”，如图4所示。双层板的关键信号线地投影平面上应有大面积铺地，或者同单层板地处理办法，设计“Guide Ground Line”，如图5所示。关键信号线两侧地“保卫地线”一方面可以减小信号回路面积，另外，还可以防止信号线与其他信号线之间地串扰。
图4单层板的关键信号线两侧布“Guide Ground Line”
图5 双层板的关键信号线地投影平面上大面积铺地
总的来说，PCB板的分层可以依据下表来设计。
PCB布局技巧 PCB布局设计时，应充分遵守沿信号流向直线放置的设计原则，尽量避免来回环绕，如图6所示。这样可以避免信号直接耦合，影响信号质量。此外，为了防止电路之间、电子元器件之间的互相干扰和耦合，电路的放置和元器件的布局应遵从如下原则：
图6 电路模块沿信号流向直线放置
  单板上如果设计了接口“干净地”，则滤波、隔离器件应放置在“干净地”和工作地之间的隔离带上。这样可以避免滤波或隔离器件通过平面层互相耦合，削弱效果。此外，“干净地”上，除了滤波和防护器件之外，不能放置任何其他器件。
  多种模块电路在同一PCB上放置时，数字电路与模拟电路、高速与低速电路应分开布局，以避免数字电路、模拟电路、高速电路以及低速电路之间的互相干扰。另外，当线路板上同时存在高、中、低速电路时，为了避免高频电路噪声通过接口向外辐射，应该遵从图7中的布局原则。
  图7 高、中、低速电路布局原则
线路板电源输入口的滤波电路应应靠近接口放置，避免已经经过了滤波的线路被再次耦合。  图8 电源输入口的滤波电路应应靠近接口放置
接口电路的滤波、防护以及隔离器件靠近接口放置，如图9所示，可以有效的实现防护、滤波和隔离的效果。如果接口处既有滤波又有防护电路，应该遵从先防护后滤波的原则。因为防护电路是用来进行外来过压和过流抑制的，如果将防护电路放置在滤波电路之后，滤波电路会被过压和过流损坏。此外，由于电路的输入输出走线相互耦合时会削弱滤波、隔离或防护效果，布局时要保证滤波电路（滤波器）、隔离以及防护电路的输入输出线不要相互耦合。  图9接口电路的滤波、防护以及隔离器件靠近接口放置
敏感电路或器件（如复位电路等）远离单板各边缘特别是单板接口侧边缘至少1000mil。 存在较大电流变化的单元电路或器件（如电源模块的输入输出端、风扇及继电器）附近应放置储能和高频滤波电容，以减小大电流回路的回路面积。 滤波器件需并排放置，以防止滤波后的电路被再次干扰。 晶体、晶振、继电器、开关电源等强辐射器件远离单板接口连接器至少1000mil。这样可将干扰直接向外辐射或在外出电缆上耦合出电流来向外辐射。  PCB布线规则 除了元器件的选择和电路设计之外，良好的印制电路板（PCB）布线在电磁兼容性中也是一个非常重要的因素。既然PCB是系统的固有成分，在PCB布线中增强电磁兼容性不会给产品的最终完成带来附加费用。任何人都应记住一个拙劣的PCB布线能导致更多的电磁兼容问题，而不是消除这些问题，在很多例子中，就算加上滤波器和元器件也不能解决这些问题。到最后，不得不对整个板子重新布线。因此，在开始时养成良好的PCB布线习惯是最省钱的办法。下面将对PCB布线的一些普遍规则和电源线、地线及信号线的设计策略进行介绍，最后，根据这些规则，对空气调节器的典型印制电路板电路提出改进措施。
 布线分离  布线分离的作用是将PCB同一层内相邻线路之间的串扰和噪声耦合最小化。3W规范表明所有的信号（时钟，视频，音频，复位等等）都必须象图10所示那样，在线与线，边沿到边沿间予以隔离。为了进一步的减小磁耦合，将基准地布放在关键信号附近以隔离其他信号线上产生的耦合噪声。
图10 线迹隔离
保护与分流线路  设置分流和保护线路是对关键信号，比如对在一个充满噪声的环境中的系统时钟信号进行隔离和保护的非常有效的方法。在图21中，PCB内的并联或者保护线路是沿着关键信号的线路布放。保护线路不仅隔离了由其他信号线上产生的耦合磁通，而且也将关键信号从与其他信号线的耦合中隔离开来。分流线路和保护线路之间的不同之处在于分流线路不必被端接（与地连接），但是保护线路的两端都必须连接到地。为了进一步的减少耦合，多层PCB中的保护线路可以每隔一段就加上到地的通路。
图11 分流和保护线路
电源线设计  根据印制线路板电流的大小，尽量加粗电源线宽度，减少环路电阻。同时、使电源线、地线的走向和数据传递的方向一致，这样有助于增强抗噪声能力。在单面板或双面板中，如果电源线走线很长，应每隔3000mil对地加去耦合电容，电容取值为10uF＋1000pF。
地线设计  地线设计的原则是：
 数字地与模拟地分开。若线路板上既有逻辑电路又有线性电路，应使它们尽量分开。低频电路的地应尽量采用单点并联接地，实际布线有困难时可部分串联后再并联接地。高频电路宜采用多点串联接地，地线应短而租，高频元件周围尽量用栅格状大面积地箔。 接地线应尽量加粗。若接地线用很纫的线条，则接地电位随电流的变化而变化，使抗噪性能降低。因此应将接地线加粗，使它能通过三倍于印制板上的允许电流。如有可能，接地线应在2~3mm以上。 接地线构成闭环路。只由数字电路组成的印制板，其接地电路布成团环路大多能提高抗噪声能力。  信号线设计  对于关键信号线，如果单板有内部信号走线层，则时钟等关键信号线布在内层，优先考虑优选布线层。另外，关键信号线一定不能跨分割区走线，包括过孔、焊盘导致的参考平面间隙，否则会导致信号回路面积的增大。而且关键信号线应距参考平面边沿≥3H（H为线距离参考平面的高度），以抑制边缘辐射效应。
对于时钟线、总线、射频线等强辐射信号线和复位信号线、片选信号线、系统控制信号等敏感信号线，应远离接口外出信号线。从而避免强辐射信号线上的干扰耦合到外出信号线上，向外辐射；也避免接口外出信号线带进来的外来干扰耦合到敏感信号线上，导致系统误操作。
对于差分信号线应同层、等长、并行走线，保持阻抗一致，差分线间无其它走线。因为保证差分线对的共模阻抗相等，可以提高其抗干扰能力。
根据以上布线规则，对空气调节器的典型印制电路板电路进行改进优化，如图12所示。
图12 改进空气调节器的典型印制电路板电路
总体来说，PCB设计对EMC的改善是：在布线之前，先研究好回流路径的设计方案，就有最好的成功机会，可以达成降低EMI辐射的目标。而且在还没有动手实际布线之前，变更布线层等都不必花费任何钱，是改善EMC最便宜的做法。
]]></content>
  </entry>
  
  <entry>
    <title>Linux内核内存管理架构</title>
    <url>/post/linux/linux-kernel-memory-management-architecture.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Linux</tag>
      <tag>Kernel</tag>
      <tag>Memory Management</tag>
    </tags>
    <content type="html"><![CDATA[内存管理子系统可能是 linux  内核中最为复杂的一个子系统，其支持的功能需求众多，如页面映射、页面分配、页面回收、页面交换、冷热页面、紧急页面、页面碎片管理、页面缓存、页面统计等，而且对性能也有很高的要求。
本文从内存管理硬件架构、地址空间划分和内存管理软件架构三个方面入手，尝试对内存管理的软硬件架构做一些宏观上的分析总结。
内存管理硬件架构 因为内存管理是内核最为核心的一个功能，针对内存管理性能优化，除了软件优化，硬件架构也做了很多的优化设计。
下图是一个目前主流处理器上的存储器层次结构设计方案。
从图中可以看出，对于逻辑cache架构读写内存，硬件设计了3条优化路径。
 首先L1 cache支持虚拟地址寻址，保证CPU出来的虚拟地址（VA）不需要转换成物理地址（PA）就可以用来直接查找L1 cache，提高cache查找效率。当然用VA查找cache，有安全等缺陷，这需要CPU做一些特别的设计来进行弥补，具体可以阅读《计算机体系结构：量化研究方法》了解相关细节。 如果L1 cache没有命中，这就需要进行地址转换，把VA转换成PA。linux的内存映射管理是通过页表来实现的，但是页表是放在内存中的，如果每次地址转换过程都需要访问一次内存，其效率是十分低下的。这里CPU通过TLB硬件单元（在MMU中）来加速地址转换。 获得PA后，在L2 cache中再查找缓存数据。L2 cache一般比L1 cache大一个数量级，其查找命中率也更高。如果命中获得数据，则可避免去访问内存，提高访问效率。  可见，为了优化内存访问效率，现代处理器引入多级cache、TLB等硬件模块。每个硬件模块内部还有大量的设计细节，这里不再深入，如有兴趣可以阅读《计算机体系结构：量化研究方法》等书籍进一步了解。
内存映射空间划分 根据不同的内存使用方式和使用场景需要，内核把内存映射地址空间划分成多个部分，每个划分空间都有自己的起止地址、分配接口和使用场景。下图是一个常见的32位地址空间划分结构。
 DMA内存动态分配地址空间：  一些DMA设备因为其自身寻址能力的限制，不能访问所有内存空间。如早期的ISA设备只能在24位地址空间执行DMA，即只能访问前16MB内存。
所以需要划分出DMA内存动态分配空间，即DMA zone。其分配通过加上GFP_ATOMIC控制符的kmalloc接口来申请。
 直接内存动态分配地址空间：  因为访问效率等原因，内核对内存采用简单的线性映射，但是因为32位CPU的寻址能力（4G大小）和内核地址空间起始的设置（3G开始），会导致内核的地址空间资源不足，当内存大于1GB时，就无法直接映射所有内存。
无法直接映射的地址空间部分，即highmem zone。在DMA zone和highmem zone中间的区域即normal zone，主要用于内核的动态内存分配。其分配通过kmalloc接口来申请。
 高端内存动态分配地址空间：  高端内存分配的内存是虚拟地址连续而物理地址不连续的内存，一般用于内 核动态加载的模块和驱动，因为内核可能运行了很久，内存页面碎片情况严重，如果要申请大的连续地址的内存页会比较困难，容易导致分配失败。根据应用需要，高端内存分配提供多个接口:
 vmalloc：指定分配大小，page位置和虚拟地址隐式分配； vmap：指定page位置数组，虚拟地址隐式分配； ioremap：指定物理地址和大小，虚拟地址隐式分配。   持久映射地址空间：  内核上下文切换会伴随着TLB刷新，这会导致性能下降。但一些使用高端内存的模块对性能也有很高要求。
持久映射空间在内核上下文切换时，其TLB不刷新，所以它们映射的高端地址空间寻址效率较高。其分配通过kmap接口来申请。
kmap与vmap的区别是：vmap可以映射一组page，即page不连续，但虚拟地址连续，而kmap只能映射一个page到虚拟地址空间。kmap主要用于fs、net等对高端内存访问有较高性能要求的模块中。
 固定映射地址空间：  持久映射的问题是可能会休眠，在中断上下文、自旋锁临界区等不能阻塞的场景中不可用。为了解决这个问题，内核又划分出固定映射，其接口不会休眠。固定映射空间通过kmap_atomic接口来映射。
kmap_atomic的使用场景与kmap较为相似，主要用于mm、fs、net等对高端内存访问有较高性能要求而且不能休眠的模块中。
不同的CPU体系架构在地址空间划分上不尽相同，但为了保证CPU体系差异对外部模块不可见，内存地址空间的分配接口的语义是一致的。
因为64位CPU一般都不需要高端内存（当然也可以支持），在地址空间划分上与32位CPU的差异较大，下图是一个X86_64的内核地址空间划分图:
内存管理；软件架构 内核内存管理的核心工作就是内存的分配回收管理，其内部分为2个体系：页管理和对象管理。
页管理体系是一个两级的层次结构，对象管理体系是一个三级的层次结构，分配成本和操作对CPU cache和TLB的负面影响，从上而下逐渐升高。
页管理层次结构： 由冷热缓存、伙伴系统组成的两级结构。负责内存页的缓存、分配、回收。
对象管理层次结构： 由per-cpu高速缓存、slab缓存、伙伴系统组成的三级结构。负责对象的缓存、分配、回收。这里的对象指小于一页大小的内存块。
除了内存分配，内存释放也是按照此层次结构操作。如释放对象，先释放到per-cpu缓存，再释放到slab缓存，最后再释放到伙伴系统。
框图中有三个主要模块，即伙伴系统、slab分配器和per-cpu（冷热）缓存。他们的对比分析如下。
]]></content>
  </entry>
  
  <entry>
    <title>PCB上怎么画GND</title>
    <url>/post/hardware/how-to-draw-gnd-on-pcb.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>PCB</tag>
      <tag>GND</tag>
    </tags>
    <content type="html"><![CDATA[在电路原理设计阶段，为了降低电路之间的互相干扰，工程师一般会引入不同的GND地线，作为不同功能电路的0V参考点，形成不同的电流回路。
GND地线的分类 细究GND的原理 一个地线GND怎么会有这么多区分，简单的电路问题怎么弄得这么复杂？为什么需要引入这么多细分的GND地线功能呢？工程师一般针对这类GND地线设计问题，都简单的统一命名为GND，在原理图设计过程中没有加以区分，导致在PCB布线的时候很难有效识别不同电路功能的GND地线，直接简单地将所有GND地线连接在一起。虽然这样操作简便，但这将导致一系列问题：
 信号串扰: 假如将不同功能的地线GND直接连接在一起，大功率电路通过地线GND，会影响小功率电路的0V参考点GND，从而产生不同电路信号之间的串扰。 信号精度: 模拟电路的考核核心指标就是信号的精度。失去精度，模拟电路也就失去了原本的功能意义。交流电源的地线CGND由于是正弦波，是周期性的上下波动变化，它的电压也是上下波动，不是像直流地线GND一样始终维持在一个0V上不变。将不同电路的地线GND连接在一起，周期性变化的交流地线CGND会带动模拟电路的地线AGND变化，这样就影响了模拟信号的电压精度值了。 EMC实验: 信号越弱，对外的电磁辐射EMC也就越弱；信号越强，对外的电磁辐射EMC也就越强。假如将不同电路的地线GND连接在一起，信号强电路的地线GND，直接干扰了信号弱电路的地线GND，后果是原本信号弱的电磁辐射EMC，也成为了对外电磁辐射强的信号源，增加了电路处理EMC实验的难度。 电路可靠性: 电路系统之间，信号连接的部分越少，电路独立运行的能力越强；信号连接的部分越多，电路独立运行的能力就越弱。试想，如果两个电路系统A和电路系统B，没有任何的交集，电路系统A的功能好坏是不能影响电路系统B的正常工作，同样电路系统B的功能好坏也不能影响电路系统A的正常工作。假如在电路系统中，将不同功能的电路地线连接在一起，就相当于增加了电路之间干扰的一个联系纽带，也即降低了电路运行的可靠性。  手把手教你画“GND” “GND”在一块PCB板上的重要程度，不亚于水对人体的重要程度。怎么画好“GND”呢？只要注意下面这几点就可以了。
做好分区“GND” 在PCB板上，不同的模块功能会分布在不同的位置，而对应模块的“GND”要求也会不一样。下图是一个电源地与信号地冲突的画板，此电路中电源的GND实际作用是“电源负极”而不是“0V参考地”，而信号部分的GND实际作用是“0V参考地”。在这种情况下，电源地的不干净就导致了信号部分受干扰！这样的情况处理的方式分两种：1.将信号部分的地与电源部分的GND分开，不要直接连接；2.将信号部分的GND掏空，如果需要供电，就以走线的方式去供电。
不要跨步“GND” 还有一些受制于结构导致的，某一个模块本应完整的GND，被其他走线分割成多个区域的跨步GND。例如下图的PCB电路所示，电源输入的负极接上PCB板后直接变成“GND”也就是①位置，往电源模块过去的方向上，①与②之间被信号线隔断；②与③之间被5V输出隔断；而③与④之间被芯片的使能隔断。这样布局的GND虽然用万用表上测量是连通的，但是从原理图上的走线先后顺序，以及高频状态下的“地阻抗”来说都是不合理的。尤其是电源这个模块作为EMC问题的核心之一，地的布局一定要在同一层是完整的！
拒绝小蛮腰“GND” 在给PCB整个板子覆铜或者铺地时，经常会有一些地方因为其他位置的走线或者过孔导致“GND”与“GND”之间出现“小蛮腰”！例如下面两张图片里面，左边是“小蛮腰”类型的GND，右边是“猪尾巴”类型的GND，这两种样式的GND对于EMI和EMS来说，都不是一个好的layout！“小蛮腰”类型的GND可以对其进行加宽，或者过于狭小的区域直接禁止铺铜，而“猪尾巴”类型的GND最好不去铺铜切掉，如果是其他功能需要，就多增加过孔，确保接地OK。
PCB板上的“GND”需要工程师的反复检查，以及全局布局的考虑，不要图方便而敷衍了事，也不要为了接地而接地！在铺铜“GND”的时候，一定要注意区分各个部分的GND能否通铺，密密麻麻的走线之间是否有“不合理”的GND，以及“GND”在各个区域的实际作用！
原文地址： PCB上怎么画GND  
]]></content>
  </entry>
  
  <entry>
    <title>I2C总线基础知识分享</title>
    <url>/post/hardware/basic-knowledge-of-i2c-bus-sharing.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>I2C</tag>
    </tags>
    <content type="html"><![CDATA[很多电子工程师都应该从 EEPROM 通信了解到的I²C总线，其实，I²C总线远不止于 EEPROM 存储器，它也有类似485、SPI等应用场景。
EEPROM存储器系统架构图
位传输 I2C总线是由飞利浦(Philips)公司开发的一种双向二线制同步串行总线，实现有效的IC间的控制，它只需要两根线(SDA和SCL)即可在连接于总线上的器件之间传送信息。
I2C总线在传输数据都是按照bit来传送。SCL为时钟线，SDA为数据线；在SCL时钟线为高电平时，SDA数据线上的电平不允许被修改，SCL时钟线为低电平时，SDA数据线上的电平可为高/低。
I2C总线的位传输
起始条件：SCL为高电平时，SDA由高电平向低电平切换；表示开始传送数据。
停止条件：SCL为高电平时，SDA由低电平向高电平跳变；表示结束传送数据。
空闲条件：I2C总线的SDA和SCL两条信号线同时处于高电平时；表示空闲状态。
起始和停止条件
数据传输 字节传输 发送数据时，由主机先发送一个起始信号，再将SDA信号切换为输出模式，然后将8位数据依次由高到低发送出去；
发送完成后，主机将SDA信号切换为输入模式，等待丛机回应ACK或NAK；再发下一笔数据
I2C总线数据传输
丛机地址 在I2C总线系统中，每个设备都有它的固定地址，一般由芯片的A0,A1和A2决定。丛机地址字节由七位地址位(D7-D1位)和一位方向位(为D0位)组成。
器件地址的D7-D4一般都是被厂家固定了为1111，余下的D3，D2和D1连接到芯片的A2，A1和A0决定；D0为0x00表示写，D0为0x01表示读。大家看例程都是些0xA0和0xA1就是这个原因。
EEPROM的器件地址
读写过程 写数据过程  主机发送I2C总线停止信号，防止总线忙写数据失败 主机发送I2C总线复位信号，确保写数据之前总线处于空闲状态 主机发送I2C总线开始信号，启动一次数据的写入 主机发送I2C丛机地址和写模式(W/R=0)信号，并且等待一个丛机的应答信号 主机接收到ACK的应答信号后，开始多个字节的写入，每写完一个字节需要等待一个丛机的应答信号 主机接收到ACK的应答信号后，发送2IC总线停止信号，确保总线处于空闲状态  读数据过程  主机发送I2C总线停止信号，防止总线忙写数据失败 主机发送I2C总线复位信号，确保读数据之前总线处于空闲状态 主机发送I2C总线开始信号，启动一次数据读取 主机发送I2C丛机地址和读模式(W/R=1)信号，并且等待一个丛机的应答信号 主机接收到ACK的应答信号后，开始多个字节的读取，每读完一个字节需要给丛机发送一个ACK应答信号 主机接收到ACK的应答信号后，发送I2C总线停止信号，确保总线处于空闲状态  主机读/写数据过程
结语 I2C总线在嵌入式应用中非常广泛，基本上所有的电力电子设备都会用到这个总线。
]]></content>
  </entry>
  
  <entry>
    <title>无线通信大比拼：Wi-Fi、蓝牙和NFC三巨头</title>
    <url>/post/hardware/wireless-communication-competition.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>NFC</tag>
      <tag>Bluetooth</tag>
      <tag>Wifi</tag>
    </tags>
    <content type="html"><![CDATA[在当今数字化飞速发展的时代，我们身边充斥着各种无线通信技术，它们如同无形的纽带，将设备、人与人之间连接在一起。而其中的三位主角——Wi-Fi、蓝牙和NFC，正是无线通信领域的明星选手，各自擅长不同的舞台，各有千秋。
随着智能家居、物联网和移动支付等领域的蓬勃发展，Wi-Fi、蓝牙和NFC已经成为我们日常生活中不可或缺的存在。那么，这三者究竟有何异同，各自又有哪些独特之处呢？本文瑞哥就带着大家好好了解一下这三种技术。
WIFI Wi-Fi（无线保真）是一种无线局域网技术，允许设备通过无线信号进行互联。WIFI的出现极大地改变了人们对于互联网和网络连接的认知，成为现代社会不可或缺的一部分。
WIFI技术起源于20世纪90年代，最初由IEEE（电气和电子工程师协会）提出，用于简化设备之间的连接。最早的WIFI标准是802.11，而后续的版本不断推出，如802.11a、802.11b、802.11g、802.11n、802.11ac和802.11ax。每个版本都引入了新的技术和性能提升，满足了不同应用场景的需求。
WIFI技术基于无线电波进行通信，使用2.4GHz和5GHz频段。设备通过WIFI芯片发送和接收数据包，使用CSMA/CA（载波监听多路访问/碰撞避免）协议来协调多个设备之间的通信。不同版本的WIFI标准支持不同的最大传输速率，从几Mbps到几Gbps不等。
不同WIFI标准的比较  802.11a：首个支持5GHz频段的WIFI标准，提供更高的传输速率。 802.11b：采用2.4GHz频段，较大的覆盖范围，但速率较低。 802.11g：在2.4GHz频段上提高了速率，向下兼容802.11b。 802.11n：引入多天线技术（MIMO），提升覆盖范围和传输速率。 802.11ac：运用更广泛的5GHz频段，实现更高速率和更大容量。 802.11ax：强调高密度环境下的性能和效率，支持更多设备同时连接。  WIFI的安全性 WIFI网络的安全性一直备受关注，特别是在公共网络中。采取适当的安全措施是至关重要的：
 加密方式： 使用WPA3（Wi-Fi Protected Access 3）等高级加密方式保护网络通信。 密码设置： 设置强密码，并定期更改，防止未经授权的访问。 防火墙： 在网络中使用防火墙，限制不必要的数据流量。  蓝牙 蓝牙技术是一种短距离无线通信技术，旨在实现设备之间的便捷连接和数据传输。蓝牙的发展历程跨足多个阶段，每个阶段都带来了新的特性和改进。
蓝牙技术最早于1994年由瑞典的爱立信公司提出，其名称源自10世纪丹麦国王哈拉尔德·布鲁图。最初版本的蓝牙技术主要用于连接手机和耳机，随着技术的不断发展，蓝牙已经演变为一个多功能、广泛应用的无线通信标准。
蓝牙技术使用短波无线电信号在设备之间进行通信。它采用频率跳跃技术（Frequency Hopping Spread Spectrum, FHSS）来减少干扰，允许设备在不同频率上跳跃，提高通信的稳定性。蓝牙的通信范围通常为几米至十几米，适用于近距离通信场景。
蓝牙的不同版本  蓝牙1.x： 首个版本，用于音频设备的连接。 蓝牙2.x： 引入了更高的传输速率和增强数据同步功能。 蓝牙3.x： 提升了传输速率和能效，引入了高速蓝牙（HSB）。 蓝牙4.x： 引入低功耗蓝牙（BLE），广泛用于物联网设备。 蓝牙5.x： 提供更远的通信距离、更高的传输速率和更强大的广播功能。  蓝牙的发展趋势 未来蓝牙技术将面临更多的创新和发展：
 Mesh网络： 蓝牙Mesh网络使得多个设备能够相互通信，扩展了蓝牙的应用范围。 低功耗蓝牙（BLE）： 在物联网领域，BLE将继续发挥关键作用，支持低功耗设备的长时间运行。 高速蓝牙（HSB）： 针对需要更高速率的应用，如高清音频、视频传输等。  NFC 近场通信（NFC）技术是一种短距离无线通信技术，允许设备之间进行快速、安全的数据传输。NFC在智能手机和其他设备中得到广泛应用，为用户提供了更便捷的互联体验。
NFC技术最早于20世纪80年代提出，但直到近年来才在智能手机和其他设备中得到广泛应用。NFC的发展始于RFID（射频识别）技术，逐渐演变为一种支持更多应用场景的通信技术。
NFC的工作模式 NFC技术包括卡模式、读卡器模式和点对点模式：
 卡模式： 设备可以像传统磁卡一样被读取，用于支付和身份验证。 读卡器模式： 设备可以读取其他NFC设备的信息，类似于扫描二维码。 点对点模式： 两个NFC设备之间可以直接进行数据交换，用于快速文件传输等。  NFC的应用场景 NFC技术在多个领域得到广泛应用：
 支付领域： NFC技术支持移动支付，用户可以通过手机或卡片进行快速支付。 身份验证： NFC卡片用于身份验证，如进入办公楼或公共交通工具。 智能标签： NFC标签被广泛用于商品标签、展览信息等，用户可通过手机轻松获取相关信息。 社交互动： NFC设备可以用于社交互动，例如两部手机通过NFC分享联系方式或社交媒体信息。  WIFI、蓝牙和NFC对比 WIFI、蓝牙和NFC是三种不同的无线通信技术，它们在多个方面有着明显的区别。
通信范围  WIFI： 通常覆盖范围较广，可以达到数十米至数百米，具有较大的覆盖面积，适用于建立大范围网络。 蓝牙： 通信范围较短，一般在几米至十几米之间，更适合近距离设备互联。 NFC： 最短的通信范围，通常在几厘米之内，需要设备之间非常接近才能进行通信。  传输速率  WIFI： 提供较高的传输速率，从几Mbps到数Gbps不等，适合大文件传输和高带宽需求的应用。 蓝牙： 传输速率中等，取决于使用的蓝牙版本，通常在数十Kbps到几百Mbps之间，适用于音频、视频和一般数据传输。 NFC： 传输速率相对较低，一般在数十Kbps到几百Kbps，适用于小文件传输和简单的数据交换。  应用场景  WIFI： 主要应用于建立无线局域网络，用于互联网接入、大文件传输等，适用于家庭、企业和公共场所。 蓝牙： 适用于近距离设备互联，如耳机、音响、智能家居、医疗设备、汽车等。 NFC： 主要用于快速、短距离的数据传输和交互，如移动支付、身份验证、社交互动等。  安全性  WIFI： 提供多种加密和安全协议，但公共WIFI网络存在一定的安全风险，需要注意安全设置。 蓝牙： 提供安全特性，但存在一些潜在的安全漏洞，需要谨慎使用，尤其是在公共场所。 NFC： 通过加密通信和短距离通信的特性，提供相对较高的安全性，适用于一些敏感领域的应用。  功耗  WIFI： 消耗相对较大，适用于供电充足的设备。 蓝牙： 低功耗蓝牙（BLE）版本提供了较低的功耗，适用于长时间运行的设备。 NFC： 功耗相对较低，适合在资源受限的设备上使用。  连接方式  WIFI： 支持多设备同时连接，适用于构建大规模网络。 蓝牙： 支持点对点连接和星型连接，新版本的蓝牙支持Mesh网络，适用于设备之间灵活的连接。 NFC： 主要用于一对一的点对点连接，适合快速数据传输和交互。  总结 Wi-Fi、蓝牙和NFC是三种不同的无线通信技术，各自适用于不同的应用场景，具有各自独特的优势和限制。选择使用哪种技术通常取决于具体的需求和环境。
 Wi-Fi（无线局域网）：适用于需要高速数据传输的场景，比如在家里或办公室中传输大量数据、流媒体等。Wi-Fi通常有较长的传输距离和高传输速率，但相对而言功耗较高。 蓝牙：适用于连接各种设备，例如耳机、键盘、鼠标等。蓝牙通常具有较低的功耗，适合短距离通信，但传输速率相对较低。 NFC（近场通信）：适用于短距离的数据传输，常用于移动支付和身份识别等场景。NFC的传输距离相对较短，但由于其近场特性，更容易确保安全性。  根据具体需求选择合适的技术是很重要的，而且在实际应用中，这三种技术也可以互相补充。例如，在一个智能家居系统中，可能同时使用Wi-Fi连接高带宽设备、蓝牙连接低功耗设备，以及NFC进行一些简单的配置或身份验证操作。
]]></content>
  </entry>
  
  <entry>
    <title>来区分一下这些算力单位：TOPS、FLOPS、MIPS、DMIPS</title>
    <url>/post/soc/computer-computing-power-unit-introduction.html</url>
    <categories><category>SOC</category>
    </categories>
    <tags>
      <tag>Computing Power</tag>
      <tag>TOPS</tag>
      <tag>FLOPS</tag>
      <tag>MIPS</tag>
      <tag>DMIPS</tag>
    </tags>
    <content type="html"><![CDATA[算力也被称之为计算能力（Computing Power），算力既然是一种“能力”——即其执行计算任务的速度和效率，那么算力就会有大小的衡量指标，同时对于算力的衡量就需要依托于芯片的类别来进行，但是无论怎样算力是需要有一个基准的单位的，常见具体的请见下表：
算力的计算方式 FLOPS FLOPS——（Floating Point Operations Per Second）来衡量算力通常以每秒钟可以执行的浮点运算次数。常见的FLOPS的计算算力单位根据数量级还有不同的表达，具体如下：
 KFLOPS-kilo Floating Point Operations Per Second（千次浮点运算每秒） MFLOPS-Mega Floating Point Operations Per Second（百万次浮点运算每秒） GFLOPS-Giga Floating Point Operations Per Second（十亿次浮点运算每秒） TFLOPS-Tera Floating Point Operations Per Second（万亿次浮点运算每秒） PFLOPS-Peta Floating Point Operations Per Second（千万亿次浮点运算每秒）  FLOPS的计算方式：计算机的算力取决于处理器的性能、核心数量、频率以及并行计算能力等因素。一般来说，可以通过以下公式来计算处理器的FLOPS数值：算力 = 处理器核心数量 x 每个核心的频率 x 每个时钟周期的指令数量 x 每个指令的执行
例如，如果一台处理器有4个核心，每个核心的频率为3.5 GHz，每个时钟周期执行4条指令，每条指令的执行时间为0.5纳秒，则该处理器的算力为：算力 = 4核 x 3.5 GHz x 4 x (0.5 ns) = 56 GFLOPS
TOPS TOPS（Tera Operations Per Second）是指每秒进行的万亿次运算，是衡量人工智能（AI）处理器性能的重要指标，也是评估处理器在深度学习和神经网络推理任务中的计算能力的重要指标。
计算TOPS的基本原理是根据处理器的时钟频率、每个时钟周期执行的指令数量以及每个指令的计算量来计算。一般来说，可以使用以下公式来计算处理器的TOPS：TOPS = 处理器时钟频率 x 每个时钟周期的指令数量 x 每个指令的计算量
其中，处理器时钟频率表示处理器的工作频率，每个时钟周期的指令数量表示每个时钟周期处理器执行的指令数量，每个指令的计算量表示每个指令的浮点运算量。
某些情况下我们也是通过TOPS/W来评价处理器算力的一个性能指标，表达的含义是在1W功耗的情况下处理器可以执行多少亿万次操作。
MIPS MIPS（Million Instructions Per Second）是指每秒钟能够执行的指令数，单位为百万条指令每秒（Million Instructions Per Second）。
计算MIPS的方法如下：
 首先，确定在一个特定时间段内处理器执行的总指令数（例如1秒内）。 将总指令数除以1,000,000（即1百万），得到每秒钟能够执行的指令数，即MIPS值。  举例说明：
假设一个处理器在1秒钟内执行了总共500,000条指令，那么它的MIPS值为：
MIPS = 500,000 / 1,000,000 = 0.5 MIPS这表示该处理器每秒钟能够执行0.5百万条指令。通过计算MIPS值，可以评估处理器的指令执行速度和性能表现，但是对于特定的应用场景和任务可能不是最准确的性能指标。
DMIPS DMIPS（Dhrystone Million Instructions Per Second）是指每秒钟能够执行的Dhrystone基准测试指令数，单位为百万条指令每秒。Dhrystone是一种通用的CPU性能测试工具，DMIPS是基于Dhrystone测试结果计算得出的性能指标。DMIPS更适用于衡量通用处理器在特定测试条件下的性能，可以提供更具体的性能评估结果。
计算DMIPS的方法如下：
 首先，进行Dhrystone基准测试，得到处理器在测试条件下执行的总指令数（例如1秒内）。 将总指令数除以1,000,000（即1百万），得到每秒钟能够执行的Dhrystone基准测试指令数，即DMIPS值。  举例说明：
假设一个处理器在进行Dhrystone基准测试时，在1秒钟内执行了总共800,000条指令，那么它的DMIPS值为：
DMIPS = 800,000 / 1,000,000 = 0.8 DM
这表示该处理器在Dhrystone基准测试条件下，每秒钟能够执行0.8百万条指令。通过计算DMIPS值，可以评估处理器在特定测试条件下的性能现。
Hash/ &ldquo;hash/s&quot;通常用来衡量计算机或网络设备的哈希计算速度，特别是在加密货币挖矿等领域常被使用。基本原理是通过计算机执行哈希函数并输出哈希值的速度来衡量设备的计算能力。
哈希函数是一种将任意长度的输入数据（消息）转换为固定长度的输出数据（哈希值）的函数。在加密货币挖矿中，通常会使用哈希函数来寻找符合特定条件的哈希值，这需要大量的计算。
计算&quot;hash/s&quot;的算力通常是通过以下步骤进行估算：
 选择哈希函数：确定要使用的哈希函数，如SHA-256（比特币挖矿中常用的哈希函数）。 执行哈希计算：在设备上执行哈希函数，将输入数据进行哈希运算，得到哈希值。 计算速度：记录设备在单位时间内执行哈希计算的次数，即&quot;hash/s&rdquo;。  例如，如果一个计算机在1秒内执行了100,000次SHA-256哈希计算，那么它的哈希计算速度就是100,000 hash/s。这个速度可以用来衡量设备的计算能力，特别是在加密货币挖矿等需要
总结 ]]></content>
  </entry>
  
  <entry>
    <title>全球首款，NPU+GPU+CPU三位一体AI加速！</title>
    <url>/post/soc/NPU-GPU-CPU-trinity-AI-acceleration.html</url>
    <categories><category>SOC</category>
    </categories>
    <tags>
      <tag>AMD</tag>
      <tag>Ryzen</tag>
      <tag>NPU</tag>
      <tag>CPU</tag>
      <tag>GPU</tag>
    </tags>
    <content type="html"><![CDATA[ AMD  前不久发布了全球首款桌面AI处理器：锐龙8000G系列，代表着桌面PC平台正式进入了 AI  时代。而锐龙8000G目前受到玩家们的广泛关注，销售十分火热，大有成为新一代装机爆款甜品U的节奏。那么，锐龙8000G的AI功能到底实力如何，能否为我们的日常应用带来不一样的体验呢？接下来就让我们实战了解一下吧。
Ryzen AI引擎加持的全球首款桌面AI处理器 不管是硬件还是应用环境的变化，都可以让人感受到AI时代已经来临。由于在效率和便利性方面本地AI计算优势突出，所以很多日常应用以及未来的新版 Windows  系统都会用到电脑本地的AI计算能力。因此，AMD去年率先在移动平台上推出了内置Ryzen AI引擎的锐龙7040系列处理器，打响X86平台进军AI的第一枪。
锐龙8000G系列则是AMD布局桌面AI PC的先锋。锐龙8000G系列与今年同时登场的锐龙8040移动处理器一样，同时具备Ryzen AI引擎打造的NPU、Zen4架构处理器核心与RDNA3核显，组成了三位一体的AI加速方案，让锐龙8000G成为了史上首款桌面全能AI处理器。此外，由于它定位桌面平台，它的功率设定更加宽泛，整体的性能释放更加彻底。根据AMD官方数据来看，锐龙8000G的整合计算能力高达39 TFLOPS。
目前，锐龙8000G已经在市场中热卖，成为了人人触手可及的AI PC装机明星。不过，锐龙8000G在实际的AI应用中表现如何呢？下面我们就以锐龙7 8700G为例，给大家实际体验一番。
实战AI应用：NPU创造新玩法，GPU效率很抢眼 测试平台 处理器：AMD锐龙7 8700G 主板：ROG STRIX B650-A GAMING WIFI吹雪 显卡：Radeon 780M（处理器内置） 内存：金士顿DDR5 6000 16GB×2 硬盘：WD_BLACK SN850X 2TB 电源：ROG雷神1200W 操作系统：Windows 11 23H2
首先，作为桌面平台首款内置AI引擎的处理器，相信锐龙8000G的NPU如何工作是大家特别想了解的内容。实际上，现在已经有不少的应用可以识别和利用到锐龙8000G的NPU了，这里我们选择一个代表应用来举例。
目前游戏辅助工具《游戏加加》测试版已经添加了对锐龙8000G内置NPU的支持，在程序界面的超能时刻菜单中可以选择Ryzen AI NPU模式来录制精彩击杀视频，使用NPU的话就不会额外占用系统资源，可以保证玩家的流畅游戏体验不受干扰。此外，知名的硬件监测工具HWINFO64也加入了对NPU频率与占用率的监测功能，玩家可以通过图形化的方式来了解其数值的变动。
根据实际体验，在《游戏加加》的游戏内监测中打开对NPU频率、占用率的显示，从《无畏英豪》的游戏截图可以看到，画面顶部显示NPU工作频率为800MHz，占用率为0%。从HWINFO64的监控图形可以看到，在进行击杀视频录制的时候，NPU的占用率会上升。
作为全能型AI处理器，锐龙8000G显然不会只依靠Ryzen AI NPU来进行AI加速，其内置的RDNA3核显在AI加速性能方面也非常给力，并得到了众多AI应用软件的支持。如图所示，Topaz Photo AI就可以选择使用Radeon 780M核显来进行加速，实测用Radeon 780M的效率比纯使用处理器加速更高。
当然，RDNA3核显最大的买点还是支持Stable Diffusion出图，不但支持DirectML模式，还可以通过Olive优化模型以实现效率翻倍。此外，锐龙8000G的核显可以在主板BIOS中手动指定显存容量，在系统配备32GB内存的情况下，最高可以分配16GB显存，更大的显存可以在Stable Diffusion中使用更高分辨率出图，这也是AMD核显独有的优势。
我们使用麦橘唯美V1.0模型进行SD出图测试，迭代步数为20、CFG为7、采样方式为Euler a、分辨率为512×512。可以看到，Radeon 780M大约每分钟出图1.7张，即便是连续出图5批次也不会爆显存导致崩溃，这一点别说竞品的核显做不到，甚至一些入门独显在这里也是会崩溃的，由此也充分凸显了Radeon 780M支持分配最高16GB大显存的好处。此外，Radeon 780M的出图速度已经相当于旗舰处理器的5倍~6倍，效率优势非常明显，对于有轻量级AI出图需求的用户来说是性价比非常高的解决方案。
因此，综合来看，锐龙8000G的CPU（Zen4）+ NPU（XDNA）+ GPU（RDNA3）三大架构终极融合的AI加速方案确实可以做到高兼容、高能效、高性能的全面AI加速，堪称当下最为全能的桌面AI处理器。
总结：入手锐龙8000G，立刻享受高效AI全能加速！ 简单总结一下。随着AI应用逐渐普及，AI PC时代已经到来。AMD使用锐龙8000G将自己在移动平台的AI应用方面领先的优势延伸到了桌面平台，锐龙8000G同时拥有高效AI引擎和最强核显让它成为当下桌面全能AI处理器的唯一选择。因此，AMD在全能AI这条赛道上已经做到了暂时没有对手。作为桌面AI处理器的先行者，锐龙8000G堪称为后来者和竞争者树立了一个追赶的标杆。
目前来看，已经有超过100个AI应用或功能提供了对AMD Ryzen AI引擎的支持，包括4个微软工作室特效、超过50个Adobe软件功能、超过25个TOPAZ LABS软件功能、超过20个达芬奇软件功能和超过50个的各种主流生产力软件与功能。由此可见，Ryzen AI生态圈发展是非常迅速的，它已经逐渐覆盖了我们日常工作的大多数应用范围。
而且，锐龙AI处理器布局完成代表着AMD已经是业内拥有最全面AI解决方案的制造商，完美覆盖了从数据中心、边缘计算到客户端的全平台。它拥有针对HPC与数据中心的Instinct加速器、针对数据中心与边缘计算的Alevo加速卡、性能遥遥领先的第四代EPYC处理器、嵌入式Versal AI Edge、锐龙8000系列处理器——目前来看，业内还没有其他厂商能够拥有如此全面的AI产品线。总而言之，锐龙8000G系列桌面处理器不但拥有强大的全面AI计算性能，也拥有潜力非常大的生态圈成长空间，这也是AMD在AI方面独有的核心优势。
和我们大众用户密切相关的是，在不久就会登场的新版Windows操作系统里，AI更是系统的核心功能之一，AI将会深度参与到性能调配、操作辅助等多项工作中。在AI的帮助下，系统能够学习并预判用户的日常使用习惯，为应用程序提供更快的响应速度；根据屏幕显示的内容，智能判断用户的使用场景并优化系统设置。此外，AI还可以帮助用户规避诈骗网站、诈骗信息，快速寻找需要的信息。通过AI的帮助，用户可以更轻松地使用电脑。而且，基于AI大模型的自然语言理解能力，新版Windows 有望部署真正成熟的语音助手，并且与底层功能相融合，实现真正的语音操控电脑。
而想要第一时间享受到这些功能，就必须要拥有一款内置AI引擎的PC处理器才行，而目前不管是移动平台还是桌面平台，拥有Ryzen AI引擎NPU的锐龙处理器都是值得优先考虑的解决方案，而锐龙8000G更是桌面平台享受全能AI加速的唯一选择。
再来看看最近火热的锐龙7 8700G装机配置与电商平台热门的酷睿i5+GTX 1650独显配置对比。从整机价格来看，目前由于锐龙7 8700G正在热销，电商平台有处理器+主板的超值套装，因此性价比进一步凸显，甚至相对同定位的酷睿i5+GTX 1650独显配置还有49元的价格优势。此外，从这两套配置的实用性、升级空间来对比，也是锐龙7 8700G主机更优，毕竟除了Radeon 780M核显在DX12U引擎的新游戏中表现优于GTX 1650独显之外，它还能无缝升级任何强力独显（核显也能同时使用，甚至还能通过AFMF功能给独显插帧提升性能），或者在使用核显的情况下使用显卡插槽加上扩展卡升级两条PCIe 4.0×4的SSD，这一点酷睿i5+GTX 1650这套配置是做不到的，毕竟GTX 1650独显已经占掉了PCIe 5.0显卡插槽，未来升级强力独显，这块GTX 1650独显也只能被淘汰，等于浪费了投资，非常不划算。
总而言之，现在组建主流台式机，锐龙7 8700G不但可以让你抢先进入AI时代、在未来各种AI应用中提供NPU的高效加速，还能提供更佳的升级性、实用性，性价比也更加突出，确实值得玩家优先考虑。
]]></content>
  </entry>
  
  <entry>
    <title>带你认识PCIe插槽！除了插显卡它还能插什么</title>
    <url>/post/hardware/pcie-slot-introduction.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>PCIe</tag>
      <tag>PCIe Gen5</tag>
    </tags>
    <content type="html"><![CDATA[前言 关注我们的玩家或者经常接触台式机的玩家应该对 PCIe  这个词不陌生，它经常出现在主板、显卡甚至是硬盘上。可以说它是你电脑里最重要的接口或通道之一，不过似乎很多人并不知道它是个什么东西，到底能够干什么，觉得它只能用来插显卡，但实际上它的用途非常广泛，今天就一起简单了解一下这个神秘的“PCIe”。
什么是PCIe插槽？ PCle的全称为PCI总线(PCI Express)， PCIe是计算机的一种高速总线，而总线就相当于计算机里的一条道路，提供给不同的设备和硬件进行数据交互。而最早的PCIe是2001年由Intel提出的，甚至在那时还不叫“PCIe”，而叫“3GIO”，用于替代PCI、PCI-X和AGP等老式总线。
这么说你可能还不太明白，那我们找个主板看看就知道了，图片中圈起来的地方就是我们所说的PCIe插槽。
PCIe有什么用？ PCIe 接口通常用于将高性能外围设备连接到您的计算机。最常见的例子是你的显卡 (GPU) ，因为现代游戏、科学、工程和机器学习应用程序涉及处理大量数据。而PCIe能够很好的在CPU与GPU之间构筑桥梁，让它们能够数据交互。
不过显卡也不是唯一能够接入PCIe插槽的设备，还有很多外设也能够利用PCIe插槽，下面我们会给各位详细解读。值得一提的是，PCIe也不是一成不变的，它也会升级迭代，毕竟最初的版本已经是2001年了，现在都3202年了，PCIe早就经过了几次迭代了。
PCIe时代的发展史 截至目前，PCIe插槽一共有多代标准，最超前的PCIe 6.0的规范已经在2022年1月22日发布，但目前仅停留在理论阶段，尚未有产品适用或者测试。
别看PCIe现在风光无量，它的前身其实是ISA，在那个电脑没有标准化，各种硬件接口、协议不统一的时代，电脑上的硬件就由于六国纷争，我的硬件不兼容你的，你的硬件不支持我的，为了解决这种情况，当时业内统一了一个规格，就是ISA接口，也算得上是PCIe的爷爷了，它最早诞生在1981年，搭载在IBM的电脑上，并且一度统治了当时的PC领域。
不过好景不长，作为初代数据总线，仅有8MB/s的传输速率，放在今天来看，U盘都快过它。因此ISA的传输速率很快就不能满足其他硬件的需求了，并且当时这款接口还有CPU占用率过高等问题，因此大家开始寻求ISA的继任者。
所以PCI诞生了，PCI相较于ISA在带宽速率上有了不小的提升，32bit位宽下可以做到128MB/s，如果将数据位宽升级到64位，那速率还可以翻倍至256MB/s。并且做到了即插即用，就好像我们现在插显卡上主机一样，系统可以自动寻找相应的驱动程序。要知道在ISA那个年代，我们接入任何设备在ISA接口上还需要手动配置，相当麻烦。不过PCI总线也不是没有缺点，一个就是它采用共享总线设计，所以多设备容易造成抢带宽的情况，其次它也不支持热插拔。
而我们今天的主角PCIe就是在PCI的基础上演变而来的，PCIe后缀上的e又叫Express，一看就知道是PCI的升级版。它与PCI的区别主要体现在总线类型不同、颜色不同、规格不同以及传输速率不同。
目前PCIe规范已经发展出6个大版本，每一次大版本的进化，都能带来相比上一版本近乎于翻倍的带宽。第一个PCIe的正式规范也就是PCIe 1.0诞生于2003年，其信号速率为2.5GT/s，采用8b/10b编码方式，单通道单向带宽达到250MB/s，16通道双向带宽为8GB/s。该规范随后还发展出PCIe 1.0a和PCIe 1.1版本，虽然细节上有不少改进但是带宽并没有改变。
PCIe 2.0规范则在2007年正式发布，其相比于PCIe 1.0规范最大的变化是信号速率翻倍至5GT/s，因此其带宽也跟随着一起翻倍，单通道单向带宽为500MB/s，16通道双向带宽为16GB/s。此外PCIe 2.0规范还将对应插槽的供电能力翻倍至最高150W的水平，但出于对兼容性以及主板供电压力等多方面的考虑，最终无论主板厂商、显卡厂商又或者其它PCIe设备的厂商，在产品开发时都是按照PCIe 1.0规范的供电要求也就是75W执行的，供电需求高于75W者一律配置外接供电，这个行业规则一直沿用至今。
PCIe 3.0虽然是2010年发布的标准，但至今依旧很多设备在用，相比PCIe 2.0规范不仅信号速率提升至8GT/s，而且编码方式也改成了更高效的128b/130b模式，因此单通道单向带宽依然实现了接近翻倍的提升，达到985MB/s的水平，16通道双向带宽高达32GB/s。
PCIe 4.0可以算作是目前的主流标准，其再一次实现了信号速率的翻倍，16通道双向带宽达到64GB/s的水平，PCIe 4.0将允许更快地传输正在GPU内存中加载的数据，并减少PCIe总线上的延迟。随着视频游戏的文件大小和图形复杂性不断增加，并且机器学习应用程序继续需要越来越大的数据集，PCIe 4.0将在提高帧速率和减少计算时间方面发挥重要作用。
PCIe 5.0早早就在2019年就提出了，但是直到去年AMD的X670、B650等主板上市，才真正应用在硬件上，加上现在也逐渐有PCIe 5.0的固态硬盘现身，玩家才得以见到PCIe 5.0的性能。PCIe 5.0 最重要的一个特性——也是每个人都会关心的特性——是速度。PCIe 5.0 的速度是PCIe 4.0 的两倍，单向带宽高达约64GB/s，双向带宽高达128GB/s。
值得一提的是，在数据传输中，PCIe 5.0还使用了从3.0标准时代所导入的NRZ 128b/130b编码技术，不再采用8bit/10bit的小包校验方式，转而采用了全新算法的128bit/130bit的大包校验方式以及全新的硬件加扰和解码模块等，其校验带宽开销从之前的20%降低至1.54%。即便是扣除损耗的带宽后，PCIe 5.0 X16、PCIe 5.0 X4接口下也能分别提供63.0 GB/s、15.75 GB/s的传输带宽。
PCIe 6.0则是2022年才提出的标准，新鲜出炉。传输速度是PCIe 5.0的两倍，单向带宽高达约128GB/s，双向带宽高达256GB/s。面对数据传输量大幅成长，相比PCIe 5.0，PCIe 6.0强化传输频宽与能源使用效益，同时具备低延迟与减少频宽消耗的功能。
至于7.0 版的PCIe规范，今年6月PCI-SIG敲定了 PCIe Gen7（PCIe 7.0）v0.3 版本的草案，届时它的数据传输速率将再次翻倍，达到单向带宽高达约256GB/s，双向带宽高达512GB/s。不过有一说一，家用PC在很多年内应该也用不上这样的速度，而PCIe 7.0的普及，那就更不知道要等到猴年马月了，毕竟现在5.0都还没普及。
为什么PCIe有不同的长度？ PCIe接口的总线带宽是按长度划分的PCIe X1、PCIe X2、PCIe X4、PCIe X8、PCIe X16。虽然我们可以把任意长度的PCIe设备插到PCIe X1或者PCIe X16的插槽中去运行，但是这样很明显会造成一个问题，带宽要求小的设备会浪费PCIe X16的超大带宽，而带宽要求大的设备在PCIe X1插槽内又“吃不饱”。
当然啦，有些玩家可能会说“我的主板上没有PCIe X1的插槽”，其实这也正常，在一些MATX、ITX甚至是旗舰主板上，由于空间布局的问题，导致PCIe X1插槽没有办法塞下，因此如果你想使用PCIe X1外设，在没有PCIe X1插槽的情况下，也是可以将较小的扩展卡安装在较大的插槽中，这仍然会工作得非常好。
总的来说，PCIe区分不同长度是为了让各种设备都能够在合适的带宽下运行，并且不同长度的插槽所能承受的带宽不同，相同长度不同版本的PCIe所能承受的带宽也不一样。下面给大家盘点一下各个版本的PCIe下，不同插槽的带宽能够有多大的区别。
从图中就可以看到，PCIe X1速度最慢，PCIe X2是X1的2倍，X4是X2的2倍，以此类推X16是X8的2倍。而每次PCIe版本的迭代也都在前代的速率基础上进行提升，几乎每一代都比上一代速度提升了一倍。而且PCIe是可以向下兼容的，PCIe 1.0的设备可以插到2.0接口上用，2.0的设备也可以插到1.0接口上用，只是不能发挥全部性能。
除了插显卡，还能插什么？ 上面我们介绍到了，PCIe插槽有不同的长度，显卡往往插在PCIe X16的插槽上，那除了插显卡还能插什么呢？当然PCIe X16插槽也会用来接RAID阵列卡，因为其与CPU直连的特性，加上物理上距离更靠近CPU，因此显卡或RAID阵列卡在与CPU之间数据交互时，延迟会更低，性能也能更好的释放。
PCIe X8的插槽在主板上大多也是PCIe X16插槽的形状，不过数据针脚只有一半是有效的，也就是说实际带宽只有真正的PCIex16插槽的一半。主要用来接M.2 NVME的扩展卡，毕竟在以前，主板的M.2接口不像现在的主板那么多，加上当时的固态价格也没有今天那么低，大家买硬盘都是省吃俭用，才用上了500G，甚至以前1T都是富哥才用得起的装备。因此想要在老主板上装更多的M.2固态就需要用这种扩展卡，并且只要协议版本和通道数量与硬盘保持一致，理论速度与板载M.2并无区别。
与PCIe X8插槽一样，PCIe X4插槽为了兼容性，现在多数也是做成PCIe X16插槽的形式，或是扩展为M.2接口，用于安装M.2SSD、M.2无线网卡或者其它M.2接口设备。
最后就是最万能的PCIe X1的这个短小精悍的插槽了，你几乎可以在网上买到所有想用它进行扩展转接的接口！比如安装USB 2.0/3.0扩展卡、安装千兆/2.5Gbps高速网卡、安装高性能声卡、扩展更多的SATA口、安装Wi-Fi网卡等等。
PCIe除了插槽形式，它还可以作为通道的形式存在，我们最常用的M.2固态硬盘接口，表面上是M.2连接我们的固态硬盘，但起到数据传输作用的却是PCIe通道。简单的讲，M.2接口就是换了外形的PCIe接口。你看它的接口是不是很像缩小了的PCIe接口，这也是为什么我们老是能够听到PCIe 4.0固态硬盘、PCIe 5.0固态硬盘的原因。
结语 虽然现在主流的应用还在PCIe 3.0和PCIe 4.0，但我们看到在有些数据中心，以及新的GPU、CPU，或固态硬盘都开始采用PCIe 5.0了。未来，PCIe 6.0乃至PCIe 7.0的出现，也势必让更多的硬件能够释放更强的性能。不过，按照目前的情况来看，PCIe 5.0更像是战未来的产品，虽然我们也说了有固态硬盘支持PCIe 5.0甚至是显卡也支持了PCIe 5.0，但是即便是PCIe 3.0也不会让目前的显卡的性能出现瓶颈。PCIe 5.0的出现更像是有了生态基础，给未来的硬件铺路，也是为了让现在的硬件能够适应忽然爆火的AIGC领域，毕竟现在的AI计算模型大得令人难以想象。
]]></content>
  </entry>
  
  <entry>
    <title>一篇关于CPU的入门知识</title>
    <url>/post/hardware/basic-knowledge-of-cpu.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>CPU</tag>
    </tags>
    <content type="html"><![CDATA[不管你玩硬件还是做软件，你的世界都少不了计算机最核心的 —— CPU。
CPU是什么？ CPU与计算机的关系就相当于大脑和人的关系，它是一种小型的计算机芯片，通常嵌入在电脑的主板上。
 CPU  的构建是通过在单个计算机芯片上放置数十亿个微型晶体管来实现。
这些晶体管使它能够执行运行存储在系统内存中的程序所需的计算，所以，也可以说CPU决定了你电脑的计算能力。
CPU实际做什么？ CPU的工作核心是从程序或应用程序中获取指令并且执行计算。
这个过程一共有三个关键阶段：提取，解码和执行。
CPU先从系统的RAM中提取指令，随后解码该指令的实际内容，最后再由CPU的相关部分执行该指令。
CPU的内部结构 刚才提到了很多CPU的重要性，那么CPU的内部结构是什么呢？又是由什么组成的呢？
下图展示了一般程序的运行流程（以C语言为例），一般来说，了解程序的运行流程是掌握程序运行机制的基础和前提。
在这个流程中，CPU负责解释和运行最终转换成机器语言的内容，CPU主要由两部分构成：控制单元和算数逻辑单元（ALU）。
 控制单元：从内存中提取指令并解码执行； 算数逻辑单元（ALU）：处理算数和逻辑运算。  CPU和内存都是由许多晶体管组成的电子部件，可以把它比作计算机的心脏和大脑。
它能够接收数据输入、执行指令并且处理相关信息，它与输入/输出（I/O）设备进行通信，这些设备向 CPU 发送数据和从 CPU 接收数据。
从功能上来看，CPU的内容是由寄存器、控制器、运算器和时钟四部分组成的，各个部分之间通电信号来连通。
接下来简单介绍一下内存，为什么说到CPU需要讲一下内存呢？
因为内存是与CPU进行沟通的桥梁，计算机中所有程序的运行都在内存中得到运行的。
内存一般又被称为主存，它的作用是存放CPU中的运算数据，以及与硬盘等外部存储设备交换的数据。
CPU会在计算机运转时，把需要运算的数据调到主存中进行运算。
在运算完成之后，CPU将结果传送出来，主存的运行也决定了计算机的稳定运行。
主存一般通过控制芯片与CPU相连，由可读写的元素构成，每个字节都有一个地址编号。
CPU通过地址从主存中读取数据和指令，也可以根据地址写入数据，注意一点：当计算机关机时，内存中的指令和数据也会被清除。
CPU是寄存器的集合体 在CPU的四个结构中，寄存器的重要性远远高于其余三个，为什么这么说？因为程序通常是把寄存器作为对象来进行描述的。
而说到寄存器，就不得不说到汇编语言，说到汇编语言，就不得不说到高级语言，说起高级语言也就不得不提及语言的概念。
计算机语言 人和人之间最古老和直接的沟通媒介是语言，但是和计算机沟通，就必须按照计算机指令来交换，其中就涉及到语言的问题。
最早，为了解决计算机和人类的交流的问题，出现了汇编语言。
但是汇编语言晦涩难懂，所以又出现了像是C、C++、Java的这种高级语言，因此计算机语言一般分为低级语言和高级语言。
使用高级语言编写的程序，经过编译转换成机器语言后才能运行，而汇编语言经过汇编器才能转换为机器语言。
汇编语言 我们先来看一段采用汇编语言表示的代码清单：
这是采用汇编语言编写程序的一部分，汇编语言采用助记符来编写程序，每个原本是电信号的机器语言指令会有一个与其对应的助记符。
比如，mov,add分别是数据的存储（move）和相加（addition）的简写。
汇编语言和机器语言一一对应，这点和高级语言不同，我们通常把汇编语言编写的程序转换为机器语言的这个过程，称之为汇编。
与之相反，将机器语言转化为汇编语言的过程称之为反汇编。
汇编语言可以帮助你理解计算机做了什么工作，机器语言级别的程序通过寄存器来处理，上面代码中的eax,ebp都是表示的寄存器，它们是CPU内部寄存器的名称。
因此，可以说 CPU 是一系列寄存器的集合体。
一般，在内存中的存储通过地址编号来表示，寄存器的种类是通过名字来区分。
那些不同类型的CPU，其内部寄存器的种类、数量以及寄存器存储的数值范围也都是不同的。
不过，根据功能的不同，我们可以将寄存器划分为下面几类：
其中，程序计数器、标志寄存器、累加寄存器、指令寄存器和栈寄存器只有一个，其他寄存器一般有好几个。
程序计数器 程序计数器是用来存储下一条指令所在单元的地址。
程序在执行时，PC的初值作为程序第一条指令的地址，在顺序执行程序时，控制器先按照程序计数器所指出的指令地址，从内存中取出一条指令，随后分析和执行该指令，并同时将PC的值加1指向下一条要执行的指令。
我们可以通过一个事例来仔细看一下程序计数器的执行过程：
这是一段进行相加的操作，程序启动，在经过编译解析后，会经由操作系统把硬盘中的程序复制到内存中。
以上示例程序，就是将123和456执行相加的操作，随后将结果输出到显示器上，因为使用机器语言很难描述，所以这些都是经过翻译后的结果。
事实上，每个指令和数据都有可能分布在不同的地址上，但是为了更好的说明，就把组成一条指令的内存和数据放在了一个内存地址上。
地址0100是程序运行的起始位置，Windows等操作系统把程序从硬盘复制到内存以后，就会将程序计数器作为设定为起始位置0100，然后再执行程序，每次执行一条指令后，程序计数器的数值就会增加1，或者是直接指向下一条指令的地址。
随后，CPU会根据程序计数器的数值，从内存中读取命令并且执行，换言之，程序计数器控制着程序的流程。
条件分支和循环机制 小伙伴们都学过高级语言，高级语言汇总的条件控制流程主要分为顺序执行、条件分支、循环判断三种。
 顺序执行是按照地址的内容顺序的执行命令。 条件分支是根据条件执行任意地址的指令。 循环是重复执行同一地址的指令。  一般情况下，顺序执行的情况较简单，每次执行一条指令程序计数器的值就是+1。
条件和循环分支会使得程序计数器的值指向任意的地址，这样一来，程序就可以返回到上一个地址来重复执行同一个指令，或者跳转到其它任意指令。
下面，我们就以条件分支举例来说明程序的执行过程：
程序的开始过程和顺序流程是一样的，程序的顺序流程和开始过程相同。
CPU从0100处就开始执行命令，在0100和0101中都是顺序执行，PC的值顺序+1，执行到0102地址的指令时，判断0106寄存器的数值大于0，跳转到0104地址的指令，再将数值输到显示器中，随后结束程序，0103的指令就被跳过了。
这和我们程序中的if（）判断相同，在不满足条件的情况下，指令一般会直接跳过。
因此，PC的执行过程没有直接+1，而是下一条指令的地址。
标志寄存器 条件和循环分支会使用到 jump（跳转指令），会根据当前的指令来判断是否跳转，上面我们提到了标志寄存器，无论当前累加寄存器的运算结果是正数、负数还是零，标志寄存器都会将其保存。
CPU在进行运算时，标志寄存器的数值会根据当前运算的结果自动设定，运算结果的正、负和零三种状态由标志寄存器的三个位表示。
标志寄存器的第一个字节位、第二个字节位、第三个字节位各自的结果都为1时，分别代表着正数、零和负数。
CPU的执行机制比较有意思，假设累加寄存器中存储的XXX和通用寄存器中存储的YYY做比较，执行比较的背后，CPU的运算机制就会做减法运算。
而无论减法运算的结果是正数、零还是负数，都会保存到标志寄存器中。
结果为正表示 XXX 比 YYY 大，结果为零表示 XXX 和 YYY 相等，结果为负表示 XXX 比 YYY 小，程序比较的指令，实际上是在 CPU 内部做减法运算。
函数调用机制 函数的调用和条件分支，循环机制有所不同，单纯的跳转指令无法实现函数的调用。
函数的调用需要在函数内部处理后，处理流程在返回到函数调用点（函数调用指令的下一个地址）。
函数的调用处理是通过把程序计数器的值设定成函数的存储地址来实现的。
通过地址和索引实现数组 接下来是基址寄存器和变址寄存器，通过这两个寄存器，可以对主存上的特定区域进行划分，以此实现类似数组的操作。
首先，可以用十六进制数将计算机内存上的 00000000 - FFFFFFFF 的地址划分出来。
这样，凡是该范围的内存地址，只要有一个 32 位的寄存器，就可以查看全部地址。
但是，要是想像数组那样，分割特定的内存区域以达到连续查看的目的的话，使用两个寄存器会更方便一些，比如，我们用两个寄存器来表示内存的值。
这种表示方式很像数组的构造，数组是指同样长度的数据，在内存中进行连续排列的数据构造。
用数组名表示数组全部的值，通过索引来区分数组的各个数据元素，例如: a[0] - a[4]，[]内的 0 - 4 就是数组的下标。
CPU指令执行过程 那说了这么多，CPU到底是怎么一条条的执行指令的呢？几乎全部的冯·诺伊曼型计算机的CPU，工作都可以分为5个阶段：取指令、指令译码、执行指令、访存取数、结果写回。
取指令阶段就是将内存中的指令读取到CPU中寄存器的过程，程序寄存器用于存储下一条指令所在的地址；
 在取指令完成后，立马进入指令译码阶段，在指令译码阶段，指令编码器按照预先的指令格式，对取回的指令进行拆分和解释，识别区分出不同的指令类别和各种获取操作数的方法； 执行指令阶段的任务是完成指令所规定的各种操作，具体实现指令的功能； 访问取数阶段的任务是：根据指令地址码，得到操作数在主存中的地址，并从主存中读取该操作数用于运算； 结果写回阶段作为最后一个阶段，把执行指令阶段的运行结果数据“写回”到某种存储形式：结果数据经常被写到CPU的内部寄存器中，以便被后续的指令快速地存取。 ]]></content>
  </entry>
  
  <entry>
    <title>AMD Zen6架构继续飞跃！核显跨越下下代RDNA5</title>
    <url>/post/news/AMD-Zen6-architecture-continues-to-leap-forward.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>AMD</tag>
      <tag>Zen6</tag>
      <tag>RDNA5</tag>
    </tags>
    <content type="html"><![CDATA[AMD的下一代Zen5 CPU架构还没来，Zen6的消息就已经多次传出，现在又提到了所集成的GPU核显，居然将会搭配同样下下一代的RDNA5。
据曝料，AMD正在同时研发Zen5、Zen6两代架构，其中Zen5的桌面版会继续集成RDNA2 GPU，移动版则会升级到RDNA3.5。
Zen6的服务器版代号为Morpehus(古希腊神话梦神摩耳甫斯)，消费级版代号则是“Medusa”(美杜莎)。
最新说法称，Medusa Zen6会进入新的2.5D互连封装技术，取代现在传统式多die互连的chiplet小芯片，从而提升传输带宽与性能。
Zen6架构的锐龙还会集成RDNA5架构的GPU核显，这也意味着，尚未露面的RDNA4会被跳过去。
至于为何这么做，尚不清楚，可能是RDNA4提升不够大，也可能是RDNA5的进展更快。
有趣的是，就在不久前，微软关于Xbox主机的一份文档中，就提到了Zen6、Navi5——后者就是RDNA5家族。
根据此前曝料，Zen6架构将会全面升级，制造工艺升级到CCD 2nm、IOD 3nm，CCD再次升级为原生32核心，IPC性能再提升10％，支持16通道内存，加入AI/ML FP16浮点指令等等。
理论上，Zen6 EPYC可以做到庞大的256核心512线程！
相关消息：
AMD官方宣布，即将发布的Windows更新，会在任务管理器中增加新的监控标签，可显示锐龙8040系列处理器内置NPU AI引擎的使用情况。
AMD 2023年初发布的锐龙7040系列，首次在x86处理器中加入了独立的NPU AI引擎，分担原本由CPU或GPU执行的部分AI推理任务，从而提高运行效率、延长电池寿命。
2023年底，AMD又发布了升级版的锐龙8040系列，AI性能大幅提升60％，相关笔记本已经陆续发布上市。
AMD也一直在与微软合作，在锐龙8040系列移动处理器上启用MCDM（微软计算驱动程序模型）。
这是WDDM（Windows显示驱动程序模型）的衍生工具，面向NPU等非GPU、计算设备，使其够利用现有的GPU设备管理功能，包括调度、电源管理、内存管理、使用任务管理器等工具进行性能调试。
MCDM作为基础层，可确保AI工作负载在NPU设备上的顺利执行。
至于首发的锐龙7040系列是否也能在Windows任务管理器中监控NPU，暂时不详。
Intel Meteor Lake酷睿Ultra处理器也集成了NPU AI引擎，并且已经抢先显示在Windows任务管理器中，命名为“Intel AI Boost”，可实时显示NPU利用率、内存占用等数据。
]]></content>
  </entry>
  
  <entry>
    <title>中国合规RTX 4090D正式发布：性能只差5%</title>
    <url>/post/news/China-compliant-RTX-4090D-officially-released-by-NVIDIA.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>NVIDIA</tag>
      <tag>GPU</tag>
      <tag>RTX 4090D</tag>
    </tags>
    <content type="html"><![CDATA[刚刚， NVIDIA  官网发布了针对中国市场定制的RTX 4090D——D就是传说中的Dragon，对应即将到来的中国龙年。
RTX 4090D的设计和规格完全符合美国出口管制标准，可以在中国内地和港澳市场正常生产、销售。
它牺牲了一部分规格和性能，但不多：
核心芯片还是AD102，编号改为AD102-250。
CUDA核心数量从16384个减少到14592个，核心基础频率反而从2235MHz小幅提高到2280MHz，加速频率则维持在2520MHz，只可惜不开放超频。
显存维持不变，还是384-bit 24GB GDDR6X，等效频率21GHz。
显卡总功耗设定从450W降低到425W，常规游戏功耗为302W，视频播放功耗为26W，空闲功耗为19W。
游戏性能方面，根据NVIDIA官网图表，RTX 4090D相比于RTX 4090只损失了大约5％，几乎没有明显影响。
RTX 4090D将从2024年1月起陆续上市，官方建议零售价12999元起，和当初RTX 4090首发时完全相同，其实很大程度上是汇率的影响。
需要注意的是，RTX 4090D没有公版卡，将由各大AIC品牌厂商推出各自的非公版产品。等。
另外，NVIDIA还在1月份陆续推出RTX 40 SUPER系列，RTX 4070 SUPER、RTX 4070 Ti SUPER、RTX 4080 SUEPR先后登场，构成新一代完整的龙年RTX 40家族布局。
没想到，AMD也有新卡。
RX 7600发布之后，AMD高管曾说过RDNA3家族到此为止不会有新卡了，但旅程显然没有结束，RX 7600 XT就要来了。
据悉，RX 7600 XT将在1月22日开始的一周正式发布，最大可能是1月24日，也就是和RTX 4070 Ti SUPER基本同时，当然二者不是一个档次的产品。
有趣的是，RX 7600 XT不会提供公版，只有非公版。
RX 7600 XT的具体规格暂时不详，但是考虑到RX 7600已经使用了完整版的Navi 33芯片，没有提升空间，它显然不可能只是提升频率了事，几乎肯定会用更大的Navi 32芯片，进一步屏蔽规格。
这大概可以解释为什么RX 7600已经发布了足足半年，RX 7600 XT才姗姗来迟——需要准备好足够数量的残次版Navi 32芯片，充分实现“废物”再利用。
至于RX 7800、RX 7700，目前不在AMD的规划内，RX 7600 XT应该是明年上半年唯一的新品。
RDNA4架构的下一代，如果顺利的话有望在明年下半年看到，支持PCIe 5.0，但不排除拖延到2025年。
]]></content>
  </entry>
  
  <entry>
    <title>全球第二款24TB大硬盘诞生</title>
    <url>/post/news/the-world-second-24TB-large-hard-drive-is-born.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Seagate</tag>
      <tag>24TB</tag>
      <tag>Exos X24</tag>
    </tags>
    <content type="html"><![CDATA[继银河Exos X24之后，希捷近日发布了全球第二款24TB超大容量的机械硬盘，这次隶属于SkyHawk AI酷鹰人工智能系列，主要面向边缘和安防领域的视频图像应用(VIA)。
希捷酷鹰AI 24TB硬盘采用CMR传统磁记录技术，充氦设计，缓存512MB，最大数据传输率285MB/s，平均读写功耗7.8W，空闲功耗6.3W，待机功耗1.1W。
该硬盘可智能适应用户的AI环境扩展，支持多达64路高清视频流、32路AI流，每秒可捕获最多120个AI实时事件、对比96个AI实时事件，支持10000小时的视频和分析。
同时采用ImagePerfect AI固件，支持AI全天候安防，而且零丢帧。
作为企业级解决方案，该硬盘具有极高的可靠性，平均故障间隔时间250万小时，最大写入负载550TB每年，是普通VIA硬盘的3倍、台式机硬盘的10倍，并提供长达5年有限质保、3年数据恢复服务。
希捷还提供兼容NVR网络录像机系统的酷鹰健康管理（SkyHawk Health Management）功能，主动监测环境和使用条件，并在必要时提供预防措施建议。
希捷酷鹰AI硬盘还有20TB、16TB、12TB等不同容量版本。
银河Exos X24 24TB硬盘则是面向大规模企业级市场的，不但是行业首款商用24TB机械硬盘，单碟容量也提升到2.4TB，因此只需10张碟片就能达成，是迄今为止存储密度最高的硬盘。
同样令人惊叹的是，银河X24并没有使用HAMR热辅助磁记录等新的存储技术，而依然是CMR传统磁记录，能达到如此密度和容量殊为不易，但也可能是真正的尽头了。
银河X24依然是典型的3.5英寸硬盘，充氦封装，转速7200RPM，缓存512MB，接口可选SATA、SAS，待机功耗6.3W，最大功耗8.9W。
性能方面，持续数据传输率高达285MB/s，4K QD16随机读写速度168/550 IOPS，平均延迟4.16ms，并增强缓存功能，比仅使用读写缓存的高出3倍。
可靠性上，支持7 x 24全年不间断运行，年故障率0.35％，平均故障间隔时间250万小时，质保5年。
安全方面支持Secure加密、自加密硬盘(SED)、自加密和即时擦除(SED-FIPS)、即时安全擦除(ISE)等等。
希捷银河X24 24TB硬盘现已发货，将于12月面向渠道供货。
希捷还预告，将把同样的底层技术平台应用于30TB以上容量硬盘，并加入HAMR技术，2024年初开始大规模量产。
]]></content>
  </entry>
  
  <entry>
    <title>将 CPU 与 FPGA Fabrics 结合使用的案例</title>
    <url>/post/fpga/example-of-using-cpu-and-fpga.html</url>
    <categories><category>FPGA</category>
    </categories>
    <tags>
      <tag>FPGA</tag>
      <tag>eFPGA</tag>
      <tag>CPU</tag>
      <tag>Fabric</tag>
    </tags>
    <content type="html"><![CDATA[如何在规模效益下降时继续推动性能提升，本文就介绍一个将 CPU 与 FPGA Fabrics 结合使用的案例。
鉴于半导体行业通过进一步缩小工艺几何尺寸在物理和经济上所能实现的目标已开始达到极限，缩小特征尺寸和增加晶体管数量已不再能达到以往的效果。取而代之的是，整个行业都在关注全新的系统架构，并通过重新思考如何在每个设备中完成任务来更好地利用现有硅片。当我们进入这个新的技术时代时，以 嵌入式   FPGA 的形式将 FPGA   结构与 CPU 集成成为一种极具吸引力的解决方案。
虽然 FPGA 和 CPU 都使用内存和逻辑组合来保存和处理数据和指令，但两者之间存在重要的根本区别。中央处理器针对快速上下文切换进行了优化，而 FPGA 的配置速度较慢，但能以类似于硬连线电路的速度模拟数字逻辑。因此，CPU 擅长执行各种任务，而 FPGA 则擅长执行重复性（尤其是高度并行化）任务，这些任务重复执行数千次，偶尔才会重新定义。
市场指标 有明显的证据表明，CPU 和 FPGA 技术的结合可以通过更紧密的集成带来真正的价值。这方面的第一个例子是英特尔公司斥资 167 亿美元收购 Altera 公司，用于加速数据中心功能。第二个例子是微软的 Catapult 计划，该计划表明，通过在每台服务器中集成 FPGA 来加速必应搜索、Azure 和 Microsoft 365，可以将数据中心服务器的计算能力提高一倍。这些例子表明，业界已经开始认识到 CPU 和 FPGA 异构架构的优势。这些类型的异构架构将不可避免地转移到同一设备上，FPGA 结构将作为 IP 块集成到 ASIC 中。
迈入 eFPGA 时代 Achronix 现在是这一领域的主要推动者，它已经推出了嵌入式 FPGA IP，这些 IP 源自其早期以高性能和复杂路由架构著称的独立 FPGA 系列。Speedcore™ eFPGA IP 展示了将 FPGA 结构集成到 CPU/SoC 中的许多可能优势。下图显示了如何将 Speedcore IP 集成到 SoC 子系统中。
图 1：SoC 子系统中的速核 eFPGA
由于 eFPGA 位于同一设备上，信号无需经过 SerDes 和 PCIe 等协议编码。因此，延迟时间要低一个数量级。此外，由于片上互连的带宽更高，采用 eFPGA 结构的 SoC 比采用独立 FPGA 的 SoC 性能更高。
集成 FPGA 结构还大大降低了功耗。由于在片上集成了 FPGA 结构，去除了时钟发生器、无源元件等一些辅助元件，因此这种功耗节省还延伸到了系统层面。
分立 FPGA 的尺寸和性能范围是固定的，而 eFPGA IP 块中逻辑门和存储器之间的组合则不同，可以由客户自行定义，这样就能获得适当数量的 FPGA，以加速相应的功能。这种功能可确保在 SoC 中实现最佳的 CPU 大小与 FPGA 资源比，从而优化硅面积、功耗和成本。
总之，毫无疑问，eFPGA 将成为未来几年的主要架构趋势。其优势实在是太引人注目了。
]]></content>
  </entry>
  
  <entry>
    <title>tr, 一个非常实用的linux命令</title>
    <url>/post/linux/tr-a-useful-command-in-linux.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>tr</tag>
    </tags>
    <content type="html"><![CDATA[在 Linux  操作系统中，tr命令是一个强大的字符转换工具，它可以在文件或输入流中替换、删除或压缩字符。该命令的名称“tr”是“translate”的缩写，它可以通过多种选项来执行不同的字符转换操作。本文将介绍tr命令的用法，并提供至少几个常用选项的示例，包括代码、输出和解析。
删除字符 tr命令的一个常见用法是删除指定的字符。使用选项-d，我们可以删除输入中出现的指定字符。
示例代码：
echo &#34;Hello, World!&#34; | tr -d &#39;o&#39; 输出：
Hell, Wrld! 解析：上述示例中，我们使用了管道符号（|）将字符串“Hello, World!”传递给tr命令。选项-d告诉tr命令删除指定的字符，这里我们删除了字符&rsquo;o'。因此，输出中的所有&rsquo;o&rsquo;都被删除了。
压缩字符 tr命令还可以用于压缩字符。使用选项-s，我们可以将输入中连续出现的重复字符压缩成一个字符。
示例代码：
echo &#34;Hello, World!&#34; | tr -s &#39; &#39; 输出：
Hello, World! 解析：在上述示例中，我们使用tr命令将连续出现的空格字符压缩成一个空格。选项-s告诉tr命令进行字符压缩。因此，输出中的多个连续空格被替换为一个空格。
字符集取补集 tr命令还可以用于字符集的取补集。使用选项-c，我们可以对输入进行字符集求反操作。
示例代码：
echo &#34;Hello, World!&#34; | tr -c &#39;A-Za-z&#39; &#39;\n&#39; 输出：
Hello World 解析：在上述示例中，我们使用了选项-c来对输入进行字符集求反操作。第一个参数&rsquo;A-Za-z&rsquo;指定了一个字符集，它包含了所有的字母字符。第二个参数'\n&rsquo;指定了替换字符，这里是换行符。因此，输出中的所有非字母字符都被替换为换行符。
大小写转换 tr命令还可以用于大小写转换。使用选项-u和-l，我们可以将输入中的字母字符转换为大写或小写。
示例代码：
echo &#34;Hello, World!&#34; | tr &#39;[:lower:]&#39; &#39;[:upper:]&#39; 输出：
HELLO, WORLD! 解析：在上述示例中，我们使用tr命令将输入中的所有小写字母转换为大写字母。选项&rsquo;u&rsquo;和&rsquo;l&rsquo;用于指定转换为大写和小写。参数'[:lower:]&lsquo;和&rsquo;[:upper:]&lsquo;是字符类，用于表示所有的小写和大写字母。
总结： tr命令是Linux中一个功能强大的字符转换工具。它可以删除、替换、压缩字符，进行字符集转换和大小写转换等操作。本文介绍了tr命令的几个常用选项，包括删除字符、替换字符、字符集转换、大小写转换和字符集补集。通过这些示例，读者可以了解tr命令的基本使用方法，并根据实际需求在Linux系统中灵活应用该命令。
]]></content>
  </entry>
  
  <entry>
    <title>ip, 一个强大的Linux命令</title>
    <url>/post/linux/ip-a-powerful-command-in-linux.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>ip</tag>
    </tags>
    <content type="html"><![CDATA[在现代计算机网络中，对 网络  进行管理和配置是至关重要的。 Linux  操作系统作为广泛使用的服务器和嵌入式设备平台，提供了丰富的工具和命令来管理网络。其中，IP命令是网络管理的核心工具之一。它可以帮助管理员完成诸如配置网络接口、设置路由规则、管理地址分配等任务。下面我们将深入探索IP命令的用法，并提供相应的代码示例、输出和解析。
IP命令概述 IP命令是Linux系统中网络管理的一种工具，它可以用于配置网络接口、设置路由规则、管理地址分配等。它的优点在于灵活性高、功能强大且易于使用。
IP命令的常用用法示例  显示网络接口信息  $ ip link show 输出：
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 00:11:22:33:44:55 brd ff:ff:ff:ff:ff:ff 解析：该命令将显示当前系统上的所有网络接口及其状态、MAC地址、MTU等信息。输出中的每一行代表一个网络接口。
配置网络接口  $ ip addr add 192.168.0.10/24 dev eth0 解析：该命令将为名为eth0的网络接口分配IP地址192.168.0.10，子网掩码为24。执行该命令后，网络接口将具有指定的IP地址。
启用或禁用网络接口  $ ip link set eth0 up $ ip link set eth0 down 解析：第一个命令将启用名为eth0的网络接口，使其处于活动状态。第二个命令将禁用该接口，使其不可用。
设置默认网关  $ ip route add default via 192.168.0.1 解析：该命令将设置默认网关为192.168.0.1，所有不在本地子网的数据包都将通过该网关进行转发。
显示路由表  $ ip route show 输出：
default via 192.168.0.1 dev eth0 192.168.0.0/24 dev eth0 proto kernel scope link src 192.168.0.10 解析：该命令将显示当前系统的路由表，包括目标网络、网关、接口等信息。输出中的每一行代表一条路由规则。
添加静态路由  $ ip route add 10.0.0.0/24 via 192.168.1.1 dev eth0 解析：该命令将添加一个静态路由，将目标网络10.0.0.0/24通过网关192.168.1.1发送到eth0接口。
IP命令的实际应用场景  配置网络接口：IP命令可用于为服务器配置静态IP地址，使其在网络中具有固定的身份。管理员可以使用IP命令为网络接口分配IP地址、设置子网掩码、启用或禁用接口等。 管理路由规则：IP命令提供了添加、删除和修改路由规则的能力，可以根据需要设置静态路由或调整默认路由。这对于构建复杂的网络拓扑和实现网络分段非常有用。 调试网络连接问题：IP命令可以用于诊断网络连接问题。管理员可以使用IP命令检查网络接口的状态、查看路由表、查找丢包等信息，帮助排除网络故障。  总结： IP命令是Linux中网络管理的重要工具，它提供了丰富的功能来配置网络接口、设置路由规则和管理地址分配。本文介绍了IP命令的常用用法示例，并对其输出进行了解析。了解和熟练掌握IP命令的用法，将帮助管理员更好地管理和维护Linux系统中的网络环境。通过灵活应用IP命令，可以构建稳定、高效的网络架构，并快速解决网络故障。
]]></content>
  </entry>
  
  <entry>
    <title>数据中心液冷技术阐述</title>
    <url>/post/datacenter/data-center-liquid-cooling-technology-introduction.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Liquid Cooling</tag>
      <tag>Heatsink</tag>
    </tags>
    <content type="html"><![CDATA[液冷技术的首要分支之一是间接接触型液冷技术，其中以冷板式液冷系统为代表。
在这一技术体系中， 服务器  芯片等发热器件并非直接接触冷却液，而是通过安装在电子元器件上的冷板实现热量的间接传导。这种系统架构主要分为室外和室内两部分，通过冷却液循环动力和冷却液体流量分配单元（CDU）实现高效的热交换。
在冷板式液冷系统中，室外冷却塔中的冷却液通过室内CDU提供循环动力，经过CDU二次侧输出并与服务器中的发热元器件直接进行热交换，形成的热液再经CDU输出到室外冷却塔进行循环冷却。对于一些发热相对较小的设备，仍然采用传统的风冷方式，实现了系统内外的高效能量利用。
直接接触型液冷技术的代表是浸没式液冷技术，而浸没式液冷技术则分为单相液冷和相变液冷两大类。在单相液冷中，冷却液保持液态，通过循环泵导入热交换单元，实现了高效的散热。而相变液冷通过冷却液体的相变过程提高传热效率，成为最节能、最高效的制冷模式。
在单相液冷技术中，要求冷却液的沸点较高，以确保冷却液在循环散热过程中始终保持液态，提高了系统的可维护性。相比之下，相变液冷技术的传热效率更高，但也面临着挥发流失控制复杂、系统密封性要求高等挑战。因此，在液冷技术的道路上，间接接触型液冷技术通过其巧妙的构架为数据中心提供了可靠而高效的散热解决方案。
直接接触型液冷技术中，浸没式液冷技术成为了引领创新的前卫。该技术将服务器完全浸没在设计独特的箱体中，使发热部件直接接触冷却液体，实现了更高效的散热。浸没式液冷分为单相液冷和相变液冷两种形式。
在单相液冷技术中，通过循环泵循环冷却液，维护简便，对电气兼容性较好。而相变液冷则通过冷却液体相变的方式，提高了传热效率，但同时也增加了系统的复杂性和维护难度。浸没式相变液冷技术通过相变过程，使冷却液体的蒸发和冷凝实现了更为高效的传热，为高发热元器件提供了极端要求下的散热保障。
然而，相变液冷在控制冷却液挥发流失和维护上面临一些挑战。系统密封性和箱体的压力成为了关键问题，同时相变过程的气压变化也需要仔细考虑。对于液冷技术的商用化进程，浸没式液冷技术仍需不断突破创新，解决技术难题，才能更好地为数据中心提供可持续、高效的散热解决方案。
综合来看，直接接触型液冷技术的浸没式创新，无疑为数据中心提供了更为可靠、高效的散热解决方案。无论是间接接触型还是直接接触型，液冷技术已经成为数据中心散热的新时代代表，为未来数字化社会的持续发展提供了强大动力。
]]></content>
  </entry>
  
  <entry>
    <title>ss, 一个比netstat好用N倍的命令</title>
    <url>/post/linux/ss-a-command-that-is-much-easier-to-use-than-netstat.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Vim</tag>
    </tags>
    <content type="html"><![CDATA[在 Linux  系统中，了解当前的网络连接状态对于故障排查、网络性能调优和安全性评估至关重要。ss（socket statistics）命令是Linux系统中一种功能强大的工具，用于检查和显示网络连接相关的信息。
ss命令简介 ss命令是net-tools软件包的替代品，提供了比传统的netstat命令更强大和更快速的网络连接信息检查功能。ss命令可以列出当前的网络连接、显示监听的端口、过滤和排序连接信息等。
列出当前的网络连接 要列出当前的网络连接，可以使用以下命令：
ss -t 上述命令将显示所有TCP连接的详细信息。类似地，您可以使用-u选项来显示所有UDP连接的信息。
ss -u 现在，让我们来执行ss -t命令，并解析输出结果：
State Recv-Q Send-Q Local Address:Port Peer Address:Port ESTAB 0 0 192.168.0.1:22 192.168.0.100:12345 TIME-WAIT 0 0 192.168.0.1:443 192.168.0.200:56789 输出结果的每一列都提供了有用的信息。例如，&ldquo;State&quot;列显示连接的状态，&ldquo;Local Address:Port&quot;列显示本地地址和端口，&ldquo;Peer Address:Port&quot;列显示远程地址和端口。
过滤和排序连接信息 ss命令提供了强大的过滤和排序功能，以便更好地分析和查找特定的连接信息。以下是一些常用的选项：
 -s：按照连接状态统计连接信息。  ss -s 输出示例：
State Total ESTAB 10 TIME-WAIT 5 上述输出结果显示了不同连接状态的数量。
 -p：显示与连接关联的进程信息。  ss -pt 输出示例：
State Recv-Q Send-Q Local Address:Port Peer Address:Port Process ESTAB 0 0 192.168.0.1:22 192.168.0.100:12345 sshd 上述输出结果显示了与每个连接相关的进程名称。
 -o：显示计时器信息。  ss -to 输出示例：
State Recv-Q Send-Q Local Address:Port Peer Address:Port Timer ESTAB 0 0 192.168.0.1:22 192.168.0.100:12345 off (0.00/0/0) 上述输出结果显示了与每个连接关联的计时器信息。
 -n：以数字格式显示IP地址和端口号。  ss -tn 输出示例：
State Recv-Q Send-Q Local Address:Port Peer Address:Port ESTAB 0 0 192.168.0.1:22 192.168.0.100:12345 上述输出结果中的IP地址和端口号以数字格式显示，而不是解析为主机名和服务名称。
 -l：显示正在监听的端口。  ss -l 输出示例：
State Recv-Q Send-Q Local Address:Port LISTEN 0 128 0.0.0.0:22 LISTEN 0 128 0.0.0.0:80 上述输出结果显示了正在监听的端口。
总结 ss命令是Linux系统中一种强大的网络连接信息检查工具，通过使用ss命令，我们可以轻松地列出当前的网络连接、过滤和排序连接信息、查看进程关联的连接等。本文通过代码示例、输出和解析向您展示了ss命令的用法，希望能够帮助您更好地理解和应用ss命令进行网络连接信息的全面检查。无论是系统管理员还是网络工程师，掌握ss命令都是非常重要的技能，它能够为您提供有关网络连接的深入洞察和分析。
]]></content>
  </entry>
  
  <entry>
    <title>掌握vim编辑器这些实用命令，让编辑效率翻倍</title>
    <url>/post/linux/master-these-practical-commands-of-vim-editor-to-improve-efficiency.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Vim</tag>
    </tags>
    <content type="html"><![CDATA[vim编辑器是 Linux  系统中备受推崇的文本编辑器之一，其丰富的命令为编辑工作提供了极大的便利。本文将介绍一些常用的vim命令，掌握这些命令，可以在编辑过程中更加得心应手，提高工作效率。
vim编辑器介绍 vi（Visual Interface）是一款功能强大的文本编辑器，广泛应用于Linux和Unix系统。vim（Vi IMproved）是vi的增强版，如果系统中已经安装了vim，那么在使用&quot;vi 文件名&quot;命令时（本文绿色部分的内容为命令及其参数），实际上会调用vim来打开文件。虽然它的界面相对简陋，但是通过熟练使用其命令，我们可以大大提高编辑效率。
vim编辑器主要有两种工作模式：命令模式和编辑模式。
命令模式：这是使用vi编辑器打开文件后的默认模式，也可以在任何时候通过按下Esc键从编辑模式切换到该模式。在此模式下，用户可以进行保存、恢复、退出、查找和替换等操作，也可以。
编辑模式：在命令模式下，输入“i、a、o、s”等字母可以进入编辑模式。在此模式下，可以在光标所在位置插入新的文本或修改现有的文本。可以通过光标键来移动光标到新的位置，然后继续编辑。
进入/退出vim编辑器 vi file 打开文件，文件不存在则新建文件
:q 未修改时退出 :q! 放弃修改并退出 :x 保存修改并退出 :x! 强制保存修改并退出 :e! 放弃修改重新载入文件 命令模式基本操作 显示/隐藏行号：
:set nu 显示行号，nu是number的简写 :set nonu 隐藏行号，nonu是nonumber的简写 光标移动：
h 左移一个字符 j 下移一行 gj 在折行文本中下移一行 k 上移一行 gk 在折行文本中上移一行 l 右移一个字符 e 跳到下一个单词词尾 ge 跳到上一个单词词尾 w 跳到下一个单词词首 b 跳到上一个单词词首 0 跳到行首 ^ 跳行首第一个非空字符处 $ 跳到行尾 :n n为具体数字，跳到第n行 gg 跳到文件首行 G 跳到文件末行 ctrl+f 向下翻页 ctrl+b 向上翻页 编辑：
i 进入编辑模式，可以在光标位置插入字符 I （大写i）进入编辑模式，可以在光标所在行首插入字符 a 光标后移一位进入编辑模式，可以在光标位置插入字符 A 进入编辑模式，可以在光标所在行尾插入字符 o 进入编辑模式，在光标所在行下方新建一行 O 进入编辑模式，在光标所在行上方新建一行 r 进入编辑模式，可以在光标位置替换一个字符 R 进入编辑模式，可以从光标位置开始替换字符 s 删除光标位置的字符然后进入编辑模式，可以在光标位置插入字符 S 删除光标所在行的字符然后进入编辑模式，可以在光标位置插入字符 x 删除光标位置的字符 X 删除光标位置前面的字符 D 删除光标位置及之后的字符 dd 剪切光标所在的行 diw 剪切光标处的单词 daw 剪切光标处的单词和其后的空白字符 yy 复制光标所在的行 yiw 复制光标处的单词 yaw 复制光标处的单词和其后的空白字符 p 粘贴剪切/复制的行到光标所在的行下方，或者粘贴剪切/复制的单词到光标位置后面 u 撤销上一次操作 命令前面可以添加数字，表示重复执行该命令的次数。例如：
4j 表示光标向下移动四行 3dd 表示剪切当前行及其下面的两行共三行内容 2p 表示将复制的内容粘贴二次 3u 表示撤销前面三次操作 搜索/替换：
/pattern 从当前位置向文件尾开始搜索，按回车键跳转到第一个匹配项 ?pattern 从当前位置向文件头开始搜索，按回车键跳转到第一个匹配项 n 跳转到下一个匹配项 N 跳转到上一个匹配项 :s/pattern/新字符串/g 将当前行中的所有与pattern匹配的内容替换为新字符串，g表示全局替换，省略g或者省略/g，则仅替换第一个匹配项 :7,9s/pattern/新字符串/g 在第7-9行进行替换，省略g或者省略/g，则每行仅替换第一个匹配项 :%s/pattern/新字符串/g 在整个文件中进行替换，省略g或者省略/g，则每行仅替换第一个匹配项 命令模式高级操作 分屏查看：
:split 将文件分为上下两个窗口查看 :vsplit 将文件分为左右两个窗口查看 ctrl+w+w （按住ctrl，然后按两次w）在不同窗口之间切换 ctrl+w+h/j/k/l 切换到左、下、上、右一个窗口 ctrl+w+v （按住ctrl，先后按w和v）将当前窗口分为左右两个窗口查看 :q 退出当前窗口 标签页编辑多个文件：
:tabedit file1 在同一个窗口中打开另一个文件，以标签页显示 :tabn 切换到下一文件 :tabp 切换到上一文件 一次编辑多个文件：
vi file1 file2 ... 一次打开多个文件 :n 切换到下一文件 :N 切换到上一文件 ]]></content>
  </entry>
  
  <entry>
    <title>将AMD CPU 3D缓存当磁盘，性能碾压PCI-E 5.0 SSD</title>
    <url>/post/hardware/use-AMD-CPU-3D-cache-as-solida-state-disk.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>AMD</tag>
      <tag>PCIe Gen5 SSD</tag>
    </tags>
    <content type="html"><![CDATA[可能很多人都忘记了，过去有一些技术可以将内存当做磁盘，这样能达到很快的速度，像技嘉这样的厂商甚至推出了相应的软件。不过这个技术一直没有流行起来，一个是因为当时的内存容量大家都不够大，另外一个还是SSD发展起来之后，用内存当磁盘的意义就不大了。毕竟PCI-E 5.0的磁盘速度都达到15GB/s了。
不过AMD的3D缓存似乎又有把类似技术推向高峰的迹象。尽管目前AMD的X3D处理器大多数只有96MB的3D缓存，但由于这是连接在高速线路上，如果将其作为磁盘来读写，那么性能可想而知。现在还真有人做了一些方案，将AMD的3D缓存弄成磁盘并且可以连续读写，结果跑出来的性能堪称逆天，直接把PCI-E 5.0的SSD碾压成渣！
有人将锐龙7 7800X3D的96MB 3D缓存切割了32MB当做磁盘，而通过软件对其进行测试，其连续读写能力达到了178GB/s以及163GB/s，比目前最快的PCI-E 5.0 SSD高出了十倍以上，这无疑是个让人震惊的磁盘性能。现在的问题是，要如何才能将AMD的X3D处理器中的3D缓存变成磁盘？
通过了解，现在的确是有方法。用户可以下载OSFMount这个免费软件，并且通过这个软件创建RAM磁盘并挂载不同格式的映像文件。比如用户可以将处理器中的3D缓存创建为FAT32格式或者NTFS的RAM磁盘。不过想要进行测试，用户需要在CrystalDiskMark上使用精确的设置才能正常工作。根据测试者的说法，用户必须将测试值配置为SEQ 256KB、队列深度为1、线程数为16，同时用户还必须将数据填充设置为零而不是随机。
而且AMD 3D 缓存这个读写数据虽然非常出色，但它们仍远未发挥 3D V-Cache的全部潜力。要知道第一代3D V-Cache的峰值吞吐量为2TB/s，AMD 随后将第二代变体的带宽提高到 2.5TB/s ，所以现在这个读写数据其实是可以继续提升的，只要软件能适配上。当然在我们看来，这的确很酷但没什么实用性，一个是AMD的3D缓存容量不大，装不了什么东西；另一个原因是这个方法似乎还不是那么安全，至少对于存储数据而言。
目前锐龙9 7950X3D的3D缓存是128MB，而最高的EPYC则可以达到1.3GB的3D缓存容量，这个玩法算是将老派的方案和最新的技术结合在一起，至少让我们觉得比较有趣。如果AMD有兴趣的话，并且有一些相对应的方案，那么3D缓存可做的事情就比较多了，这应该让AMD的处理器获得更多应用的场景。
]]></content>
  </entry>
  
  <entry>
    <title>AMD锐龙8040产品全球首发</title>
    <url>/post/news/AMD-Ryzen-8040-product-launches-globally.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>AMD</tag>
      <tag>Ryzen</tag>
      <tag>8040</tag>
    </tags>
    <content type="html"><![CDATA[AMD日前官宣了代号Hawk Point的新一代锐龙8040系列移动处理器，是现有锐龙7040系列的升级版，工艺、架构不变，重点提升NPU AI性能，并调整了功耗设定。
现在，华擎全球第一家发布了基于锐龙8040系列的产品，确切地说是低功耗的锐龙8040U系列，包括迷你机、迷你主板。
迷你机有两款，型号分别为4X4 BOX-8840U、4X4 BOX-8640U，主板也有两款，与迷你机相对应，型号分别为4X4-8840U-1U、4X4-8640U-1U，搭载处理器分别为锐龙7 8840U、锐龙5 8640U。
锐龙7 8840U 8核心16线程，最高频率5.1GHz，集成核显Radeon 780M。
锐龙5 8640U 6核心12线程，最高频率4.9GHz，集成核显Radeon 760M。
二者的热设计功耗范围都是15-30W，比现在的固定28W更有弹性。
迷你机设计很朴素，尺寸为117.5x110.0×49毫米。
核心规格方面，迷你机和主板具备两条DDR5-5600 SO-DIMM独立内存插槽(最大容量96GB)、一条M.2 2280和一条M.2 2242(均支持PCIe 4.0 x4)、Realtek RTL8125BG 2.5千兆和Realtek RTL8111H千兆网卡、Wi-Fi 6E无线网卡、Realtek ALC256声卡。
接口有两个HDMI 1.4b和两个USB-C/DP 1.4(四屏4K)、一个USB-A 3.0、两个USB 2.0，供电为12-24V DC。
另外，零刻新款迷你主机SEi12已上架开售，准系统售价为1985元。
据悉，这款 SEi12迷你主机拥有“藏青蓝”、“千禧灰”两种配色，采用包布工艺，防水防溅，尺寸约为传统台式机的1/40倍。
处理器采用的是英特尔酷睿i7-12650H，10核心16线程，睿频至高可达4.7GHz，内存最大支持双通道64GB DDR4 3200MHz标配双通道16GB(8*2)DDR或无DDR。
存储支持M.22280 NVMe PCle4.0 SSD，至高可扩展至2TB，标配500GB/1TB/无SSD。
此外，这款迷你主机支持Wi-F i6、蓝牙5.2技术，拥有双风扇散热（CPU冷却风扇 + 系统风扇）。
接口包括一个RJ45接口、两个USB-A 3.2 Gen2接口、两个USB-A 2.0接口、一个 USB-C接口、一个HDMI 2.0接口、一个DP1.4接口。
]]></content>
  </entry>
  
  <entry>
    <title>NVIDIA AI芯片可以卖给中国！只有一条件</title>
    <url>/post/news/NVIDIA-AI-chips-can-be-sold-to-China-with-one-condition.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>NVIDIA</tag>
      <tag>AI</tag>
    </tags>
    <content type="html"><![CDATA[为了满足美国政府的出口管制条件，尽可能向中国销售高端 AI  芯片， NVIDIA  可以说操碎了心，不断研究各种特供版。
但美国政府对此穷追不舍，美国商务部长雷蒙多就直接威胁NVIDIA，这种试图绕过限制的做法是危险的，他们可以随时调整管制条件。
在最近的一次谈话中，雷蒙多又指出，美国商务部正在与NVIDIA和其他美国芯片公司共同研究更精确的管制条件，允许他们继续向中国客户销售产品，唯一的条件就是不能卖最先进的产品。
雷蒙多称：&ldquo;NVIDIA可以，将会，也应该向中国销售AI芯片，因为大多数AI芯片都是商业用途，但我们不允许销售最先进、最高性能的AI芯片，不能让中国用它们训练前沿大模型。&rdquo;
乍一看，雷蒙多的态度似乎软化了，但其实本质上并没有变，仍然不允许中国接触最先进的AI芯片，只能用一些中低端产品而已。
NVIDIA CEO黄仁勋此前也曾向雷蒙多保证，会严格遵守出口管制。
此外，雷蒙多在谈话中对中国批准博通690亿美元收购VMware的做法表示了赞赏。
除了来自政府的压力，NVIDIA还在面临竞争对手的冲击。
据业内传闻，继第三季度被微软砍掉20％的订单之后，NVIDIA GPU AI加速卡被曝再次遭到某大型云服务商的砍单，据说还是微软，NVIDIA也不得不下调了2024年下半年在台积电的订单量。
对于砍单原因，供应链的普遍看法是AMD新发布的Instinct MI300系列非常有竞争力，客户也不希望将鸡蛋都放在NVIDIA这一个篮子里。
AMD MI300系列包括GPU加速器MI300X、CPU+GPU融合加速器MI300A，后者是独家技术，性能都非常突出，部分指标已经远远超过NVIDIA竞品，而且按照惯例价格更低。
AMD的发布会上，除了Google、亚马逊两大巨头之外，微软、甲骨文、Meta(Facebook)、戴尔、惠普、联想、超威等都纷纷站台支持，相关产品也第一时间出炉。
AMD MI300X已经出货，MI300A也投入量产，预计能满足行业对于AI大模型训练推理的需求。
NVIDIA目前在AI加速市场上握有90％的份额，但产品价格与成本越来越高，让很多大企业也吃不消，都在寻求替代解决方案，AMD MI300系列无疑是目前的最佳选择，除了ROCm开发生态相比CUDA还差太多，迁移是个麻烦。
AMD现在预计，2023年数据中心AI加速器市场规模可达450亿美元，年复合增长率超过70％，2027年可超4000亿美元，比此前预计的300亿美元、50％、1500亿美元大大增加。
在这个市场上，AMD即便只拿下10％的份额，也是每年几百亿美元的收入。
不过也有观点认为，微软连续砍单是在等待NVIDIA明年将要推出的下一代芯片B100，会升级3nm工艺、Blackwell架构，性能可达目前H200的两倍以上。
最后放松一下。
NVIDIA创始人兼CEO黄仁勋最近首次到访越南，除了谈正事，还不忘忙里偷闲，享受一下街头美食。
据越南媒体报道称，在越南河内结束正式活动后，黄仁勋没有在豪华酒店享受豪华晚宴，而是逛了Luong Ngoc Quyen、Hang Non、Hang Thiec、Nguyen Huu Huan等美食街，先后点了牛肉河粉、椰子汁、火锅、蛋咖啡等当地特色小吃。
大快朵颐的黄仁勋还连称，那里的美味无可媲美。
此外，黄仁勋还参加了微星举办的一个LAN Party电竞活动，并自掏腰包，为玩家和观众颁奖，让大家受宠若惊。
一位Reddit网友就感慨，没想到作为如今最有影响力的风云人物，黄仁勋会到他们这么个小国家，参加这么一个小活动，非常有趣，也非常敬佩他。
得益于在AI领域的遥遥领先，黄仁勋的身价一再暴涨，如今已经超过3000亿元人民币，但一直以来黄仁勋都生活朴素、平易近人，经常会参加一些小型活动和聚会。
]]></content>
  </entry>
  
  <entry>
    <title>长江存储致态TiPlus7100 4TB评测</title>
    <url>/post/storage/Yangtze-Storage-TiPlus7100-4TB-Review.html</url>
    <categories><category>Storage</category>
    </categories>
    <tags>
      <tag>SSD</tag>
      <tag>Yangtze</tag>
      <tag>TiPlus7100</tag>
    </tags>
    <content type="html"><![CDATA[其实早在年初，就有不少搭载长江存储闪存颗粒的国产4TB SSD  ，不过长江存储的自有品牌致态，直到现在才推出这款致态TiPlus7100 4TB。
前言：长江存储首款自有品牌致态4TB SSD 其实早在年初，就有不少搭载长江存储闪存颗粒的国产4TB SSD，不过长江存储的自有品牌致态，直到现在才推出这款致态TiPlus7100 4TB。
当然有句话叫好货不怕晚，致态TiPlus7100 4TB是一款非常优秀的PCIe 4.0 SSD。
致态TiPlus7100 4TB采用了长江存储自家的晶栈Xtacking 3.0架构NAND闪存颗粒，I/O速度高达2400MT/s，相比市面上同类产品速度提升50%以上。
只需要搭配一个简单的4通道主控，就能达到7000MB/s的读取速度。
另外，晶栈Xtacking 3.0架构NAND闪存颗粒拥有极高的存储密度，单颗容量就能达到1TB，只需要4颗就能做到4TB容量，双面M.2 2280甚至能轻松做到8TB。
在性能方面，致态TiPlus7100 4TB拥有7000MB/s的顺序读取速度、6000MB/s的顺序写入速度，4K随机读取能力为900KIOPS，4K写入能力时间800KIOPS，写入寿命2400TBW，质保5年。
外观：只需4颗基于长江存储晶栈Xtacking 3.0构架的闪存颗粒 SSD正面有一张标贴，上面标注了SSD的各种信息。
这张贴纸下面还有一张散热贴，能让主控的温度均匀地分散出去，降低高负载下主控的温度，这是此前上市的512GB、1TB、2TB版本上所没有的。
更低的温度不仅能让SSD的性能表现更加额稳定，同时也能延长SSD的使用寿命。
管中窥豹、可见一斑，从4TB增加散热贴这一点也能看出长江存储致态对待用户的用心程度。
揭下正面的标贴，可以看到四颗长江存储的闪存整齐排列。仅用4颗就达到了4TB容量。如果做成双面8颗NAND闪存，是能够做到8TB的。
另外一面是空的。
长江存储原厂闪存颗粒，基于长江存储晶栈Xtacking 3.0架构。
性能测试：顺序读取7434MB/s、高负载稳定写入 在CrystalDiskMark测试中，致态TiPlus7100 4TB的顺序读取速度突破了7000MB/s，达到了7434MB/s，写入速度也有6512MB/s；4K随机读取80MB/s，4K随机写入282MB/s。
在进行64GB测试时，各项数据有一定幅度的降低，对于无DRAM缓存的SSD来说是正常现象。
AS SSD Benchmark太老了，其实已经不太适合测试PCIe 4.0 SSD，测试数据看看就好。
左边是1GB容量的测试，AS SSD Benchmark总分为8848，最高顺序读取、写入速度分别为6325MB/s、55778MB/s，4K随机读写速度为83MB/s、242MB/s。
右边是10GB容量的测试，总分为6945。最高顺序读取、写入速度分别为6356MB/s、5805MB/s，4K随机读写速度分别为70MB/s、220MB/s。
ATTO Disk Benchmark 分别测试了队列深度4和16的数据。
4队列深度测试中，对于0.5KB的小文件读写，致态TiPlus7100 4TB也能达到90MB/s以上的写入和67MB/s的读取。
32K的时候，写入达到了4.7GB/s，读取速度为4.5GB/s。从512KB开始，写入速度稳定在5.7GB/s左右，读取速度则能达到6.6GB/s以上。
3DMark 3DMark存储测试主要是测试游戏读写表现，致态TiPlus7100 4TB总分是4399分，这也是 PCIe   4.0 SSD的旗舰水准，平均带宽756MB/s，在移动游戏时，带宽能达到3266MB/s。
AIDA64 Disk Benchmark Linear Write AIDA64 Disk Benchmark Linear Write是一项非常严苛和残酷的测试，也是一面照妖镜，目前市面上的消费级SSD用这个软件都能暴露出真实的缓外速度。
在33%之前，能保持5000MB/s左右的写入速度，此后会掉到900MB/s左右并一直以此速度完成全部测试。
全盘拷贝与温度测试：最高温度仅45度、缓外速度2.3GB/s 全盘写入测试 为了还原最真实读写场景，我们进行了一次全盘文件拷贝测试。
致态TiPlus7100 4TB二进制真实容量为3.72TB，我们一次性写入3.63TB的文件，写完后仅剩下80GB的空间，不到全部容量的3%。
刚开始写入是，速度保持在3.4GB/s左右。
写了600GB之后，速度掉到2GB/s左右。
在写入的最后阶段，此时SSD的可用空间不到100GB，但是写入速度依然高达2.3GB/s。
完成拷贝后，致态TiPlus7100 4TB的可用空间会剩下87GB，不到总容量的3%。
温度测试 我们使用的是ROG Z790 DARK HERO主板再带的M.2散热片进行散热测试，测试室温26度。
在待机状态下，致态TiPlus7100 4TB温度在41度左右。在高负载读写状态下，最高温度也只有44度。
这个表现让人惊讶，此前大部分PCIe 4.0 SSD高负载下屋内都超过了60度。
总结：就算没有缓存 也是综合性能最好的PCIe 4.0 SSD之一 毫无疑问，这是我们测试过的综合性能最好的长江存储自有品牌致态SSD产品，大部分性能指标达到甚至超越了旗舰级PCIe 4.0 SSD的水准。
首先就是温度，此前我们测试过致态TiPlus7100 2TB，它在高负载下温度为59度，其实大部分高端PCIe 4.0 SSD也都差不多是60度左右。
致态TiPlus7100 4TB在长时间7400MB/s读取、6000MB/s下写入的状态下，最高温度仅仅只有45度，这是目前为止，温度表现最好的PCIe 4.0 SSD。
另外一个就是大家非常关注的缓外写入速度。
我们先用AIDA64 Disk Benchmark Linear Write进行了残酷的全盘写入测试，致态TiPlus7100 4TB的表现非常稳定，以近900MB/s的写入速度结束了测试，并且在将近2个小时的持续写入过程中，整体表现也稳如一条直线。
接着我们进行了全盘拷贝测试，向这块拥有3.72TB可用容量的SSD中一次性拷贝进去3.63TB数据。
这个测试最让我们惊讶的地方是，致态TiPlus7100 4TB竟然以2.3GB/s的写入速度完成了最后的测试，并且整个写入过程都非常稳定。
相比此前我们测试过的其他SSD，在进行全盘拷贝时，写入速度通常低于1000MB/s，就算是“高贵”的三星980 Pro，也只有1.8GB/s。
也就是说，在缓外写入性能方面，致态TiPlus7100 4TB是当今表现最好的消费级PCIe 4.0 SSD，至少也是之一。
至于顺序读取速度，致态TiPlus7100 4TB能达到7430MB/s，这同样也是PCIe 4.0 SSD中的顶级表现。
最后，将致态TiPlus7100 4TB和其他一线大厂的竞品放在一起，尤其是对比海外品牌，无论是产品表现力，还是品牌属性，长江存储都值得我们每个人去大力支持！
]]></content>
  </entry>
  
  <entry>
    <title>VAST Data新一轮1.18亿美元融资</title>
    <url>/post/news/Vast-Data-founder-on-new-118M-us-dollar-funding.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Vast Data</tag>
    </tags>
    <content type="html"><![CDATA[对于科技行业最大的“独角兽”之一，总部位于纽约的VAST Data，本周标志着一个重要时刻的到来。这家公司开发了一种将存储、数据库和计算融合在一起，以支持 AI  和GPU加速应用的平台。它刚刚宣布完成E轮融资，不仅为其带来额外的1.18亿美元现金，还将其估值几乎提升了两倍，达到91亿美元。
经过这轮融资，VAST Data现在在银行账户中坐拥3亿美元。不过，该公司已经实现了正现金流收益，所以这些资金目前只是闲置在银行中赚取利息，公司联合创始人兼营销和产品管理团队执行官Jeff Denworth透露。
VAST Data并未将前三轮融资的所有资金都投入使用，新一轮的E轮融资并不是为了填补资金缺口，而是更多地展示了公司的成熟度，Denworth表示。
考虑到新融资轮的规模相对较小，仅占VAST Data总估值的约1%，他表示，这是一种轻松的方式，可以在显著提高公司估值的同时提升其在市场中的影响力。
他说道：“此类事件具有极其重要的意义，因为实际上在市场上进行‘上一轮’融资的公司为数不多，且看到估值大幅增加。因此，这本质上是我们在任何给定时刻在市场上树立的一个标杆，用以表明‘看看发生了什么。我们得到了一些最好的私募市场投资者的验证。’我们还不是一家上市公司。我们展示业务绩效的最佳方式是通过这些估值事件向世界展示。”
不过，VAST Data其实并不需要那么多的额外外部验证。该公司与Nvidia紧密合作，而Nvidia恰好也是VAST Data的投资者，它们共同推出了一款平台，旨在与围绕大型语言模型的AI平台合作，该平台可以使用非结构化数据。
Denworth表示：“世界上的非结构化数据比数据库和数据仓库中的数据多20倍，而且大多数组织几乎未触及或无法使用这些数据。你不能回到你的文件中提出有意义的新问题，因为当时没有相应的工具。现在，借助GPU和神经网络，你有了能够理解来自自然界的数据的技术。因此，AI是一个重要的触发器。”
 你如何定义VAST Data？  VAST Data正致力于构建适应AI时代的数据平台。在市场上，许多人使用“数据平台”这一术语。我们将其定义为一个集成了统一计算环境、数据库环境和存储环境的软件，可在本地、通用基础设施和云中部署。我们将各种可能进行计算的地方连接在一起。与市场上其他一些数据平台（如Snowflake或Databricks）相比，我们的独特之处在于，我们在这一理念上保持一致，即可以整合一切，使客户能够轻松进行计算。我们的产品真正被设计为优化非结构化数据，或者说文件和对象数据，例如视频、图像、声音等，这些数据不符合企业数据仓库或数据库的规范。
世界上的非结构化数据大约比数据库和数据仓库中的数据多出20倍，这是一个巨大的市场机会。然而，目前大多数组织几乎无法触及或使用这些数据，因为没有相应的工具来提出有意义的新问题。但现在，借助GPU和神经网络技术的支持，我们已经能够理解自然界中的数据了。因此，AI成为了一个重要的触发因素。我们正处于市场变革的中心，客户现在希望整理这些数据，并将其提供给GPU，以便在新的工具上进行处理和分析。这也正是我们设计VAST Data平台的初衷所在。
多年来，我们一直在拓展销售业务，并与众多AI领域的专业人士建立了紧密的合作关系。在ChatGPT崭露头角之前，我们的业务已经展现出良好的增长态势。而如今，一些顶尖的AI云平台和AI驱动的企业纷纷选择部署在我们的系统上，我们因此看到了一个在原本庞大且成熟的业务基础上实现加速增长的机会。这一趋势让许多投资者都感到极度兴奋。
 VAST Data的业务与超融合基础设施有何不同？  这两者在某种程度上存在相似性，但其区别在于超融合基础设施更专注于虚拟化管理，而非数据管理。我们的系统提供了高性能的文件和对象存储系统，与具有事务性和分析性能力的数据库紧密耦合。让我们想象一下，你有一个深度学习应用程序，例如用于查找照片中的猫。你手上有一个图像形式的文件，其中可能包含一只猫，但你能向数据集提出问题的唯一方式是当这些标签和学习内容被注释和编目在一个大规模、高性能的数据库中时。因此，当这两个要素结合在一起时，你不仅拥有原始的数据负载，还有数据库中的上下文层协同工作。这就是事情变得极具趣味之处。在过去的20年里，非结构化数据从未能够被放入数据仓库或类似的存储系统。但如今，我们实际上可以对这些数据进行策划、注释和理解。信息必须存在于某个地方，而这就是VAST数据库发挥作用的地方。
 你提到VAST Data的技术是基于GPU的。你们使用哪家公司的GPU？  我们是一家软件供应商。实际上，我们并不直接向客户提供硬件。不过，Nvidia是我们的投资者，我们与它们合作已有多年。我们目前为一些全球最大的Nvidia计算机提供支持。考虑到Nvidia是市场的领导者，我们的许多合作项目都与它们密切相关。
 VAST Data还有哪些战略投资者？  我们还有其他两家，分别是戴尔科技和高盛。
 你还提到了客户使用多种与VAST Data技术相辅相成的工具，从存储在VAST上的非结构化数据中获取信息。能否请你分享一些具体的工具名称？  市场上，那些正在大规模投资部署AI的企业，目前正进行着一些堪称巨额的投资，例如单个客户投入数万亿美元的资本来解决这个问题。因此，我们将它视为一个堆栈，其中包括在计算层的Nvidia。我们观察到一些真正具有实力的公司，当我说“真正具有实力”时，我指的是估值达到50亿美元的公司，它们在整个年度内都见证了估值的实质性增长。所以，Nvidia显然正在崛起。但如果你关注部署这些系统的平台，有一家名为CoreWeave的云端GPU公司，就在几天前宣布它们的估值实质性增长。事实上，它们是我们的合作伙伴。
所以您拥有云平台。您拥有计算平台。然后您拥有数据层。我们正在与CoreWeave合作，但我们还与一些新兴的AI云合作伙伴展开合作，这些云开始渗透市场，例如总部位于中东的Core42，现在正在进入美国并进行大规模投资。Lambda Labs是另一家崭露头角的AI开发云企业。所以现在您拥有了云层。
最后一块是应用程序部署在这些基础上。主要是生成式AI大型语言模型，但我不能透露任何合作伙伴。我们可以通过我们与CoreWeave合作的工作来谈谈一些案例。我认为您应该这样考虑，我们在市场上看到的大型语言模型公司之一，它们的估值也在不断增长，其中只有两家宣布它们的估值超过50亿美元或更大，并且增值超过一倍。它们是OpenAI和Anthropic。
所以这是一个非常特殊的组织类别。如果您将它们全部堆叠起来，我们看到这是一个新的AI堆栈，您拥有应用程序是语言模型，您拥有计算层是Nvidia，您拥有云层是像CoreWeave这样的公司，然后横跨所有这些不同云上部署的企业的数据层就是VAST Data。
 VAST Data刚刚完成了其第五轮1.18亿美元的融资，使其总融资达到3.81亿美元，估值达到91亿美元。你们计划如何利用这笔新的融资？  什么都不做。它将静待在银行中，并积累利息。这些融资轮次也用于在市场上引起对VAST的关注。我们构建了一个非常独特的业务，专门服务于全球最大的数据消费者，因此我们的平均售价相当高，超过100万美元。通过与最大的客户合作，我们无需建立与那些通常是初创公司的销售和营销团队相同规模的团队，这些初创公司进入市场主要面向市场低端销售。正因为如此，我们成功地建立了一家不消耗风险资本的企业。实际上，我们没有花费我们的B轮、C轮或D轮融资。现在，我们也不预计会花费任何E轮融资的资金。这让我们在银行账户中留有超过3亿美元。
那么，为什么我们会采取这样的行动呢？让我们来算一下。1亿美元加到90亿美元上，结果仅仅稀释了超过1%。实际上，我们的估值几乎增加了三倍。从市场影响力的角度来看，这类事件显得尤为重要，因为实际上在市场上进行“上一轮”融资的公司并不多，而且这些公司中能够看到估值大幅增加的更是少之又少。因此，这实际上是我们为了在市场上树立一个标杆而采取的行动，向外界展示“看看我们取得了什么成果。我们已经得到了顶级私募市场投资者的认可。”我们目前还不是一家上市公司，因此，通过这些估值事件来展示我们的业务绩效是最佳的方式。所以，这笔资金将会继续存放在银行中。
 有关VAST Data进行首次公开募股的传言。有时间表吗？  我可以告诉你的是，我们正在建设一个非常专业的组织。我们现在在全球有700多名员工。我们试图像一家上市公司一样经营业务。我们每个季度都会结账，并制定会计控制措施，以便在选择时能够上市。所以现在只是时间的问题。在合适的时候，我们会采取正确的举措。
 VAST Data已经实现了正的现金流，是这样吗？  是的。这是我们商业模式的一个优势：高平均销售价格和客户重复购买。与主要面向市场低端销售的公司相比，我们没有大量的销售和市场营销人员。对比我们的收入规模，我们的团队相对精简。
 公司实际上是盈利的吗？  目前还没有实现盈利。但我们的现金流为正。
 VAST Data如何与间接渠道合作？  我们是一家以渠道为先的公司，目前拥有近500个全球积极参与的合作伙伴。我们的业务主要通过渠道合作伙伴和服务提供商进行。过去六个月里，我们开始与一些新的系统集成商合作，之前我们与这类公司的合作并不多。在接下来的两个季度中，我们将宣布与几家大型系统集成商的重大合作消息，所以请大家关注我们的动态，期待一些重大新闻。另外，我们与位于圣路易斯的WWT（World Wide Technology）的合作关系也将得到进一步加强，这无疑是我们合作关系的黄金标准。
目前，我们正积极寻找那些认识到AI和深度学习将彻底颠覆市场的合作伙伴。我们明白，工具集和平台都需要进行相应的变革。我们致力于帮助每家企业实现业务和数据的现代化，确保它们在准备好迎接AI的时代时能够从容应对。
]]></content>
  </entry>
  
  <entry>
    <title>2024年数据管理和存储的四大预测</title>
    <url>/post/datacenter/4-predictions-for-data-management-and-storage-in-2024.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Storage</tag>
      <tag>Data Management</tag>
    </tags>
    <content type="html"><![CDATA[鉴于持续上升的通货膨胀、动荡的经济环境以及劳动力市场的诸多挑战，2023年成为充满变数的一年。
尽管如此，根据Gartner的估算，全球IT支出预计将以8%的增长率达到2024年的5.1万亿美元。值得一提的是，许多企业正积极将 人工智能  应用融入业务中，以应对不断增长的数据激增所带来的种种挑战。
预测1：人工智能推动数据管理创新 人工智能的崛起不仅凸显了商业数据的真正价值，而且实际上已经成为了企业将这些数据转化为资产的催化剂。因此，预计在接下来的12个月内，我们将看到企业从以 存储  为中心转变为以数据为中心。智能企业将不再是简单地将数据存储连接起来，而是进入清理数据、实现生产效益、降低成本和推动数据驱动增长的阶段。
为了推动这一转变，数据管理员的角色正在逐渐转变为数据资产管理者，并成为创收的关键角色。传统的资产管理者一直以来为企业所熟知和赞誉，并享受着诱人的薪酬待遇。然而，这种认可直到现在才开始渗透到传统的数据管理员身上。
企业明白，不能适应数据处理的变革将导致失去竞争优势。因此，随着数据资产管理者角色的演变，数据管理员在市场上的影响力将迅速扩大。我们预计在未来五年内，他们的薪酬水平将翻倍。
预测2：非结构化数据增长与数据清理相结合，有望降低数据存储成本、促进IT创新并释放IT预算 根据Gartner的预测，未来三年内非结构化数据将激增三倍，这令企业感到忧心忡忡。然而，通过将增长率降低到一倍，我们仍然有望实现积极的结果。
随着企业发现维持不断增长的数据需求所需的预算越来越困难（2023年IT支出预计仅增长5.1%），领导层开始意识到仅仅扩大存储基础设施并不是解决问题的有效对策。预计到2024年，减少不必要的数据，例如重复的文件，将成为应对指数级数据增长的关键解决方案。
这一目标可以通过两个核心过程来实现：数据整合和数据清理。这意味着要消除重复和陈旧的数据，并更好地理解专有数据及其使用方式。云迁移通常是企业进行数据整合的主要驱动力。如果没有进行数据整合和清理，非结构化数据的存储成本将迅速上升。过时的传统存储厂商希望保持数据不变，因为他们认为使用的数据越多越好，这有助于推动更多的基础设施销售。
通过数据整合和清理，将非结构化数据的增长控制在一倍以内，企业将不再被迫将原本用于产生收入的IT预算重新分配，而是能够释放这些资金用于推动IT创新。
预测3：在充满挑战的经济环境中，负责任的增长将成为科技行业的关键 尽管面临着宏观经济上的诸多挑战，科技行业依然展现出耀眼的光芒。在人工智能创新的推动下，这是我们在技术领域经历的第三次主要浪潮，该行业目前正处于引领我们迈向更有效未来的有利位置。
然而，在人工智能竞赛的激烈背景下，技术公司为了获取竞争优势，急于大规模扩张至新市场。在某些情况下，这种扩张是在不充分考虑盈利能力的情况下进行的。在过去的12个月里，这些公司一直在努力纠正这些错误，以平衡财务状况，这导致了大规模的裁员——而这一趋势预计将继续。
到2024年，市场上取得成功的企业将是那些已经并将继续将负责任的增长视为首要任务的企业。这些公司将能够吸引并留住市场上不断增长的人才，领先于竞争对手，并持续乘着科技行业持续的势头前进。
预测4：2024年开启全新的数据共享时代 随着企业日益认识到专有非结构化数据在推动公司发展中的关键作用，以及这些数据所蕴含的巨大潜力，2024年将迎来一个崭新的数据共享时代。对数据的深入理解将为企业带来前所未有的机遇，同时形成独特的竞争优势。以能源行业为例，非结构化文件数据一直是该行业的核心资源，其规模已超过PB级，需要在多个国家和区域的用户之间进行高效的共享、访问和保护。对于上游运营商，全球数据共享意味着地质学家可以在更短的时间内更准确地做出钻井决策。能源公司需要采用能够使他们更快、更准确地做出关于在全球哪里配置资源的战略决策的技术，利用比过去更大的资源池。
展望2024年，我们将迈出勇敢的一步，共享数据和经验，推动整个行业的发展。为了建立健康的生态系统，我们必须付出更多的努力将数据引入其中，并通过集中管理的方式与多个行业特定的应用程序和SaaS平台共享数据，同时遵守当地的法规和治理，帮助组织发现新趋势，并将这些趋势更迅速地应用于市场，例如人工智能的突破。之前孤立并锁定在技术基础设施中的数据智能和创新力的价值正在释放，这将改变组织的能力和整体价值。
然而，随着这一变革的到来，我们不能忽视勒索病毒带来的挑战，这仍然是所有企业普遍面临的威胁。在接下来的12个月里，勒索病毒威胁的增加将给提供全球有效数据保护和恢复策略和方法的组织带来更大的压力，使其有必要明智地选择他们的技术合作伙伴。
]]></content>
  </entry>
  
  <entry>
    <title>锐龙8040官宣！AMD首创的AI PC性能飙升60%</title>
    <url>/post/news/ryzen-8040-official-announcement-AMD-first-AI-PC.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>AMD</tag>
      <tag>Ryzen 8040</tag>
      <tag>AI PC</tag>
    </tags>
    <content type="html"><![CDATA[ AMD   2023年初发布的锐龙7040系列处理器，不但为笔记本带来了迄今最先进的4nm制造工艺、Zen4 CPU架构、RDNA3 GPU架构，还第一次为x86处理器加入了独立的NPU AI  引擎硬件单元。
这就是Ryzen AI，也创造了AI PC这一全新概念和应用。
北京时间12月7日凌晨，美国加州圣何塞，AMD Advancing AI大会上，AMD一方面公布了AI PC的最新应用进展，另一方面公布了下一代锐龙8040系列、处理器，AI PC正在进化到新的高度。
AMD Ryzen AI引擎基于全新设计的XDNA 架构，可以脱离网络和云端，在本地执行AI工作负载，进而降低延迟、保护隐私。
当然，它也可以在端-云混合场景中运行，在云服务器和本地笔记本之间分配任务、协同加速。
目前，基于锐龙7040系列处理器的笔记本产品已经有几十款，出货量数百万台，涵盖宏碁、华硕、戴尔、联想、惠普、雷蛇、小米等各大合作品牌。
Ryzen AI的应用也不断拓展，合作伙伴包括Adobe、微软、Avid、Bori SFX、OBS Studio、Topaz Labs、Zoom、Cyberlink、XSplit VCam、Luminar、Audacity、Arkrunr、Blackmagic Design、Capcut，以及国内的字节跳动、爱奇艺，等等。
Ryzen AI驱动的加速功能目前也已有100多个，尤其是在Adobe Photoshop、Premiere Pro、After Effect、Lightroom等创意设计软件中，大量的日常操作都可以从中获得极大的效率提升。
以上这些，都是在轻薄笔记本上第一次实现，无论日常娱乐、办公还是内容创作，都可以获得成本的效率提升。
如今，AMD又全新打造了Ryzen AI软件平台，支持TensorFlow、PyTorch、ONNX等眼下最为火热的AI模型。
开发者可以藉此充分挖掘Ryzen AI引擎的性能，更轻松地部署生成式等AI应用和加速。
为了鼓励开发者创新，AMD还发起了首届Pervasive AI开发者挑战赛，为数据中心、工作站、笔记本电脑、游戏、机器人等领域打造创新的AI应用。
其中生成式AI竞赛基于Radeon Pro W7900专业显卡或者Instinct MI210加速器，机器人AI基于Kria KR620机器人套件，PC AI就是基于锐龙7040系列处理器，每个类别的最高奖金都有1万美元，总奖金达15.5万美元。
面向未来，AMD今天正式宣布了下一代移动处理器锐龙8040系列，开发代号Hawk Point，在锐龙7040系列(开发代号Pheonix)的基础上继续进化。
锐龙8040系列仍然基于Zen4 CPU架构、RDNA3 GPU架构、XDNA NPU架构，通过进一步挖掘潜力，尤其是提升各部分的频率和效率，带来更上一层的AI性能。
其中，NPU AI性能算力从10TOPS提升到16TOPS，幅度达到了惊人的60％，也使得整体算力从33TOPS增加到39TOPS。
比如Llama 2大语言模型，性能可提升最多40％，再比如视觉模型方面，性能也可提升最多40％。
当然基础性能也有不小的进步，官方数据称多线程性能提升最多10％，游戏性能提升最多80％，内容创作性能提升最多40％。
以上是锐龙8040系列的完整型号、规格表，和现有锐龙7040系列有对应关系但并非一一对应，而针对国内市场应该还有H系列。
锐龙9 8945HS、锐龙7 8845HS、锐龙5 8645HS是新增加的序列，基础规格分别对应锐龙9 7940HS、锐龙7 7840HS、锐龙5 7640HS，CPU核心数量与加速频率、GPU单元数量、热设计功耗范围等都保持一致。
官方宣称，顶级型号锐龙9 8945HS相比竞品酷睿i9-13900H，视频编辑性能领先最多64％、3D渲染性能领先最多37％，游戏性能领先最多77％。
锐龙7 8840HS、锐龙5 8640HS分别对应现在的锐龙7 7840HS、锐龙5 7640HS，热设计功耗范围从35-54W收缩到20-30W。
随着功耗范围的降低，核心频率自然不可避免地要更低一些，其中锐龙7 8840HS 3.3-5.1GHz，基础频率降低500MHz，锐龙5 8640HS 3.5-4.9GHz，分别降低800MHz、100MHz。
这样一来，8045HS、8040HS系列就在性能上拉开了档次，方便用于不同档次和价位的笔记本产品。
锐龙7 8840U、锐龙5 8640U、锐龙5 8540U、锐龙3 8440U分别对应现在的锐龙7 7840U、锐龙5 7640U、锐龙5 7540U、锐龙3 8440U，基础规格不变，但是热设计功耗从固定的28W扩展到了15-30W的范围，更加灵活。
需要注意的是，锐龙5 8540U、锐龙3 8440U应该是继续采用小号核心，Zen4、Zen4c混合架构，并没有集成NPU单元。
锐龙8040系列已经出货，相关笔记本产品将在2024年第一季度陆续上市，OEM品牌包括宏碁、华硕、戴尔、惠普、联想、雷蛇。
更激动人心的则要等到再下一代的“Strix Point” ，2024年内登场，将会升级到XDNA 2架构的新一代NPU，号称生成式AI性能猛增超过3倍。
此前有消息称，Strix Point还将升级Zen5 CPU架构、RDNA3.5 GPU架构，预计2024年中左右就会发布。
]]></content>
  </entry>
  
  <entry>
    <title>NVIDIA 收到商务部长警告, 勿为中国重新设计芯片</title>
    <url>/post/news/NVIDIA-receives-warning-from-Commerce-Secretary.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>NVIDIA</tag>
      <tag>GPU</tag>
    </tags>
    <content type="html"><![CDATA[虽然美国禁止了 NVIDIA 一些 AI   芯片出口中国，不过 NVIDIA 正试图看能否不放弃中国这块大饼，推出一些管制边缘能符合条件的卡，如 HGX H20 或将要推出的特规 RTX 4090D。
不过最近美国商务部长 Gina Raimondo 对 NVIDIA 发出了警告，声明任何之前禁止的芯片产品即便重新设计仍受到限制，虽然没有表明涉及那些产品，但 NVIDIA RTX 系列都有 Tensor Core ，这对技术意义上而言，就是能为 AI 人工智能工作负载提供一定程度的加速。
就早前2022年第一波制裁之后，NVIDIA 在中国被限制了 H100 / A100 HPC 加速器出口，不过随后 NVIDIA 推出了 H800 / A800 作为替代方案，但随后也受到了禁止，而今年则是禁止 HGX H20 、L20、L2 芯片，以及包括 RTX 4090 GPU。
NVIDIA RTX 4090 虽然是游戏卡，但它仍是相当好的 AI 加速器，可以有 4800 TOPS 的运算能力，也因如此这张卡在中国是被禁止，而 NVIDIA 则想要推 RTX 4090D 来符合禁令规范，但目前并不确定规格如何。
]]></content>
  </entry>
  
  <entry>
    <title>AMD PC全面出击，英特尔危险了</title>
    <url>/post/news/amd-pc-attacks-in-full-force.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>AMD</tag>
      <tag>Intel</tag>
    </tags>
    <content type="html"><![CDATA[ AMD  最近两年在消费级PC市场的日子并不好过，相比起往年的驰骋风云，如今不管是市占率还是市场声量都小了很多，从CPU到 GPU  ，AMD在两个消费级业务上都面临严峻挑战。
虽然 服务器  市场的份额有所上升，但是消费级市场也是一个大蛋糕，更不可能拱手让人。但是在GPU领域，目前AMD与英伟达的差距短时间内是无法跨越的（不仅仅是硬件性能，软件生态差距更大），对比之下CPU领域的差距则小很多，是AMD最有机会的方向。
为了能够从英特尔手中抢到更多的份额，AMD一直在对自己的CPU产品线做细分，用不同规格、价格的CPU去满足不同的PC消费群体需求，比如近期热门的掌上PC，清一色使用的都是AMD处理器，独占整个市场。
而在近日的一则消息中曝光了AMD将会在明年初发布多款全新的处理器，而且分别对应三个产品线，加上刚刚发布的七款线程撕裂者系列处理器，一股硝烟味正在CPU市场中蔓延。
分工明确，AMD全面出击 2023年11月，AMD久违地更新了旗下的消费级服务器处理器系列，也就是我们常说的Threadripper（线程撕裂者）产品线，以最高96核192线程的恐怖配置，为消费级市场带来一些“小小”的震撼。
虽然英特尔也提供部分消费级的服务器产品，但是从性能来看，目前与AMD的Threadripper系列有着不少差距，不过，Threadripper 7000系列的起售价就高达15999元，显然不是普通用户需要关注的产品。
至于将在明年初发布的多款处理器，从型号来看基本是瞄准的中低端市场，精准填充英特尔的弱势市场。目前曝光的型号分别为：Ryzen 3 8300G、Ryzen 5 8500G、Ryzen 5 8600G、Ryzen 7 8700G、Ryzen 5 5500GT、Ryzen 5 5600GT、Ryzen 7 5700X3D、Ryzen 7 5700 NPU。
其中，Ryzen 8000G系列还细分出了多个型号，因为各个型号间差距不大且过于重复，所以我们只看标准版就好，有兴趣深入了解的可以自行查看下方表格。
即使不算上零零碎碎的细分型号，本次曝光的新处理器型号也多达8个，用一句“U海战术”来形容都不为过，不过其中除了Ryzen 8000G系列是新的架构外，其余基本是旧型号的再就业，只不过在结构、性能上与以前的型号有所不同。
先来看看Ryzen 8000G系列，从规格来看，基本上与过去一样是Ryzen 7000系列的核显版本， 将会根据规格、主频分别供应桌面和移动PC市场。单看主频提升并不小，以Ryzen 7系列为例，Ryzen 7 5700G的最高主频为4.6GHz，而Ryzen 7 8700G的主频则提升到了5.1GHz，基础主频也有0.4GHz的提升。
此外，核显从织女星架构升级到RDNA3架构，拥有12个CU单元，是Ryzen 7 5700G的1.5倍，考虑到RDNA3架构的性能提升，加上暴涨的CU单元，此前Ryzen 8000G系列核显性能将暴涨的传闻总算是得到了证实。
单从硬件规格来推测，Ryzen 7 8700G的核显性能应该不输目前主流的入门级独显，而且将支持光追功能，如果只是打算玩一些独立游戏或是网游，甚至可以不需要独立显卡就能流畅游玩。至于剩余的几个型号，目前还没有曝光确切的核显规格，所以不好估算，不过以惯例来看，应该会分别采用8CU、4CU的RDNA3核心。
除了Ryzen 8000G系列，剩下的几个型号也十分值得关注，首先是Ryzen 5 5600GT/5700GT，这是第一次出现以GT为后缀的Ryzen系列处理器，目前仅有型号曝光，不过根据命名规范来看，可能会是Ryzen 5 5600G/5500G的超频升级版。
而Ryzen 7 5700X3D倒是让人有点意外，或许是为了进一步填补价位区间所开发的一款产品，具体参数目前也不明确。最后的Ryzen 7 5700 NPU，同样是一个全新的后缀，NPU一般代指处理器中为AI运算所准备的特殊核心，如果型号属实，那么可能是AMD为PC AI功能所设计的一款试水产品。
AMD的“U海战术”已经起步了，至于效果如何，还得等明年见分晓。
你的PC，如何买“芯”？ AMD一口气曝光这么多款处理器，加上英特尔前段时间发布的14代酷睿处理器，一些网友估计都被绕晕了，如果想装机，该如何从一堆处理器中挑出合适自己的型号呢？
实际上，虽然型号众多，但是处理器的分工还是十分明确的，不同的用途、预算，都有大致的选择区间。对于多数用户而言，选择英特尔和AMD都不会让你的体验有所不同，唯一需要考虑的就是性能和价格，简单来说就是“性价比”。
如果你的需求是日常娱乐和上网，不玩3A大作什么的，那么英特尔的酷睿i5系列和AMD的Ryzen 5000G系列都是不错的选择。当然，即将发布的Ryzen 8000G则更好，但是相对的售价也会高很多，如果你对游戏的要求不高，那么Ryzen 5000G系列完全足够满足你的需求。
再来看看游戏方面，对于游戏玩家来说，主频越高，体验就越好，预算无上限的话，英特尔的i9-14900K就是你最好的选择。除此之外，AMD的Ryzen 7 7800X3D和Ryzen 9 7900X3D也是不错的选择，虽然主频低于i9-14900K，但是超大的缓存非常适合游戏，实际帧数与i9-14900K打得有来有回，若是预算不太够，Ryzen 5 5800X3D也是不错的选择。
游戏之外，如果你的主要需求是生产力工具，而且要求性能越高越好，那么下血本买Threadripper 7000系列或许是不错的选择，就目前的消费级处理器中，还没有哪个处理器可以与Threadripper系列在多核性能及周边功能配置上掰手腕的。
实际上，Threadripper系列就是AMD的服务器产品线下放后的产物，拥有多种企业级的功能配置，可以让用户用更低的成本轻松搭建服务器级别的系统。当然，多数人估计也不需要如此高的性能，如果单看多核性能，那么i9-14900K（也可以选13900K）、Ryzen7950X等处理器都十分不错。
最后，给大家一个省流版：办公随便挑，i3、R3（Ryzen 3）都可以，游戏就看i5和i9，R5、R7也不错，你要生产力？14代i7/i9和R9是你最好的朋友，预算够，买最新，预算不够，退一代。
]]></content>
  </entry>
  
  <entry>
    <title>Intel全线涨价！12/13/14代、酷睿Ultra无一例外</title>
    <url>/post/datacenter/amd-processor-will-subvert-SSD-technology.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>AMD</tag>
      <tag>3D V-Cache</tag>
      <tag>SSD</tag>
      <tag>Intel</tag>
      <tag>Optane</tag>
    </tags>
    <content type="html"><![CDATA[当你拥有 AMD 的 3D V-Cache 时，谁还需要英特尔的 Optane？
谁会想到最好的 CPU可以与最好的 SSD相媲美？显然，可以在 AMD 的 Ryzen 3D V-Cache处理器上运行 RAM 磁盘，并实现连续读写速度，甚至可以击败最快的 PCIe   5.0 SSD  。
这个谜团始于我们的冷却专家Albert Thomas分享的一个有趣的屏幕截图，其中 RAM 磁盘在 CrystalDiskMark 中的顺序读取和写入速度分别约为 178 GB/s 和 163 GB/s。值得注意的是，据报道，结果来自运行在 AMD Ryzen 7 7800X3D处理器上的 RAM 磁盘。
起初，人们对这一说法持怀疑态度，因为您需要将 L3 公开为块存储设备才能运行 CrystalDiskMark 基准测试。假定的容量 (508MB) 大于 Ryzen 7 7800X3D 上的 3D V-Cache，后者具有 96MB 的 L3 缓存。然而，似乎有一种合法的方法可以将 3D V-Cache 用于 RAM 磁盘。
Nemez是 X（以前称为 Twitter）上的用户，发现了该方法。这位热心人在二月份分享了让它发挥作用的步骤，但它并没有引起人们的注意。结果甚至比 Thomas 的结果更加引人注目，在上一代 Ryzen 7 5800X3D 上，RAM 磁盘的顺序读取和写入速度分别达到 182 GB/s 和 175 GB/s左右。
该方法基于 OSFMount，这是一个免费软件，允许您创建 RAM 磁盘并挂载不同格式的映像文件。创建采用 FAT32 格式的 RAM 磁盘听起来很简单。
但是，您需要在 CrystalDiskMark 上使用精确的设置才能使其正常工作。根据 Nemez 的说法，您必须将测试值配置为 SEQ 256KB、队列深度为 1、线程数为 16。您还必须将数据填充设置为零而不是随机。由于系统负载的性质，该方法可能在第一次尝试时不起作用，因此您可能需要运行基准测试几次。
看到爱好者发现 AMD 3D V-Cache 的新用途真是令人着迷。虽然性能数据看起来非常出色，但它们仍远未发挥 3D V-Cache 的潜力。例如，第一代3D V-Cache的峰值吞吐量为2 TB/s。AMD 随后将第二代变体的带宽提高到 2.5 TB/s 。
这个实验很酷，但在实际使用中并不实用，因为没有一致的方法来利用 3D V-Cache。该方法并非万无一失，有时需要反复试验。此外，消费者 Ryzen 芯片上的 3D V-Cache 太小，没有什么帮助。例如，旗舰Ryzen 9 7950X3D只有128MB的L3缓存。另一方面，AMD 的 EPYC 处理器（例如具有 1.3GB L3 缓存的Genoa-X）可能是一个有趣的用例。
尽管如此，我们认为 3D V-Cache 和 RAM 磁盘还是有潜力的。这是将老派技术和新技术结合在一起的巧妙方法。SSD 已经让 RAM 磁盘过时了，但也许大量的 3D V-Cache 可以让它们复活。试想一下，如果 AMD 接受这个想法并推出故障安全实施方案，消费者可以通过翻转开关将 3D V-Cache 转变为 RAM 磁盘，那么这种可能性将会有多大。
]]></content>
  </entry>
  
  <entry>
    <title>一个轻量级的嵌入式框架</title>
    <url>/post/mcu/a-lightweight-embedded-program-framework.html</url>
    <categories><category>MCU</category>
    </categories>
    <tags>
      <tag>Embedded</tag>
      <tag>Framework</tag>
    </tags>
    <content type="html"><![CDATA[Zorb Framework是一个基于面向对象的思想来搭建一个轻量级的嵌入式框架。
Zorb简介 Zorb Framework是一个基于面向对象的思想来搭建一个轻量级的嵌入式框架。
搭建Zorb Framework的目的是为在不能运行Linux的芯片上快速开发应用，不用反复造轮子。
Zorb Framework的初步设计功能有：
 时间系统功能zf_time 环形缓冲区功能zf_buffer 列表功能zf_list 状态机功能zf_fsm 事件功能zf_event 定时器功能zf_timer 任务功能zf_task  前6个功能，就可以实现纯事件驱动的程序，基本可以满足中小型嵌入式应用程序开发的需求。加上任务功能，是为了满足部分程序对实时性要求较高的需求。
当然，也可以将前6个功能裁剪出来，然后运行在现有的嵌入式系统上面，这样子也可以满足实时性的需求。
嵌入式环境搭建 采用STM32F429开发板作为硬件运行环境，硬件资源用到串口1和systick，其中串口1提供调试打印功能，systick提供系统时间计数功能。
关于硬件环境的搭建不多说，可以参照开发板提供的例程来搭建，板级初始化完成了调试串口和systick的初始化。
/****************************************************************************** * 描述 ：硬件环境初始化 * 参数 ：无 * 返回 ：无 ******************************************************************************/ void BSP_init(void) { /* 嵌套向量中断控制器组选择 */ NVIC_PriorityGroupConfig(NVIC_PriorityGroup_2); /* 初始化调试串口 */ Debug_USART_init(); /* Systick初始化 */ SystemTick_init(); } /****************************************************************************** * 描述 ：硬件底层程序 * 参数 ：无 * 返回 ：无 ******************************************************************************/ void BSP_process(void) { } 调试输出 开发一个程序，最开始也最重要的是搭建调试的环境，我们采用串口1作为调试输出（printf映射），然后调试信息分为三个等级，后续上位机可以根据不同等级进行高亮提示：
/** ***************************************************************************** * @file zf_debug.h * @author Zorb * @version V1.0.0 * @date 2018-06-28 * @brief 调试输出的头文件 ***************************************************************************** * @history * * 1. Date:2018-06-28 * Author:Zorb * Modification:建立文件 * ***************************************************************************** */ #ifndef __ZF_DEBUG_H__ #define __ZF_DEBUG_H__  #ifdef __cplusplus extern &#34;C&#34; { #endif  #include &#34;stdio.h&#34;#include &#34;stdbool.h&#34; #define LOG_D 0; /* 信息等级：正常 */#define LOG_W 1; /* 信息等级：告警 */#define LOG_E 2; /* 信息等级：错误 */ #define _ZF_DEBUG /* 定义调试功能 */#define ZF_DEBUG_ON true /* 启用调试功能 */ #ifdef _ZF_DEBUG  #if ZF_DEBUG_ON  #define ZF_DEBUG(rank, x...) do \ { \ char code[10] = &#34;[rank=0]&#34;; \ code[6] = &#39;0&#39; + (char)rank; \ if (code[6] != &#39;0&#39;) \ { \ printf(&#34;%s&#34;, code); \ } \ printf(x); \ } while(0)  #else  #define ZF_DEBUG(rank, x...)  #endif /* ZF_DEBUG_ON */#endif /* _ZF_DEBUG */ #ifdef __cplusplus } #endif  #endif /* __ZF_DEBUG_H__ */ /******************************** END OF FILE ********************************/ 实现断言 在开发过程中，在关键地方进行一些断言，可以方便定位bug。
/** ***************************************************************************** * @file zf_assert.h * @author Zorb * @version V1.0.0 * @date 2018-06-28 * @brief 断言的头文件 ***************************************************************************** * @history * * 1. Date:2018-06-28 * Author:Zorb * Modification:建立文件 * ***************************************************************************** */ #ifndef __ZF_ASSERT_H__ #define __ZF_ASSERT_H__  #ifdef __cplusplus extern &#34;C&#34; { #endif  #include &#34;stdint.h&#34; #define _ZF_ASSERT /* 定义断言功能 */#define ZF_ASSERT_ON true /* 启用断言功能 */ #ifdef _ZF_ASSERT  #if ZF_ASSERT_ON  #define ZF_ASSERT(expression_) ((expression_) ?\ (void)0 : ZF_assertHandle((uint8_t *)__FILE__, (int)__LINE__));  #else  #define ZF_ASSERT(expression_)  #endif /* ZF_ASSERT_ON */#endif /* _ZF_ASSERT */ /* 断言产生时的处理 */ void ZF_assertHandle(uint8_t *pFileName, int line); #ifdef __cplusplus } #endif  #endif /* __ZF_ASSERT_H__ */ /******************************** END OF FILE ********************************/ 断言的处理很简单，就是告诉我们在哪个文件哪一行出错就可以，实现如下
/** ***************************************************************************** * @file zf_assert.c * @author Zorb * @version V1.0.0 * @date 2018-06-28 * @brief 断言的实现 ***************************************************************************** * @history * * 1. Date:2018-06-28 * Author:Zorb * Modification:建立文件 * ***************************************************************************** */ #include &#34;zf_assert.h&#34;#include &#34;zf_debug.h&#34; /****************************************************************************** * 描述 ：断言产生时的处理 * 参数 ：(in)-pFileName 文件名 * (in)-line 行数 * 返回 ：无 ******************************************************************************/ void ZF_assertHandle(uint8_t *pFileName, int line) { ZF_DEBUG(LOG_E, &#34;file:%s line:%d:asserted\r\n&#34;, pFileName, line); while (1); } /******************************** END OF FILE ********************************/ 建立时间系统 为了减少框架对资源的消耗，所以初步设定框架的最小时间周期为1ms，因此我们需要设置systick的定时周期为1ms，然后每次进入中断为我们的框架计数即可。
/****************************************************************************** * 描述 ：SysTick中断服务程序 * 参数 ：无 * 返回 ：无 ******************************************************************************/ void SysTick_Handler(void) { /* 为zorb framework提供计时 */ ZF_timeTick(); } 现在时间系统提供的功能比较基础，只有系统滴答计数和系统死等待延时，后面我们开发定时器功能和任务功能的时候会重新扩展时间系统。
/** ***************************************************************************** * @file zf_time.h * @author Zorb * @version V1.0.0 * @date 2018-06-28 * @brief 系统时间的头文件 ***************************************************************************** * @history * * 1. Date:2018-06-28 * Author:Zorb * Modification:建立文件 * ***************************************************************************** */ #ifndef __ZF_TIME_H__ #define __ZF_TIME_H__  #ifdef __cplusplus extern &#34;C&#34; { #endif  #include &#34;stdbool.h&#34;#include &#34;stdint.h&#34; /* 系统滴答周期(ms) */ #define ZF_TICK_PERIOD 1  /* 获取系统滴答数 */ #define ZF_SYSTICK() ZF_getSystemTick()  /* 获取系统时间(ms) */ #define ZF_SYSTIME_MS() ZF_getSystemTimeMS()  /* 系统延时(ms) */ #define ZF_DELAY_MS(ms_) do \ { \ if (ms_ % ZF_TICK_PERIOD) \ { \ ZF_delayTick((ms_ / ZF_TICK_PERIOD) + 1); \ } \ else \ { \ ZF_delayTick(ms_ / ZF_TICK_PERIOD); \ } \ } while(0)  /* 获取系统滴答数 */ uint32_t ZF_getSystemTick(void); /* 获取系统时间(ms) */ uint32_t ZF_getSystemTimeMS(void); /* 系统延时 */ void ZF_delayTick(uint32_t tick); /* 系统滴答程序(需挂在硬件的时间中断里边) */ void ZF_timeTick (void); #ifdef __cplusplus } #endif  #endif /* __ZF_TIME_H__ */ /******************************** END OF FILE ********************************/ 最后 本篇实现的功能比较基础，但是整个框架开发的根基，后面所有扩展的功能都需要在此环境下进行开发。
搭建良好的调试输出环境，可以帮我们快速定位bug的所在，从而提高开发效率。
Github 地址 https://github.com/54zorb/Zorb-Framework ]]></content>
  </entry>
  
  <entry>
    <title>单片机数字滤波算法</title>
    <url>/post/mcu/microcontroller-digital-filtering-algorithm.html</url>
    <categories><category>MCU</category>
    </categories>
    <tags>
      <tag>Digital Filter</tag>
      <tag>Algorithm</tag>
    </tags>
    <content type="html"><![CDATA[单片机主要作用是控制外围的器件，并实现一定的通信和数据处理。但在某些特定场合，不可避免地要用到数学运算，尽管单片机并不擅长实现算法和进行复杂的运算。下面主要是介绍如何用单片机实现数字滤波。
在单片机进行数据采集时，会遇到数据的随机误差，随机误差是由随机干扰引起的，其特点是在相同条件下测量同一量时，其大小和符号会现无规则的变化而无法预测，但多次测量的结果符合统计规律。为克服随机干扰引起的误差，硬件上可采用滤波技术，软件上可采用软件算法实现数字滤波。滤波算法往往是系统测控算法的一个重要组成部分，实时性很强。
采用数字滤波算法克服随机干扰的误差具有以下优点：
 数字滤波无需其他的硬件成本，只用一个计算过程，可靠性高，不存在阻抗匹配问题。尤其是数字滤波可以对频率很低的信号进行滤波，这是模拟滤波器做不到的。 数字滤波使用软件算法实现，多输入通道可共用一个滤波程序，降低系统开支。 只要适当改变滤波器的滤波程序或运算，就能方便地改变其滤波特性，这对于滤除低频干扰和随机信号会有较大的效果。 在单片机系统中常用的滤波算法有限幅滤波法、中值滤波法、算术平均滤波法、加权平均滤波法、滑动平均滤波等。  限幅滤波算法 该运算的过程中将两次相邻的采样相减，求出其增量，然后将增量的绝对值，与两次采样允许的最大差值A进行比较。A的大小由被测对象的具体情况而定，如果小于或等于允许的最大差值，则本次采样有效;否则取上次采样值作为本次数据的样本。
算法的程序代码如下：
/* A值根据实际调，Value有效值，new_Value当前采样值，程序返回有效的实际值 */ #define A 10 char Value; char filter() { char new_Value; new_Value = get_ad(); //获取采样值  if (abs(new_Value - Value) &gt; A) return Value; if ((datanew - data) &gt; A || (data - datanew &gt; A)) //abs()取绝对值函数  return new_Value; } 说明：限幅滤波法主要用于处理变化较为缓慢的数据，如温度、物体的位置等。使用时，关键要选取合适的门限制A。通常这可由经验数据获得，必要时可通过实验得到。
中值滤波算法 该运算的过程是对某一参数连续采样N次(N一般为奇数)，然后把N次采样的值按从小到大排列，再取中间值作为本次采样值，整个过程实际上是一个序列排序的过程。
算法的程序代码如下：
#define N 11 char filter() { char value_buf[N]; char count, i, j, temp; for (count = 0; count &lt; N; count++) //获取采样值  { value_buf[count] = get_ad(); delay(); } for (i = 0; i &lt; n - 1; i++) { for (j = i + 1; j &lt; n - 1; j++) { if (value_buf[i] &gt; value_buf[i + 1]) { temp = value_buf[i]; value_buf[i] = value_buf[i + 1]; value_buf[i + 1] = temp; } } } return value_buf[(N - 1) / 2]; } 说明：中值滤波比较适用于去掉由偶然因素引起的波动和采样器不稳定而引起的脉动干扰。若被测量值变化比较慢，采用中值滤波法效果会比较好，但如果数据变化比较快，则不宜采用此方法。
算术平均滤波算法 该算法的基本原理很简单，就是连续取N次采样值后进行算术平均。
算法的程序代码如下：
#define N 12 char filter() { int sum = 0; for (count = 0; count &lt; N; count++) sum += get_ad(); return (char)(sum / N); } 说明：算术平均滤波算法适用于对具有随机干扰的信号进行滤波。这种信号的特点是有一个平均值，信号在某一数值附近上下波动。信号的平均平滑程度完全到决于N值。当N较大时，平滑度高，灵敏度低;当N较小时，平滑度低，但灵敏度高。为了方便求平均值，N一般取4、8、16、32之类的2的整数幂，以便在程序中用移位操作来代替除法。
加权平均滤波算法 由于前面所说的“算术平均滤波算法”存在平滑度和灵敏度之间的矛盾。为了协调平滑度和灵敏度之间的关系，可采用加权平均滤波。它的原理是对连续N次采样值分别乘上不同的加权系数之后再求累加，加权系数一般先小后大，以突出后面若干采样的效果，加强系统对参数变化趋势的认识。各个加权系数均小于1的小数，且满足总和等于1的结束条件。这样加权运算之后的累加和即为有效采样值。设D为N个采样值的加权平均值：XN-i为第N-i次采样值;N为采样次数;Ci为加权系数。加权系数Ci体现了各种采样值在平均值中所占的比例。一般来说采样次数越靠后，取的比例越大，这样可增加新采样在平均值中所占的比重。加权平均值滤波法可突出一部分信号抵制另一部分信号，以提高采样值变化的灵敏度。
样例程序代码如下：
/* coe数组为加权系数表 */ #define N 12 char code coe[N] = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 }; char code sum_coe = { 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 11 + 12 }; char filter() { char count; char value_buf[N]; int sum = 0; for (count = 0; count &lt; N; count++) { value_buf[count] = get_ad(); } for (count = 0; count &lt; N; count++) sum += value_buf[count] * coe[count]; return (char)(sum / sum_coe); } 滑动平均滤波算法 以上介绍和各种平均滤波算法有一个共同点，即每获取一个有效采样值必须连续进行若干次采样，当采速度慢时，系统的实时得不到保证。这里介绍的滑动平均滤波算法只采样一次，将一次采样值和过去的若干次采样值一起求平均，得到的有效采样值即可投入使用。如果取N个采样值求平均，存储区中必须开辟N个数据的暂存区。每新采集一个数据便存入暂存区中，同时去掉一个最老数据，保存这N个数据始终是最新更新的数据。采用环型队列结构可以方便地实现这种数据存放方式。
程序代码如下：
char value_buff[N]; char i = 0; char filter() { char count; int sum = 0; value_buff[i++] = get_data(); if (i == N) i = 0; for (count = 0; count &lt; N; count++) sum = value_buff[count]; return (char)(sum / N); } 低通滤波 将普通硬件RC低通滤波器的微分方程用差分方程来表求，变可以采用软件算法来模拟硬件滤波的功能，经推导，低通滤波算法如下：
Yn=a* Xn+(1-a) *Yn-1
式中
 Xn —— 本次采样值 Yn-1 —— 上次的滤波输出值 a —— 滤波系数，其值通常远小于1 Yn —— 本次滤波的输出值  由上式可以看出，本次滤波的输出值主要取决于上次滤波的输出值(注意不是上次的采样值，这和加权平均滤波是有本质区别的)，本次采样值对滤波输出的贡献是比较小的，但多少有些修正作用，这种算法便模拟了具体有教大惯性的低通滤波器功能。滤波算法的截止频率可用以下式计算：
fL=a/2Pit pi为圆周率3.14…
式中
 a —— 滤波系数; t —— 采样间隔时间;  例如：当t=0.5s(即每秒2次)，a=1/32时;
fL=(1/32)/(23.140.5)=0.01Hz
当目标参数为变化很慢的物理量时，这是很有效的。另外一方面，它不能滤除高于1/2采样频率的干搅信号，本例中采样频率为2Hz，故对1Hz以上的干搅信号应采用其他方式滤除，低通滤波算法程序于加权平均滤波相似，但加权系数只有两个：a和1-a。为计算方便，a取一整数，1-a用256-a，来代替，计算结果舍去最低字节即可，因为只有两项，a和1-a，均以立即数的形式编入程序中，不另外设表格。虽然采样值为单元字节(8位A/D)。为保证运算精度，滤波输出值用双字节表示，其中一个字节整数，一字节小数，否则有可能因为每次舍去尾数而使输出不会变化。
设Yn-1存放在30H(整数)和31H(小数)两单元中，Yn存放在32H(整数)和33H(小数)中。
小结一下吧：数字滤波器，说白了，就是多次采样求平均值的一个过程，精确一点的，就是再顺序排列，去掉首位再求平均值，就是求平均数！
]]></content>
  </entry>
  
  <entry>
    <title>美芯片巨头全球大裁员,最新人数近3000!内部称或多达2万</title>
    <url>/post/news/us-chip-giant-lays-off-employees-worldwide.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Broadcom</tag>
      <tag>VMWare</tag>
    </tags>
    <content type="html"><![CDATA[集微网消息，博通公司斥资690亿美元收购VMware后进行裁员的官方数字上升至2838人，数据来自博通在美国多个州提交的WARN（工人调整和再培训通知）通知。
弗吉尼亚州就业委员会发布WARN通知称，从1月26日开始，弗吉尼亚州雷斯顿市的一处地点将有116人失业。合并前，雷斯顿办事处属于VMware。
得克萨斯州劳动力委员会证实，博通将解雇与VMware得州奥斯汀办事处相关的577名员工。随后，来自马萨诸塞州的WARN通知显示博通预计将裁员150名。
随着州机构逐渐发布来自博通和VMware的WARN通知，来自得克萨斯州、弗吉尼亚州和马萨诸塞州的843名用户加入了该不断增长的数字。值得注意的是，博通正在加利福尼亚州帕洛阿托的VMware总部裁员1267人。博通还将在华盛顿州裁员158人，在纽约州裁员169人，在佐治亚州裁员217人，在科罗拉多州裁员184人。这些数字加起来已达2838人。
然而，这些数字并不完整，因为州WARN法案通常只要求公司在有100名或更多员工受到影响时才进行备案。
有VMware内部人士表示，裁员总数将在1万到2万之间。许多裁员是交错进行的，时间长达9个月。
VMware的3.8万多名员工遍布全球，博通收购VMware交易后的裁员也将带来全球影响。
VMware位于爱尔兰科克办事处的员工收到了一封电子邮件，称该公司已启动协商流程。VMware在爱尔兰雇佣了1000多名员工，科克办事处是VMware的全球第三大办事处。就在博通正式与VMware讨论收购事宜之前，该公司在都柏林还设立了一个研发中心。
据报道，该协商流程将持续六周，并“确定将在何处进行变更和重组”。一位不愿透露姓名的员工表示：“事实上，协商期将持续到圣诞节，这确实会稍微减轻一些影响，但对于有家人和孩子的员工来说，面临着压力更大的情况。”
据悉，博通遵循其通常的模式，在收购后取消支持职位以削减成本。博通CEO Hock Tan通过一系列交易建立了半导体行业最大的公司之一，这些交易越来越侧重于软件。
Hock Tan的策略是找出那些拥有强大市场份额但增长前景不佳的公司，这提高了利润并赢得投资者支持。收购这些公司后，Hock Tan整合销售、人力资源和其他支持组织等业务，以削减成本，同时努力留住工程人才。
VMware已成为博通软件运营的核心。该公司之前通过收购CA Technologies和赛门铁克企业安全业务建立了该部门。
VMware成立于1998年，是虚拟化程序的先驱，该程序允许软件更有效地利用服务器计算机。在交易完成之前，该公司拥有约3.83万名员工。
博通的其他业务包括制造苹果和Alphabet旗下谷歌等公司使用的芯片，该公司将于下周公布财报。
]]></content>
  </entry>
  
  <entry>
    <title>NVIDIA为中国打造特供RTX 4090D</title>
    <url>/post/news/NVIDIA-creates-special-RTX-4090D-for-China.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>AI</tag>
      <tag>NVIDIA</tag>
      <tag>GPU</tag>
      <tag>4090D</tag>
    </tags>
    <content type="html"><![CDATA[RTX 4090在国内遭到禁售， NVIDIA  正在规划特供版本RTX 4090 D，性能降级以满足出口管制——D代表Dragon，因为明年是中国龙年。
根据曝料，RTX 4090D将采用不同的GPU核心编号AD102-250，相比于RTX 4090上的AD102-300/301在数字上就低了一个等级，按照命名规则规格也会降低。
但是目前，RTX 4090D的真正规格仍然没有明确说法，猜测[CUDA(https://www.gaitpu.com/tag/cuda)核心数量会少于16384个，显存有望保持在24GB GDDR6X，反正计算性能不会高于4800TOPS——这是被禁售的底线。
当然，另一方面，它肯定得明显高于RTX 4080 SUPER，后者采用AD103-400核心，10240个CUDA核心，16GB GDDR6X显存。
另外，NVIDIA很可能会强行限制RTX 4090D的超频，至少不会公然开放太多，不然一超性能就上去了，还是违规。
价格方面还没有确定，但是不排除继续卖12999元的可能，那就太无语了。
由于美国是按照算力密度来禁售产品的，AMD RX 7900系列显卡是否会步其后尘也让人担心，尤其是日前戴尔发出通知，声称不会继续在中国销售RX 7900 XTX/XT、Pro W7900，更加剧了这种忧虑。
但是据快科技最新了解，AMD从未向合作伙伴发布过RX 7900系列在中国停售的通知，也从未接到美国有关部门的通知，所谓禁售都是误读。
当然，不排除以后情况可能会发生变化，就像RTX 4090开始也说能继续销售，结果还是被禁了，但至少就目前而言，RX 7900系列是安全的。
至于戴尔为何会发布那样的通知，无从知晓，可能是为了自保、以防万一吧，但无论如何，都是其自己的行为。
如今的局面下，RTX 4090系列被禁，RX 7900系列就成了最佳替补，无论玩游戏还是做AI，不是最好的，但也能满足基本需求。
特别是AMD ROCm 5.7开发平台已经完整支持RX 7900系列、Pro W7900，Windows Olive也支持AMD显卡，AI和计算应用情况大大改善。
但是，RX 7900系列对于中国市场的供应量一直有限，而其销量最近一路看涨，结果缺货非常严重，价格也有所上涨，预计这种情况将一直持续到明年第一季度。
AMD应该会尽量加大RX 7900系列对中国市场的供应量，但暂时没有明确消息。
]]></content>
  </entry>
  
  <entry>
    <title>x86指令集和ARM指令集的区别</title>
    <url>/post/soc/the-difference-between-Arm-instruction-set-and-X86-instruction-set.html</url>
    <categories><category>SOC</category>
    </categories>
    <tags>
      <tag>x86</tag>
      <tag>Arm</tag>
      <tag>Instruction Set</tag>
    </tags>
    <content type="html"><![CDATA[现代处理器的主要指令集架构（ISA）包括： x86  指令集架构、RISC指令集架构。其中，x86可以划分为x86-32（英特尔）、x86-64（英特尔）、AMD64（AMD）等三种。RISC可以划分为 ARM  、MIPS、Alpha、RISC-V。
现代处理器的指令集架构 现代处理器的主要指令集架构（ISA）包括：x86指令集架构、RISC指令集架构。其中，x86可以划分为x86-32（英特尔）、x86-64（英特尔）、AMD64（AMD）等三种。RISC可以划分为ARM、MIPS、Alpha、RISC-V。
“观其源可以知其流，而因其流亦可溯其源”。如果仅仅像教材一样泛泛而谈，则无法掌握处理器芯片的底层原理。下面，我们首先谈一下x86指令集的问题。
x86指令集 第一个方面，x86指令集为了保持二进制的兼容性，即：上一代芯片的应用程序仍然能运行在下一代芯片中，使之前后系列的芯片成为一个“系列机”，扩展了许多新的指令，导致x86指令集的规模不断膨胀。
第二个方面，x86指令集在设计时，采取了一种强指令的方式，即：一条指令可以完成非常强大的功能。例如，一条指令可以完成内存不同位置的整个运算过程，或将一块数据直接从内存的一个位置移动到另一个位置，而且这种强大的操作是在1个时钟周期内完成的。
在这里，我个人强调CISC指令集的定义，不应该仅仅是指令规模庞大，更重要的是，单条指令的效率较高，也应该是CISC的含义之一。许多教材只强调前者是不严谨的说法。
上图列出了英特尔官方统计的指令数量变化。在将近40年的发展历史中，x86由不足200条指令到今天超过1600条指令。
上图列出了x86指令集的MOV指令种类，其可以跳过一系列底层的微操作，实现一个较为复杂的指令功能。
基于以上的设计理念，x86指令集有以下的优点或缺点。注意，缺点也许是优点，优点也许是缺点，这是马克思辩证唯物主义的思想。
 由于需要兼容以往的历史版本，x86的硬件设计复杂，这也成为了其历史包袱。但是，塞翁失马、焉知非福，二进制的兼容性获得了IBM的强力支持，让英特尔快速占领了PC和台式机市场。 一条指令的功能很强，这降低了对编译器的要求。另外，不需要考虑那么多的底层指令，早期的汇编程序员面对x86编程时也较为轻松。 多条高效率的指令并行，让单个核的性能强大。早期的处理器，单核架构足以满足应用需求。 必须要大量的冗余晶体管，以实现这种CISC复杂指令集设计。冗余的晶体管带来了大量的面积和功耗开销。  如今，x86指令集在笔记本、台式机、服务器等场景取得了近乎垄断的优势，和IBM有着紧密的联系。可以说，IBM的选择成就了x86指令集的今天，并成就了英特尔这家曾经名不见经传的小公司。
引文1：在20世纪80年代，IBM在计划研制第一台个人计算机（PC）时，考虑过德州仪器的TMS9900、Motorola 68000、英特尔8088。尽管，Motorola 68000被后世认为是最佳选项，但是，IBM出于已经熟悉了英特尔8085、处理器制造授权的考虑，选择了8088这一注定名垂青史的x86处理器。从此，计算机工业界笼罩在了x86指令集的乌云之下。
引文2：IBM PC上市后大受欢迎，销售额超出IBM预期「八倍」，一度每月出货高达4万台。1983年IBM卖出超过75万台个人电脑，刺激IBM进入这市场的DEC仅售出6万9千台。到1984年，IBM个人电脑营收为40亿美元，是同期苹果两倍多。拜开放规格之赐，距离IBM PC首次亮相还不到一年，1982年6月市场就出现其他品牌的相容（Clone）机种，从此开启个人电脑的大航海时代。
以上两段文字引自technews、半导体行业观察，其简要描述了IBM早期的选择及其巨大的市场成功。
ARM指令集 在上世纪80年代，计算机科学家们分析了大量的程序，发现80%的指令是很少用到的，处理器频繁地使用20%的那部分指令，比如Load指令、条件分支指令、Store指令、比较指令。如下图所示，该表格引自《计算机体系结构：量化研究方法（第六版）》。作者是David Patterson、John Hennessy。
因此，早期的RISC指令集、MIPS指令集应运而生，它们砍掉了大量不常用的冗余指令，只保留了最基本、最常用、功能最简单的指令集合。基于这种RISC指令集设计的处理器架构代表是ARM架构，为了便于不同客户进行定制化修改，其每代均会推出以下三个授权版本：
 Cortex-A内核，面向高性能应用。 Cortex-R内核，面向实时系统。 Cortex-M内核，面向嵌入式设备场景。  相比于x86指令集（CISC复杂指令集），ARM这种RISC精简指令集的变化主要是：
第一个方面，原来大量的冗余指令，以及由于历史原因兼容的指令，都在统计结果的基础上予以删除。
第二个方面，原来的一条x86强指令，在ARM中被多条基本的简单指令替代。
举一个例子：CISC提供的乘法指令，调用时可完成内存a和内存b中的两个数相乘，结果存入内存a，需要多个CPU周期才可以完成；而RISC不提供“一站式”的乘法指令，需调用四条单CPU周期指令完成两数相乘：内存a加载到寄存器，内存b加载到寄存器，两个寄存器中数相乘，寄存器结果存入内存a。
基于以上的设计理念，或者说，在这样的底层逻辑下，ARM指令集的处理器架构有如下优点或缺点：
 砍掉了大量的x86冗余硬件设计，使得DEC译码器的设计更加简单，节省了大量的面积和功耗开销。 一条ARM指令的功能更加单一和基本，这种指令相比于x86的强指令可以称之为”弱指令“，执行这样的弱指令所需的功耗进一步降低。 由于原来一条x86强指令就可以搞定的事情，在ARM这里需要多条弱指令组合来做，大幅提高了编译器的设计难度，同时提高了汇编语言编程的难度。 由于ARM指令集的处理器，相比于x86指令集的处理器，其在硬件结构上更加简单，因此，单个ARM核的面积和功耗更小，但是其性能也更弱小。这就导致了我们堆叠多个弱小的ARM核来打一个强大的x86核。  显然，ARM是ISA指令集架构的后起之秀，其由于出色的低功耗处理器设计技术，在手机、嵌入式设备、平板电脑、移动设备等领域，取得了十分巨大的成功。
我们熟悉的苹果M系列芯片、高通骁龙系列芯片、华为麒麟系列芯片、三星Exynos系列芯片，均是在ARM架构的基础上针对移动端设计的。
后记 下一节，我们深入谈一谈x86指令集、ARM指令集更加细节的区别。
本文的作者是岳诗鹏。
岳诗鹏，湖北孝感人，清华大学硕士，处理器芯片设计专业，吉林大学本科，电子信息工程专业。现专注于算力芯片的架构和设计。
]]></content>
  </entry>
  
  <entry>
    <title>关于芯片大厂裁员的几点思考</title>
    <url>/post/soc/some-thoughts-on-layoffs-at-major-chip-manufacturers.html</url>
    <categories><category>SOC</category>
    </categories>
    <tags>
      <tag>Chip Manufacturer</tag>
      <tag>Layoff</tag>
      <tag>Gartner</tag>
      <tag>Marvell</tag>
      <tag>NAND</tag>
    </tags>
    <content type="html"><![CDATA[近年来，半导体产业经历了波诡云谲的变革。疫情时代，远程办公技术激增带来了行业的暂时繁荣，但随后，我们见证了这一波潮的退却。
Gartner的数据指出，2023年全球半导体收入可能会收缩11.2%，与2022年度的小幅增长形成鲜明对照。不同于以往的周期性波动，这一次的下滑受到了市场动态和多重非市场因素的双重影响，包括疫情的持续冲击、地缘政治的复杂局势，以及劳动力市场的变动。这些因素综合作用，导致需求的减退和库存的积压，促使半导体企业普遍下修收入预期。
在全球半导体市场格局的重塑过程中，行业内部的调整和重组已成为必然。面对长期的投资周期和巨额的资本及人力成本，不少企业选择裁员作为应对行业下行期的策略。
进入2023年，裁员的消息依然甚嚣尘上。最近据报道，Graphcore宣布裁掉中国区大部分员工，并停止在华销售产品。
在此之前，Marvell Technology宣布将裁减台湾NAND控制IC团队。Marvell主要提供NAND原厂开发Enterprise SSD控制芯片ASIC设计服务，收取NRE设计服务费。这已经是今年Marvell的多次裁员之一，2023年3月，Marvell宣布，由于全球半导体市场放缓，将裁撤其在中国的整个研发团队。当然不仅仅是国内地区，2023年4月4日，Marvell计划在其硅谷总部裁掉近100名员工，作为更广泛裁员计划的一部分。
此外，还有高通、安森美、德州仪器、SiFive、imagination等大型芯片公司在今年也陆续发布了裁员计划。
在这些大厂的裁员背后，除了下行周期的因素之外，还有许多值得深思的点。
裁员背后的原因 其中一个关键因素是本土企业的快速崛起与日益激烈的市场竞争。近年来，国内半导体企业发展步伐很快，在不少市场取得了一定的成果。例如在Marvell所想要裁减的SSD控制IC市场，目前全球市场呈现Marvell、群联、慧荣三分天下局面；TI的MCU和电源芯片领域，都是竞争激烈之地。面对本土的企业竞争，这些厂商可能需要调整其市场策略和技术发展方向，以应对来自本土厂商的竞争压力。这可能包括资源的重新分配、市场重点的调整或产品线的优化。
此外，高毛利率的追求和成本控制也是驱动裁员决策的经济逻辑。国际芯片厂商一般都看重高毛利，例如，德州仪器的毛利率在全球半导体行业中是相对较高的，约为64%。TI一直致力于提升毛利率，这主要通过采用成熟制程和提高生产效率来实现。比如将老旧的6英寸和8英寸晶圆厂转变为更高效的12英寸晶圆厂，以降低成本并提高产量。在维持毛利的同时，必须进行成本控制，这常常意味着舍弃不再盈利的业务。
地缘政治和供应链考虑：国际政治环境的变化和供应链的稳定性也可能是影响这些芯片厂商在中国市场策略的因素之一。据业界人士指出，Marvell将原本大陆的研发资源转移改到越南设立研发中心，这反映了企业对地缘政治和成本的双重应对。
对中国半导体产业的影响 中国半导体产业面临的，则是一个多维度的影响局面。
从短期影响来看，国际公司将一些研发部门裁撤或者缩减，可能导致中国在某些高端芯片技术领域的发展放缓，尤其是在那些高度依赖国际技术和知识产权的领域。此外，大规模裁员可能导致市场信心下降，对投资者和其他利益相关者的信心造成短期冲击。
但是，这种裁员对中国的技术发展存在着长期机遇。国际公司的撤退可能促使中国加大对本土半导体行业的支持，鼓励自主研发和创新，减少对外依赖。同时，被裁撤的人才可能被国内的半导体公司吸纳，这为中国企业带来了高质量人才资源，有助于提升本土企业的技术能力和创新水平。
这种外部压力可能会促使中国政府和企业加大在本土半导体技术的投资，特别是在基础研究和应用技术开发方面。这可能会加快中国从技术引进者向技术创新者的转变。
对全球半导体行业的影响 随着芯片大厂开始逐渐减弱在国内市场的研发重心，这可能会对全球半导体行业产生以下影响：
 全球供应链的重塑：  供应链可能会出现重新配置，新的供应商和生产基地可能会在其他国家出现，如越南、非洲、马来西亚等地，以应对地缘政治风险和供应链的多元化需求。近年来，芯片大厂押注东南亚的案例不胜枚举。供应链的多元化也可能导致成本和效率上的变化，这可能影响全球半导体产品的价格和供应。
技术竞争和创新的加剧：  全球范围内可能会出现技术竞争的加剧。如今的一个不争的事实是，各大半导体生产国和地区都可能加大对创新和研发的投入，以保持或提升在全球市场的竞争力。如来势汹汹的美国不仅拉拢台积电等晶圆代工厂赴美国建厂，近日还正在大力发展此前所忽视的先进封装产业，拨出30亿美元推动先进封装的发展，美国NIST主任表示要在十年内制造和封装世界上最先进的芯片；一向以存储产业为生的韩国半导体产业，开始重视Fabless的发展，韩国出现了不少一些新兴领域的Fabless芯片公司，如AI芯片领域、DPU领域等等；日本对2nm和1nm的激进布局，以及对台积电赴日的重视和真金白银的补助诚意。。。
尤其是东南亚各国，正在紧紧抓住这波半导体发展机遇。如2023年11月的越南国际创新展上，越南军事工业和电信集团 (Viettel) 推出了首个5G芯片，这款5G DFE芯片系列每秒能够执行1,000万亿次计算，受到EDA公司Synopsys等信誉良好的合作伙伴的高度评价。
对于人才的建议 芯片大厂的裁员对国内的人才影响颇大，处于动荡之际，人心惶惶。对于在全球芯片行业变革中被裁减的人才，这一变化既带来了挑战，亦暗藏着新的职业机会。
首先，这波芯片最坏的时期，已经熬过去。这点从消费电子的回暖可见一斑，手机相关芯片供应链已经逐渐看到客户端拉货潮。这个行业可能会经历起伏，但每一次低潮都是积累和跳跃的契机。
其次，半导体人才荒还是整个产业面临的基本现状，所以人才自是不必过分担忧。在快速变化的半导体行业，不断学习和适应是成功的关键。被裁的人员可能一时面临着职业发展的不确定性。尽管他们拥有宝贵的行业经验和技术知识，但在当前市场环境下，重新找到合适的工作可能需要时间和努力。这些人才可能拥有高度专业化的技能，这些技能在其他相关领域，如新兴的科技行业、数据分析、人工智能等领域仍然具有很高的价值。
半导体行业既是全球经济的重要支柱，也是科技创新的重要基础。随着全球经济的不断发展，半导体行业仍具有巨大的增长潜力。预计到2030年，全球市场规模将达到1万亿美元已经成为行业内的共识。而据国际数据公司（IDC）的最新预测，预计到明（2024）年，全球半导体市场将同比增长20.2%，达到6330亿美元，整个半导体市场正在朝着1万亿美元的目标稳步前进中。
这一增长背后是智能手机、电动汽车、数据中心和物联网等领域的蓬勃发展，它们对先进半导体的需求日益增长。技术的快速进步要求半导体工程师和技术人员不断更新知识和技能。持续学习成为这一行业从业者的必要条件。
虽然就业机会多，但这也意味着竞争非常激烈。市场的快速变化和技术的不断迭代使得半导体产业周期性波动，从业人员需要准备好面对行业的起伏。
个人应对挑战的能力，企业的战略布局，以及国家的政策支持，将共同决定能否充分利用这一波澜壮阔的产业发展机遇。
结语 在半导体产业的变革浪潮中，每个人都不能独善其身。半导体是一个快速发展且日新月异的领域，今天的前沿技术可能明天就成为过时的知识。永远保持学习的热情和好奇心是至关重要的。另外，要保持灵活和适应性，行业的变化可能导致某些岗位变得不再需要，但同时也会创造出新的机会。对变化保持开放的态度，并准备好接受和适应这些变化。
总之，对于个人和企业来说，全面而周到的准备与战略部署，是把握这1万亿美元市场机遇、在竞争中取得领先的决定性因素。
本文作者： 杜芹DQ
]]></content>
  </entry>
  
  <entry>
    <title>GPU市场分析：全球视野下的竞争格局与未来发展</title>
    <url>/post/datacenter/gpu-competitive-landscape-and-future-development-from-a-global-perspective.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>SR-IO</tag>
      <tag>GPU</tag>
    </tags>
    <content type="html"><![CDATA[谈到计算机与GPU的关系，就不得不提到IBM公司在1981年发布的世界上第一台个人电脑IBM5150，该电脑配备了黑白显示适配器和彩色图形适配器，这是最早的图形显示控制器。
前言 谈到计算机与GPU的关系，就不得不提到IBM公司在1981年发布的世界上第一台个人电脑IBM5150，该电脑配备了黑白显示适配器和彩色图形适配器，这是最早的图形显示控制器。
在20世纪80年代初期，以GE芯片为代表的图形处理器开始出现，它具备四位向量的浮点运算功能，可以完成图形渲染过程中的矩阵、裁剪、投影等运算，标志着计算机图形学进入以图形处理器为主导的阶段。随着GE等图形处理器功能的不断完善，图形处理功能也逐渐从CPU向GPU（前身）转移。随着时间进入上世纪90年代，如今GPU的王者英伟达进入个人电脑3D市场，并于1999年推出具有标志意义的图形处理器GeForce256，真正意义上的GPU第一次出现。
与以往的图形处理器相比GeForce256将T&amp;L硬件从CPU中分离，并将其单独组合成硬件，这种创新使得GPU可以独立进行三维顶点的空间坐标变换，从而将CPU从繁重的光照计算中解放出来。这意味着，即使是低端CPU，如果搭配了支持硬件T&amp;L的显卡，也可以流畅地玩游戏。这一技术革新使得NVIDIA在市场竞争中获得了较大的优势，并且其市占率也持续提升。
当今全球GPU市场分析 目前，全球局势动荡，前几年新冠疫情席卷全球，整个世界逐渐转入“线上”模式，随着家用电脑需求量的增加和因疫情等原因带来的芯片紧缺，加之中美冲突，当下北约内部矛盾，一度激化上演全武行至今也不见降温迹象。再到如今的后疫情时代，大语言模型AI的火爆，加之汽车智能化浪潮和智能驾驶对于AI算力的需求，一下子就把GPU推到了世界半导体硬件的舞台中央。
就当下而言，GPU应用市场可划分为三大应用领域分别为：PC市场（游戏）、AI&amp;数据中心和智能汽车。
首先，我们先来看看GPU的主战场，也就是PC领域。
根据Jon Peddie Research的数据，2023年的整体出货量包括NVIDIA、AMD和英特尔的出货量都呈现大幅下降的情况。根据该报告，2023年第一季GPU的总出货量达到了630万片，但与上一季相比下降了12.6%，与2022年同期相比则下降了38.2%。其中，2023年第一季出货量相较上一季下降了12.6%，低于 -4.9%的10年平均水平。而2023年的第二季度情况则更让人瞩目，GPU市场规模达到了6160万美元，但同比下降了惊人的23%。究其原因，笔者认为一是后疫情时代消费者普遍预算吃紧，消费意愿较低；二是NVIDIA和AMD都有较旧型号的库存，可用于吸引部分主流和预算型市场的游戏玩家，加上英特尔的高性比显卡的入局；三是虽然新的显卡已经推出，但在性价比明显不佳情况下，消费者对它们的兴趣不大，更多的消费者宁可选择前代3090系列或是英特尔的产品。
尽管整体趋势呈下降态势，但该报告还是给出了GPU市场乐观的预测：2022年至2026年期间，PC用GPU的复合年增长率将稳定在3.70%，并有望在2026年年末达到总规模2998万片。与此同时，JPR认为PC用独立显卡(DGPU)的渗透率有望增长到32%，为市场带来新的发展契机。
其次，我再来关注一下GPU拥有巨大潜力的市场，那便是2023年初被推上风口的AI&amp;数据中心领域。
在数据中心，GPU被广泛应用于人工智能的训练、推理、高性能计算（HPC）等领域。以ChatGPT、文心一言等为代表的生成式大语言模型AI的涌现，其展现出了高度的拟人化和智能化，背后离不开利用GPU进行的海量基础训练，自GPT-3模型推出后，大规模自然语言模型进入了拥有千亿个参数的时代。在2021年之后，出现了许多达到千亿级别的自然语言模型，这些模型的训练算力显著增加。例如，ChatGPT模型拥有1.75亿个参数，其训练所需的算力为3.14x10^23次浮点运算。
当前，各种预训练语言模型仍在不断快速更新和迭代，不断刷新自然语言处理任务的最佳表现记录。同时，单个模型的训练算力需求也不断创新高。以ChatGPT的训练为例，单次模型训练就可能需要超过2000张英伟达A100显卡持续不断地训练27天。不仅如此，大模型的日常运行中每一次用户调用都需要一定的算力和带宽作为支撑。还是以ChatGPT为例，近期，ChatGPT官网每日吸引的访客数量接近5000万，每小时平均访问人数约为210万人。在高峰时期，同时在线人数达到450万人。假设每个人在一小时内提问8个问题，每个问题的回答长度为200个字，那么需要使用14,000块英伟达A100芯片来提供日常算力支持。大模型在融入搜索引擎或以app形式提供其他商业化服务过程中，其AI芯片需求将得到进一步的显著拉动。
根据相关数据显示，2023年全球AI芯片市场规模达到了1500亿美元，预计到2028年将达到2500亿美元。全球范围内，企业在人工智能市场的技术投资从2019年的612.4亿美元增长至2021年的924.0亿美元，2022年同比增长26.6%至1,170.0亿美元，并有望到2025年突破2,000亿美元，增幅高于企业数字化转型（DX）支出整体增幅。
在数据中心领域，则是GPU市场保持持续增长的另一个强劲支柱，根据Omdia数据，2019年全球人工智能服务器市场规模为23亿美金，2026年将达到376亿美金，CAGR为49%。而根据IDC的数据显示，2020年中国数据中心用于人工智能推理的芯片的市场份额已经超过50%，预计到2025年，用于人工智能推理工作负载的芯片将达到60.8%。人工智能服务器在搭建时，一般选用CPU+各种加速处理器的组合。而常用的加速处理器有FPGA、ASIC、GPU、NPU等。而就目前来看，GPU凭借其出色的深度学习能力、并行计算能力还有其不俗的通用性，在数据中心加速芯片市场中占比一骑绝尘，超过90%的AI服务器将GPU作为了加速芯片。因此相关数据指出，在2026年AI服务器市场规模将达到376亿美元，随着AI服务器迅速增长GPU在这一部分市场也将迎来需求的大爆发。
最后，我们来看一下GPU在汽车电子中的应用情况。
在如今新能源汽车崛起高速增长的今天，随着汽车智能化浪潮的到来，自动驾驶和智能座舱是最具有发展前景的两个方向。
在自动驾驶领域，使用的芯片不计其数，各个类型的芯片都有涉及，根据Yole数据，全球自动驾驶市场2025年将达到780亿美金，其中用于自动驾驶的AI芯片超过100亿美元。在这之中GPU在自动驾驶的渗透率随着自动驾驶的等级的提升而提升。根据ICVTank的自动驾驶渗透数据：假设GPU在L2中的渗透率达到15%，在L3-L5中的渗透率达到50%，那么可以估算出GPU在自动驾驶领域的市场规模。整体规模预计将从2020年的7.1亿美元增长到2025年的44亿美元，年复合增长率为44%。
在智能座舱领域，独立GPU往往不会被应用，往往是以SoC的形式以集成GPU的形态被应用的。在一片完整的SoC中一般由GPU、CPU、AI引擎、DPU等组成。而目前的智能座舱往往向着一芯多屏的方向发展，这就是GPU部分在智能座舱中有着无可替代且越来越重要的优势。
国内外GPU相关企业概况 谈到GPU方面的市场份额，那么就不得不提目前几乎是一家独大的英伟达。在消费级独立PC市场上，根据 Jon Peddie Research 调查数据显示，2023 年第一季度，全球桌面级显卡销量约为 630 万块，其中英伟达显卡销量约为 529 万张，以 84%的市场份额占据领先地位，此外销量位居前三的 GPU 供应商还包括 AMD 及 Intel，其销量分别为 76 万张、25 万张。而国内方面则在消费级独立GPU上起步较晚，我国国产GPU厂商市场份额普遍较小，但也不乏如芯动科技、摩尔线程等勇于挑战国外厂商垄断地位的国内GPU新兴厂商，未来随着国内数据中心、智能驾驶及终端侧 GPU 市场需求的提升，国产 GPU 市场份额有望实现渗透。
而在盈利能力方面，英伟达凭借其绝对的市场地位遥遥领先，根据权威机构Jon Peddie Research GPU市场统计数据，2023年第二季度英伟达以68%的PC显卡市场份额位居市场第一。与此相对，2024财年第二季度游戏业务实现了24.9亿美元的营收，同比增长了22%。而老对手AMD的游戏业务则以15.81亿美元，同比下降4%，环比下降了10%，营业利润2.25亿美元的成绩紧随其后。
在数据中心市场，英伟达依旧是处于领导地位，根据研究机构TrendForce的预测，目前数据中心市场上，英伟达的GPU已成为主流芯片，市场份额约占60-70%。而国内在该领域布局者较多，产品性能逐渐向英伟达靠拢。比如，目前国内算力最高的产品是由壁仞科技推出的BR100P，其采用了台积电7nm工艺。在峰值状态下，它具有单精度浮点算力240TFLOPS和整型算力1920TOPS。BR100芯片的性能比英伟达的A100高出3倍以上，虽说对比英伟达的旗舰产品H100还有不小的差距，但是依旧未来可期。
在盈利方面，据英伟达 2024 财年 Q2 报告，其数据中心营收已达到 103.2 亿美元，同比增长171%，约占总营收比例为 76%，目前数据中心业务已经成为英伟达最高的收入来源。而我国壁仞科技的 BR100P 系列芯片同样由台积电代工，预计于 2023 年量产，若量产计划顺利推进，也有望在数据中心这一广阔的市场中分一杯羹。
在智能驾驶领域，国内大多数智能驾驶车型选用英伟达产品，大多都采用其旗下的ORIN 产品。但在智能驾驶领域，华为也在之中提供了自动驾驶的全栈解决方案，其发布的昇腾 610、MDC810 已经量产，MDC610平台，单组算力为200 TOPS，与英伟达 ORIN 产品差距较小。在市场结构方面，2023年上半年，中国市场乘用车自动驾驶计算方案市场份额中，英伟达以52.57%的份额位居第一，地平线占据30.71%的市场份额位居第二，华为海思占据4.05%的市场份额。
在盈利能力方面，英伟达的智能驾驶业务在2024财年第二季度的营收较第一季度出现了下滑，但与去年同期相比，仍然增长了15%，总营收达到2.53亿美元。
结语 就目前的GPU市场而言，英伟达在市场中凭借自己多年的技术积累大杀四方，占据了绝对的领导地位（80%以上的市场份额），而AMD和重新归来的英特尔在其后瓜分剩下的一点市场份额。而国内GPU厂商则是刚刚起步，正在完成从0到1的突破阶段，总体来看还有很长的路要走。但是在诸如智能驾驶等国内重点发展的领域也能从英伟达口中分得一点点市场份额。
未来看我国的GPU发展，作为数据处理的重要工具，GPU具有重要的战略地位，并受到国家的高度重视。在中美科技摩擦的背景下，自主研发和掌握GPU显得尤为重要。从成长性的角度来看，全球市场空间广阔，国内市场规模也相当庞大，并随着下游需求的增加而呈现加速增长的趋势。在数字化驱动的总需求提升背景下，结合国产化的发展趋势，为国产GPU产业提供了总量和份额双提升的机遇。因此，国产GPU厂商得到了快速发展的机会。在大市场需求下，GPU的国产化进程具有广阔的空间，而优秀厂商的稀缺性也日益凸显，有望实现加速成长，部分厂商甚至有望实现爆发式增长。
]]></content>
  </entry>
  
  <entry>
    <title>一个超级精简高可移植的shell命令行C实现</title>
    <url>/post/mcu/a-super-streamlined-and-highly-portable-shell-command-line-implemented-by-c.html</url>
    <categories><category>MCU</category>
    </categories>
    <tags>
      <tag>Shell</tag>
    </tags>
    <content type="html"><![CDATA[在嵌入式开发中,一般需要使用shell命令行来进行交互。在一些资源非常受限的平台比如8位单片机平台，则各种开源的shell框架都显得过于大和复杂。此时实现一个超级精简的shell命令行则显得很必要。我们这里就来实现一个，方便以后快速集成到任何平台中使用。
前言 有几点开发前提:
  考虑可移植性，不使用任何编译器扩展。
  考虑可移植性，不依赖任何的库函数，仅依赖stdint的数据类型。
  考虑可移植性, 核心代码无需任何修改，只需要调用初始化配置接口来适配不同平台。
  尽可能少的代码量和ram使用量。
  精简，一切都刚好够用，可以快速嵌入任何平台，针对不同需要可以直接在此基础上增加实现。
  实现 首先定义一个数据结构，用于记录命令字符串和对应的执行函数和帮助字符串的对应关系
typedef void (*shell_command_pf)(uint8_t *); /**&lt; 命令回调函数 */ typedef uint32_t (*shell_read_pf)(uint8_t *buff, uint32_t len); /**&lt; 底层收接口 */ typedef void (*shell_write_pf)(uint8_t *buff, uint32_t len); /**&lt; 底层接口 */ /** * \struct shell_cmd_cfg * 命令信息 */ typedef struct { uint8_t * name; /**&lt; 命令字符串 */ shell_command_pf func; /**&lt; 命令回调函数 */ uint8_t * helpstr; /**&lt; 命令帮助信息 */ }shell_cmd_cfg; 对接口抽象，初始化指定输入输出接口
/** * \fn shell_set_itf * 设置底层输入输出接口,以及命令列表 * 调用shell_exec_shellcmd之前,需要先调用该接口初始化 * \param[in] input \ref shell_read_pf 输入接口 * \param[in] output \ref shell_write_pf 输出接口 * \param[in] cmd_list \ref shell_cmd_cfg 命令列表 * \param[in] enableecho 0:不使能回显, 其他值:使能回显 */ void shell_set_itf(shell_read_pf input, shell_write_pf output, shell_cmd_cfg* cmd_list, uint8_t enableecho); 非阻塞，主循环调用以下函数进行处理
/** * \fn shell_exec * 周期调用该函数,读取底层输入,并判断是否有命令进行处理 * 非阻塞 */ void shell_exec(void); 通过以下宏配置缓存大小，这里实际可以修改下改为调用者分配，而不是静态数组，这样可以由调用者决定分配和释放，更灵活，也在不用的时候不占用空间。
#define SHELL_CMD_LEN 64 /**&lt; 命令缓冲区大小 */核心逻辑
shell_exec调用 shell_read_line从输入接口读数据到缓冲区,读到一行后调用
shell_exec_cmdlist查询命令信息表，匹配字符串再调用对应的实现函数。
所以核心代码是读命令行，直接看注释即可
/** * 读取一行命令 */ static uint32_t shell_read_line(void) { uint8_t ch; uint32_t count; /* 初始打印sh&gt; */ if(s_cmd_buf_au8[0]==&#39;\r&#39;) { shell_putstring(&#34;sh&gt;\r\n&#34;); s_cmd_buf_au8[0] = 0; } /* 非阻塞读取一个字符 */ if(shell_getchar(&amp;ch) !=0 ) { return 0; } /* 遇到除了退格之外的不可打印字符,则认为收到一行命令 * 退格需要单独处理,需要删除一个字符 */ if((ch == &#39;\r&#39; || ch == &#39;\n&#39; || ch &lt; &#39; &#39; || ch &gt; &#39;~&#39;) &amp;&amp; (ch != &#39;\b&#39;)) { if(s_cmd_buf_index_u32==0) { /* 缓冲区没有数据就收到了非打印字符串,则打印提示sh&gt; */ shell_putstring(&#34;sh&gt;\r\n&#34;); } else { /* 收到了非打印字符,且缓冲区有数据则认为收到了一行 * 返回缓冲区数据长度,并清零计数,打印回车换行 * 并且添加结束符0 */ count = s_cmd_buf_index_u32; s_cmd_buf_au8[s_cmd_buf_index_u32]=0; s_cmd_buf_index_u32 =0; shell_putstring(&#34;\r\n&#34;); return count; } } else { if(ch == &#39;\b&#39;) { /* 退格处理,注意只有有数据才会删除一个字符，添加结束符 */ if(s_cmd_buf_index_u32 != 0) { s_cmd_buf_index_u32--; shell_putchar(&#39;\b&#39;); shell_putchar(&#39; &#39;); shell_putchar(&#39;\b&#39;); s_cmd_buf_au8[s_cmd_buf_index_u32]= &#39;\0&#39;; } } else { /* 可打印字符，添加到缓冲区 * 如果数据量已经到了缓冲区大小-1,则也认为是一行命令 * -1是保证最后有结束符0空间 */ if(s_enableecho_u8 != 0) { shell_putchar(ch); } s_cmd_buf_au8[s_cmd_buf_index_u32++] = ch; if(s_cmd_buf_index_u32&gt;=(sizeof(s_cmd_buf_au8)-1)) { count = s_cmd_buf_index_u32; s_cmd_buf_au8[s_cmd_buf_index_u32]=0; s_cmd_buf_index_u32 =0; shell_putstring(&#34;\r\n&#34;); return count; } } } return 0; } 代码量很少，直接看源码即可
shell.c/h为核心代码，无需修改
shell_func.c/h为命令实现，需要自己额添加
shell.c #include &lt;stdint.h&gt;#include &#34;shell.h&#34; shell_read_pf s_input_pf = 0; /* 输入接口指针 */ shell_write_pf s_output_pf = 0; /* 输出接口指针 */ shell_cmd_cfg* s_cmd_cfg_pst = 0; /* 命令列表指针 */ uint8_t s_enableecho_u8 = 0; /* 是否使能echo标志 */ static uint8_t s_cmd_buf_au8[SHELL_CMD_LEN]=&#34;\r&#34;; /* 命令缓冲区 */ static uint32_t s_cmd_buf_index_u32 = 0; /* 当前命令缓冲区中字符数 */ /** * 输出字符接口 */ static void shell_putchar(uint8_t val) { uint8_t tmp; if(s_output_pf != 0) { tmp = val; s_output_pf(&amp;tmp, 1); } } /** * 输出字符串接口 */ static void shell_putstring(char* str) { uint32_t len = 0; uint8_t*p = (uint8_t*)str; while(*str++) { len++; } s_output_pf(p, len); } /** * 读字符接口 */ static int shell_getchar(uint8_t *data) { if(s_input_pf == 0) { return -1; } if(0 == s_input_pf(data, 1)) { return -1; } else { return 0; } } /** * 判断命令字符串的长度 * 命令字符串不能有空格 */ static uint32_t shell_cmd_len(uint8_t *cmd) { uint8_t *p = cmd; uint32_t len = 0; while((*p != &#39; &#39;) &amp;&amp; (*p != 0)) { p++; len++; } return len; } /** * 判断两个字符串是否相等,相等返回0 */ static int shell_cmd_check(uint8_t *cmd, uint8_t *str) { uint32_t len1 = shell_cmd_len(cmd); uint32_t len2 = shell_cmd_len(str); if(len1 != len2) { return -1; } for(uint32_t i=0; i&lt;len1; i++) { if(*cmd++ != *str++) { return -1; } } return 0; } /** * 读取一行命令 */ static uint32_t shell_read_line(void) { uint8_t ch; uint32_t count; /* 初始打印sh&gt; */ if(s_cmd_buf_au8[0]==&#39;\r&#39;) { shell_putstring(&#34;sh&gt;\r\n&#34;); s_cmd_buf_au8[0] = 0; } /* 非阻塞读取一个字符 */ if(shell_getchar(&amp;ch) !=0 ) { return 0; } /* 遇到除了退格之外的不可打印字符,则认为收到一行命令 * 退格需要单独处理,需要删除一个字符 */ if((ch == &#39;\r&#39; || ch == &#39;\n&#39; || ch &lt; &#39; &#39; || ch &gt; &#39;~&#39;) &amp;&amp; (ch != &#39;\b&#39;)) { if(s_cmd_buf_index_u32==0) { /* 缓冲区没有数据就收到了非打印字符串,则打印提示sh&gt; */ shell_putstring(&#34;sh&gt;\r\n&#34;); } else { /* 收到了非打印字符,且缓冲区有数据则认为收到了一行 * 返回缓冲区数据长度,并清零计数,打印回车换行 * 并且添加结束符0 */ count = s_cmd_buf_index_u32; s_cmd_buf_au8[s_cmd_buf_index_u32]=0; s_cmd_buf_index_u32 =0; shell_putstring(&#34;\r\n&#34;); return count; } } else { if(ch == &#39;\b&#39;) { /* 退格处理,注意只有有数据才会删除一个字符，添加结束符 */ if(s_cmd_buf_index_u32 != 0) { s_cmd_buf_index_u32--; shell_putchar(&#39;\b&#39;); shell_putchar(&#39; &#39;); shell_putchar(&#39;\b&#39;); s_cmd_buf_au8[s_cmd_buf_index_u32]= &#39;\0&#39;; } } else { /* 可打印字符，添加到缓冲区 * 如果数据量已经到了缓冲区大小-1,则也认为是一行命令 * -1是保证最后有结束符0空间 */ if(s_enableecho_u8 != 0) { shell_putchar(ch); } s_cmd_buf_au8[s_cmd_buf_index_u32++] = ch; if(s_cmd_buf_index_u32&gt;=(sizeof(s_cmd_buf_au8)-1)) { count = s_cmd_buf_index_u32; s_cmd_buf_au8[s_cmd_buf_index_u32]=0; s_cmd_buf_index_u32 =0; shell_putstring(&#34;\r\n&#34;); return count; } } } return 0; } /** * 搜寻命令列表处理命令 */ static int shell_exec_cmdlist(uint8_t* cmd) { int i; if(s_cmd_cfg_pst == 0) { return -1; } for (i=0; s_cmd_cfg_pst[i].name != 0; i++) { if (shell_cmd_check(cmd, s_cmd_cfg_pst[i].name) == 0) { s_cmd_cfg_pst[i].func(cmd); return 0; } } if(s_cmd_cfg_pst[i].name == 0) { shell_putstring(&#34;unkown command\r\n&#34;); return -1; } return 0; } /** * 对外接口,周期执行 */ void shell_exec(void) { if(shell_read_line() &gt; 0) { shell_exec_cmdlist(s_cmd_buf_au8); } } /** * 对外接口,初始化配置接口 */ void shell_set_itf(shell_read_pf input, shell_write_pf output, shell_cmd_cfg* cmd_list, uint8_t enableecho) { s_input_pf = input; s_output_pf = output; s_cmd_cfg_pst = cmd_list; s_enableecho_u8 = enableecho; } shell.h #ifndef SHELL_H #define SHELL_H  #ifdef __cplusplus extern &#34;C&#34; { #endif  #include &lt;stdint.h&gt; #define SHELL_CMD_LEN 64 /**&lt; 命令缓冲区大小 */ typedef void (*shell_command_pf)(uint8_t *); /**&lt; 命令回调函数 */ typedef uint32_t (*shell_read_pf)(uint8_t *buff, uint32_t len); /**&lt; 底层收接口 */ typedef void (*shell_write_pf)(uint8_t *buff, uint32_t len); /**&lt; 底层发接口 */ /** * \struct shell_cmd_cfg * 命令信息 */ typedef struct { uint8_t * name; /**&lt; 命令字符串 */ shell_command_pf func; /**&lt; 命令回调函数 */ uint8_t * helpstr; /**&lt; 命令帮助信息 */ }shell_cmd_cfg; /** * \fn shell_exec * 周期调用该函数,读取底层输入,并判断是否有命令进行处理 * 非阻塞 */ void shell_exec(void); /** * \fn shell_set_itf * 设置底层输入输出接口,以及命令列表 * 调用shell_exec_shellcmd之前,需要先调用该接口初始化 * \param[in] input \ref shell_read_pf 输入接口 * \param[in] output \ref shell_write_pf 输出接口 * \param[in] cmd_list \ref shell_cmd_cfg 命令列表 * \param[in] enableecho 0:不使能回显, 其他值:使能回显 */ void shell_set_itf(shell_read_pf input, shell_write_pf output, shell_cmd_cfg* cmd_list, uint8_t enableecho); #ifdef __cplusplus } #endif  #endif shell_func.c #include &lt;stdio.h&gt;#include &lt;stdint.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt; #include &#34;shell.h&#34;#include &#34;shell_func.h&#34;#include &#34;xmodem.h&#34;#include &#34;uart_api.h&#34;#include &#34;iot_flash.h&#34; static void helpfunc(uint8_t* param); static void rxmemfunc(uint8_t* param); static void sxmemfunc(uint8_t* param); static void rxflashfunc(uint8_t* param); static void sxflashfunc(uint8_t* param); static void uarttestfunc(uint8_t* param); /** * 最后一行必须为0,用于结束判断 */ const shell_cmd_cfg g_shell_cmd_list_ast[ ] = { { (uint8_t*)&#34;help&#34;, helpfunc, (uint8_t*)&#34;help&#34;}, { (uint8_t*)&#34;rxmem&#34;, rxmemfunc, (uint8_t*)&#34;rxmem addr len&#34;}, { (uint8_t*)&#34;sxmem&#34;, sxmemfunc, (uint8_t*)&#34;sxmem addr len&#34;}, { (uint8_t*)&#34;rxflash&#34;, rxflashfunc, (uint8_t*)&#34;rxflash addr len&#34;}, { (uint8_t*)&#34;sxflash&#34;, sxflashfunc, (uint8_t*)&#34;sxflash addr len&#34;}, { (uint8_t*)&#34;uarttest&#34;, uarttestfunc, (uint8_t*)&#34;uarttest&#34;}, { (uint8_t*)0, 0 , 0}, }; void helpfunc(uint8_t* param) { (void)param; unsigned int i; printf(&#34;\r\n&#34;); printf(&#34;**************\r\n&#34;); printf(&#34;* SHELL *\r\n&#34;); printf(&#34;* V1.0 *\r\n&#34;); printf(&#34;**************\r\n&#34;); printf(&#34;\r\n&#34;); for (i=0; g_shell_cmd_list_ast[i].name != 0; i++) { printf(&#34;%02d.&#34;,i); printf(&#34;%-16s&#34;,g_shell_cmd_list_ast[i].name); printf(&#34;%s\r\n&#34;,g_shell_cmd_list_ast[i].helpstr); } } uint8_t rxtx_buf[1029]; extern uint32_t g_tick_u32; static uint32_t getms(void) { return g_tick_u32; } static uint32_t io_read(uint8_t* buffer, uint32_t len) { return uart_api_read(buffer, len); } static void io_read_flush(void) { uint8_t tmp; while(0 != uart_api_read(&amp;tmp, 1)); } static uint32_t io_write(uint8_t* buffer, uint32_t len) { uart_api_write(buffer, len); return len; } static uint32_t mem_read(uint32_t addr, uint8_t* buffer, uint32_t len) { memcpy(buffer, (uint8_t*)addr, len); return len; } static uint32_t mem_write(uint32_t addr, uint8_t* buffer, uint32_t len) { memcpy((uint8_t*)addr, buffer, len); return len; } static uint32_t flash_read(uint32_t addr, uint8_t* buffer, uint32_t len) { iot_flash_read(IOT_FLASH_SFC_PORT_0, addr, buffer, len); return len; } static uint32_t flash_write(uint32_t addr, uint8_t* buffer, uint32_t len) { iot_flash_write(IOT_FLASH_SFC_PORT_0, addr, buffer, len); return len; } void rxmemfunc(uint8_t* param) { uint32_t addr; uint32_t len; uint8_t* p = param; int res = 0; //if(3 == sscanf((const char*)param, &#34;%*s %s %d %d&#34;, type, &amp;addr, &amp;len))  while(1) { if((*p &gt; &#39;z&#39;) || (*p &lt; &#39;a&#39;)) { break; } else { p++; } } while(1) { if(*p != &#39; &#39;) { break; } else { p++; } } addr = atoi((const char*)p); while(1) { if((*p &gt; &#39;9&#39;) || (*p &lt; &#39;0&#39;)) { break; } else { p++; } } while(1) { if(*p != &#39; &#39;) { break; } else { p++; } } len = atoi((const char*)p); xmodem_cfg_st cfg= { .buffer = rxtx_buf, .crccheck = 1, .getms = getms, .io_read = io_read, .io_read_flush = io_read_flush, .io_write = io_write, .start_timeout = 60, .packet_timeout = 1000, .ack_timeout = 1000, .mem_write = mem_write, .addr = addr, .totallen = len, }; xmodem_init_rx(&amp;cfg); while((res = xmodem_rx()) == 0); printf(&#34;res:%d\r\n&#34;,res); } void sxmemfunc(uint8_t* param) { uint32_t addr; uint32_t len; uint8_t* p = param; int res = 0; //if(3 == sscanf((const char*)param, &#34;%*s %s %d %d&#34;, type, &amp;addr, &amp;len))  while(1) { if((*p &gt; &#39;z&#39;) || (*p &lt; &#39;a&#39;)) { break; } else { p++; } } while(1) { if(*p != &#39; &#39;) { break; } else { p++; } } addr = atoi((const char*)p); while(1) { if((*p &gt; &#39;9&#39;) || (*p &lt; &#39;0&#39;)) { break; } else { p++; } } while(1) { if(*p != &#39; &#39;) { break; } else { p++; } } len = atoi((const char*)p); xmodem_cfg_st cfg= { .buffer = rxtx_buf, .plen = 1024, .getms = getms, .io_read = io_read, .io_read_flush = io_read_flush, .io_write = io_write, .start_timeout = 60, .packet_timeout = 1000, .ack_timeout = 1000, .mem_read = mem_read, .addr = addr, .totallen = len, }; xmodem_init_tx(&amp;cfg); while((res = xmodem_tx()) == 0); printf(&#34;res:%d\r\n&#34;,res); } void rxflashfunc(uint8_t* param) { uint32_t addr; uint32_t len; uint8_t* p = param; int res = 0; //if(3 == sscanf((const char*)param, &#34;%*s %s %d %d&#34;, type, &amp;addr, &amp;len))  while(1) { if((*p &gt; &#39;z&#39;) || (*p &lt; &#39;a&#39;)) { break; } else { p++; } } while(1) { if(*p != &#39; &#39;) { break; } else { p++; } } addr = atoi((const char*)p); while(1) { if((*p &gt; &#39;9&#39;) || (*p &lt; &#39;0&#39;)) { break; } else { p++; } } while(1) { if(*p != &#39; &#39;) { break; } else { p++; } } len = atoi((const char*)p); xmodem_cfg_st cfg= { .buffer = rxtx_buf, .crccheck = 1, .getms = getms, .io_read = io_read, .io_read_flush = io_read_flush, .io_write = io_write, .start_timeout = 60, .packet_timeout = 1000, .ack_timeout = 1000, .mem_write = flash_write, .addr = addr, .totallen = len, }; xmodem_init_rx(&amp;cfg); while((res = xmodem_rx()) == 0); printf(&#34;res:%d\r\n&#34;,res); } void sxflashfunc(uint8_t* param) { uint32_t addr; uint32_t len; uint8_t* p = param; int res = 0; //if(3 == sscanf((const char*)param, &#34;%*s %s %d %d&#34;, type, &amp;addr, &amp;len))  while(1) { if((*p &gt; &#39;z&#39;) || (*p &lt; &#39;a&#39;)) { break; } else { p++; } } while(1) { if(*p != &#39; &#39;) { break; } else { p++; } } addr = atoi((const char*)p); while(1) { if((*p &gt; &#39;9&#39;) || (*p &lt; &#39;0&#39;)) { break; } else { p++; } } while(1) { if(*p != &#39; &#39;) { break; } else { p++; } } len = atoi((const char*)p); xmodem_cfg_st cfg= { .buffer = rxtx_buf, .plen = 1024, .getms = getms, .io_read = io_read, .io_read_flush = io_read_flush, .io_write = io_write, .start_timeout = 60, .packet_timeout = 1000, .ack_timeout = 1000, .mem_read = flash_read, .addr = addr, .totallen = len, }; xmodem_init_tx(&amp;cfg); while((res = xmodem_tx()) == 0); printf(&#34;res:%d\r\n&#34;,res); } void uarttestfunc(uint8_t* param) { (void)param; uint8_t tmp[64]; uint32_t len; uint32_t total = 0; while(1) { while(0 != (len = uart_api_read(tmp, sizeof(tmp)))) { uart_api_write(tmp, len); total+=len; } if(total &gt;= 10*1024) { break; } } } shell_func.h #ifndef SHELL_FUNC_H #define SHELL_FUNC_H  #include &lt;stdint.h&gt; #ifdef __cplusplus  extern &#34;C&#34; { #endif  extern const shell_cmd_cfg g_shell_cmd_list_ast[ ]; #ifdef __cplusplus } #endif  #endif 测试 添加命令行只需要在shell_func.c中先申明
实现函数，比如
static void helpfunc(uint8_t* param); 然后在数组g_shell_cmd_list_ast中添加一行
用于指定命令行字符和对应的实现函数，以及帮助字符串。
比如
{ (uint8_t*)&#34;help&#34;, helpfunc, (uint8_t*)&#34;help&#34;}, 然后实现函数
void helpfunc(uint8_t* param) { (void)param; unsigned int i; printf(&#34;\r\n&#34;); printf(&#34;**************\r\n&#34;); printf(&#34;* SHELL *\r\n&#34;); printf(&#34;* V1.0 *\r\n&#34;); printf(&#34;**************\r\n&#34;); printf(&#34;\r\n&#34;); for (i=0; g_shell_cmd_list_ast[i].name != 0; i++) { printf(&#34;%02d.&#34;,i); printf(&#34;%-16s&#34;,g_shell_cmd_list_ast[i].name); printf(&#34;%s\r\n&#34;,g_shell_cmd_list_ast[i].helpstr); } } 然后初始化接口，执行
uart_bsp_init(115200); shell_set_itf(uart_api_read, uart_api_write, (shell_cmd_cfg*)g_shell_cmd_list_ast, 1); while(1) { shell_exec(); } 此时就可以输入对应命令字符串回车来执行对应的函数，比如
总结 以上以最简单的方式,实现了命令行shell。代码量和占用ram非常小，可以快速嵌入资源非常受限的开发平台。
Shell.c和shell.h完全可移植，无需任何修改。
只需初始化shell_set_itf设置对应的底层接口，然后shell_func.c中添加对应的命令实现即可。
非阻塞方式实现,方便应用。
以最简形式实现,支持退格，但是不支持历史命令等，但是可以快速的修改代码实现。
命令行以数组静态添加，而不是动态添加和使用编译器的扩展将代码放置于指定段的方式来添加，是因为考虑可移植性和简单性。也可以快速修改使用后者。
有很多人总是在感慨为什么嵌入式领域热衷于”造轮子”,这是由于其特定的需求决定的,在嵌入式领域有诸多前提条件的限制,比如资源就是一个必须要考虑的前提,所以很难一个”轮子”适应所有路况。比如在8位单片机上几k的ram,十几k的rom,这时候也要使用比如shell命令行,那么就不可能使用大而全的框架,则存储资源，运行占用CPU等等都会无法满足。。甚至编译器都是专用针对嵌入式场景的,可能都不能用各种花哨的处理方式编译器扩展等(比如获取段地址,大小,指定代码位于某个段),另外在嵌入式领域编译器相关的扩展也会影响可移植性，其实不建议过多使用。
正是因为有这么多的限制所以在嵌入式领域才会有这么的多需要定制开发，重复”造轮子”的事，而积累自己的轮子，积累自己的小代码库,组件库,则是嵌入式高手积累经验的一条路径。积累了足够多的自己的轮子，将来面对需求就能够得心应手,才能快速开发出不同需求的”轮子”。
]]></content>
  </entry>
  
  <entry>
    <title>你以为你的SSD硬盘正在待机，实际上它快忙坏了</title>
    <url>/post/datacenter/background-io-for-enterprise-ssd.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Background IO</tag>
      <tag>Enterprise SSD</tag>
    </tags>
    <content type="html"><![CDATA[Background IO，从字面理解即后台IO任务，它和前台IO是相对应的关系。前台IO，就是通常我们所理解的CPU访问SSD盘内数据、写数据到 SSD  闪存颗粒等工作。
Background IO是什么？ 但因为，SSD使用了相对HDD来说更加脆弱和敏感的NAND闪存颗粒，所以导致了SSD主控芯片不仅要处理数据的读写，还需要做一些额外的后台工作来保障SSD的正常工作。
一般来说，一款标准的企业级SSD，它所需要做的Background IO主要有以下几种：
 PLP电容自检 BootLoader存放区域扫描 系统区域扫描（固件、坏块表存放区） 用户数据区域后台扫描 温度传感器采样 Log日志定时写入NAND Read disturb数据搬移 后台GC垃圾回收 磨损均衡  平时没怎么总结还没发现，原来SSD固件在后台需要处理的任务这么多了，而且还有一些这里没有列出来的。
为什么需要Background IO？ PLP电容自检 正常情况下，拆开一块企业级SSD，基本都可以看到下图中这一块黑色的上面写着1000uF的电容，当然也可以是很多个并联的小电容组成。
这块电容在SSD中充当着不可或缺的角色，因为企业级对异常掉电后的数据恢复是刚需，因此需要这块电容存储电荷，在掉电时再继续让SSD工作一段时间，将仍留在DRAM中的重要数据及时写到NAND颗粒中，保证了异常掉电下的数据安全。
但是，只要是铝电解电容都会有一些缺点，那就是高温敏感、寿命短、漏电流大。因为铝电解电容内部基本是液态的电解质，当温度过高时，内部电解质会被加热导致挥发甚至是爆浆（有兴趣的读者，可以网上搜索“铝电解电容爆炸的各种姿势，还是挺有意思的”）。而且，如果长期在高温下使用，可能会造成电解质挥发至干涸，最终电容变成一具“干尸”。
当然，做SSD设计时就已经考虑到了铝电解电容的这些特征，因此做了冗余设计，保证电容能够持续工作5年以上。但，基于铝电解电容的这些特性，还是需要定时去检查一下有没有问题，防止电容失效而引起数据丢失灾难。一般每隔24小时检查一次，当然你也可以设置更长或更短时间间隔。
这就像，我们日常开的车，刹车片虽然不用每次保养都去更换，但每次保养都要去检查一下磨损程度。同样的道理。
BootLoader存放区域扫描 Bootloader的用途，做产品研发的同学应该是清楚的。它是一段非常精简的代码，用来引导整个固件或者系统代码的启动。
因此，它真的非常重要！
所以，BootLoader代码一般都不会存放在NAND颗粒中，而是在一颗很小的SPI Flash中，这个SPI Flash的可靠性要远高于NAND闪存颗粒。
但，即使这样，对于BootLoader存放区域的介质健康检查不可省略，谁叫它这么重要呢？至于扫描的时间间隔，每家公司设计不同，可自行决定。
就像，你在家里的密码箱存放了100斤黄金，就算它足够安全不会被偷，你也还是想每天晚上睡觉前打开柜子看看确保安全呢。
系统区域扫描（固件、坏块表存放区） 系统区域，一般是指固件在NAND闪存内部开辟出来的多个高安全区域，这些区域的特点是，相比于其他区域，他们更加安全，更不容易出现bit error。
在这些最安全的系统区域，我们会用来存放固件代码和备份、坏块表等数据，因为固件代码和坏块表足够重要，所以会有很多的备份存放在多个不同的系统区域。
但是，最安全的系统区域，也只是相对安全，并非绝对安全。因此，对这些区域的扫描不可省略，一旦发现某些闪存块有问题，就要立马将数据搬到新的区域。至于扫描的时间间隔，一般是2-3周，或200-300个上下电循环，企业级一般是采用固定时间间隔很少采用上下电循环，消费类盘都可以。
用户数据区域后台扫描 用户数据区域，和刚刚说的系统区域，是一个相对的概念。
相对于系统区域，用户数据区域的数据安全性相对低一些，但也是相当重要的，毕竟用户就是上帝，上帝的数据绝不能丢。
但为什么需要定时去扫描用户的数据区域呢？因为所有的NAND闪存都有一个缺点，包括英特尔家的floating gate架构、或者其他家的charge trap架构，均存在电荷泄漏的问题。
意思是，当已经保存有数据的NAND闪存断电时，其cell内部的电子会逐渐逃逸出去，导致电荷泄漏，最终数据丢失。当然，不要过度恐慌，这是一个非常缓慢的过程。正常情况下，企业级SSD断电情况下，数据可以在40摄氏度温度下安全存放3个月，即使是生命末期。而消费类的SSD，则数据可以在30摄氏度的温度下安全存放1年的时间（所以，如果有好久没有用过且存有重要数据的SSD，要赶紧拿出来溜溜了，通上电跑一跑）。
它有个专业术语，叫数据保持-data retention。
因此，一旦我们的SSD掉电一段时间后，尤其是经历过高温的情况，之前保存的数据可能会陷入险境之中。这时候，一旦重新上电，SSD固件就要开始马不停蹄地去检查，哪些数据存在风险，要赶紧搬移到新的区域。
温度传感器采样 温度传感器采样，这个比较简单，因为很多情况下，系统需要获取到SSD内部的温度，来决定散热风速的转速。
因此，温度传感器的采用必不可少，一般都是每秒钟采样一次。
Log日志定时写入NAND 都知道Windows/Linux操作系统有各种各样的log日志，来记录系统的状态和debug用途。SSD也是一样，它内部也运行了一套庞大的类似系统一样的复杂逻辑和运算，因此也会存在各种各样的Log日志来记录系统状态、错误信息等内容。
尤其是在SSD研发早期阶段，大量的debug工作非常依赖于系统log。同时，在如今的大型数据中心，越来越多的用户开始关注SSD Log，因为对于数据中心的运维来说，健全的Log可以大大提高debug效率。
但，因为很多时候，最新Log其实是存放在DRAM中的，因为这样可以更快去刷新。而DRAM是易失性存储，即掉电数据丢失，因此，系统Log日志，同样也是需要定期维护刷写到非易失性的NAND闪存中的。至于刷写的时间间隔，因需求而异，可以设置5分钟，也可以设置半小时。
Read Disturb数据搬移 Read Disturb，读干扰，也是NAND闪存的其中一个缺点。具体的原理呢，会比较复杂，一时半会很难说清楚，后续有时间单独开一篇来讲。
这里，只简单说一下读干扰所带来的结果。因为现代NAND制程的进度，密度增加，导致cell之间的距离越来越小。而读干扰的现象也越来越明显，即当我们读取某个页数据非常多次数时（可能是200万~1000万次），会对它周边的页造成干扰，而影响到周边页的数据，严重时会造成周边页数据无法读取。
因此，当我们选择一款NAND开发SSD时，NAND厂商的FAE会告诉我们，read disturb建议的次数时xx次。
当固件发现某个页被连续读取了xx次时，触发了read disturb阈值，此时就要将周边页的数据进行搬移，防止数据丢失。
后台GC垃圾回收 后台GC垃圾回收，这个已经不用说太多了吧。
因此NAND闪存本身的特性，最小写入单位是页，而擦除最小单位是块。因此随着数据不断写入，每个块上都会随机散布着各种数据，这时候如果有新的数据再写进来，则可能没有足够的空间，这时候就需要擦除旧的数据块，并搬移有效数据到新的块。
当经历过了长时间的随机写数据时，这部分GC的工作量是非常大的，而且非常影响这个SSD的性能。
磨损均衡 二八定律无处不在，又或者说，正态分布才是自然的常态。
经过不正经的调查，当没有任何认为干预时，我们随机性地往SSD中写入数据。
当我们写满全盘时，没有太大发现，但是当写了很多次全盘之后，就会发现。
NAND闪存中仅有20%的区域被写入了超过80%的数据，而另外80%的区域却并没有被写入太多数据。
这样会造成一个问题，因为NAND闪存都是有寿命的，而这种写入方法会造成20%的NAND闪存块很快被写完，寿命殆尽。而剩下80%的NAND闪存块却还可以用很久。
但这20%的闪存块的退役，会导致整个SSD系统无法正常工作，相当于一个本来完整的木桶，突然出现了一块短板，导致桶里的水大量流失。
因此，Wear Leveling磨损均衡才被需要，而且相当重要，因为它决定了你的木桶到底能装多少水！
最后 写到这里，我虽意犹未尽，但也累得腰酸背痛想要休息片刻。
不知各位读者看完，作何感想，能写SSD固件代码固然不难，但如果要做好固件真心不易。
]]></content>
  </entry>
  
  <entry>
    <title>新的架构，更快的芯片</title>
    <url>/post/soc/faster-chip-with-new-architecture.html</url>
    <categories><category>SOC</category>
    </categories>
    <tags>
      <tag>Chip</tag>
    </tags>
    <content type="html"><![CDATA[芯片行业在多个物理维度和多种架构方法方面取得了进展。基于更多模块化和异构设计、新的先进封装选项以及至少几个工艺节点的数字逻辑的持续扩展，为性能的巨大提升奠定了基础。
大规模创新，推动性能提升数量级。 在最近的会议上讨论了其中的一些变化。就部分个体而言，它们具有潜在的意义。但从整体上看，随着设备扩展的好处减少和市场需求的变化，他们指出了一些重要的趋势。其中：
 对于高性能应用，芯片的设计基于更有限的数据移动和近内存计算。这可以从I/O位于芯片周边而不是中心的平面图中看出，这种方法将通过减少数据需要传输的距离来提高性能，从而降低整体功耗。 使用高数值孔径 EUV、各种全栅极 FET（CFET、纳米片/纳米线 FET）和碳纳米管器件，数字逻辑的缩放将继续超过 3nm。同时，光罩尺寸将增加，以允许更多的组件适合封装，如果不是单个芯片。这两项举措都将通过缩小功能来增加更多的空间，从而实现更高的计算密度。此外，SRAM的扩展将继续进行，并将为高带宽存储器（HBM）模块和3D-NAND闪存添加更多层。 设计正变得越来越模块化和异构化，为更多的定制和更快的上市时间奠定了基础。所有主要的晶圆代工厂和OSAT现在都支持小芯片战略，他们根据价格和性能要求提供多种选择。  其中一些已经进行了多年，但大部分开发都是零碎的。不再有单一的行业路线图，过去一直被用作所有发展如何结合在一起的指南。在没有该路线图的情况下，所有方面的工作仍在继续，但通常很难理解大局是如何发展的，因为并非一切都在同步发展。例如，ASML甚至在EUV商业上可行之前就公开谈论高数值孔径EUV，它用变形透镜取代了平面透镜。在这十年的大部分时间里，ASE 和 Amkor 等公司一直在开发多个版本的扇出、2.5D 和 3D-IC，尽管这些封装方案的市场与最初想象的截然不同。
还有许多新的发展即将到来。台积电、联电、GlobalFoundries和三星等主要晶圆代工厂正在将先进的封装能力融入到制造后端。台积电还计划使用无凸块混合键合（称为SoIC）将小芯片添加到前端。所有这些都可能需要整个行业进行重大变革，从EDA工具到测试和硅后监控。
目前尚不清楚所有这些不同元素的结合速度有多快。没有人喜欢成为第一，在这一点上，这些方法和技术中哪一种会获胜，甚至它们是否会相互竞争，都不清楚。但是，随着数据量的持续增长，改变是必不可少的。这推动了更多定制的解决方案，以在更接近源头的地方处理和利用这些数据，其中包括几乎无处不在的某种程度的智能。
过去，解决方案是围绕最先进的硬件或软件开发的，其假设是下一代工艺将大大提高性能。这不再起作用。扩展变得越来越困难和昂贵，缩小功能的功耗/性能优势正在减弱。此外，一种尺寸不再适合所有人。根据最终客户在计算层次结构中的位置（端点、边缘或云）以及需要如何构建数据和确定其优先级，它可能会有很大差异。因此，芯片制造商已将重点转移到新的、更模块化的架构上，这些架构能够从云中的大规模模拟和训练算法，到从源头剔除无用的图像和流式视频数据。
从长远来看，需要在任何地方更快地进行更多的处理，并且需要使用相同或更少的功率来完成。此外，系统需要更快地创建，并且随着市场需求的发展和算法的不断变化，它们需要能够更快地发生变化。
架构转变 为了实现这一目标，硬件架构需要改变。芯片制造商已经看到这种情况已经有一段时间了。例如，IBM 的新型 Power 10 芯片将定制的计算元素集中在芯片的中心，并将外围设备和 I/O 移动到边缘。
“加速需要被推到处理器核心中，”该芯片的首席架构师比尔·斯塔克（Bill Starke）在最近的Hot Chips会议上说。“芯片周边是PHY。”IBM 还引入了 Pod 级集群，并添加了一个新的微架构来支持所有这些。
其他的也在采取类似的方法。英特尔推出了一种基于内部开发的小芯片的新架构，该架构使用其嵌入式多芯片互连桥接到 HBM 模块，将模块化处理元件聚集在一起。此外，它还更新了其最新的服务器芯片架构，以最大限度地减少数据移动。
同样，制造人工智能系统的 Tenstorrent 创建了一个高度模块化的系统，其中包括 120 个独立的内核，这些内核与 2D 双向环面 NoC 相连。“每个核心都按照自己的节奏发展，”Tenstorrent 软件工程总监 Jasmina Vasiljevic 说。
扩展仍在继续 数据中心芯片对成本的敏感度远低于消费类应用，因此它们在性能方面往往处于行业领先地位。例如，高性能服务器将芯片开发成本与系统价格摊销，而不是通过体积来分摊，这对于手机应用处理器来说至关重要。因此，尽管关于摩尔定律终结的预测永无止境，但出于密度原因，许多这些设备中的数字逻辑将继续使用最新的工艺几何形状。
然而，不同的是，对性能不太关键的电路以及模拟模块越来越多地被分流到单独的芯片上，这些芯片使用高速接口连接。
“你现在可以按节点进行分区，”西门子旗下公司 Mentor 的产品总监 Matt Hogan 说。“因此，您可以确定设计特定部分的正确技术是什么。这也允许你扩大一些副作用。
戈登·摩尔（Gordon Moore）在1965年首次发表他现在著名的观察结果时提到了这种方法。
Synopsys首席应用工程师Tim Kogel表示：“随着工艺技术的快速发展，使用现成的解决方案通常比开发定制芯片更便宜。“到现在为止，每个新工艺节点的高性能和低功耗的免费午餐几乎已经结束。另一方面，人工智能、自动驾驶、AR/VR等杀手级应用对处理能力和计算效率的需求不可抑制。谷歌的TPU和特斯拉的FSD芯片等著名例子表明，根据目标工作负载的特定特征定制架构，其投资回报率令人印象深刻。
尽管如此，摩尔定律的价值正在减弱，这既有经济意义，也有技术意义。平面缩放的经济效益随着finFET的引入而结束，当时每个晶体管的成本从前一个节点停止下降。同样，自90nm以来，功耗/性能优势一直在下降。台积电研发高级副总裁Y.J. Mii表示，在相同功率下，3nm将带来10%至15%的性能提升，或在相同速度下降低25%至30%的功耗。
然而，从技术角度来看，这并不是一条死胡同。架构改进，包括不同的封装方法和 3D 布局，可以将性能提高几个数量级。缩放仍然有助于将更多的密度封装到这些封装中，即使缩小的晶体管本身的运行速度并没有明显加快。
“多年来，我们一直被超越摩尔定律的话题轰炸，”Cadence设计IP营销总监Tom Wong说。“但是，真的是面积减小、功耗降低或晶体管性能改进（传统PPA）推动了这些讨论，还是硅经济性和光刻/设备的局限性导致我们撞上了砖墙？事实证明，硅经济性和掩模版尺寸的限制是推动颠覆的两大因素，这促使设计人员寻找新的芯片设计方法，并转向新的架构。
通过不同的封装方案和光罩尺寸的增加，经济性和光罩尺寸限制都得到了解决，从而允许更大的单个芯片。台积电研发副总裁Doug Yu表示，通过晶圆代工厂的InFO（集成扇出）封装方法，掩模版尺寸将增加1.7倍。此外，台积电计划在明年第一季度推出 110 x 110 mm² 的光罩，这将使光罩尺寸增加 2.5 倍。
所有这些都是必要的，因为将所有东西都放在一个芯片上的成本不断上升。模块化允许芯片制造商根据平台类型的方法相对快速地定制芯片。CPU、GPU 和 FPGA 芯片设计人员在五年多前就发现了这一点，此后开始向分解实现迈进，采用多芯片，让中介层/封装负责集成。Wong说，这就是为什么die-to-die连接IP成为当今舞台的中心舞台的原因之一。
“CPU、GPU 和 FPGA 都走上了小芯片的路线，因为这些公司自己设计芯片（小芯片），不需要依赖商业小芯片生态系统。他们可以利用基于小芯片的设计可以提供的功能，“Wong指出。“包括 CPU、GPU 和 FPGA 在内的多核设计可以从这种架构变化/趋势中受益。能够分离“核心计算”和高速 I/O 的 SoC 设计也可以从中受益。AI 加速 SoC 和加密 SoC 就是两个例子。数据中心交换机和结构（例如用于超大规模计算和云构建者的 25.6Tb/s）也可以从这种架构更改为基于小芯片的设计中受益。这些设计可以像 200 亿+ 晶体管一样复杂。
到目前为止，Intel、AMD和Marvell等IDM已经采用了这种方法，每个IDM都创建了自己的模块化方案和互连。因此，他们没有制造芯片并试图向广泛的客户推销其优势，而是使用小芯片提供一系列选项，在英特尔的情况下，还提供各种连接选项，例如高速桥接器。
变化无处不在，有大有小 正确看待所有这些变化通常很困难，因为整个行业都在运动，尽管不一定以相同的速度或出于相同的原因。因此，例如，当处理器和进程发生变化时，内存会远远滞后。
此外，有些技术需要完全重新思考，而另一些技术则保持不变。这在 GPU 中尤为明显，GPU 一直是 AI/ML 训练的首选解决方案，因为它们价格便宜且可扩展。但它们并不是最节能的方法。
Imagination Technologies产品管理和技术营销高级总监Kristof Beets表示：“我们已经看到了带宽，我们已经看到了它的强大功能，”所有这些不同的限制都会发挥作用。从 GPU 的角度来看，这是一个棘手的演变，因为显然 GPU 是巨大的数字运算器，显示器越来越大，设备越来越小。所以很多这样的问题一直在发生。有一个蛮力阶段，这有点取决于摩尔定律。我们将 GPU 翻了一番，这在一段时间内是可以的，因为工艺技术跟上了。但现在回报正在减少，所以虽然我们可以放下更多的逻辑，但我们基本上不能再打开它了，因为它消耗了太多的电量。所以蛮力的方式是行不通的。
动态电压和频率调节 （DVFS） 在一定程度上有助于降低电压，从而允许更大的 GPU 在较低频率下运行。然而，即使是这种方法也有局限性，因为在固定的功率预算内可以使用的 GPU 内核数量有限。“这为我们提供了更好的每瓦FPS（每秒帧数），但即使这样，现在也开始变慢，因为泄漏再次上升，”Beets说。“对于 GPU 来说，这就是光线追踪的有趣之处。这是一种摆脱蛮力的方式。它们非常灵活。我们在人工智能和神经网络处理方面也看到了同样的情况。这是完全相同的概念。在这里，你真正看到了比GPU好10到20倍的解决方案，考虑到数据流和具体操作，所以这很有趣。它并不像过去的固定函数处理那么糟糕。我们还没有回到那里。但其中一些肯定会开始以更专用的处理类型回归。
有许多方法可以增强扩展性能。Codasip高级营销总监Roddy Urquhart表示：“在一些领域，例如应用处理器、GPU、MCU、DSP，我们已经有相当通用的架构利用摩尔定律来做越来越多的事情。“但现在有大量的想法围绕着尝试新颖的架构，新颖的结构，具有一系列可编程性。在脉动阵列端，有些东西往往是硬连线的处理元素，或者它们具有已上传固件并处于静态状态一段时间的进程。另一个极端是特定于域的进程，这些进程是高度可编程的。我看到了高度并行、高度流水线的数组类型结构的创新回归，这与不同类型的神经网络非常契合。另一方面，人们更多地跳出框框思考，以摆脱MCU、GPU、DSP和应用处理器的孤岛，并创造一些更像这些东西的混合版本来满足特定需求。
微架构 除了这些广泛的架构转变之外，还有微架构创新。在许多方面，这是一个分区问题，在更大的系统中，某些计算功能优先于其他计算功能。这可能会对性能和计算效率产生重大影响。
“利用固有的并行性，应用程序应该映射到一组最佳的异构处理元素，”Synopsys的Kogel说。“为每个函数选择一个提供最低灵活性的处理内核，可以提供尽可能高的计算效率。此外，内存架构的组织对性能和功耗有非常大的影响。由于外部存储器访问成本高昂，因此数据应保存在片上存储器中，靠近处理位置。
然而，这说起来容易做起来难，它需要多学科和越来越多的多维规划。Kogel说：“管理复杂性并预测在具有分布式内存的异构多处理平台上运行的高度并行应用程序的动态效果是一个相当大的挑战。“我们建议在开发过程的早期使用虚拟原型来定量分析架构权衡。这使得来自应用程序和实施团队的利益相关者能够在承诺实施规范之前进行协作。
新的平衡 展望未来，如何进行功耗和性能的权衡取决于市场。一些市场对成本高度敏感，因此他们还没有解决这个问题。同时，其他软件对成本的敏感度较低，对延迟的敏感度较高。
“人们越来越不耐烦了。你想尽快得到你想要的东西，“英特尔首席技术官迈克·梅伯里（Mike Mayberry）在DARPA最近的电子复兴倡议（ERI）峰会的小组演讲中说。“但我们也看到了平衡的系统和更多的计算能力，这是我们看到的持续趋势之一。
Mayberry 指出，密度缩放没有硬性停止，但它将越来越多地包括 Z 轴。“我们还看到了新型的超越CMOS器件，这些器件将支持异构架构。十年后，你会看到这些在货架上。
英特尔等公司正在研究除了沉积和蚀刻不同材料之外的器件生长方法。多年来，人们一直在谈论定向自组装等方法。在某种程度上，这在经济上仍然是可行的，但普遍的共识可能要等到3nm之后。
除此之外，光子学也开始积聚一些动力，以最小的热量在这些日益密集的结构中和周围移动大量数据。一种更新颖的方法涉及使用光进行处理。LightMatter 首席执行官尼克·哈里斯 （Nick Harris） 表示，光学设备消除了泄漏效应，从而降低了热量并提高了性能。这种方法特别独特的是，光可以被划分为不同的波长，从而可以优先考虑不同的颜色。
“使用100GHz波长，这是一个非常小的间距，我们可以容纳1000种颜色，”哈里斯说。缺点是激光器不会永远持续下去，因此需要有足够的冗余来使这些系统在其预期的使用寿命内持续使用。
对于更传统的计算，进程节点选项的数量也在增加。晶圆代工厂提供中间节点，无需完全重新设计即可提高性能或功耗。例如，台积电（TSMC）打开了其N4工艺的瓶塞，该工艺将于明年年底进入风险生产。台积电首席执行官魏中瀛在演讲中表示，N5（5nm）和N4中使用的IP将兼容，这使公司能够以最小的重新设计来提高密度和降低功耗。
尽管如此，选项的数量仍然令人眼花缭乱。除了不同的节点数量外，还有不同的工艺选项，以实现低功耗和高性能。最重要的是，不同的衬底材料开始受到关注，包括用于功率晶体管的碳化硅和氮化镓，以及用于低成本、低功耗应用的绝缘体上硅。
所有这些都对用于防止故障的设计规则有很大影响。“如果你正在设计一个小芯片，你不知道它将如何使用或放置，”Mentor的Hogan说。“你不知道它是否会在MCU旁边，所以你必须弄清楚如何以一种深思熟虑的方式做到这一点。你需要保护它免受电磁效应和其他潜在问题的影响。
而且，由于芯片有望在更长的时间内正常运行——就汽车而言，前导节点逻辑可能需要长达 18 年——所有这些都需要在老化的背景下完成。这可能会变得非常复杂，尤其是在多芯片封装中。
Ansys半导体业务部营销副总裁兼首席策略师Vic Kulkarni表示：“您需要关注不同刺激和场景的阈值变化等因素。“你可以对寄存器进行精确分析，但如果 Vdd 没有下降，Vt 也没有下降，那么剩下的余量就不多了。您还需要考虑诸如电气过载之类的事情。晶圆厂不愿意接受这一点。
权衡范围从功耗、性能和成本到服务质量。
“我们过去总是有无损压缩，”Imagination 的 Beets 说。“大约一两年前，我们也推出了有损，这样我们就可以在质量上进行权衡。在GPU中，我们开始看到质量与成本的权衡，有损压缩可以降低质量，这也节省了带宽和功耗。在 GPU 处理中，我们开始看到同样的东西，即可变速率着色。这基本上是当你看视频时，你会说你真正关心的只是脸，你想要完整的细节，所以背景并不重要。游戏本质上是做同样的事情。例如，在赛车游戏中，汽车非常清晰，有很多细节，但其余部分都有运动模糊。
在精度方面也存在权衡。较低的精度可以大大加快处理速度，而稀疏算法可以写得不那么精确，无论是 16 位精度还是 1 位精度。但是，这种精度也可以由硬件和固件控制，并且会对整体系统性能产生重大影响，其中某些功能比其他功能更准确。
结论 在摩尔定律的前 40 年左右，功耗、性能和面积的改进对于大多数应用程序来说已经足够了，并且数据的增长通常可以通过经典扩展来管理。在90nm之后，经典缩放开始显示出压力的迹象。虽然已经很长时间没有注意到了，但它并没有被忽视。
然而，令人惊讶的是，仍然有多少途径可用于大幅提高性能、降低功耗和潜在的成本节约。工程团队正在以新颖有趣的方式进行创新。几十年来对当时看似晦涩难懂的话题或切线的研究现在正在得到回报，还有很多研究正在酝酿中。
]]></content>
  </entry>
  
  <entry>
    <title>STM32高阶应用--使用DFU方案实现固件升级</title>
    <url>/post/mcu/using-DFU-solution-to-update-the-firmware-for-stm32.html</url>
    <categories><category>MCU</category>
    </categories>
    <tags>
      <tag>ARM</tag>
      <tag>STM32</tag>
      <tag>DFU</tag>
    </tags>
    <content type="html"><![CDATA[DFU全称为Device Firmware update，是ST官方推出的一个通过USB接口进行IAP升级的方案，同串口ISP一样，他们都集成在了芯片内部的Bootloader区段，可以通过配置boot引脚来启动。（具体可参照ST文档：AN2606）。
什么是 DFU 不过内置DFU的芯片大部分型号都比较新，如果你用的型号没有内置DFU程序，没关系我们也可以通过CubeMX来快速生成和移植一个DFU功能程序到你的Flash中来使用。
DFU方案完整的组件包括单片机DFU Demo代码、PC端升级程序、PC端Demo代码以及相关资料手册等。
通过使用DFU方案，我们可以快速的集成升级功能到开发的产品中，同时还能够快速的开发与之配套的升级程序。
使用CubeMX生成初始工程 由于官方提供的DFU例程并不多，我们很难找到现成的可已使用DFU程序，但是通过CubeMX我们可以很快速的配置和生成DFU的Bootloader，下面我们正式开始。
 新建CubeMX工程  首先选定好IC的型号，进入配置界面，由于只是Bootloader代码所以这里我们只需要配置USB功能和一个做Bootloader触发的引脚就可，其余的时钟等部分一切按照正常方式配置。
设置USB引脚功能  设定USB模式为Device（HS还是FS并不影响DFU的功能，按照应用选择就可）。
开启DFU组件  在MiddleWares中加入USB DFU组件
设置DFU参数
开启DFU组件后，CubeMX的程序设置窗口的MiddleWares中就会出现DFU程序设置按钮。
点开它将APP加载的地址改为0x0800_c000，这个加载地址根据你实际的应用设置，目前我们选择让flash的前三个sector为Bootloader的区域。
第二个全是字段的参数是用来在DFU连接升级软件式传输给软件用来获取Flash结构字符串数据，很好理解这个小协议的内容，点击设置后，下方的CubeMX的参数说明也写的很清晰，这里就不多说了。
当然这些参数也在工程生成后在 usbd_conf.h 和 usbd_dfu_if.c 文件中修改。
最后的设置  最后我们添加一个外部的按键作为触发单片机启动时进入DFU的方式，按键按下后就启动DFU模式，否则直接加载后方APP程序，这里选用PA0引脚，给它设置个User Label 就叫 USER_BTN_GPIO_Port。
修改补全工程  实现 DFU 功能代码  打开 src 目录下的 usbd_dfu_if.c 文件补全其中的功能代码
Flash 解锁
uint16_t MEM_If_Init_HS(void) { HAL_FLASH_Unlock(); return (USBD_OK); } Flash 上锁
uint16_t MEM_If_DeInit_HS(void) { HAL_FLASH_Lock(); return (USBD_OK); } Flash 擦除
static uint32_t GetSector(uint32_t Address) { uint32_t sector = 0; if ((Address &lt; ADDR_FLASH_SECTOR_1) &amp;&amp; (Address &gt;= ADDR_FLASH_SECTOR_0)) { sector = FLASH_SECTOR_0; } ...... } else if ((Address &lt; ADDR_FLASH_SECTOR_23) &amp;&amp; (Address &gt;= ADDR_FLASH_SECTOR_22)) { sector = FLASH_SECTOR_22; } else { sector = FLASH_SECTOR_23; } return sector; } uint16_t MEM_If_Erase_HS(uint32_t Add) { uint32_t startsector = 0; uint32_t sectornb = 0; /* Variable contains Flash operation status */ HAL_StatusTypeDef status; FLASH_EraseInitTypeDef eraseinitstruct; /* Get the number of sector */ startsector = GetSector(Add); eraseinitstruct.TypeErase = FLASH_TYPEERASE_SECTORS; eraseinitstruct.VoltageRange = FLASH_VOLTAGE_RANGE_3; eraseinitstruct.Sector = startsector; eraseinitstruct.NbSectors = 1; status = HAL_FLASHEx_Erase(&amp;eraseinitstruct, &amp;sectornb); if (status != HAL_OK) { return (USBD_FAIL); } return (USBD_OK); } Flash 写入
uint16_t MEM_If_Write_HS(uint8_t *src, uint8_t *dest, uint32_t Len) { uint32_t i = 0; for (i = 0; i &lt; Len; i += 4) { /* Device voltage range supposed to be [2.7V to 3.6V], the operation will be done by byte */ if (HAL_FLASH_Program(FLASH_TYPEPROGRAM_WORD, (uint32_t)(dest + i), *(uint32_t *)(src + i)) == HAL_OK) { /* Check the written value */ if (*(uint32_t *)(src + i) != *(uint32_t *)(dest + i)) { /* Flash content doesn&#39;t match SRAM content */ return (USBD_FAIL); } } else { /* Error occurred while writing data in Flash memory */ return (USBD_FAIL); } } return (USBD_OK); } Flash 读取
uint8_t *MEM_If_Read_HS(uint8_t *src, uint8_t *dest, uint32_t Len) { /* Return a valid address to avoid HardFault */ uint32_t i = 0; uint8_t *psrc = src; for (i = 0; i &lt; Len; i++) { dest[i] = *psrc++; } /* Return a valid address to avoid HardFault */ return (uint8_t *)(dest); } 获取 Flash 擦写时间参数
``c uint16_t MEM_If_GetStatus_HS(uint32_t Add, uint8_t Cmd, uint8_t buffer) { / USER CODE BEGIN 11 */ uint16_t time;
 time = TimingTable[GetSector(Add)]; switch (Cmd) { case DFU_MEDIA_PROGRAM: buffer[1] = (uint8_t)time; buffer[2] = (uint8_t)(time &lt;&lt; 8); buffer[3] = 0; break; case DFU_MEDIA_ERASE: default: buffer[1] = (uint8_t)time; buffer[2] = (uint8_t)(time &lt;&lt; 8); buffer[3] = 0; break; } return (USBD_OK); /* USER CODE END 11 */  }
usbd_dfu_if.h 文件添加的宏定义 ```c /* Define flash address */ // BLANK 1 #define ADDR_FLASH_SECTOR_0 0x08000000 #define ADDR_FLASH_SECTOR_1 0x08004000 #define ADDR_FLASH_SECTOR_2 0x08008000 #define ADDR_FLASH_SECTOR_3 0x0800C000 #define ADDR_FLASH_SECTOR_4 0x08010000 #define ADDR_FLASH_SECTOR_5 0x08020000 #define ADDR_FLASH_SECTOR_6 0x08040000 #define ADDR_FLASH_SECTOR_7 0x08060000 #define ADDR_FLASH_SECTOR_8 0x08080000 #define ADDR_FLASH_SECTOR_9 0x080A0000 #define ADDR_FLASH_SECTOR_10 0x080C0000 #define ADDR_FLASH_SECTOR_11 0x080E0000 // BLANK 2 #define ADDR_FLASH_SECTOR_12 0x08100000 #define ADDR_FLASH_SECTOR_13 0x08104000 #define ADDR_FLASH_SECTOR_14 0x08108000 #define ADDR_FLASH_SECTOR_15 0x0810C000 #define ADDR_FLASH_SECTOR_16 0x08110000 #define ADDR_FLASH_SECTOR_17 0x08120000 #define ADDR_FLASH_SECTOR_18 0x08140000 #define ADDR_FLASH_SECTOR_19 0x08160000 #define ADDR_FLASH_SECTOR_20 0x08180000 #define ADDR_FLASH_SECTOR_21 0x081A0000 #define ADDR_FLASH_SECTOR_22 0x081C0000 #define ADDR_FLASH_SECTOR_23 0x081E0000 /* Flash oprate time from datasheet page 128 */ #define FLASH_SECTOR_16KB_WRITE_ERASE_TIME 500 //500 usb frame,means 500ms #define FLASH_SECTOR_64KB_WRITE_ERASE_TIME 1100 #define FLASH_SECTOR_128KB_WRITE_ERASE_TIME 2000 修改Main文件  首先在main文件前添加几个用于加载APP程序的变量和函数定义
typedef void (*pFunction)(void); pFunction JumpToApplication; uint32_t JumpAddress;1234 然后再 main 函数中加入 外部按键的判断、APP程序加载以及USB DFU初始化功能
int main(void) { /* Reset of all peripherals, Initializes the Flash interface and the Systick. */ HAL_Init(); /* Configure the system clock */ SystemClock_Config(); /* Initialize all configured peripherals */ MX_GPIO_Init(); if (HAL_GPIO_ReadPin(USER_BTN_GPIO_Port, USER_BTN_Pin) == GPIO_PIN_SET) { HAL_GPIO_WritePin(GPIOG, LD3_Pin, GPIO_PIN_SET); // For debug  /* Test if user code is programmed starting from address 0x0800C000 */ if (((*(__IO uint32_t *)USBD_DFU_APP_DEFAULT_ADD) &amp; 0x2FF80000) == 0x20000000) { HAL_GPIO_WritePin(GPIOG, LD4_Pin, GPIO_PIN_SET); // For debug  /* Jump to user application */ JumpAddress = *(__IO uint32_t *)(USBD_DFU_APP_DEFAULT_ADD + 4); JumpToApplication = (pFunction)JumpAddress; /* Reset of all peripherals */ HAL_DeInit(); /* Set interrupt vector to app code */ SCB-&gt;VTOR = USBD_DFU_APP_DEFAULT_ADD; /* Initialize user application&#39;s Stack Pointer */ __set_MSP(*(__IO uint32_t *)USBD_DFU_APP_DEFAULT_ADD); JumpToApplication(); } } MX_USB_DEVICE_Init(); while (1) { } } 编译程序下载进入单片机  使用DfuSe 从ST官网DfuSe的程序安装包，并安装。然后我们按下之前写的触发按键并复位单片机，让单片机初始 USB DFU 功能，这时如果你插着单片机的USB线，系统应该已经识别了。
如果没有右键更新驱动程序，手动指定驱动搜索路径在DfuSe安装目录下的 \Bin\Driver 内。如果直接无法识别USB设备，建议在CubeMx配置完工程后就编译下载测试一下，看看是不是你在移植过程中哪里写错了。
然后我们需要生成一个地址设定在0x0800_c000后的测试程序，就先编写一个 Blink LED 的程序吧，生成bin、hex或S19文件。
然后我们打开DfuSe软件的Dfu file manager来生成DFU软件用的.dfu格式的文件。选择第一项，第二个是用来将.dfu反向变换回来的。
大概的操作已经标在图片上了，操作比较简单就不做详细介绍了，记得把Address的地址改到偏移后的地址上否则下载会出错，其他参数可以不用修改。
然后我们打开DfuSe程序，在Upgrade中选择生成好的blink.dfu文件，勾选校验功能，下载程序。成功后复位单片机，LED开始闪烁，移植成功。
更多 仔细区看看DfuSe的安装目录，里面有DFU的资料文档，还有DFU的工程源代码，可以用来改写自己需要的DFU升级程序。
参考资料 ST官网DfuSe
http://www.stmicroelectronics.com.cn/content/st_com/zh/products/development-tools/software-development-tools/stm32-software-development-tools/stm32-programmers/stsw-stm32080.html ]]></content>
  </entry>
  
  <entry>
    <title>C语言中结构体struct的用法</title>
    <url>/post/programming/usage-of-struct-in-c.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>C</tag>
      <tag>Embedded</tag>
      <tag>Struct</tag>
    </tags>
    <content type="html"><![CDATA[本文简要介绍了c语言中结构体Struct的用法。
定义结构体变量 下面举一个例子来说明怎样定义结构体变量。
struct string { char name[8]; int age; char sex[2]; char depart[20]; float wage1, wage2, wage3, wage4, wage5; }person; 这个例子定义了一个结构名为string的结构体变量person。还可以省略变量名person，做如下定义：
struct string { char name[8]; int age; char sex[2]; char depart[20]; float wage1, wage2, wage3, wage4, wage5; }; struct string person; //定义结构名为string的结构体变量person 定义多个具有相同形式的结构变量：
struct string Liming, Liuqi, ...; 有一种结构体常常出现在函数内部，省略结构名，则称之为无名结构，如下：
struct { char name[8]; int age; char sex[2]; char depart[20]; float wage1, wage2, wage3, wage4, wage5; } Liming, Liuqi; 结构体成员的获取与赋值 结构是一个新的数据类型，结构成员的表示方式为:
结构变量.成员名 如果将&quot;结构变量.成员名&quot;看成一个整体，这样就可以像其他变量那样使用。
下面这个例子定义了一个结构变量stu1，分别给name、age、group赋值，并打印输出。
#include &lt;stdio.h&gt;int main() { struct { char *name; //姓名  int age; //年龄  char group; //所在小组  } stu1; //给结构体成员赋值  stu1.name = &#34;Tom&#34;; stu1.age = 18; stu1.group = &#39;A&#39;; //读取结构体成员的值  printf(&#34;%s的年龄是%d，在%c组\n&#34;, stu1.name, stu1.age, stu1.group); return 0; } 结构体数组 结构体数组就是具有相同结构类型的变量集合，假如要定义一个班级40个同学 的姓名、性别、年龄和住址, 可以定义成一个结构数组。如下所示：
struct { char name[8]; char sex[2]; int age; char addr[40]; }student[40]; 结构体数组成员的访问是以数组元素为结构变量的, 其形式为:
结构数组元素.成员名 例如：
student[0].name student[30].age 结构体指针 结构体指针由一个加在结构变量名前的*操作符来定义，定义一个结构体指针如下：
struct string { char name[8]; char sex[2]; int age; char addr[40]; }*student; 使用结构体指针对结构体成员的访问与结构体变量对结构体成员的访问在表达方式不同。结构体指针对结构体成员的访问方式为：
结构体指针名-&gt;结构体成员 给上面定义的结构体中name和age赋值的语句：
strcpy(student-&gt;name, &#34;acket&#34;); //student-&gt;name就是(*student).name student-&gt;age=18; 需要指出的是结构体指针是指向结构体的一个指针，即结构体中第一个成员的首地址，因此在使用之前应该对结构体指针初始化，即分配整个结构体长度的字节空间：
student=(struct string*)malloc(size of (struct string)); //size of (struct string)是自动求取string结构体的字节长度 malloc()函数定义了一个大小为结构体长度的内存区域，然后将其地址作为结构体指针返回。
位结构 位结构是一种特殊的结构体，位结构定义的一般形式为：
struct 位结构名 { 数据类型 变量名: 整型常数; 数据类型 变量名: 整型常数; }位结构变量; 其中数据类型必须是int（unsigned或signed，但当成员长度为1时, 会被认为是unsigned类型），整型常数必须是0~15的非负整数，表示二进制位个数。变量名是选择项，可以不命名，下面定义了一个位结构：
struct { unsigned incon: 8; /*incon占用低字节 的0~7共8位*/ unsigned txcolor: 4;/*txcolor占用高字节的0~3位共4位*/ unsigned bgcolor: 3;/*bgcolor占用高字节的4~6位共3位*/ unsigned blink: 1; /*blink占用高字节的第7位*/ }ch; 位结构成员的访问与结构体成员的访问相同，访问位结构中的bgcolor成员可写成：
ch.bgcolor 举个例子
struct info { char name[8]; int age; struct addr address; float pay; unsigned state: 1; unsigned pay: 1; }workers; 上面结构体定义了工资的信息，其中有两个只有1位的位结构成员，表示工人的状态以及工资是否已发放。
typedef定义结构体
typedef struct person { int age ; char *name; char *sex; }student; student stu1; //此处可以用student来定义一个结构体变量 typedef的作用就相当于给struct person取了一个别名student。
]]></content>
  </entry>
  
  <entry>
    <title>新手必看的单片机知识</title>
    <url>/post/mcu/knowledge-on-mcu.html</url>
    <categories><category>MCU</category>
    </categories>
    <tags>
      <tag>MCS-48</tag>
    </tags>
    <content type="html"><![CDATA[1946年2月15日，第一台电子数字计算机 ENIAC问世，这标志着计算机时代的到来。
前言 ENIAC 是电子管计算机，时钟频率虽然仅有 100 kHz，但能在1s 的时间内完成 5000 次加法运算。与现代的计算机相比，ENIAC有许多不足，但它的问世开创了计算机科学技术的新纪元，对人类的生产和生活方式产生了巨大的影响。
在研制 ENIAC 的过程中，匈牙利籍数学家冯·诺依曼担任研制小组的顾问，并在方案的设计上做出了重要的贡献。
1946年6月，冯·诺依曼又提出了 “程序存储”和“二进制运算”的思想，进一步构建了计算机由运算器、控制器、存储器、输入设备和输出设备组成这一计算机的经典结构。
电子计算机技术的发展，相继经历了电子管计算机、晶体管计算机、集成电路计算机、大规模集成电路计算机和超大规模集成电路计算机五个时代，但是计算机的结构仍然没有突破冯·诺依曼提出的计算机的经典结构框架。
微型计算机的组成及其应用形态 微型计算机的组成 1971 年 1 月，Intel 公司的特德·霍夫在与日本商业通信公司合作研制台式计算器时，将原始方案的十几个芯片压缩成三个集成电路芯片。
其中的两个芯片分别用于存储程序和数据，另一芯片集成了运算器和控制器及一些寄存器， 称为微处理器(即 Intel 4004)。微处理器、存储器加上 I/O 接口电路组成微型计算机。各部分通过地址总线(AB)、数据总线(DB)和控制总线(CB)相连。
微型计算机的应用形态 从应用形态上，微型计算机可以分成三种：多板机(系统机)、单板机和单片机。
 多板机(系统机)  多板机是将微处理器、存储器、I/O 接口电路和总线接口等组装在一块主机板(即微机主板)上，再通过系统总线和其它多块外设适配板卡连接键盘、显示器、打印机、软/硬盘驱动器及光驱等设备。各种适配板卡插在主机板的扩展槽上,并与电源、软/硬盘驱动器及光驱等装在同一机箱内，再配上系统软件,就构成了一台完整的微型计算机系统，简称系统机。
目前人们广泛使用的个人计算机(PC 机)就是典型的多板微型计算机。由于其人机界面好，功能强，软件资源丰富，通常作为办公或家庭的事务处理及科学计算，属于通用计算机，现在已经成为社会各领域中最为通用的工具。
另外，将系统机的机箱进行加固处理，底板设计成无 CPU 的小底板结构，利用底板的扩展槽插入主机板及各种测控板，就构成了一台工业 PC 机。由于其具有人机界面友好和软件资源丰富的优势，工业 PC 机常作为工业测控系统的主机。
单板机  将 CPU 芯片、存储器芯片、I/O 接口芯片和简单的 I/O 设备(小键盘、LED 显示器)等装配在一块印制线路板上，再配上监控程序(固化在 ROM 中),就构成了一台单板微型计算机，简称单板机。典型的产品如 TP801。
单板机的 I/O 设备简单，软件资源少，使用不方便。早期主要用于微型计算机原理的教学及简单的测控系统，现在已很少使用。
单片机  在一片集成电路芯片上集成微处理器、存储器、I/O 接口电路，从而构成了单芯片微型计算机，即单片机。
计算机原始的设计目的是为了提高计算数据的速度和完成海量数据的计算。人们将完成这种任务的计算机称为通用计算机。
随着计算机技术的发展，人们发现了计算机在逻辑处理及工业控制等方面也具有非凡的能力。在控制领域中，人们更多地关心计算机的低成本、小体积、运行的可靠性和控制的灵活性。
特别是智能仪表、智能传感器、智能家电、智能办公设备、汽车及军事电子设备等应用系统要求将计算机嵌入到这些设备中。嵌入到控制系统(或设备)中，实现嵌入式应用的计算机称为嵌入式计算机， 也称为专用计算机。
嵌入式应用的计算机可分为嵌入式微处理器(如 386EX)、嵌入式 DSP 处理器(如 TMS320 系列)、嵌入式微控制器(即单片机，如 80C51 系列)及嵌入式片上系统 SOC。
单片机体积小、价格低、可靠性高，其非凡的嵌入式应用形态对于满足嵌入式应用需求具有独特的优势。
目前，单片机应用技术已经成为电子应用系统设计最为常用的技术手段，学习和掌握单片机应用技术具有极其重要的现实意义。
综上所述，微型计算机技术的发展正趋于两个方向，一是以系统机为代表的通用计算机，致力于提高计算机的运算速度，在实现海量高速数据处理的同时兼顾控制功能；二是以单片机为代表的嵌入式专用计算机，致力于计算机控制功能的片内集成，在满足嵌入式对象的测控需求的同时兼顾数据处理。
单片机的发展过程及产品近况 单片机的发展过程 单片机技术发展十分迅速，产品种类已琳琅满目。纵观整个单片机技术发展过程，可以分为以下三个主要阶段：
单芯片微机形成阶段 1976 年，Intel公司推出了MCS-48系列单片机。该系列单片机早期产品在芯片内集成有：8 位CPU、1K 字节程序存储器(ROM)、64字节数据存储器(RAM)、27 根 I/O 线和1个8 位定时/计数器。
此阶段的主要特点是：在单个芯片内完成了 CPU、存储器、I/O 接口、定时/计数器、中断系统、时钟等部件的集成，但存储器容量较小，寻址范围小(不大于 4K)，无串行接口，指令系统功能不强。
性能完善提高阶段 1980 年，Intel 公司推出 MCS-51 系列单片机。该系列单片机在芯片内集成有：8 位 CPU、4K 字节程序存储器(ROM)、128 字节数据存储器(RAM)、4个 8 位并行接口、1 个全双工串行接口和 2 个 16 位定时/计数器。寻址范围为64 K，并集成有控制功能较强的布尔处理器完成位处理功能。此阶段的主要特点是：结构体系完善，性能已大大提高，面向控制的特点进一步突出。现在，MCS-51 已成为公认的单片机经典机种。
微控制器化阶段 1982 年，Intel 公司推出 MCS-96 系列单片机。该系列单片机在芯片内集成有：16 位 CPU、8K 字节程序存储器(ROM)、232 字节数据存储器(RAM)、 5 个 8 位并行接口、1 个全双工串行接口和 2 个 16 位定时/计数器。寻址范围最大为 64 K。片上还有 8 路 10 位 ADC、1 路 PWM(D/A)输出及高速 I/O 部件。
近年来，许多半导体厂商以 MCS-51 系列单片机的 8051 为内核，将许多测控系统中的接口技术、可靠性技术及先进的存储器技术和工艺技术集成到单片机中，生产出了多种功能强大、使用灵活的新一代 80C51 系列单片机。此阶段的主要特点是：片内面向测控系统的外围电路增强，使单片机可以&ndash;方便灵活地应用于复杂的自动测控系统及设备。因此，“微控制器”的称谓更能反应单片机的本质。
单片机产品近况 随着微电子设计技术及计算机技术的不断发展，单片机产品和技术日新月异。单片机产品近况可以归纳为以下二方面。
一、80C51 系列单片机产品繁多，主流地位已经形成 通用微型计算机计算速度的提高主要体现在 CPU 位数的提高(16 位、32 位乃至 64 位)，而单片机更注重的是产品的可靠性、经济性和嵌入性。所以，单片机 CPU 位数的提高需求并不十分迫切。而多年来的应用实践已经证明， 80C51 的系统结构合理，技术成熟。因此，许多单片机芯片生产厂商倾力于提高 80C51 单片机产品的综合功能，从而形成了 80C51 的主流产品地位。近年来推出的与 80C51 兼容的主要产品有：
 ATMEL 公司融入 Flash 存储器技术推出的 AT89 系列单片机； Philips 公司推出的 80C51、80C552 系列高性能单片机； 华邦公司推出的 W78C51、W77C51 系列高速低价单片机； LG 公司推出的 GMS90/97 系列低压高速单片机； Maxim 公司推出的 DS89C420 高速（50MIPS）单片机； Cygnal 公司推出的 C8051F 系列高速 SOC 单片机等。  由此可见，80C51 已经成为事实上的单片机主流系列，所以本书以 80C51为对象，讲述单片机的原理与接口方法。
二、非 80C51 结构单片机新品不断推出，给用户提供了更为广泛的选择空间在 80C51 及其兼容产品流行的同时，一些单片机芯片生产厂商也推出了一些非 80C51 结构的产品，影响比较大的有：
 Intel 公司推出的 MCS-96 系列 16 位单片机； Microchip 公司推出的 PIC 系列 RISC 结构单片机； TI 公司推出的 MSP430F 系列 16 位低电压、低功耗单片机； ATMEL 公司推出的 AVR 系列 RISC 结构单片机等。  单片机的特点及应用领域、 单片机的特点 控制性能和可靠性高 单片机是为满足工业控制而设计的，所以实时控制功能特别强，其 CPU 可以对 I/O 接口直接进行操作，位操作能力更是其它计算机无法比拟的。另外， 由于 CPU、存储器及 I/O 接口集成在同一芯片内，各部件间的连接紧凑，数据在传送时受到的干扰较小，且不易受环境条件的影响，所以单片机的可靠性非常高。近期推出的单片机产品，内部集成有高速 I/O 接口、ADC、PWM、WDT 等部件，并在低电压、低功耗、串行扩展总线、控制网络总线和开发方式（如在系统编程 ISP）等方面都有了进一步的增强。
体积小、价格低、易于产品化 每片单片机芯片即是一台完整的微型计算机，对于批量大的专用场合，一方面可以在众多的单片机品种间进行匹配选择，同时还可以专门进行芯片设计， 使芯片功能与应用具有良好的对应关系。在单片机产品的引脚封装方面，有的单片机引脚已减少到 8 个或更少，从而使应用系统的印制板减小，接插件减少， 安装简单方便。在现代的各种电子器件中，单片机具有良好的性能价格比。这正是单片机得以广泛应用的重要原因。
单片机的应用领域 由于单片机具有良好的控制性能和灵活的嵌入品质，近年来单片机在各种领域都获得了极为广泛的应用。概要地分成以下几个方面：
智能仪器仪表 单片机用于各种仪器仪表，一方面提高了仪器仪表的使用功能和精度，使仪器仪表智能化，同时还简化了仪器仪表的硬件结构，从而可以方便地完成仪器仪表产品的升级换代。如各种智能电气测量仪表、智能传感器等。
机电一体化产品 机电一体化产品是集机械技术、微电子技术、自动化技术和计算机技术于一体，具有智能化特征的各种机电产品。单片机在机电一体化产品的开发中可以发挥巨大的作用。典型产品如机器人、数控机床、自动包装机、点钞机、医疗设备、打印机、传真机、复印机等。
实时工业控制 单片机还可以用于各种物理量的采集与控制。电流、电压、温度、液位、流量等物理参数的采集和控制均可以利用单片机方便地实现。在这类系统中， 利用单片机作为系统控制器，可以根据被控对象的不同特征采用不同的智能算法，实现期望的控制指标，从而提高生产效率和产品质量。典型应用如电动机转速控制、温度控制、自动生产线等。
分布系统的前端模块 在较复杂的工业系统中，经常要采用分布式测控系统完成大量的分布参数的采集。在这类系统中，采用单片机作为分布式系统的前端采集模块。系统具有运行可靠，数据采集方便灵活，成本低廉等一系列优点。
家用电器 家用电器是单片机的又一重要应用领域，前景十分广阔。如空调器、电冰箱、洗衣机、电饭煲、高档洗浴设备、高档玩具等。另外，在交通领域中，汽车、火车、飞机、航天器等均有单片机的广泛应用。如汽车自动驾驶系统、航天测控系统、黑匣子等。
单片机应用系统开发简介 单片机应用系统的开发 设计单片机应用系统时，在完成硬件系统设计之后，必须配备相应的应用软件。正确无误的硬件设计和良好的软件功能设计是一个实用的单片机应用系统的设计目标。完成这一目标的过程称为单片机应用系统的开发。单片机作为一片集成了微型计算机基本部件的集成电路芯片，与通用微机相比，它自身没有开发功能，必须借助开发机(一种特殊的计算机系统)来完成如下任务：1、排除应用系统的硬件故障和软件错误；2、调试完的程序要固化到单片机内部或外部程序存储器芯片中。
指令的表示形式 指令是让单片机执行某种操作的命令。在单片机内部，指令按一定的顺序以二进制码的形式存放于程序存储器中。二进制码是计算机能够直接执行的机器码(或称目标码)。为了书写、输入和显示方便，人们通常将机器码写成十六进制形式。如二进制码 0000 0100B 可以表示为 04H。04H 所对应的指令的意义是累加器 A 的内容加 1。若写成 INC A，则要清楚得多，这就是该指令的符号表示，称为符号指令。
汇编或编译 符号指令要转换成计算机所能执行的机器码并存入计算机的程序存储器中，这种转换称为汇编。常用的汇编方法有三种，
 一是手工汇编，设计人员对照单片机指令编码表，把每一条符号指令翻译成十六进制数表示的机器码指令， 借助于小键盘送入开发机，然后进行调试，并将调试好的程序写入程序存储器芯片。 二是利用开发机的汇编程序进行汇编。 三是利用通用微型计算机配备的汇编程序进行交叉汇编，然后将目标码传送到开发机中。  另外，还可以采用高级语言(如C51)进行单片机应用程序的设计。在 PC 机中编辑好的高级语言源程序经过编译、连接后形成目标码文件，并传送到开发机中。这种方法具有周期短、移植和修改方便的优点，适合于较复杂系统的开发。
单片机应用系统的传统开发方式 单片机开发系统又称为开发机或仿真器。仿真的目的是利用开发机的资源(CPU、存储器和I/O设备等)来模拟欲开发的单片机应用系统(即目标机) 的 CPU、存储器和I/O操作，并跟踪和观察目标机的运行状态。
仿真可以分为软件模拟仿真和开发机在线仿真两大类。软件模拟仿真成本低，使用方便，但不能进行应用系统硬件的实时调试和故障诊断。下面仅介绍在线仿真方法。
利用独立型仿真器开发 独立型仿真器采用与单片机应用系统相同类型的单片机做成单板机形式， 板上配置 LED 显示器和简易键盘。这种开发系统在没有普通微机系统的支持下，仍能对单片机应用系统进行在线仿真，便于在现场对应用软件进行调试和修改。
另外，这种开发系统还配有串行接口，能与普通微机系统连接。这样， 可以利用普通微机系统配备的组合软件进行源程序的编辑、汇编和联机仿真调试。然后将调试无误的目标程序（即机器码）传送到仿真器，利用仿真器进行程序的固化。
利用非独立型仿真器开发 这种仿真器采用通用微型计算机加仿真器方式构成。仿真器与通用微机间以串行通信的方式连接。这种开发方式必须有微机的支持，利用微机系统配备的组合软件进行源程序的编辑、汇编和仿真调试。有些仿真接口上还备有EPROM 写入插座，可以将开发调试完成的用户应用程序写入 EPROM 芯片。与前一种相比，此种开发方式现场参数的修改和调试不够方便。
以上两种开发方式均是在开发时拔掉目标系统的单片机芯片和程序存储器芯片，插上从开发机上引出的仿真头，即把开发机上的单片机出借给目标机。
仿真调试无误后，拔掉仿真头，再插回单片机芯片，把开发机中调试好的程序固化到 EPROM 芯片中并插到目标机的程序存储器插座上，目标机就可以独立运行了。
单片机开发方式的发展 由于单片机贴片封装形式的广泛采用以及 Flash 存储器技术的迅速发展， 传统的单片机应用系统开发的理念将受到冲击。
采用新的单片机应用系统开发技术可以将单片机先安装到印制线路板上，然后通过 PC 机将程序下载到目标系统。
如：SST 公司推出的 SST89C54 和 SST89C58 芯片分别有 20 KB 和 30 KB 的SuperFlash 存储器，利用这种存储器可以进行高速读/写的特点，能够实现在系统编程(ISP)和在应用编程(IAP)功能。
首先在 PC 机上完成应用程序的编辑、汇编(或编译)和模拟运行，然后实现目标程序的串行下载。
Microchip 公司推出的RISC 结构单片机PIC16F87X 中内置在线调试器ICD功能，该公司还配置了具有 ICSP功能的简单仿真器和烧写器。由于芯片内置了侦测电路逻辑，所以可以不需要额外的硬件仿真器。
通过 PC 机串行电缆(含有完成通信功能的MPLAB-ICD 模块及与目标板连接的 MPLAB-ICD 头)就可以完成对目标系统的仿真调试。
]]></content>
  </entry>
  
  <entry>
    <title>嵌入式开发中的C语言——编译器</title>
    <url>/post/programming/compiler-c-programming-in-embedded-design.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>C</tag>
      <tag>Embedded design</tag>
      <tag>Compiler</tag>
    </tags>
    <content type="html"><![CDATA[如果你和一个优秀的程序员共事，你会发现他对他使用的工具非常熟悉，就像一个画家了解他的画具一样。&mdash; 比尔.盖茨
不能简单的认为是个工具  嵌入式程序开发跟硬件密切相关，需要使用C语言来读写底层寄存器、存取数据、控制硬件等，C语言和硬件之间由编译器来联系，一些C标准不支持的硬件特性操作，由编译器提供。 汇编可以很轻易的读写指定RAM地址、可以将代码段放入指定的Flash地址、可以精确的设置变量在RAM中分布等等，所有这些操作，在深入了解编译器后，也可以使用C语言实现。 C语言标准并非完美，有着数目繁多的未定义行为，这些未定义行为完全由编译器自主决定，了解你所用的编译器对这些未定义行为的处理，是必要的。 嵌入式编译器对调试做了优化，会提供一些工具，可以分析代码性能，查看外设组件等，了解编译器的这些特性有助于提高在线调试的效率。 此外，堆栈操作、代码优化、数据类型的范围等等，都是要深入了解编译器的理由。 如果之前你认为编译器只是个工具，能够编译就好。那么，是时候改变这种思想了。  不能依赖编译器的语义检查 编译器的语义检查很弱小，甚至还会“掩盖”错误。现代的编译器设计是件浩瀚的工程，为了让编译器设计简单一些，目前几乎所有编译器的语义检查都比较弱小。为了获得更快的执行效率，C语言被设计的足够灵活且几乎不进行任何运行时检查，比如数组越界、指针是否合法、运算结果是否溢出等等。这就造成了很多编译正确但执行奇怪的程序。
C语言足够灵活，对于一个数组test[30]，它允许使用像test[-1]这样的形式来快速获取数组首元素所在地址前面的数据；允许将一个常数强制转换为函数指针，使用代码(((void()())0))()来调用位于0地址的函数。C语言给了程序员足够的自由，但也由程序员承担滥用自由带来的责任。
莫名的死机 下面的两个例子都是死循环，如果在不常用分支中出现类似代码，将会造成看似莫名其妙的死机或者重启。
unsigned char i; //例程1  for(i=0;i&lt;256;i++) { //其它代码  } unsigned char i; //例程2  for(i=10;i&gt;=0;i--) { //其它代码  } 对于无符号char类型，表示的范围为0~255，所以无符号char类型变量i永远小于256（第一个for循环无限执行），永远大于等于0（第二个for循环无限执行）。需要说明的是，赋值代码i=256是被C语言允许的，即使这个初值已经超出了变量i可以表示的范围。C语言会千方百计的为程序员创造出错的机会，可见一斑。
不起眼的改变 假如你在if语句后误加了一个分号，可能会完全改变了程序逻辑。编译器也会很配合的帮忙掩盖，甚至连警告都不提示。代码如下：
if(a&gt;b); //这里误加了一个分号  a=b; //这句代码一直被执行 不但如此，编译器还会忽略掉多余的空格符和换行符，就像下面的代码也不会给出足够提示：
if(n&lt;3) return	//这里少加了一个分号 logrec.data=x[0]; logrec.time=x[1]; logrec.code=x[2]; 这段代码的本意是n&lt;3时程序直接返回，由于程序员的失误，return少了一个结束分号。编译器将它翻译成返回表达式logrec.data=x[0]的结果，return后面即使是一个表达式也是C语言允许的。这样当n&gt;=3时，表达式logrec.data=x[0];就不会被执行，给程序埋下了隐患。
难查的数组越界 上文曾提到数组常常是引起程序不稳定的重要因素，程序员往往不经意间就会写数组越界。
一位同事的代码在硬件上运行，一段时间后就会发现LCD显示屏上的一个数字不正常的被改变。经过一段时间的调试，问题被定位到下面的一段代码中：
int SensorData[30]; //其他代码  for(i=30;i&gt;0;i--) { SensorData[i]=…; //其他代码  } 这里声明了拥有30个元素的数组，不幸的是for循环代码中误用了本不存在的数组元素SensorData[30]，但C语言却默许这么使用，并欣然的按照代码改变了数组元素SensorData[30]所在位置的值， SensorData[30]所在的位置原本是一个LCD显示变量，这正是显示屏上的那个值不正常被改变的原因。真庆幸这么轻而易举的发现了这个Bug。
其实很多编译器会对上述代码产生一个警告：赋值超出数组界限。但并非所有程序员都对编译器警告保持足够敏感，况且，编译器也并不能检查出数组越界的所有情况。比如下面的例子：
你在模块A中定义数组：
int SensorData[30]; 在模块B中引用该数组，但由于你引用代码并不规范，这里没有显示声明数组大小，但编译器也允许这么做：
extern int SensorData[]; 这次，编译器不会给出警告信息，因为编译器压根就不知道数组的元素个数。所以，当一个数组声明为具有外部链接，它的大小应该显式声明。
再举一个编译器检查不出数组越界的例子。函数func()的形参是一个数组形式，函数代码简化如下所示：
char * func(char SensorData[30]) { unsigned int 1; for(i=30; i&gt;0; i--) { SensorData[i]=...; //其他代码 	} } 这个给SensorData[30]赋初值的语句，编译器也是不给任何警告的。实际上，编译器是将数组名Sensor隐含的转化为指向数组第一个元素的指针，函数体是使用指针的形式来访问数组的，它当然也不会知道数组元素的个数了。造成这种局面的原因之一是C编译器的作者们认为指针代替数组可以提高程序效率，而且，可以简化编译器的复杂度。
指针和数组是容易给程序造成混乱的，我们有必要仔细的区分它们的不同。其实换一个角度想想，它们也是容易区分的：可以将数组名等同于指针的情况有且只有一处，就是上面例子提到的数组作为函数形参时。其它时候，数组名是数组名，指针是指针。
下面的例子编译器同样检查不出数组越界。
我们常常用数组来缓存通讯中的一帧数据。在通讯中断中将接收的数据保存到数组中，直到一帧数据完全接收后再进行处理。即使定义的数组长度足够长，接收数据的过程中也可能发生数组越界，特别是干扰严重时。
这是由于外界的干扰破坏了数据帧的某些位，对一帧的数据长度判断错误，接收的数据超出数组范围，多余的数据改写与数组相邻的变量，造成系统崩溃。由于中断事件的异步性，这类数组越界编译器无法检查到。
如果局部数组越界，可能引发ARM架构硬件异常。
同事的一个设备用于接收无线传感器的数据，一次软件升级后，发现接收设备工作一段时间后会死机。调试表明ARM7处理器发生了硬件异常，异常处理代码是一段死循环（死机的直接原因）。接收设备有一个硬件模块用于接收无线传感器的整包数据并存在自己的缓冲区中，当硬件模块接收数据完成后，使用外部中断通知设备取数据，外部中断服务程序精简后如下所示：
__irq ExintHandler(void) { unsignedchar DataBuf[50]; GetData(DataBug); //从硬件缓冲区取一帧数据  //其他代码  } 由于存在多个无线传感器近乎同时发送数据的可能加之GetData()函数保护力度不够，数组DataBuf在取数据过程中发生越界。由于数组DataBuf为局部变量，被分配在堆栈中，同在此堆栈中的还有中断发生时的运行环境以及中断返回地址。溢出的数据将这些数据破坏掉，中断返回时PC指针可能变成一个不合法值，硬件异常由此产生。
如果我们精心设计溢出部分的数据，化数据为指令，就可以利用数组越界来修改PC指针的值，使之指向我们希望执行的代码。
1988年，第一个网络蠕虫在一天之内感染了2000到6000台计算机，这个蠕虫程序利用的正是一个标准输入库函数的数组越界Bug。起因是一个标准输入输出库函数gets()，原来设计为从数据流中获取一段文本，遗憾的是，gets()函数没有规定输入文本的长度。
gets()函数内部定义了一个500字节的数组，攻击者发送了大于500字节的数据，利用溢出的数据修改了堆栈中的PC指针，从而获取了系统权限。目前，虽然有更好的库函数来代替gets函数，但gets函数仍然存在着。
神奇的volatile 做嵌入式设备开发，如果不对volatile修饰符具有足够了解，实在是说不过去。volatile是C语言32个关键字中的一个，属于类型限定符，常用的const关键字也属于类型限定符。
volatile限定符用来告诉编译器，该对象的值无任何持久性，不要对它进行任何优化；它迫使编译器每次需要该对象数据内容时都必须读该对象，而不是只读一次数据并将它放在寄存器中以便后续访问之用（这样的优化可以提高系统速度）。
这个特性在嵌入式应用中很有用，比如你的IO口的数据不知道什么时候就会改变，这就要求编译器每次都必须真正的读取该IO端口。这里使用了词语“真正的读”，是因为由于编译器的优化，你的逻辑反应到代码上是对的，但是代码经过编译器翻译后，有可能与你的逻辑不符。
你的代码逻辑可能是每次都会读取IO端口数据，但实际上编译器将代码翻译成汇编时，可能只是读一次IO端口数据并保存到寄存器中，接下来的多次读IO口都是使用寄存器中的值来进行处理。因为读写寄存器是最快的，这样可以优化程序效率。与之类似的，中断里的变量、多线程中的共享变量等都存在这样的问题。
不使用volatile，可能造成运行逻辑错误，但是不必要的使用volatile会造成代码效率低下（编译器不优化volatile限定的变量），因此清楚的知道何处该使用volatile限定符，是一个嵌入式程序员的必修内容。
一个程序模块通常由两个文件组成，源文件和头文件。如果你在源文件定义变量：
unsigned int test; 并在头文件中声明该变量：
extern unsigned long test; 编译器会提示一个语法错误：变量’ test’声明类型不一致。但如果你在源文件定义变量：
volatile unsigned int test; 在头文件中这样声明变量：
extern unsigned int test; /*缺少volatile限定符*/ 编译器却不会给出错误信息（有些编译器仅给出一条警告）。当你在另外一个模块（该模块包含声明变量test的头文件）使用变量test时，它已经不再具有volatile限定，这样很可能造成一些重大错误。比如下面的例子，注意该例子是为了说明volatile限定符而专门构造出的，因为现实中的volatile使用Bug大都隐含，并且难以理解。
在模块A的源文件中，定义变量：
volatile unsigned int TimerCount=0; 该变量用来在一个定时器中断服务程序中进行软件计时：
TimerCount++; 在模块A的头文件中，声明变量：
extern unsigned int TimerCount; //这里漏掉了类型限定符volatile 在模块B中，要使用TimerCount变量进行精确的软件延时：
#include &#34;…A.h&#34; //首先包含模块A的头文件  //其他代码  TimerCount=0; while(TimerCount&lt;=TIMER_VALUE); //延时一段时间(感谢网友chhfish指这里的逻辑错误)  //其他代码 实际上，这是一个死循环。由于模块A头文件中声明变量TimerCount时漏掉了volatile限定符，在模块B中，变量TimerCount是被当作unsigned int类型变量。由于寄存器速度远快于RAM，编译器在使用非volatile限定变量时是先将变量从RAM中拷贝到寄存器中，如果同一个代码块再次用到该变量，就不再从RAM中拷贝数据而是直接使用之前寄存器备份值。
代码while(TimerCount&lt;=TIMER_VALUE)中，变量TimerCount仅第一次执行时被使用，之后都是使用的寄存器备份值，而这个寄存器值一直为0，所以程序无限循环。下面的流程图说明了程序使用限定符volatile和不使用volatile的执行过程。
为了更容易的理解编译器如何处理volatile限定符，这里给出未使用volatile限定符和使用volatile限定符程序的反汇编代码：
 没有使用关键字volatile，在keil MDK V4.54下编译，默认优化级别，如下所示（注意最后两行）：  122: unIdleCount=0; 123: 0x00002E10 E59F11D4 LDR R1,[PC,#0x01D4] 0x00002E14 E3A05000 MOV R5,#key1(0x00000000) 0x00002E18 E1A00005 MOV R0,R5 0x00002E1C E5815000 STR R5,[R1] 124: while(unIdleCount!=200); //延时2S钟  125: 0x00002E20 E35000C8 CMP R0,#0x000000C8 0x00002E24 1AFFFFFD BNE 0x00002E20  使用关键字volatile，在keil MDK V4.54下编译，默认优化级别，如下所示（注意最后三行）：  122: unIdleCount=0; 123: 0x00002E10 E59F01D4 LDR R0,[PC,#0x01D4] 0x00002E14 E3A05000 MOV R5,#key1(0x00000000) 0x00002E18 E5805000 STR R5,[R0] 124: while(unIdleCount!=200); //延时2S钟  125: 0x00002E1C E5901000 LDR R1,[R0] 0x00002E20 E35100C8 CMP R1,#0x000000C8 0x00002E24 1AFFFFFC BNE 0x00002E1C 可以看到，如果没有使用volatile关键字，程序一直比较R0内数据与0xC8是否相等，但R0中的数据是0，所以程序会一直在这里循环比较（死循环）；再看使用了volatile关键字的反汇编代码，程序会先从变量中读出数据放到R1寄存器中，然后再让R1内数据与0xC8相比较，这才是我们C代码的正确逻辑！
局部变量 ARM架构下的编译器会频繁的使用堆栈，堆栈用于存储函数的返回值、AAPCS规定的必须保护的寄存器以及局部变量，包括局部数组、结构体、联合体和C++的类。默认情况下，堆栈的位置、初始值都是由编译器设置，因此需要对编译器的堆栈有一定了解。
从堆栈中分配的局部变量的初值是不确定的，因此需要运行时显式初始化该变量。一旦离开局部变量的作用域，这个变量立即被释放，其它代码也就可以使用它，因此堆栈中的一个内存位置可能对应整个程序的多个变量。
局部变量必须显式初始化，除非你确定知道你要做什么。下面的代码得到的温度值跟预期会有很大差别，因为在使用局部变量sum时，并不能保证它的初值为0。编译器会在第一次运行时清零堆栈区域，这加重了此类Bug的隐蔽性。
由于一旦程序离开局部变量的作用域即被释放，所以下面代码返回指向局部变量的指针是没有实际意义的，该指针指向的区域可能会被其它程序使用，其值会被改变。
char * GetData(void) { char buffer[100]; //局部数组  … return buffer; } 使用外部工具 由于编译器的语义检查比较弱，我们可以使用第三方代码分析工具，使用这些工具来发现潜在的问题，这里介绍其中比较著名的是PC-Lint。
PC-Lint由Gimpel Software公司开发，可以检查C代码的语法和语义并给出潜在的BUG报告。PC-Lint可以显著降低调试时间。
目前公司ARM7和Cortex-M3内核多是使用Keil MDK编译器来开发程序，通过简单配置，PC-Lint可以被集成到MDK上，以便更方便的检查代码。MDK已经提供了PC-Lint的配置模板，所以整个配置过程十分简单，Keil MDK开发套件并不包含PC-Lint程序，在此之前，需要预先安装可用的PC-Lint程序，配置过程如下：
点击菜单Tools&mdash;Set-up PC-Lint… PC-Lint Include Folders：该列表路径下的文件才会被PC-Lint检查，此外，这些路径下的文件内使用#include包含的文件也会被检查； Lint Executable：指定PC-Lint程序的路径 Configuration File：指定配置文件的路径，该配置文件由MDK编译器提供。  菜单Tools&mdash;Lint 文件路径.c/.h 检查当前文件。  菜单Tools&mdash;Lint All C-Source Files 检查所有C源文件。 PC-Lint的输出信息显示在MDK编译器的Build Output窗口中，双击其中的一条信息可以跳转到源文件所在位置。 编译器语义检查的弱小在很大程度上助长了不可靠代码的广泛存在。随着时代的进步，现在越来越多的编译器开发商意识到了语义检查的重要性，编译器的语义检查也越来越强大，比如公司使用的Keil MDK编译器，虽然它的编辑器依然不尽人意，但在其V4.47及以上版本中增加了动态语法检查并加强了语义检查，可以友好的提示更多警告信息。建议经常关注编译器官方网站并将编译器升级到V4.47或以上版本，升级的另一个好处是这些版本的编辑器增加了标识符自动补全功能，可以大大节省编码的时间。  你觉得有意义的代码未必正确 C语言标准特别的规定某些行为是未定义的，编写未定义行为的代码，其输出结果由编译器决定！C标准委员会定义未定义行为的原因如下：    简化标准，并给予实现一定的灵活性，比如不捕捉那些难以诊断的程序错误；
  编译器开发商可以通过未定义行为对语言进行扩展
  C语言的未定义行为，使得C极度高效灵活并且给编译器实现带来了方便，但这并不利于优质嵌入式C程序的编写。因为许多 C 语言中看起来有意义的东西都是未定义的，并且这也容易使你的代码埋下隐患，并且不利于跨编译器移植。Java程序会极力避免未定义行为，并用一系列手段进行运行时检查，使用Java可以相对容易的写出安全代码，但体积庞大效率低下。作为嵌入式程序员，我们需要了解这些未定义行为，利用C语言的灵活性，写出比Java更安全、效率更高的代码来。
常见的未定义行为 自增自减在表达式中连续出现并作用于同一变量或者自增自减在表达式中出现一次，但作用的变量多次出现 自增（++）和自减（--）这一动作发生在表达式的哪个时刻是由编译器决定的，比如：  r = 1 * a[i++] + 2 * a[i++] + 3 * a[i++]; 不同的编译器可能有着不同的汇编代码，可能是先执行i++再进行乘法和加法运行，也可能是先进行加法和乘法运算，再执行i++，因为这句代码在一个表达式中出现了连续的自增并作用于同一变量。更加隐蔽的是自增自减在表达式中出现一次，但作用的变量多次出现，比如：  a[i] = i++; /* 未定义行为 */ 先执行i++再赋值，还是先赋值再执行i++是由编译器决定的，而两种不同的执行顺序的结果差别是巨大的。  函数实参被求值的顺序 函数如果有多个实参，这些实参的求值顺序是由编译器决定的，比如：  printf(&#34;%d %d\n&#34;, ++n, power(2, n)); /* 未定义行为 */ 是先执行++n还是先执行power(2,n)是由编译器决定的。  有符号整数溢出 有符号整数溢出是未定义的行为，编译器决定有符号整数溢出按照哪种方式取值。比如下面代码：  int value1,value2,sum //其它操作  sum=value1+value; /*sum可能发生溢出*/ 有符号数右移、移位的数量是负值或者大于操作数的位数 除数为零 malloc()、calloc()或realloc()分配零字节内存 如何避免C语言未定义行为 代码中引入未定义行为会为代码埋下隐患，防止代码中出现未定义行为是困难的，我们总能不经意间就会在代码中引入未定义行为。但是还是有一些方法可以降低这种事件，总结如下：    了解C语言未定义行为
标准C99附录J.2“未定义行为”列举了C99中的显式未定义行为，通过查看该文档，了解那些行为是未定义的，并在编码中时刻保持警惕；
  寻求工具帮助
编译器警告信息以及PC-Lint等静态检查工具能够发现很多未定义行为并警告，要时刻关注这些工具反馈的信息；
  总结并使用一些编码标准
1）避免构造复杂的自增或者自减表达式，实际上，应该避免构造所有复杂表达式；
  比如a[i] = i++;语句可以改为a[i] = i; i++;这两句代码。
2）只对无符号操作数使用位操作；    必要的运行时检查
检查是否溢出、除数是否为零，申请的内存数量是否为零等等，比如上面的有符号整数溢出例子，可以按照如下方式编写，以消除未定义特性：
  int value1,value2,sum; //其它代码  if((value1&gt;0 &amp;&amp; value2&gt;0 &amp;&amp; value1&gt;(INT_MAX-value2))|| (value1&lt;0 &amp;&amp; value2&lt;0 &amp;&amp; value1&lt;(INT_MIN-value2))) { //处理错误  } else { sum=value1+value2; } 上面的代码是通用的，不依赖于任何CPU架构，但是代码效率很低。如果是有符号数使用补码的CPU架构（目前常见CPU绝大多数都是使用补码），还可以用下面的代码来做溢出检查：
int value1, value2, sum; unsigned int usum = (unsigned int)value1 + value2; if((usum ^ value1) &amp; (usum ^ value2) &amp; INT_MIN) { /*处理溢出情况*/ } else { sum = value1 + value2; } 使用的原理解释一下，因为在加法运算中，操作数value1和value2只有符号相同时，才可能发生溢出，所以我们先将这两个数转换为无符号类型，两个数的和保存在变量usum中。如果发生溢出，则value1、value2和usum的最高位（符号位）一定不同，表达式(usum ^ value1) &amp; (usum ^ value2) 的最高位一定为1，这个表达式位与（&amp;）上INT_MIN是为了将最高位之外的其它位设置为0。    了解你所用的编译器对未定义行为的处理策略
很多引入了未定义行为的程序也能运行良好，这要归功于编译器处理未定义行为的策略。不是你的代码写的正确，而是恰好编译器处理策略跟你需要的逻辑相同。了解编译器的未定义行为处理策略，可以让你更清楚的认识到那些引入了未定义行为程序能够运行良好是多么幸运的事，不然多换几个编译器试试！
以Keil MDK为例，列举常用的处理策略如下：
  1）有符号量的右移是算术移位，即移位时要保证符号位不改变。
2）对于int类的值：超过31位的左移结果为零；无符号值或正的有符号值超过31位的右移结果为零。负的有符号值移位结果为-1。
3）整型数除以零返回零
了解你的编译器 在嵌入式开发过程中，我们需要经常和编译器打交道，只有深入了解编译器，才能用好它，编写更高效代码，更灵活的操作硬件，实现一些高级功能。下面以公司最常用的Keil MDK为例，来描述一下编译器的细节。  编译器的一些小知识  默认情况下，char类型的数据项是无符号的，所以它的取值范围是0～255； 在所有的内部和外部标识符中，大写和小写字符不同； 通常局部变量保存在寄存器中，但当局部变量太多放到栈里的时候，它们总是字对齐的。 压缩类型的自然对齐方式为1。使用关键字__packed来压缩特定结构，将所有有效类型的对齐边界设置为1； 整数以二进制补码形式表示；浮点量按IEEE格式存储； 整数除法的余数的符号于被除数相同，由ISO C90标准得出； 如果整型值被截断为短的有符号整型，则通过放弃适当数目的最高有效位来得到结果。如果原始数是太大的正或负数，对于新的类型，无法保证结果的符号将于原始数相同。 整型数超界不引发异常；像unsigned char test; test=1000;这类是不会报错的； 在严格C中，枚举值必须被表示为整型。例如，必须在‑2147483648 到+2147483647的范围内。但MDK自动使用对象包含enum范围的最小整型来实现（比如char类型），除非使用编译器命令‑‑enum_is_int 来强制将enum的基础类型设为至少和整型一样宽。超出范围的枚举值默认仅产生警告：#66:enumeration value is out of &ldquo;int&rdquo; range； 对于结构体填充，根据定义结构的方式，keil MDK编译器用以下方式的一种来填充结构：  I&gt; 定义为static或者extern的结构用零填充； II&gt; 栈或堆上的结构，例如，用malloc()或者auto定义的结构，使用先前存储在那些存储器位置的任何内容进行填充。不能使用memcmp()来比较以这种方式定义的填充结构！
 编译器不对声明为volatile类型的数据进行优化； __nop()：延时一个指令周期，编译器绝不会优化它。如果硬件支持NOP指令，则该句被替换为NOP指令，如果硬件不支持NOP指令，编译器将它替换为一个等效于NOP的指令，具体指令由编译器自己决定； __align(n)：指示编译器在n 字节边界上对齐变量。对于局部变量，n的值为1、2、4、8； attribute((at(address)))：可以使用此变量属性指定变量的绝对地址； __inline：提示编译器在合理的情况下内联编译C或C++ 函数；  初始化的全局变量和静态变量的初始值被放到了哪里？ 我们程序中的一些全局变量和静态变量在定义时进行了初始化，经过编译器编译后，这些初始值被存放在了代码的哪里？我们举个例子说明：  unsigned int g_unRunFlag=0xA5; static unsigned int s_unCountFlag=0x5A; 我曾做过一个项目，项目中的一个设备需要在线编程，也就是通过协议，将上位机发给设备的数据通过在应用编程（IAP）技术写入到设备的内部Flash中。我将内部Flash做了划分，一小部分运行程序，大部分用来存储上位机发来的数据。随着程序量的增加，在一次更新程序后发现，在线编程之后，设备运行正常，但是重启设备后，运行出现了故障！经过一系列排查，发现故障的原因是一个全局变量的初值被改变了。 这是件很不可思议的事情，你在定义这个变量的时候指定了初始值，当你在第一次使用这个变量时却发现这个初值已经被改掉了！这中间没有对这个变量做任何赋值操作，其它变量也没有任何溢出，并且多次在线调试表明，进入main函数的时候，该变量的初值已经被改为一个恒定值。 要想知道为什么全局变量的初值被改变，就要了解这些初值编译后被放到了二进制文件的哪里。在此之前，需要先了解一点链接原理。 ARM映象文件各组成部分在存储系统中的地址有两种：一种是映象文件位于存储器时（通俗的说就是存储在Flash中的二进制代码）的地址，称为加载地址；一种是映象文件运行时（通俗的说就是给板子上电，开始运行Flash中的程序了）的地址，称为运行时地址。 赋初值的全局变量和静态变量在程序还没运行的时候，初值是被放在Flash中的，这个时候他们的地址称为加载地址，当程序运行后，这些初值会从Flash中拷贝到RAM中，这时候就是运行时地址了。 原来，对于在程序中赋初值的全局变量和静态变量，程序编译后，MDK将这些初值放到Flash中，位于紧靠在可执行代码的后面。在程序进入main函数前，会运行一段库代码，将这部分数据拷贝至相应RAM位置。 由于我的设备程序量不断增加，超过了为设备程序预留的Flash空间，在线编程时，将一部分存储全局变量和静态变量初值的Flash给重新编程了。在重启设备前，初值已经被拷贝到RAM中，所以这个时候程序运行是正常的，但重新上电后，这部分初值实际上是在线编程的数据，自然与初值不同了。  在C代码中使用的变量，编译器将他们分配到RAM的哪里？ 我们会在代码中使用各种变量，比如全局变量、静态变量、局部变量，并且这些变量时由编译器统一管理的，有时候我们需要知道变量用掉了多少RAM，以及这些变量在RAM中的具体位置。 这是一个经常会遇到的事情，举一个例子，程序中的一个变量在运行时总是不正常的被改变，那么有理由怀疑它临近的变量或数组溢出了，溢出的数据更改了这个变量值。要排查掉这个可能性，就必须知道该变量被分配到RAM的哪里、这个位置附近是什么变量，以便针对性的做跟踪。 其实MDK编译器的输出文件中有一个“工程名.map”文件，里面记录了代码、变量、堆栈的存储位置，通过这个文件，可以查看使用的变量被分配到RAM的哪个位置。要生成这个文件，需要在Options for Targer窗口，Listing标签栏下，勾选Linker Listing前的复选框，如下图所示。  默认情况下，栈被分配到RAM的哪个地方？ MDK中，我们只需要在配置文件中定义堆栈大小，编译器会自动在RAM的空闲区域选择一块合适的地方来分配给我们定义的堆栈，这个地方位于RAM的那个地方呢？ 通过查看MAP文件，原来MDK将堆栈放到程序使用到的RAM空间的后面，比如你的RAM空间从0x4000 0000开始，你的程序用掉了0x200字节RAM，那么堆栈空间就从0x4000 0200处开始。 使用了多少堆栈，是否溢出?  有多少RAM会被初始化？ 在进入main()函数之前，MDK会把未初始化的RAM给清零的，我们的RAM可能很大，只使用了其中一小部分，MDK会不会把所有RAM都初始化呢？ 答案是否定的，MDK只是把你的程序用到的RAM以及堆栈RAM给初始化，其它RAM的内容是不管的。如果你要使用绝对地址访问MDK未初始化的RAM，那就要小心翼翼的了，因为这些RAM上电时的内容很可能是随机的，每次上电都不同。  MDK编译器如何设置非零初始化变量？ 对于控制类产品，当系统复位后（非上电复位），可能要求保持住复位前RAM中的数据，用来快速恢复现场，或者不至于因瞬间复位而重启现场设备。而keil mdk在默认情况下，任何形式的复位都会将RAM区的非初始化变量数据清零。 MDK编译程序生成的可执行文件中，每个输出段都最多有三个属性：RO属性、RW属性和ZI属性。对于一个全局变量或静态变量，用const修饰符修饰的变量最可能放在RO属性区，初始化的变量会放在RW属性区，那么剩下的变量就要放到ZI属性区了。 默认情况下，ZI属性区的数据在每次复位后，程序执行main函数内的代码之前，由编译器“自作主张”的初始化为零。所以我们要在C代码中设置一些变量在复位后不被零初始化，那一定不能任由编译器“胡作非为”，我们要用一些规则，约束一下编译器。 分散加载文件对于连接器来说至关重要，在分散加载文件中，使用UNINIT来修饰一个执行节，可以避免编译器对该区节的ZI数据进行零初始化。这是要解决非零初始化变量的关键。 因此我们可以定义一个UNINIT修饰的数据节，然后将希望非零初始化的变量放入这个区域中。于是，就有了第一种方法：   修改分散加载文件，增加一个名为MYRAM的执行节，该执行节起始地址为0x1000A000，长度为0x2000字节（8KB），由UNINIT修饰：  LR_IROM1 0x00000000 0x00080000 { ; load region size_region ER_IROM1 0x00000000 0x00080000 { ; load address = execution address *.o (RESET, +First) *(InRoot$$Sections) .ANY (+RO) } RW_IRAM1 0x10000000 0x0000A000 { ; RW data .ANY (+RW +ZI) } MYRAM 0x1000A000 UNINIT 0x00002000 { .ANY (NO_INIT) } } 那么，如果在程序中有一个数组，你不想让它复位后零初始化，就可以这样来定义变量：  unsigned char plc_eu_backup[32] __attribute__((at(0x1000A000))); 变量属性修饰符__attribute__((at(adde)))用来将变量强制定位到adde所在地址处。由于地址0x1000A000开始的8KB区域ZI变量不会被零初始化，所以位于这一区域的数组plc_eu_backup也就不会被零初始化了。 这种方法的缺点是显而易见的：要程序员手动分配变量的地址。如果非零初始化数据比较多，这将是件难以想象的大工程（以后的维护、增加、修改代码等等）。所以要找到一种办法，让编译器去自动分配这一区域的变量。  分散加载文件同方法1，如果还是定义一个数组，可以用下面方法：  unsigned char plc_eu_backup[32] __attribute__((section(&#34;NO_INIT&#34;),zero_init)); 变量属性修饰符__attribute__((section(“name”),zero_init))用于将变量强制定义到name属性数据节中，zero_init表示将未初始化的变量放到ZI数据节中。因为“NO_INIT”这显性命名的自定义节，具有UNINIT属性。   将一个模块内的非初始化变量都非零初始化
假如该模块名字为test.c，修改分散加载文件如下所示：
  LR_IROM1 0x00000000 0x00080000 { ; load region size_region ER_IROM1 0x00000000 0x00080000 { ; load address = execution address *.o (RESET, +First) *(InRoot$$Sections) } RW_IRAM1 0x10000000 0x0000A000 { ; RW data .ANY (+RW +ZI) } RW_IRAM2 0x1000A000 UNINIT 0x00002000 { test.o (+ZI) } } 在该模块定义时变量时使用如下方法： 这里，变量属性修饰符__attribute__((zero_init))用于将未初始化的变量放到ZI数据节中变量，其实MDK默认情况下，未初始化的变量就是放在ZI数据区的。 ]]></content>
  </entry>
  
  <entry>
    <title>传三星斩获AMD部分订单</title>
    <url>/post/news/samsung-is-rumored-to-have-won-some-orders-from-AMD.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>AMD</tag>
      <tag>Samsung</tag>
      <tag>Zen 5c</tag>
    </tags>
    <content type="html"><![CDATA[据报道，三星已获得 AMD  的订单，将使用其的4nm工艺技术，为AMD生产基于Zen 5c架构的处理器。目前，这一消息在多个渠道得到了证实。
如果这笔交易最终达成，这标志着AMD对其制造战略进行了调整，以实现多元化供应。其中，AMD的考量已经从工艺节点、生产良率、成本等因素，扩展到产能、生态链等多个角度。
据悉，AMD的“Prometheus”处理器将由三星和台积电共同生产。其中，三星将使用其先进的4nm工艺技术制造基础版本的“Prometheus”处理器，而台积电则将使用更先进的3nm工艺技术来生产更高级别的“Prometheus”处理器。
如今，三星正在积极扩展其4nm工艺，以能够从台积电那里获得更多订单。在关键的良率方面，业内人士普遍认为，台积电的4nm工艺良率达到了80%，而三星的4nm工艺已经从年初的50%提升到75%，与台积电相当。对此，外界猜测高通、英伟达等大客户可能会回流采用三星工艺。
据此前曝出的苹果高层会议纪录显示，台积电3nm制程的良率接近63%，报价比4nm高出一倍。因此，随着三星今年较大幅度改善良率和产能，再加上台积电今年决定涨价，很多大客户可能会从成本等角度考量，寻找第二供应商以分散外包生产订单。
]]></content>
  </entry>
  
  <entry>
    <title>AMD能否借助MI300加速器再次加速</title>
    <url>/post/soc/can-AMD-accelerate-again-with-the-MI300-accelerator.html</url>
    <categories><category>SOC</category>
    </categories>
    <tags>
      <tag>AMD</tag>
      <tag>MI300</tag>
    </tags>
    <content type="html"><![CDATA[ AMD  宣布将于 2023 年 12 月 6 日举办“ Advancing AI ”现场直播活动，AMD董事会主席兼首席执行官苏姿丰博士（Dr. Lisa Su）将携手其他AMD高管、AI生态系统合作伙伴和客户共同探讨AMD产品和软件将如何重塑AI和自适应高性能计算领域。
基本上板上钉钉的是，这场活动中的“聚焦点”，是AMD将推出下一代 AMD Instinct MI300 数据中心 GPU 加速器系列。
总结：
 AMD的 MI300 AI 加速器可能成为该公司未来几年的主要增长动力，使其能够与NVIDIA等公司竞争。 MI300A以及MI300X加速器预计将明显增强AMD的数据中心业务，并有可能产生10亿美元的销售额。 AMD 在数据中心人工智能市场的强势地位，加上对 ROCm 软件生态系统的投资，将进一步支持其增长潜力。  MI300加速器预计成为AMD最快达成10亿美元销售额产品 在2023 财年第三季度财报电话会议上，AMD 管理层预计第四季度数据中心 GPU 收入约为 4 亿美元，2024 年将超过 20 亿美元，全年收入将持续增长。
22财年，AMD在数据中心业务上创造了60亿美元的收入，占集团总收入的25.6%。MI300A和MI300X加速器的出货极大地促进了他们的数据中心业务。Lisa Su表示，MI300 预计将成为公司历史上销售额最快达到 10 亿美元的产品。
MI300A是一款由1460亿个晶体管组成的CPU+GPU加速器，而MI300X则是一款专为数据中心市场设计的纯GPU产品。据透露，AMD有望在未来几周内开始向领先的云和 OEM 客户生产 Instinct MI300X GPU 加速器。此外，Instinct MI300A APU 于 10 月初开始生产发货，以支持 El Capitan Exascale 超级计算机。
MI300系列和NVIDIA H100的AI争夺战 MI300A 和 MI300X 都将成为 AMD 未来几年的重要增长动力。首先，尽管 AMD 的 GPU 产品比 NVIDIA更晚进入市场，但 MI300X 将8个加速器集成到一个平台上，并具有1.5TB HBM3 内存。凭借如此强大的计算能力，MI300X非常适合AI机器学习中的大型语言模型。
业界谈论到AI加速器，NVIDIA是绕不开的话题。
AMD 的 MI300X 使该公司能够与NVIDIA的 H100 GPU 产品竞争。Lisa Su声称MI300X提供5.2TBps的内存带宽，比NVIDIA H100 GPU好1.6倍。
值得注意的是，Lisa Su指的是NVIDIA H100 SXM版本，但NVIDIA同样具有高版本的H100 NVL——通过NVLink桥接2个GPU，提供7.8 TBps的内存带宽——仍然略高于AMD的MI300X。
不过，AMD MI300X的强大，足以满足大型语言模型的计算需求。
数据中心人工智能市场规模巨大，AMD预计今年的潜在市场总额将达到300亿美元，预计到2027年将增长至1500亿美元。
这些都给AMD在此领域留下足够想象的扩展空间。
ROCm 软件生态系统 成功的软件对于人工智能加速器的重要性不可低估。
NVIDIA 的 CUDA 软件已经成功建立了其生态系统，涵盖硬件、软件和外部合作伙伴。同样，AMD 也一直在投资其 ROCm 软件。最新的 ROCm 软件套件完全支持 AMD 的 MI300 加速器。
由于 CUDA 的先发优势，GitHub 上的许多现有代码主要基于 CUDA，为弥补这一差距，AMD 一直在为 AMD GPU 开发 PyTorch/TensorFlow 代码环境。该环境可与 AMD GPU 上基于 CUDA 的代码存储库兼容，从而促进 AMD 生态系统的扩展，并帮助客户更高效地构建机器学习应用程序。
顺应AI计算的浪潮。新推出的 MI300 AI 加速器预计将维持收入增长的高速势头。许多企业优先考虑云和人工智能投资，极大地推动了对加速器的需求。
研究和市场预测 GPU 市场的复合年增长率将达到 29.57%，将从 2021 年的 310 亿美元增至 2028 年的 1900 亿美元。
]]></content>
  </entry>
  
  <entry>
    <title>AMD下一代处理器进化！正式迈入3nm，混合使用三星4nm工艺</title>
    <url>/post/soc/amd-next-generation-chips-will-use-3nm-and-4nm-manufacture-technology.html</url>
    <categories><category>SOC</category>
    </categories>
    <tags>
      <tag>AMD</tag>
      <tag>CPU</tag>
      <tag>GPU</tag>
    </tags>
    <content type="html"><![CDATA[ AMD  这几年的处理器和GPU芯片一直和台积电有着积极的合作，处理器和显卡都已经用上了台积电的5nm工艺。而从最新的报道来看，在下一代处理器，AMD很可能用上台积电的3nm工艺，不过具体是不是Zen 5现在还要打上一个问号，目前来看更大可能是Zen 5C这样的小核心会用上台积电最新的先进工艺。
事实上在之前的AMD路线图中，在下一代Zen 5架构的处理器中，就已经明确表示有Zen 5和Zen 5C，就和当下有Zen 4以及Zen 4C两种架构一样。两者在IP架构部分其实区别并不大，只是规模和尺寸大小，功能部分其实差异不多，和Intel的大小核设计区别还是很大的。而在最新的了解的AMD下一代处理器IP所运用的制程技术中，则包括了台积电N3制程及三星4nm制程，这多少暴露了AMD的设计思路。
过去AMD的路线图中，明确表示Zen 5和Zen 5C会使用4nm和3nm的制程，Zen 5明年就要发布，我们认为赶上台积电N3工艺的可能性不是太大，目前台积电主要是为苹果生产3nm的芯片。所以更可能是Zen 5采用台积电4nm工艺，而更晚发布对能耗要求更高的Zen 5C才会用上台积电的3nm工艺。
先前就有传言表示，AMD在寻求和三星的合作，或许会把部分产能交给三星代工运用其4nm制程技术，但具体规模仍不确定。不过AMD不太可能让三星4nm代工任何重要的IP，所以或许会利用三星4nm进行试产或代工特定I/O晶片。如果是这样，那么未来Zen 5C的主芯片还是采用台积电的3nm，而I/O芯片则会使用三星4nm，类似于目前RDNA 3的设计。
除此之外，Zen 4架构代号为Persephone、Zen 5代号为Nirvana、Zen 6代号为Morpheus。Zen4C代号为Dionysus，而Zen 5C代号则是Prometheus。AMD的Zen 5及Zen 5C核心架构是2024~2025年的重头戏，将运用于代号为Strix Point的笔记本处理器、Granite Ridge的桌机处理器）及Turin服务器处理器等系列的产品上。
无论如何，明年我们会看到Intel和AMD在工艺上的双双进步，Intel的桌面处理器会进入到Intel 4时代，实际也就是Intel自己的7nm工艺；而AMD的新品则会在台积电4nm和3nm之间，届时两家公司的产品在性能部分相信都会有新的进展。而对于AMD用户的好消息是，至少内存和主板都不用更换，就能升级到新一代处理器上！
]]></content>
  </entry>
  
  <entry>
    <title>嵌入式高级工程师10年经验总结</title>
    <url>/post/mcu/summary-of-10-years-of-experience-as-a-senior-embedded-engineer.html</url>
    <categories><category>MCU</category>
    </categories>
    <tags>
      <tag>Embedded</tag>
    </tags>
    <content type="html"><![CDATA[本文是一位 嵌入式  高级工程师对10年工作经验的总结。
嵌入式系统的概念 理解“嵌入”的概念 ，主要从三个方面展开。
 从硬件上，“嵌入”将基于CPU的处围器件，整合到CPU芯片内部，比如早期基于X86体系结构下的计算机，CPU只是有运算器和累加器的功能，一切芯片要造外部桥路来扩展实现，象串口之类的都是靠外部的16C550/2的串口控制器芯片实现，而目前的这种串口控制器芯片早已集成到CPU内部，还有PC机有显卡，而多数嵌入式处理器都带有LCD控制器，但其种意义上就相当于显卡。  比较高端的ARM类Intel Xscale架构下的IXP网络处理器CPU内部集成PCI控制器(可配成支持4个PCI从设备或配成自身为CPI从设备);还集成3个NPE网络处理器引擎，其中两个对应于两个MAC地址， 可用于网关交换用，而另外一个NPE网络处理器引擎支持DSL，只要外面再加个PHY芯片即可以实现DSL上网功能。IXP系列最高主频可以达到 1.8G，支持2G内存，1G×10或10G×1的以太网口或Febre channel的光通道。IXP系列应该是目标基于ARM体系统结构下由intel进行整合后成Xscale内核的最高的处理器了。
 从软件上，嵌入就是在定制操作系统内核里将应用一并选入，编译后将内核下载到ROM中。而在定制操作系统内核时所选择的应用程序组件就是完成了软件的“嵌入”，比如Win在内核定制时，会有相应选择，其中就是wordpad,PDF,MediaPlay等等选择，如果我们选择了在CE启动后，就可以在界面中找到这些东西，如果是以前PC上装的windows操作系统，多半的东西都需要我们再装。
  把软件内核或应用文件系统等东西烧到嵌入式系统硬件平台中的ROM中就实现了一个真正的“嵌入”。
  以上的定义是我在6、7年前给嵌入式系统下自话侧重于理解型的定义，书上的定义也有很多，但在“嵌入式”这个领域范围内，谁都不敢说自己的定义是十分确切的，包括那些专家学者们，历为毕竟嵌入式系统是计算机范畴下的一门综合性学科。
嵌入式系统的分层与专业的分类 嵌入式系统分为4层，硬件层、驱动层、操作系统层和应用层。
 硬件层，是整个嵌入式系统的根本，如果现在单片机及接口这块很熟悉，并且能用C和汇编语言来编程的话，从嵌入式系统的硬件层走起来相对容易，硬件层也是驱动层的基础，一个优秀的驱动工程师是要能够看懂硬件的电路图和自行完成CPLD的逻辑设计的，同时还要对操作系统内核及其调度性相当的熟悉的。但硬件平台是基础，增值还要靠软件。  硬件层比较适合于，电子、通信、自动化、机电一体、信息工程类专业的人来搞，需要掌握的专业基础知识有，单片机原理及接口技术、微机原理及接口技术、C语言。
驱动层，这部分比较难，驱动工程师不仅要能看懂电路图还要能对操作系统内核十分的精通，以便其所写的驱动程序在系统调用时，不会独占操作系统时间片，而导致其它任务不能动行，不懂操作系统内核架构和实时调度性，没有良好的驱动编写风格，按大多数书上所说添加的驱动的方式，很多人都能做到，但可能连个初级的 驱动工程师的水平都达不到，这样所写的驱动在应用调用时就如同windows下我们打开一个程序运行后，再打开一个程序时，要不就是中断以前的程序，要不就是等上一会才能运行后来打开的程序。想做个好的驱动人员没有三、四年功底，操作系统内核不研究上几编，不是太容易成功的，但其工资在嵌入式系统四层中可是最高的。  嵌入式的驱动层比较适合于电子、通信、自动化、机电一体、信息工程类专业尤其是计算机偏体系结构类专业的人来搞，除硬件层所具备的基础学科外，还要对数据结构与算法、操作系统原理、编译原理都要十分精通了解。
 操作系统层，对于操作系统层目前可能只能说是简单的移植，而很少有人来自已写操作系统，或者写出缺胳膊少腿的操作系统来，这部分工作大都由驱动工程师来完成。操作系统是负责系统任务的调试、磁盘和文件的管理，而嵌入式系统的实时性十分重要。据说，XP操作系统是微软投入300人用两年时间才搞定的，总时工时是600人年，中科院软件所自己的女娲Hopen操作系统估计也得花遇几百人年才能搞定。因此这部分工作相对来讲没有太大意义。
  应用层，相对来讲较为容易的，如果会在windows下如何进行编程接口函数调用，到操作系统下只是编译和开发环 境有相应的变化而已。如果涉及Jave方面的编程也是如此的。嵌入式系统中涉及算法的由专业算法的人来处理的，不必归结到嵌入式系统范畴内。但如果涉及嵌 入式系统下面嵌入式数据库、基于嵌入式系统的网络编程和基于某此应用层面的协议应用开发(比如基于SIP、H.323、Astrisk)方面又较为复杂， 并且有难度了。
  目标与定位 先有目标，再去定位。
学 ARM  ，从硬件上讲，一方面就是学习接口电路设计，另一方面就是学习汇编和C语言的板级编程。如果从软件上讲，就是要学习基于ARM处理器的操作系统层面 的驱动、移植了。这些对于初学都来说必须明确,要么从硬件着手开始学，要么从操作系统的熟悉到应用开始学，但不管学什么，只要不是纯的操作系统级以上基于 API的应用层的编程，硬件的寄存器类的东西还是要能看懂的，基于板级的汇编和C编程还是要会的。因此针对于嵌入式系统的硬件层和驱动程的人，ARM的接 口电路设计、ARM的C语言和汇编语言编程及调试开发环境还是需要掌握的。
因此对于初学者必然要把握住方向，自己学习嵌入式系统的目标是什么，自己要在那一层面上走。然后再着手学习较好，与ARM相关的嵌入式系统的较为实际的两个层面硬件层和驱动层，不管学好了那一层都会很有前途的。
如果想从嵌入式系统的应用层面的走的话，可能与ARM及其它体系相去较远，要着重研究基嵌入式操作系统的环境应用与相应开发工具链，比如WinCe操作系统下的EVC应用开发(与windows下的VC相类似)，如果想再有突破就往某些音视频类的协议上靠，比如VOIP领域的基于SIP或H.323协议的应用层开发，或是基于嵌入式网络数据库的开发等等。
对于初学者来讲，要量力而行，不要认为驱动层工资高就把它当成方向了，要结合自身特点，嵌入式系统四个层面上那个层面上来讲都是有高人存在，当然高人也对应 的高工资，我是做硬件层的，以前每月工资中个人所得税要被扣上近3千大元，当然我一方面充当工程师的角色，一方面充当主管及人物的角色，两个职位我一个人 干，但上班时间就那些。硬件这方面上可能与我PK的人很少了，才让我拿到那么多的工资。
开发系统选择 很多ARM初学者都希望有一套自己能用的系统，但他们住住会产生一种错误认识就是认为处理器版本越高、性能越高越好，就象很多人认为ARM9与ARM7好， 我想对于初学者在此方面以此入门还应该理智，开发系统的选择最终要看自己往嵌入式系统的那个方向上走，是做驱动开发还是应用，还是做嵌入式系统硬件层设计与板级测试。如果想从操作系统层面或应用层面上走，不管是驱动还是应用，那当然处理器性能越高越好了，但嵌入式系统这个东西自学，有十分大的困难，不是几个月或半年 或是一年二年能搞定的事。
在某种意义上请，ARM7与9的差别就是在某些功能指令集上丰富了些，主频提高一些而已，就比如286和386。对于用户来讲可能觉查不到什么，只能是感觉速度有些快而已。
ARM7比较适合于那些想从硬件层面上走的人，因为ARM7系列处理器内部带MMU的很少，而且比较好控制，就比如S3C44B0来讲，可以很容易将 Cache关了，而且内部接口寄存器很容易看明白，各种接口对于用硬件程序控制或AXD单步命令行指令都可以控制起来，基于51单片机的思想很容易能把他搞懂，就当成个32位的单片机，从而消除很多51工程师想转为嵌入式系统硬件ARM开发工程师的困惑，从而不会被业界某此不是真正懂嵌入式烂公司带到操作 系统层面上去，让他们望而失畏，让业界更加缺少这方面的人才。
而嵌入式系统不管硬件设计还是软件驱动方面都是十分注重接口这部分的，选择平台还要考察一个处理器的外部资源，你接触外部资源越多，越熟悉他们那你以后就业成功的机率就越高，这就是招聘时 所说的有无“相关技能”，因为一个人不可能在短短几年内把所有的处理器都接触一遍，而招聘单位所用的处理器就可能是我们完全没有见过的，就拿台湾数十家小公司(市价几千万)的公司生产的ARM类处理器，也很好用，但这些东西通用性太差，用这些处理器的公司就只能招有相关工作经验的人了，那什么是相关工作经验，在硬件上讲的是外围接口设计，在软件上讲是操作系统方面相关接口驱动及应用开发经验。我从业近十年，2000年ARM出现，我一天始做ARM7,然后 直接跑到了Xscale(这个板本在ARM10-11之间)，一做就是五年，招人面试都不下数百人，在这些方面还是深有体会的。
#3 高级嵌入式系统硬件工程师必备技能
对于硬件来讲有几个方向，就单纯信号来分为数字和模拟，模拟比较难搞，一般需要很长的经验积累，单单一个阻值或容值的精度不够就可能使信号偏差很大。因此年轻人搞的较少，随着技术的发展，出现了模拟电路数字化，比如手机的Modem射频模块，都采用成熟的套片，而当年国际上只有两家公司有此技术，自我感觉模拟功能不太强的人，不太适合搞这个，如果真能搞定到手机的射频模块，只要达到一般程度可能月薪都在15K以上。
另一类就是数字部分了，在大方向上又可分为51/ARM的单片机类，DSP类，FPGA类， 国内FPGA的工程师大多是在IC设计公司从事IP核的前端验证，这部分不搞到门级，前途不太明朗，即使做个IC前端验证工程师，也要搞上几年才能胜任。
DSP硬件接口比较定型，如果不向驱动或是算法上靠拢，前途也不会太大。而ARM单片机类的内容就较多，业界产品占用量大，应用人群广，因此就业空间极 大，而硬件设计最体现水平和水准的就是接口设计这块，这是各个高级硬件工程师相互PK，判定水平高低的依据。而接口设计这块最关键的是看时序，而不是简单的连接，比如PXA255处理器I2C要求速度在100Kbps，如果把一个I2C外围器件，最高还达不到100kbps的与它相接，必然要导致设计的失败。
这样的情况有很多，比如51单片机可以在总线接LCD，但为什么这种LCD就不能挂在ARM的总线上，还有ARM7总线上可以外接个Winband的SD卡控制器，但为什么这种控制器接不到ARM9或是Xscale处理器上，这些都是问题。因此接口并不是一种简单的连接，要看时序，要看参数。
一个优秀的硬件工程师应该能够在没有参考方案的前提下设计出一个在成本和性能上更加优秀的产品，靠现有的方案，也要进行适当的可行性裁剪，但不是胡乱的来，我遇到一个工程师把方案中的5V变1.8V的DC芯片，直接更换成LDO，有时就会把CPU烧上几个。
前几天还有人希望我帮忙把他们以前基于PXA255平台的手持GPS设备做下程序优化，我问了一下情况，地图是存在SD卡中的，而SD卡与PXA255的MMC控制器间采用的SPI接口，因此导致地图读取速度十分的慢，这种情况是设计中严重的缺陷，而不是程序的问题，因此我提了几条建议，让他们更新试下再说。因此想成为一个优秀的工程师，需要对系统整体性的把握和对已有电路的理解，换句话说，给你一套电路图你 终究能看明白多少，看不明白80%以上的话，说明你离优秀的工程师还差得远哪。其次是电路的调试能力和审图能力，但最最基本的能力还是原理图设计PCB绘制，逻辑设计这块。
这是指的硬件设计工程师，从上面的硬件设计工程师中还可以分出ECAD工程师，就是专业的画PCB板的工程师，和EMC设计工程师，帮 人家解决EMC的问题。硬件工程师再往上就是板级测试工程师，就是C语功底很好的硬件工程师，在电路板调试过程中能通过自已编写的测试程序对硬件功能进行 验证。然后再交给基于操作系统级的驱动开发人员。
总之，硬件的内容很多很杂，硬件那方面练成了都会成为一个高手，我时常会给人家做下方案评估，很多高级硬件工程师设计的东西，经常被我一句话否定，因此工程师做到我这种地步，也会得罪些人，但硬件的确会有很多不为人知的东西，让很多高级硬件工程师也摸不到头脑。
那么高级硬件件工程师技术技能都要具备那些东西哪，首先要掌握EDA设计的辅助工具类如Protelor、CAD、PowperPCB、Maplux2、ISE、VDHL语言，要能用到这些工具画图画板做逻辑设计，再有就是接口设计审图能力，再者就是调试能力，如果能走到总体方案设计这块，那就基本上快成为资深工程师了。
硬件是要靠经验，也要靠积累的，正所谓十年磨一剑。
]]></content>
  </entry>
  
  <entry>
    <title>GH200，来自英伟达的AI超级计算机</title>
    <url>/post/news/gh200-from-nvidia.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Nvidia</tag>
      <tag>GH200</tag>
    </tags>
    <content type="html"><![CDATA[Nvidia的GH200 AI超级计算机是一款巨大的进步，相比于之前的H100，其在使用TB级数据训练和部署AI模型方面的性能提升了3-7倍。
GH200拥有144TB的共享内存和141GB HBM3e内存，这个共享内存比标准的A100多出了500倍。
每一个GH200 GPU都能够访问其他GPU的内存，以及所有NVIDIA Grace CPU的扩展GPU内存，速度高达每秒900GB。这大大加快了大型语言模型（LLM）的训练速度，并显著降低了LLM推理的成本。有了这些超级计算机，就不再需要成千上万的机器来训练和运行LLM的推理。
有了GH200，就不再需要成千上万台机器来训练LLM和进行推理，这对初创公司和大型企业都是一个变革性的时刻。特别是对那些旨在训练、微调和部署LLM的初创公司来说，GH200为它们提供了以前只有大公司才能拥有的计算能力。
随着时间的推移，硬件将变得更加强大和便宜，不久的将来，我们甚至可能在我们的笔记本电脑上运行我们自己的强大的个人LLM。
]]></content>
  </entry>
  
  <entry>
    <title>英伟达发布H200, 高性价比全面升级</title>
    <url>/post/news/nvidia-launch-h200-gpu.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Nvidia</tag>
      <tag>H200</tag>
      <tag>H100</tag>
    </tags>
    <content type="html"><![CDATA[在11月13日的S23大会上， NVIDIA  宣布推出NVIDIA HGX H200，为生成式人工智能和高性能计算（HPC）工作负载提供强大支持。
根据介绍，新的H200 GPU是当前H100的升级产品，作为第一款搭载 HBM3e 的 GPU，H200 集成了141GB的内存，更大更快的内存推动生成式人工智能和大型语言模型（LLM）的加速，同时推进科学计算在 HPC 工作负载中的发展，在用于推理或生成问题答案时，性能较H100提高60%至90%。
H200主要在内存带宽和内存容量上进行提升 为了最大化计算性能，H200 是全球首款搭载 HBM3e 内存的 GPU，拥有4.8TB/s的内存带宽，较 H100 增加了1.4倍。H200 还将 GPU 的内存容量扩展到141GB，几乎是 H100 的80GB 的两倍。更快速和更大容量的 HBM 内存的结合，加速了计算密集型的生成式人工智能和高性能计算应用的性能，同时满足了不断增长的模型大小的需求。对比 H100 和 H200 的性能参数，在算力指标上H200和H100完全一样。H200 的所有改进都集中在更快速和更高容量的141GB HBM3e 的引入。
H200相较于H100在推理性能上最高提高至2倍 H200的推理表现相较于H100在各大不同的模型都具有明显的提升。与处理类似 Llama2（一个 700 亿参数的 LLM）的LLMs时相比，相较于 H100 GPU，H200 在推理速度上提高了最多 2 倍。
高内存带宽激发高性能计算表现 内存带宽对于高性能计算应用至关重要，因为它能够实现更快的数据传输，减少复杂的问题处理。对于内存密集型的高性能计算场景，如仿真、科学研究和AI，H200 的更高内存带宽确保数据可以被高效地访问和操作，从而在与 CPU 相比，实现高达 110 倍更快的生成结果时间。
降低能耗和总拥有成本（TCO） H200 的引入使得AI数据中心能源效率和TCO进一步提升。H200在性能相较H100具有明显提升的背景下，保持了与 H100 相同的功耗。以Llama2 70B这样的模型去推理衡量看，H200的性能是H100的两倍，在功耗相同的情况下，成本是原来单位成本的一半。
]]></content>
  </entry>
  
  <entry>
    <title>GPT4.0 终于可以免费使用了</title>
    <url>/post/news/gpt-4.0-is-finally-free-to-use.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>GPT 4.0</tag>
      <tag>Copilot</tag>
      <tag>Microsoft Ignite</tag>
    </tags>
    <content type="html"><![CDATA[2023年末将至，今年几乎每个产业都在经历着集体转型， AI  技术的发展为工作方式带来了新的变革。在最近的Microsoft Ignite技术大会上，微软展示了他们最新的技术和产品进展，让每个人都能体验到科技创新的力量。
Microsoft Ignite是一年一度针对开发者和IT专业人士举办的大会。AIGC无疑是本届大会的最大的热点。而最重磅的莫过于微软宣布，Bing Chat更名为Copilot，面向所有用户免费开启GPT-4功能。
Bing Chat正式更名为Copilot，品牌升级意味着Copilot正在成为一个独立的新产品，用户无需先导航到Bing就可以访问。Copilot现在可以通过Bing和Windows使用，与ChatGPT一样，微软Copilot也有了自己的独立域名：copilot.microsoft.com，但与ChatGPT不同，Copilot上像GPT-4、DALL-E 3等功能全部免费开放！你只需登录微软账户即可使用（而ChatGPT需要订阅会员，每月20刀）。
就连OpenAI上周发布的GPT自定义版本，微软也开始免费提供，命名为Copilot Studio。
此前，微软内部曾有十几种产品共享“Copilot”品牌，Copilot品牌起源于2021年的 GitHub Copilot，今年微软又推出了Dynamics 365 Copilot、Windows Copilot、Microsoft Security Copilot和Microsoft 365 Copilot等，似乎每项业务都有自己的“Copilot”。
那为什么要在Copilot家族中再添两名新成员？微软表示是为了给消费者和企业客户带来统一的Copilot体验。
现在，Bing Chat成为了微软的第六个Copilot，也可以视作是最通用的一个Copilot。未来，Bing将为其提供支持。另外，Copilot基于最新OpenAI模型，包括GPT-4和DALL-E 3，提供统一的文本和图像生成功能。
有分析认为，Bing Chat本身并未为Bing带来太多效益，所以微软将其从搜索引擎中剥离出来。此举颇为讽刺，因为微软此前大力利用AI产品来提升自家搜索引擎，意图从谷歌手中夺取市场份额。今年初，微软CEO纳德拉甚至称“谷歌是搜索领域800磅重的大猩猩”，微软将让它“跳舞”。但谷歌并未像微软那样急于在搜索中加入生成式AI。从现在看来，这种“不变”并未对谷歌造成什么不良影响：在Bing Chat推出近10个月后，谷歌搜索引擎的市场份额仍超过91%。
微软特别提到，从12月1日起，使用Microsoft Entra登录时，在Bing、Edge和Windows中使用Copilot的用户将享有商业数据保护功能。Copilot不会保存用户的prompt和回复，微软无法直接访问这些数据，也不会用于模型训练。
此外，微软还推出了其他Copilot产品：Microsoft Copilot Studio、Copilot for Azure、Copilot for Service和Copilot in Dynamics 365 Guides。
其中，Microsoft Copilot Studio对应OpenAI上周发布的GPTs产品。该新平台提供将微软AI应用与第三方数据连接的工具，企业可以用它创建定制的Copilot或集成自定义的ChatGPT对话机器人。该产品已向现有Copilot for Microsoft 365订阅用户开放公测版。
现在，无论是Edge、Chrome、Safari，还是移动端，都可以使用Copilot。需要注意的是，虽然Copilot仅需要登录微软账户就可免费使用，但Microsoft 365等其他Copilot产品仍需付费。
与OpenAI GPTs相比，微软Copilot Studio还有些不同。它的主要设计目的是扩展Microsoft 365 Copilot。用户可以利用该应用自定义集成不同数据集和自动化流程的Copilot。
除了将Bing Chat改名Copilot，微软还发布了期待已久的自研芯片。这是为云基础设施设计的两款高端定制芯片，分别是Azure Maia 100 AI芯片和Cobalt 100 CPU。两款芯片均由微软内部构建，深度针对整个云服务器堆栈进行了优化，在性能、功耗和成本方面具有竞争优势，计划2024年推出。
]]></content>
  </entry>
  
  <entry>
    <title>恒流电路的三种设计方案</title>
    <url>/post/hardware/three-design-options-for-constant-current-circuits.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>Circuit Design</tag>
      <tag>Constant Current</tag>
    </tags>
    <content type="html"><![CDATA[作为硬件研发工程师相信对恒流电路不会陌生，本文介绍下三种恒流电路的原理图。
三极管恒流电路 三极管的恒流电路，主要是利用Q2三极管的基级导通电压为0.6~0.7V这个特性；当Q2三极管导通，Q1三极管基级电压被拉低而截止，负载R1不工作；负载R1流过的电流等于R6电阻的电流（忽略Q1与Q2三极管的基级电流），R6电阻的电流等于R6电阻两端的0.6~0.7V电压除以R6电阻阻值（固定不变），因此流过R1负载的电流即为恒定不变，即使R1负载的电源端VCC电压是可变的，也能达到恒流的电路效果。
运放恒流电路 运放的恒流电路，主要是利用运放的“电压跟随特性”，即运放的两个输入引脚Pin3与Pin2电压相等电路特性；当在电阻R4输入Vin稳定电源电压时，电阻R7两端的电压也为Vin不变，因此无论外界电路如何变化，流过R7电阻的电流是不变的；同三极管恒流电路原理分析一样，R2负载的电流等于R7电阻的电流，所以即使R2负载的电源为可变电压电源，R2负载的电流也是保持固定不变，达到恒流的效果。
除去运用三极管与运放设计的恒流电路，芯片哥介绍另外一种恒流电路设计方案，主要是利用稳压二极管的稳压特性。
稳压二极管恒流电路 稳压二极管的恒流电路中，三极管Q4的基级电压被限定在稳压二极管工作的稳定电压Uzd下，因此R10电阻的电压等于Uzd减去三极管基级与发射级的导通压降0.7V，即U=Uzd-0.7保持恒定不变，所以流过R10电阻的电流在VCC电源即使可变的条件下也是固定不变，也就是R8负载的电流保持不变，达到恒流的效果。
原文引用： 恒流电路的三种设计方案  
]]></content>
  </entry>
  
  <entry>
    <title>一图说清8 种流行的网络协议</title>
    <url>/post/datacenter/8-popular-network-protocols.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Network Protocol</tag>
      <tag>HTTP</tag>
      <tag>HTTPS</tag>
      <tag>WebSocket</tag>
      <tag>TCP</tag>
      <tag>UDP</tag>
      <tag>SMTP</tag>
      <tag>FTP</tag>
    </tags>
    <content type="html"><![CDATA[网络协议是在网络中的两台计算机之间传输数据的标准方法。
 HTTP（超文本传输协议：HyperText Transfer Protocol)  HTTP 是一种用于获取 HTML 文档等资源的协议。它是 Web 上任何数据交换的基础，是一种客户端-服务器协议。
HTTP/3 HTTP/3  HTTP/3 是 HTTP 的下一个主要修订版。它运行在 QUIC 上，这是一种专为移动互联网使用而设计的新传输协议。它依赖于 UDP 而不是 TCP，从而实现更快的网页响应。VR 应用程序需要更多带宽来渲染虚拟场景的复杂细节，并且可能会从迁移到由 QUIC 提供支持的 HTTP/3 中受益。
HTTPS（安全超文本传输协议：HyperText Transfer Protocol Secure)  HTTPS 扩展了 HTTP 并使用加密技术实现安全通信。
WebSocket  WebSocket 是一种通过 TCP 提供全双工通信的协议。客户端建立WebSocket来接收来自后端服务的实时更新。与总是“拉取”数据的 REST 不同，WebSocket 允许“推送”数据。在线游戏、股票交易和消息传递应用程序等应用程序利用 WebSocket 进行实时通信。
TCP（传输控制协议：Transmission Control Protocol)  TCP 旨在通过互联网发送数据包并确保通过网络成功传送数据和消息。许多应用层协议构建在 TCP 之上。
UDP（用户数据报协议：User Datagram Protocol)  UDP 直接将数据包发送到目标计算机，无需先建立连接。UDP 通常用于时间敏感的通信，其中偶尔丢弃数据包比等待更好。语音和视频流量通常使用此协议发送。
SMTP（简单邮件传输协议：Simple Mail Transfer Protocol)  SMTP 是一种用于将电子邮件从一个用户传输到另一个用户的标准协议。
FTP（文件传输协议：File Transfer Protocol)  FTP 用于在客户端和服务器之间传输计算机文件。它具有单独的控制通道和数据通道连接。
]]></content>
  </entry>
  
  <entry>
    <title>大型语言模型入门必知术语汇总</title>
    <url>/post/datacenter/terminology-of-large-scale-language-models.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Large Scale</tag>
      <tag>Terminology</tag>
      <tag>AI</tag>
      <tag>LLM</tag>
    </tags>
    <content type="html"><![CDATA[在本文中，我将以一种非数据科学家易于理解的方式分解与LLM和AI相关的一些基本术语和概念。我将涵盖从神经网络到数据增强的所有内容，并为每个术语提供简单的解释和示例。
   人工智能（AI）  ：它就像一个智能机器人，可以像人类一样思考和做事。人工智能帮助计算机解决问题、做出决策并理解我们的语言。示例：iPhone 上的 Siri。
  深度学习：这是计算机从许多例子中学习的一种方式，比如你如何从经验中学习。深度学习使用称为神经网络的特殊计算机程序来查找数据中的模式。示例：计算机学习识别图片中的猫。
  神经网络：一种像人脑一样工作的计算机程序，使用连接的节点（如脑细胞）分层。示例：可以玩视频游戏的计算机“大脑”。
  Transformer：谷歌创建的一种特殊类型的神经网络，用于更好地理解和生成语言。示例：可以像朋友一样与您聊天的计算机。
  大型语言模型（LLM）：一种计算机程序，通过研究大量文本来学习理解和创建人类语言。示例：可以编写故事或回答问题的计算机。
  参数：神经网络中在训练期间进行调整以帮助其学习的部分。示例：就像调整吉他以使其听起来更好。
  位置编码：Transformer 记住句子中单词顺序的一种方式。示例：记住“狗追猫”与“猫追狗”不同。
  自我注意（Self-Attention）：Transformer专注于句子中最重要的部分的一种方式。示例：知道“蛋糕”是“我想吃蛋糕”中的关键词。
  编码器：Transformer 的一部分，帮助它理解并记住你告诉它的内容。示例：计算机记住问题“今天的天气怎么样？
  解码器：Transformer 的一部分，可帮助它创建响应或答案。示例：计算机回答：今天的天气晴朗而温暖。
  BERT：一种 Transformer 模型，可帮助计算机理解语言，以执行诸如猜测人们对电影的看法之类的任务。示例：知道评论是正面还是负面的计算机。
  GPT-3 和 GPT-4：一种 Transformer 模型，可帮助计算机像人类一样生成文本，例如完成句子或撰写摘要。示例：一台计算机为您编写读书报告。
  T5：擅长理解和生成文本的 Transformer 模型，例如将一种语言翻译成另一种语言。示例：可以将英语翻译成西班牙语的计算机。
  无监督学习：当计算机在没有被告知什么是对或错的情况下学习模式时。示例：一台计算机学习将相似的图片组合在一起。
  基础模型：大型AI模型，如LLM，可用于许多不同的任务。示例：可以帮助完成家庭作业、写电子邮件和讲笑话的计算机。
  零样本学习(Zero-Shot Learning)：当计算机无需接受训练即可完成任务时。示例：无需先练习即可玩新游戏的计算机。
  少样本学习(Few-Shot Learning)：当计算机只需几个例子就可以学习新任务时。示例：一台计算机，可以在听一两次后学习您喜欢的歌曲。
  微调：调整经过训练的模型以更好地完成特定任务。示例：教计算机理解和回答有关恐龙的问题。
  提示调优：改变您向计算机提问以获得更好答案的方式。示例：问“法国的首都是什么？”而不是“巴黎在哪里？”
  适配器(Adapters)：您可以添加到训练模型的微小部件中，以帮助它完成特定任务，而无需对其进行太多更改。示例：在不更改整个游戏的情况下向电脑游戏角色添加新技能。
  自然语言处理（NLP）：教计算机理解、解释和创建人类语言。示例：可以与您聊天或阅读您的论文的计算机。
  自然语言理解（NLU）：教计算机理解和寻找人类语言的含义。示例：一台知道“我喜欢猫”和“我不喜欢猫”之间区别的计算机。
  自然语言生成（NLG）：教计算机创建类似人类的文本。示例：可以写故事或诗歌的计算机。
  token化（Tokenization）：将文本分解为单词或单词的一部分，称为标记，以帮助计算机理解语言。示例：将句子“我有一只狗”拆分为标记：“我”、“有”、“一只”和“狗”。
  词汇：计算机程序可以理解的独特单词或标记集。示例：计算机知道单词“苹果”、“香蕉”和“橙子”，但不知道“猕猴桃”。
  预训练：训练LLM的第一步，它从大量文本中学习语言。示例：一台计算机阅读大量书籍和文章以学习如何写作。
  迁移学习：使用计算机从一个任务中学到的知识来帮助它完成另一个相关的任务。示例：一台计算机，它学会了识别猫，使用该知识来识别狗。
  序列到序列 （Seq2Seq） 模型：一种将一个序列（如文本）更改为另一个序列（如翻译）的模型。示例：计算机将英语文本转换为法语文本。
  注意力机制：计算机在创建输出时专注于输入重要部分的一种方式。示例：计算机知道“披萨”是“我想吃披萨”中最重要的词。
  波束搜索(Beam Search)：一种在计算机生成文本时查找最佳单词序列的方法。示例：计算机选择句子中最有可能的下一个单词。
  困惑（Perplexity）：一种衡量计算机预测文本能力的方法。示例：较低的困惑度意味着计算机更善于猜测句子中下一个单词。
  上下文学习：当计算机可以根据输入更改其行为时，无需额外的培训。示例：一台计算机在谈论体育后知道如何回答有关体育的问题。
  数据增强：通过创建新样本（如改写句子）使数据集更大、更多样化。示例：将“猫在垫子上”更改为“猫坐在垫子上”。
  偏差：当计算机因其训练数据不平衡或不具有代表性而犯错误时。示例：计算机认为所有医生都是男性，因为它主要阅读有关男性医生的信息。
  可解释的AI（XAI）：使计算机的决策过程更容易被人类理解。示例：一台计算机解释为什么它认为某部电影是喜剧。
 ]]></content>
  </entry>
  
  <entry>
    <title>Marvell再传裁员！同行争抢订单和人才</title>
    <url>/post/news/marvell-announces-layoffs-again.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Marvell</tag>
      <tag>NAND</tag>
    </tags>
    <content type="html"><![CDATA[集微网消息，受NAND Flash市况不佳以及中美贸易战限制影响，IC设计厂Marvell（美满电子）的NAND Flash控制IC业务受到冲击，业界传出Marvell会裁撤中国台湾的NAND Flash控制IC的团队，且近期已经生效，行业争抢NAND Flash控制IC订单及人才。
业界推断，虽然Marvell并非全面撤出NAND Flash控制IC业务，不过裁撤中国台湾的NAND Flash团队，表示存储业务仍较困难。
据了解，目前原厂也在争夺NAND Flash控制IC市场，以自行开发或委外设计量产的模式进行，让面向企业端市场的Marvell运营受到影响，这也导致Marvell缩编相关团队。
法人认为，Marvell裁撤中国台湾NAND Flash控制IC团队，群联受惠最大，不只有望抢下相关人才，还可扩大市占率，扩大相关企业用SSD控制IC出货量。
近两年，Marvell不断传出裁员消息。2022年10月，Marvell裁撤了上海和成都分公司的大部分研发成员。今年5月，有消息称该公司计划在全球范围内裁撤约15%左右研发人员，约1000人，其中美国本土仅占5%左右，剩余大部分裁撤名额位于中国大陆。
从全球控制芯片市场来看，三星占据约45%的市场份额；慧荣科技、群联电子、Marvell等厂商，合计占据约40%的市场份额。Marvell性能较好，一直是主控市场上性能的佼佼者，其SSD控制器基于NAND Edge LDPC引擎，支持在企业和超大规模数据中心环境中使用高性能和大容量的SSD，并兼容TLC、QLC和SLC存储器，数据安全及数据纠错能力领先。
]]></content>
  </entry>
  
  <entry>
    <title>智能网卡（SmartNIC）才是未来</title>
    <url>/post/datacenter/smart-nic-is-the-trend-for-future.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>SmartNIC</tag>
    </tags>
    <content type="html"><![CDATA[网络技术和硬件设备的不断发展已经改变了数据中心和云计算的格局。随着虚拟化、微服务架构的不断激增，以及移动设备和云服务的高度利用率，网络工作负载的增加速度已经远远超过了传统数据中心CPU的处理速度。这使得寻找更高性能、更灵活性以及更强大功能的网络解决方案变得迫切。
当涉及到网络硬件的选择时，智能网卡（SmartNIC）和标准网卡（NIC）是两个备受关注的选项。本文将深入探讨这两种网卡的特性、差异以及如何在特定用例中选择合适的选项。
标准网卡（NIC）：传统的网络连接 几十年来，标准网卡一直是网络连接的主要方式。它们是将计算机连接到网络的硬件组件，实现设备和互联网之间的通信。标准网卡主要用于数据传输和接收，通常因其可靠性和鲁棒性而在数据中心环境中得到广泛应用。
标准网卡的功能 标准网卡具有以下基本功能：
 数据包传输和接收。 数据包校验和校正。 数据包分段和重组。 MAC地址管理。  这些功能足以支持常见的网络连接需求，但在面对数据密集型应用、虚拟化环境、云计算和高性能计算等要求更高性能和功能的情况下，标准网卡可能显得力不从心。
传统硬件的限制 传统的数据中心硬件包括基于NIC的网络卡，这些硬件已经无法满足现代数据中心的需求。
CPU负担过重 传统基于NIC的架构将网络任务交给服务器的CPU来处理，这导致CPU负担过重，无法应对高强度网络工作负载。
难以满足高性能需求 计算密集型应用程序需要更高的性能和低延迟。传统硬件无法提供足够的性能，导致性能瓶颈。
难以管理和维护 随着数据中心规模的扩大，管理和维护传统硬件变得更加困难。虚拟化和微服务的增长也增加了管理的复杂性。
智能网卡（SmartNIC）：超越传统网络 智能网卡是一种新型网卡，它超越了传统的数据传输功能。它们配备了强大的处理器、内存和专用硬件，可以执行各种高级功能。这些功能包括网络增强、存储加速、安全功能等等。智能网卡的主要目标是从主机CPU上卸载和加速各种网络相关任务，提供专用处理能力和硬件功能，以增强网络性能、安全性和效率。
智能网卡的功能 智能网卡提供了一系列高级功能，包括：
 数据包过滤和负载平衡。 服务质量（QoS）实施。 存储加速，包括远程直接内存访问（RDMA）、iSCSI和NVMe over Fabrics。 安全功能，如防火墙处理和入侵检测系统（IDS）检查。  这些功能使得智能网卡成为适用于各种高级用例的强大工具，从提高网络性能到加强安全性，再到存储加速。
早期SmartNIC实现 早期的SmartNIC实现使用寄存器驱动的ASIC逻辑，这些设计在性能方面表现出色，具有极低的延迟、高数据包吞吐量和低功耗（通常在15瓦到35瓦之间）。然而，尽管在性能上具有优势，但它们通常缺乏必要的可编程性和灵活性。它们通常需要深奥的命令行工具来设置寄存器，缺乏以编程方式管理数据包和流的能力。
SmartNIC的多种部署方式 SmartNIC在多种不同的部署场景中发挥着关键作用，其中包括存储、安全和网络处理。SmartNIC可能承担的特定任务包括隧道协议（例如VxLAN）的处理以及复杂虚拟交换，如图1所示。它的最终目标是消耗更少的主机CPU处理器内核，同时以更低的成本提供更高的性能解决方案。
标准网卡与智能网卡的区别 联网功能 标准网卡主要设计用于处理基本网络任务，例如数据包传输、分段和校验和校正。虽然智能网卡也执行这些任务，但它们在卡本身上实现了更复杂的网络功能，如数据包过滤、负载平衡和服务质量（QoS）实施，从而从主机CPU上卸载了这些任务。这降低了延迟、降低了CPU利用率，并提高了整体网络性能。
存储功能 智能网卡在存储加速方面表现出色，特别适用于现代数据密集型应用。它们可以卸载远程直接内存访问（RDMA）、iSCSI和NVMe over Fabrics等存储功能，从而提高数据传输速度并减少CPU开销。相比之下，标准网卡缺乏这些存储密集型任务所需的硬件和处理能力。
任务卸载到SmartNIC 智能网卡最显著的优势之一是能够将各种任务从主机CPU卸载到网卡上。这不仅包括网络和存储功能，还包括防火墙处理和入侵检测系统（IDS）检查等安全任务。通过将这些任务转移到智能网卡，主机CPU可以腾出时间来专注于应用程序特定的处理，从而提高应用程序性能并减少服务器蔓延。
如何选择标准网卡或智能网卡 在选择标准网卡或智能网卡时，需要评估特定网络需求和用例。以下是需要考虑的一些因素：
性能和速度 如果您的应用程序需要卓越的网络性能、更低的延迟以及负载平衡、存储加速等高级功能，那么智能网卡可能是更好的选择。它们可以卸载并加速各种任务，从而提高整体性能。
工作负载和用例 考虑工作负载和用例的性质。智能网卡在数据密集型场景、虚拟化环境、机器学习和云服务中尤其具有优势，因为在这些场景中，高级网络和存储功能至关重要。如果您的环境需要更多的功能和性能，那么智能网卡可能是更合适的选择。
预算和成本 在选择网卡时，还需要考虑您的预算限制。智能网卡通常比标准网卡更昂贵，因为它们提供了更多功能和性能。如果成本是一个关键因素，那么标准网卡可能是更经济的选择。
结论 在网络连接领域，我们常常发现自己在标准网卡（NIC）和智能网卡（SmartNIC）之间做出选择。不同的用例和需求将决定您的最佳选择。一些基于Intel的以太网适配器，例如E810CAM2-2CP，具备许多高级功能，包括以太网、TCP/IP、UDP/IP等多种网络协议的支持，片上QoS、流量管理、iWARP/RDMA、RoCEv2/RDMA、智能卸载以及iSCSI和NFS的存储功能。这些适配器经过严格测试和认证，确保与各种操作系统和管理程序全面兼容，为那些寻求最佳网络性能和功能的用户提供了平衡且可靠的解决方案。
除了这些特色的NIC解决方案外，还有Mellanox SmartNIC可供选择。与Mellanox Technologies合作，这些SmartNIC开启了网络增强、卸载和安全性的无限可能。无论您选择高级NIC还是SmartNIC，都可以根据您的网络需求，确保网络以最佳、安全和高效的方式运行。
在不断演进的网络领域，选择合适的网卡是至关重要的，它将直接影响到您的网络性能、安全性和应用程序的运行。因此，深入了解标准网卡和智能网卡的优劣势以及如何根据特定需求进行选择，将有助于确保您的网络能够满足不断变化的需求。
]]></content>
  </entry>
  
  <entry>
    <title>芯片设计中的DRAM</title>
    <url>/post/fpga/DRAM-in-chip-design.html</url>
    <categories><category>FPGA</category>
    </categories>
    <tags>
      <tag>Chip</tag>
      <tag>DRAM</tag>
    </tags>
    <content type="html"><![CDATA[本文将介绍芯片设计中动态随机存取存储器（DRAM）的相关知识，包括其工作原理、分类以及在现代电子设备中的应用。
 DRAM 的基本概念  DRAM（Dynamic Random Access Memory，动态随机存取存储器）是一种用于存储数据的半导体芯片。它的基本工作原理是在一个存储单元中存储一个比特（0 或 1）的信息，并通过刷新机制来保持这些信息的稳定性。DRAM 广泛应用于各种电子设备中，如个人电脑、智能手机等。
DRAM 的工作原理  DRAM 的工作原理相对简单。它由一个存储单元阵列组成，每个存储单元都包含一个存储电容。当一个存储单元被选中时，它的电容中存储的电荷代表数据位（0 或 1）。为了读取或写入数据，DRAM 需要通过行地址和列地址进行寻址。
DRAM 的工作过程中，刷新是关键。由于存储电容会随着时间的推移而泄漏电荷，因此需要定期刷新以保持数据稳定。刷新过程通常是在读取或写入数据之后进行的。
DRAM 的分类  根据不同的应用场景和性能要求，DRAM 分为多种类型。以下是一些常见的 DRAM 类型：
 SDRAM（Synchronous DRAM）：同步 DRAM，用于高速数据传输。SDRAM 使用一个时钟信号来控制数据传输，速度较快。 DDR SDRAM（Double Data Rate SDRAM）：双倍数据速率 SDRAM，可以在每个时钟周期中读取或写入两个数据位。DDR SDRAM 进一步细分为 DDR、DDR2、DDR3 等不同版本，速度逐渐提高。 RDRAM（Rapid I/O DRAM）：快速 I/O DRAM，用于图像处理和视频卡等领域。RDRAM 具有高带宽和低延迟，但功耗较高。 LPDDR（Low Power DDR）：低功耗 DDR，用于移动设备。LPDDR 通过降低功耗来提高电池续航能力。  DRAM 在现代电子设备中的应用  随着科技的不断发展，对存储容量的需求不断增长。DRAM 在现代电子设备中的应用越来越广泛，如：
 个人电脑：DRAM 作为主存储器，用于临时存储操作系统、应用程序和用户数据。 智能手机：DRAM 用于存储操作系统、应用程序和用户数据，以及运行时的缓存。 服务器：DRAM 作为内存，用于支持大量数据处理和高速 I/O。 嵌入式设备：DRAM 用于存储程序代码和数据 ]]></content>
  </entry>
  
  <entry>
    <title>梳理STM32芯片的内部架构</title>
    <url>/post/soc/STM32-chip-internal-architecture.html</url>
    <categories><category>SOC</category>
    </categories>
    <tags>
      <tag>SOC</tag>
      <tag>UART</tag>
    </tags>
    <content type="html"><![CDATA[STM32芯片主要由内核和片上外设组成，STM32F103采用的是Cortex-M3内核，内核由 ARM  公司设计。STM32的芯片生产厂商ST，负责在内核之外设计部件并生产整个芯片。这些内核之外的部件被称为核外外设或片上外设，如 GPIO、USART（串口）、I2C、SPI 等。
 STM32 Internal Architecture  
芯片内部架构示意图 芯片内核与外设之间通过各种总线连接，其中驱动单元有 4 个，被动单元也有 4 个，具体如上图所示。可以把驱动单元理解成是内核部分，被动单元都理解成外设。
ICode 总线 ICode总线是专门用来取指令的，其中的I表示Instruction（指令），指令的意思。写好的程序编译之后都是一条条指令，存放在 FLASH中，内核通过ICode总线读取这些指令来执行程序。
DCode总线 DCode这条总线是用来取数的，其中的D表示Data（数据）。在写程序的时候，数据有常量和变量两种。常量就是固定不变的，用C语言中的const关键字修饰，放到内部FLASH当中。变量是可变的，不管是全局变量还是局部变量都放在内部的SRAM。
系统System总线 我们通常说的寄存器编程，即读写寄存器都是通过系统总线来完成的，系统总线主要是用来访问外设的寄存器。
DMA总线 DMA总线也主要是用来传输数据，这个数据可以是在某个外设的数据寄存器，可以在SRAM，可以在内部FLASH。
因为数据可以被Dcode总线，也可以被DMA总线访问，为了避免访问冲突，在取数的时候需要经过一个总线矩阵来仲裁，决定哪个总线在取数。
内部的闪存存储器Flash 内部的闪存存储器即FLASH，编写好的程序就放在这个地方。内核通过ICode总线来取里面的指令。
内部的SRAM 内部的SRAM，是通常所说的内存，程序中的变量、堆栈等的开销都是基于内部SRAM，内核通过DCode总线来访问它。
FSMC FSMC的英文全称是Flexible static memory controller（灵活的静态的存储器控制器）。通过FSMC可以扩展内存，如外部的SRAM、NAND-FLASH和NORFLASH。但FSMC只能扩展静态的内存，不能是动态的内存，比如就不能用来扩展SDRAM。
AHB 从AHB总线延伸出来的两条APB2和APB1总线是最常见的总线，GPIO、串口、I2C、SPI 这些外设就挂载在这两条总线上。这个是学习STM32的重点，要学会对这些外设编程，去驱动外部的各种设备。
]]></content>
  </entry>
  
  <entry>
    <title>详细讲解MMU—为什么嵌入式linux没他不行</title>
    <url>/post/linux/why-linux-need-mmu.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>mmu</tag>
    </tags>
    <content type="html"><![CDATA[MMU（Memory Management Unit，内存管理单元）是一种硬件模块，用于在CPU和内存之间实现虚拟内存管理。
MMU内存管理 其主要功能是将虚拟地址转换为物理地址，同时提供访问权限的控制和缓存管理等功能。MMU是现代计算机操作系统中重要的组成部分，可以提高系统的稳定性和安全性。
在内存管理方面，MMU可以通过页面表（Page Table）实现虚拟内存管理。页面表是一种数据结构，记录了每个虚拟页面和其对应的物理页面之间的映射关系。
当CPU发出一个虚拟地址时，MMU会通过页面表查找并将其转换为对应的物理地址。
此外，MMU还可以通过页面表实现内存保护和共享等功能，从而提高系统的安全性和效率。
总之，MMU是内存管理中一个重要的硬件组件，可以实现虚拟内存管理、内存保护、共享和缓存等功能，为现代计算机操作系统的稳定性和安全性提供支持。
举个例子 假设我们有一个程序，它需要访问两个内存区域：一个是只读的代码区域，一个是可读写的数据区域。
我们现在想要在一个没有 MMU 的系统上运行这个程序。如果没有 MMU，代码区域和数据区域就只能被映射到两个固定的物理地址上。这就意味着，如果程序尝试访问一个不正确的地址，可能会导致系统崩溃。
现在，如果我们在一个具有 MMU 的系统上运行这个程序，情况会有所不同。MMU 可以将程序尝试访问的地址映射到不同的物理地址，这样可以使得代码区域和数据区域在物理内存中不再是固定的位置。
这意味着，如果程序尝试访问不正确的地址，MMU 可以通过重新映射来保护系统不崩溃。
MMU 还可以将多个虚拟地址映射到同一个物理地址上，这就是所谓的页共享（page sharing），可以减少物理内存的使用。
如果没有MMU，程序访问内存时只能使用物理地址，而物理地址是直接映射到内存芯片上的地址，程序可以随意访问任何一个物理地址。
这种情况下，程序如果访问了错误的地址或试图访问未被授权的地址，就会产生访问错误或非法访问，可能导致系统崩溃、数据丢失等问题。
而有了MMU，程序访问的是虚拟地址，由MMU负责将虚拟地址映射到物理地址上，这样程序就无法直接访问物理地址。
同时，MMU可以根据内存访问权限来限制程序对内存的访问，确保系统的安全性和稳定性。
因此，没有MMU时，程序可能会访问到其他地址，而有了MMU，程序只能访问被允许访问的地址，可以有效地避免非法访问的问题。
为什么相同的虚拟地址空间在物理地址不会发生冲突呢？ 相同的虚拟地址空间在不同的进程中可能会映射到不同的物理地址，这个映射的过程是由MMU完成的。在操作系统中，每个进程都有独立的虚拟地址空间，且这些虚拟地址空间互不干扰。
MMU会将每个进程的虚拟地址映射到对应的物理地址上，使得不同进程间的内存访问不会相互干扰。同时，MMU也会提供一些安全机制，如页面保护等，来防止进程越界访问内存或访问其他进程的内存。
因此，MMU起到了保护进程间内存互不干扰的作用，也是现代操作系统的重要组成部分。
页表是什么？ 页表是一种用于存储虚拟内存地址与物理内存地址映射关系的数据结构。在使用虚拟内存的系统中，每个进程都有自己的虚拟地址空间，而这些虚拟地址空间被分割成许多页（通常大小为4KB或更大），而不是一整块连续的内存。
因此，当进程需要访问某个虚拟地址时，需要将其翻译成对应的物理地址。这个翻译过程就是通过页表来完成的。
页表的基本原理是将虚拟地址划分成一个页号和一个偏移量。
页号用于在页表中查找对应的物理页帧号，而偏移量则用于计算该虚拟地址在物理页帧中的偏移量。通过这种方式，就可以将虚拟地址映射到物理地址，使得进程可以访问对应的内存区域。
页表一般由操作系统来维护，因为操作系统需要掌握虚拟地址和物理地址之间的映射关系。
在使用MMU（Memory Management Unit）的硬件支持的系统中，当进程访问虚拟地址时，MMU会通过页表将虚拟地址转换为物理地址，并将访问指向正确的物理地址。这样，进程就可以在不知道自己真实物理地址的情况下访问内存。
为什么没有MMU就无法运行Linux系统？ 这是因为 Linux   内核将虚拟地址空间分为多个页面，并将这些页面映射到物理地址空间上，以实现内存隔离、保护和虚拟内存等功能。
没有 MMU，就无法实现这种映射，从而无法运行 Linux 系统。
为什么有些较为简单的SOC可能没有MMU，但仍然可以运行一些嵌入式操作系统或者裸机程序？
RTOS可以运行在没有MMU的系统上，因为RTOS通常不需要进行内存保护和虚拟地址映射等高级特性。
相反，RTOS的设计侧重于实时性和低延迟，因此通常只需要简单的内存管理和任务调度即可。
这使得RTOS可以运行在许多嵌入式系统上，包括一些没有MMU的系统。
]]></content>
  </entry>
  
  <entry>
    <title>CXL开启高性能计算的新纪元</title>
    <url>/post/datacenter/CXL-opens-a-new-era-of-high-performance-computing.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>CXL</tag>
      <tag>PCIe</tag>
    </tags>
    <content type="html"><![CDATA[随着科学研究和工业生产对数据处理能力的需求不断攀升，高性能计算（HPC）已成为推动这些领域进步的重要力量。在这样的背景下，计算机技术尤其是互连技术的进步显得尤为关键。Compute Express Link（ CXL  ）作为新一代的高速互连技术，因其在带宽、延迟和扩展性方面的优势，正展现出在HPC领域的广泛应用潜力，有可能彻底改变数据中心和加速器之间的通信方式，从而推动HPC领域的革命性进步。
然而，为了充分发挥CXL的价值，软件必须得及时跟得上技术的发展，所以，其中的关键是开发针对SoC（System on Chip）设计师所需的硬件生态系统的软件框架。这意味着我们需要构建支持CXL的全面软件解决方案，以简化应用开发，确保硬件的充分利用，并最终实现系统级的优化。
三种CXL设备解锁HPC全新潜能 CXL主要由三种协议构成：CXL.io、CXL.cache 和 CXL.mem。具体来说，CXL.io被视为是PCIe通信的低延迟备选，它们在很多场合上可以互相替代。CXL.io为I/O设备带来了增强且非一致的加载/存储接口。CXL.cache为相关设备提供了在系统内存中建立一致性缓存的能力，并通过请求与响应方式为低延迟I/O事务创建缓存。而CXL.mem则与CXL.cache的功能相反，它允许主机处理器直接访问通过CXL连接的设备，尤其是那些具备低延迟加载/存储指令集的存储设备。
这三种协议能够以不同方式组合，支持三种不同类型的设备，为方便起见，分别简称为类型1、类型2和类型3。
  类型1设备融合了CXL.io和CXL.cache协议，允许无内部存储器的智能 NIC 或加速器等设备直接控制和访问系统存储器的区域。
  类型2设备在类型1的基础上，拥有额外的或内置的存储，并利用所有协议来允许系统或设备以两者间硬件支持的一致性在另一个存储器中分配区域。这两个不同的方向（由系统启动或由主机启动）分别称为主机偏置或设备偏置模式。
  类型3设备是由CXL.io和CXL.mem协议支持的存储设备，它们能够在DRAM、NVRAM和其他各类持久性及易失性存储设备上实现字节级内存寻址。通过这种架构，主机能够像访问本地内存一样访问其他系统的额外内存和专用内存扩展器。
  下图显示了这三种设备类型。
图 1：CXL设备类型，摘自 Compute Express Link Specification r.3.0，v1.0
相较于传统的PCIe事务，CXL.mem和CXL.cache拥有更低的延迟。例如，PCIe 5.0常见的延迟大约是100纳秒，而CXL 2.0只有20-40纳秒。CXL.mem所带来的延迟降低，使得通过利用类型3设备可以进行内存扩展。这意味着应用程序线程可以访问系统外的存储器，同时避免了因内存不足而导致作业失败的风险，这一功能被称为“内存池”。通过CXL 2.0得到加强，并可通过支持交换机附加内存来拓展此范式。这不仅让系统能外加内存，还可以向其他系统提供未使用的本地内存，从而提高利用率和降低初始系统成本。
CXL 3.0 规范中引入的内存共享允许多个主机访问给定的 CXL 附加内存分配。它还定义了结构连接内存扩展器的概念——这些设备可以包含各种类型的内存，以便进行池化和共享，并且可以实现本地内存分层，以代表主机优化池的性能特性。这为Cray Research定义的 SHMEM协议创造了一个有趣的替代方案，使多个主机对共享内存池进行极低延迟访问。由于本机总线互连介质，这不仅提供了比 SHMEM 库例程更好的性能，而且还为该共享内存池上的并行计算提供了可能简单得多的编程模型。
CXL延迟降低的另一个内在价值是，它还具有促进设备到设备内存事务的潜力，例如在一个或多个系统中使用多个 GPU，不需要花费专有的辅助总线或软件层来互连这些设备。这一点在小型AI训练场景中尤为明显，因为可以轻松展示即时性能影响。这种通过结构性连接将远程硬件直接整合进共享内存系统的方法，可能会开启AI训练的新篇章，特别是在数据中心，因为 CXL 引入了对称的对等设备通信功能，从而减少了对CPU的持续依赖。
总的来说，CXL结构为服务器分解提供了机会，有助于克服由于资源不在本地系统架构中而限制特定应用程序工作流的问题。例如，当存储可以被集中到通过结构连接的扩展器中，每个系统对独立（且孤立）存储的需求就会减少。专用内存总线和CXL提供的附加带宽有助于解决核心内存带宽瓶颈，允许设计和配置单个服务器，更注重性能而不是容量。要实现这一愿景，关键在于开发具有低延迟的CXL交换功能和灵活的内存分层系统，这些功能需要在支持软件和扩展器硬件中实现。
新思科技新方案，保障CXL的完整性和数据加密 鉴于 CXL 2.0中的外部交换和 CXL 3.0 中的增强结构的引入，随着数据在服务器外部电缆上传输，增强的总线安全性变得至关重要。因此，为了保护数据免受未经授权的访问或篡改，PCIe和CXL控制器可以使用完整性和数据加密 (IDE) 安全IP模块，即使是在数据被外部人员接触的情况下，也能确保数据的安全和隐私。
面对日益增长的安全需求，新思科技推出了一种创新方案，将高安全性的CXL控制器与符合标准的、可定制的IDE安全模块相结合。这一技术的目的是确保数据在SoC内部的传输过程中免遭篡改和物理攻击的威胁。更具体地说，该方案可以在 CXL.cache/.mem 协议情况下为 FLIT 提供保密性、完整性和重放保护，并在 CXL.io 的情况下为事务层数据包 (TLP) 提供保密性、完整性和重放保护。值得一提的是，这个系统不仅与控制器的数据接口总线宽度和通道配置相匹配，且在面积、性能和延迟方面都经过了精心优化，甚至在CXL.cache/.mem滑动模式情况下，可以实现几乎零延迟的数据传输。
CXL的未来展望 在经历了多年对OpenCAPI、GenZ等一致性协议的标准化努力后，整个行业开始聚焦于CXL。CXL控制器将采用一种名为“信用可扩展流”(CXS)的流接口协议，这种协议通过封装更新的CCIX版本，为多处理器架构带来了对称一致性。这一做法最初因为一致性成本的上升而在原生形态下遭遇了较高的延迟，尤其是在处理小型写入操作时。CXS.B（CXS的CXL托管版本）通过提供专门用于CPU间对称通信的流媒体通道对，巧妙地解决了这一挑战。
CXL技术的发展和应用正在深刻改变高性能计算领域，对于降低延迟、实现内存资源共享和服务器的功能解耦，都展现出了明显的进步和潜力。随着软件框架的不断完善，这种技术有望开启计算性能和效率的新纪元，满足日益增长的硬件生态系统的需求。作为PCIe和CXL物理层、控制器以及IDE和验证IP的领先提供商，新思科技凭借在超过1800个设计项目中的集成和验证经验，可以显著降低风险，协助SoC工程师加快产品推向市场的进程。
]]></content>
  </entry>
  
  <entry>
    <title>Linux内存管理 | ioremap</title>
    <url>/post/linux/linux-memory-management-ioremap.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Linux</tag>
      <tag>Memory</tag>
      <tag>MMU</tag>
    </tags>
    <content type="html"><![CDATA[ioremap是 Linux  内核中用于将IO物理地址映射到内核虚拟地址空间的函数，对于搭载Linux内核的ARM芯片来说，它通常用于访问芯片内部的寄存器。
ioremap的用法 ioremap函数接受两个参数：映射的起始物理地址和映射的内存区域大小，函数返回一个void类型的指针（虚拟地址）。
void __iomem *addr = ioremap(phys_addr, size); 通过ioremap函数返回的指针，内核就可以访问这片映射的物理区域。
u32 val = readl(addr); val |= (0x1 &lt;&lt; ENABLE_SHIFT); writel(val, addr); 操作完毕后，可以释放映射。
iounmap(addr); ioremap的内部实现 和之前讲过的其他内存分配函数类似，ioremap首先要找到一段空闲的虚拟地址区域，然后建立虚拟地址到物理地址的映射。
ioremap() 的虚拟地址从 vmalloc 区分配，这一点与vmalloc函数的实现一致。
area = get_vm_area_caller(size, VM_IOREMAP, caller); 然后通过修改内核页表的方式将其映射到 I/O 地址空间。
err = ioremap_page_range(addr, addr + size, phys_addr, prot); 与 vmalloc() 不同的是，ioremap() 并不需要通过伙伴系统去分配物理页，因为ioremap() 要映射的目标地址是IO地址，不是物理内存。
]]></content>
  </entry>
  
  <entry>
    <title>芯片设计中的uart模块</title>
    <url>/post/soc/uart-module-in-chip-design.html</url>
    <categories><category>SOC</category>
    </categories>
    <tags>
      <tag>SOC</tag>
      <tag>UART</tag>
    </tags>
    <content type="html"><![CDATA[在芯片设计中，UART（Universal Asynchronous Receiver/Transmitter，通用异步接收/发送器）模块是一个非常重要的外设模块。
UART模块负责处理芯片与外部设备之间的异步串行通信，广泛应用于各种嵌入式系统、微控制器和通信设备中。本文将详细介绍芯片设计中的UART模块及其关键技术。
UART 模块的基本原理 UART 模块主要负责实现异步串行通信，它包括一个发送器（TX）和一个接收器（RX）。发送器将内部并行数据转换为串行数据，并按照一定的时序和速率发送到外部设备；接收器则从外部设备接收串行数据，并将其转换为并行数据，供内部系统使用。
UART 模块的工作原理如下：
 并行数据输入：将内部并行数据输入到 UART 模块。 数据位处理：UART 模块将并行数据的每个位进行处理，如添加起始位、数据位、校验位等。 串行发送：将处理后的数据位按照一定的时序和速率发送给外部设备。 串行接收：从外部设备接收串行数据，并将其转换为并行数据。 数据位处理：UART 模块对接收到的并行数据进行处理，如校验位、数据位、停止位等。 并行数据输出：将处理后的并行数据输出到内部系统。  UART 模块的关键技术  数据位：UART 模块支持不同的数据位，如 5 位、6 位、7 位或 8 位。数据位决定了通信速率和传输效率。 停止位：UART 模块支持不同的停止位，如 1 位、2 位或 3 位。停止位用于标识一个数据帧的结束。 校验位：UART 模块支持不同的校验位，如奇校验、偶校验、高位校验等。校验位用于检测数据传输过程中的错误。 波特率：波特率是指每秒钟传输的位数，它决定了通信速率。UART 模块需要根据外部设备的波特率进行配置。 中断处理：UART 模块支持中断处理，如接收中断、发送中断等。中断处理可以提高系统的实时性和响应速度。 双向通信：UART 模块支持双向通信，既可以发送数据，也可以接收数据。  UART 模块在芯片设计中的应用 在芯片设计中，UART 模块广泛应用于各种嵌入式系统、微控制器和通信设备中。例如，在智能手机、平板电脑、路由器等设备中，UART 模块用于与外部传感器、显示器、Modem 等设备进行通信。此外，UART 模块还可以与其他通信接口（如 I2C、SPI、CAN 等）配合使用，实现更丰富的功能。
UART 模块是芯片设计中的一个重要外设模块，它负责处理异步串行通信。通过掌握 UART 模块的基本原理和关键技术，设计人员可以为芯片提供高效、可靠的通信功能。
]]></content>
  </entry>
  
  <entry>
    <title>通过NAT Server从内网向外网提供服务</title>
    <url>/post/linux/NAT-server-how-to.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Boot</tag>
      <tag>NAT Server</tag>
    </tags>
    <content type="html"><![CDATA[今天给大家介绍一下NAT Server  ，包括NAT Server的原理、工作过程、配置（华为、思科、Juniper）。
什么是 NAT Server NAT（Network Address Translation）服务器是一种网络设备或服务，用于在内部网络和外部网络之间进行地址转换。在网络通信中，每个设备都需要一个唯一的IP地址来进行识别和通信。然而，IPv4地址空间有限，而且经常会出现IP地址不足的情况。为了解决这个问题，引入了NAT技术。
NAT具有“屏蔽”内部主机的作用，将内部网络的私有IP地址转换成公网IP地址，从而在有限的公网IP地址下支持多个内部设备与外部网络通信。这种转换过程隐藏了内部网络的实际IP地址，提高了网络安全性，并减少了对公网IP地址的需求。
然而，有时内网需要向外网提供服务，比如提供WWW服务或FTP服务。这种情况下需要内网的服务器不被“屏蔽”，外网用户可以随时访问内网服务器。这就需要一种机制来允许外部用户访问内网服务器，而不会影响内网其他设备的安全性。
NAT Server（NAT服务器）可以很好地解决这个问题。当外网用户访问内网服务器时，NAT Server通过事先配置好的“公网IP地址+端口号”与“私网IP地址+端口号”之间的映射关系，将服务器的“公网IP地址+端口号”根据映射关系替换成对应的“私网IP地址+端口号”。这样，外网用户的请求就能正确地传递到内网服务器，实现了内网服务器对外提供服务的需求。
NAT Server工作原理 NAT Server的地址转换过程如下：
  在Router上配置NAT Server的转换表项：在网络中的路由器（Router）上配置NAT Server的转换表项。这些表项记录了内网服务器的私网IP地址和端口号与对应的公网IP地址和端口号之间的映射关系。
  外网用户访问内网服务器：当外网用户发起访问请求时，请求会到达路由器。路由器根据该请求的“目的IP地址+端口号”查找NAT Server转换表项，找出对应的“私网IP地址+端口号”。
  替换报文的目的IP地址+端口号：路由器将请求报文中的“目的IP地址+端口号”替换为转换表项中对应的“私网IP地址+端口号”，从而确保报文能够正确地传递给内网服务器。
  内网服务器回应报文：当内网服务器收到外网用户的请求并生成回应报文时，回应报文会发送给路由器。
  查找NAT Server转换表项：路由器根据回应报文的“源IP地址+源端口号”查找NAT Server转换表项，找出对应的“公网IP地址+端口号”。
  替换报文的源IP地址+源端口号：路由器将回应报文中的“源IP地址+源端口号”替换为转换表项中对应的“公网IP地址+端口号”，以确保回应报文能够正确地传递给外网用户。
  通过这个地址转换过程，NAT Server实现了外网用户与内网服务器之间的通信，同时确保内网服务器的私网IP地址和端口号不会被外部用户直接暴露。
NAT Server的地址转换过程（简易拓扑图） NAT Server的地址转换过程可以通过以下简易说明图来表示：
在此简易说明图中，外部用户通过公网IP地址和端口号发起访问请求。请求经过路由器，根据转换表项进行地址转换，将目的IP地址和端口号替换为对应的内网IP地址和端口号。内网服务器处理请求并生成响应，响应经过路由器，根据转换表项进行源地址替换，将源IP地址和端口号替换为对应的公网IP地址和端口号。最终，响应传递给外部用户。
NAT Server的工作原理可以更详细地描述如下：
  配置NAT Server转换表项：在Router上进行配置，将内网服务器的私网IP地址和端口号与公网IP地址和端口号建立映射关系。
  外网用户发起请求：当外网用户通过公网IP地址和端口号发起请求时，请求报文会传递到路由器。
  转换表项查找：路由器根据请求报文中的目的IP地址和端口号查找NAT Server转换表项。
  目的地址替换：路由器使用转换表项中对应的私网IP地址和端口号替换请求报文中的目的IP地址和端口号。
  内网服务器响应：请求报文被路由器转发给内网服务器，内网服务器处理请求并生成响应报文。
  源地址替换：内网服务器的响应报文返回到路由器，路由器根据响应报文中的源IP地址和端口号查找NAT Server转换表项。
  源地址替换：路由器使用转换表项中对应的公网IP地址和端口号替换响应报文中的源IP地址和端口号。
  响应传递给外网用户：经过地址转换后的响应报文通过公网IP地址和端口号传递给外网用户，完成了内网服务器与外网用户之间的通信。
  通过以上步骤，NAT Server实现了内网服务器对外提供服务的需求，同时保护了内网服务器的私网IP地址和端口号，增加了网络安全性。
NAT Server 配置 这里瑞哥提供三个厂商的NAT Server配置：
华为设备配置示例：
 创建NAT地址池：  nat address-group 1 10.0.0.0 255.255.255.0 202.0.0.0 255.255.255.0 创建ACL：  acl number 2001 rule permit tcp source any destination 202.0.0.0 0.0.0.255 创建NAT实例并关联地址池和ACL：  nat instance 1 acl 2001 address-group 1 配置接口并启用NAT Server功能：  interface GigabitEthernet 0/0/1 nat server global 1 应用NAT实例：  interface GigabitEthernet 0/0/1 nat server protocol tcp global 80 inside 10.0.0.10 80 思科设备配置示例：  创建NAT地址池：  ip nat pool NAT_POOL 202.0.0.1 202.0.0.254 netmask 255.255.255.0 创建ACL：  access-list 101 permit tcp any host 202.0.0.10 eq 80 配置NAT实例并关联地址池和ACL：  ip nat inside source list 101 pool NAT_POOL overload 配置接口并启用NAT Server功能：  interface GigabitEthernet0/0 ip nat inside 应用NAT实例：  ip nat inside source static tcp 10.0.0.10 80 202.0.0.10 80 Juniper设备配置示例：  创建NAT地址池：  set security nat pool NAT_POOL address 202.0.0.0/24 创建ACL：  set security nat source rule-set SOURCE_RULE_SET from zone trust set security nat source rule-set SOURCE_RULE_SET to zone untrust set security nat source rule-set SOURCE_RULE_SET rule ALLOW_TCP match source-address any set security nat source rule-set SOURCE_RULE_SET rule ALLOW_TCP match destination-address any set security nat source rule-set SOURCE_RULE_SET rule ALLOW_TCP match application junos-http set security nat source rule-set SOURCE_RULE_SET rule ALLOW_TCP then source-nat pool NAT_POOL 配置接口并启用NAT Server功能：  set security zones security-zone trust interfaces ge-0/0/1 set security zones security-zone untrust interfaces ge-0/0/2 应用NAT实例：  set security nat source rule-set SOURCE_RULE_SET rule ALLOW_TCP then source-nat interface 以上示例仅供参考，实际配置可能因设备型号、固件版本和网络拓扑而有所不同。在进行设备配置时，请根据厂商的官方文档和设备型号，参考相关文档并根据实际情况进行配置。
NAT Server 挑战 NAT Server的使用也存在一些限制和考虑因素。例如，NAT转换表项的数量和管理可能会带来一定的复杂性。此外，如果有大量外部用户同时访问内网服务器，NAT Server可能会面临负载压力和网络拥塞的挑战。因此，在设计和配置NAT Server时，需要综合考虑网络规模、性能需求以及安全性等因素，确保其正常运行和适应实际需求。
总结 综上所述，NAT Server（NAT服务器）是一种用于在内部网络和外部网络之间进行地址转换的设备或服务。它通过建立映射关系，将外部用户的请求转发到内网服务器，并将内网服务器的回应传递回外部用户。以下是关于NAT Server的更详细总结：
  NAT的基本原理：NAT通过将内部网络的私有IP地址转换成公网IP地址，实现了多个内部设备通过有限的公网IP地址与外部网络通信的能力。
  NAT Server的作用：NAT Server充当了内网服务器与外网用户之间的桥梁，允许外部用户访问内网服务器提供的服务。
  地址转换过程：NAT Server的地址转换过程包括配置转换表项和实施转换。
    配置转换表项：在路由器上配置NAT Server的转换表项，记录内网服务器的私网IP地址和端口号与对应的公网IP地址和端口号之间的映射关系。
  外网用户访问内网服务器：外网用户发起访问请求，请求到达路由器，根据目的IP地址和端口号查找转换表项。
  目的地址替换：路由器使用转换表项中的私网IP地址和端口号替换请求报文中的目的IP地址和端口号。
  内网服务器回应：内网服务器生成响应报文，响应报文返回到路由器。
  源地址替换：路由器根据响应报文的源IP地址和端口号查找转换表项，并使用转换表项中的公网IP地址和端口号替换响应报文中的源IP地址和端口号。
  响应传递给外网用户：经过地址转换后的响应报文通过公网IP地址和端口号传递给外网用户。
   增强网络安全性：NAT Server通过隐藏内网服务器的实际IP地址，提高了网络的安全性，阻止外部用户直接访问内网服务器，从而减少了潜在的攻击风险。
  考虑因素：在使用NAT Server时，需要综合考虑网络规模、性能需求和安全性。管理和配置转换表项可能会带来一定的复杂性，同时，大量外部用户访问内网服务器可能导致负载压力和网络拥塞的问题。
  通过使用NAT Server，内网服务器能够提供服务给外部用户，同时保护了内网的隐私和安全性。这为企业和组织提供了一种有效的方式来实现内网与外网之间的双向通信。
]]></content>
  </entry>
  
  <entry>
    <title>C语言的编译过程</title>
    <url>/post/programming/the-compile-process-of-c-languageg.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>C</tag>
      <tag>Compile</tag>
    </tags>
    <content type="html"><![CDATA[C语言的编译链接过程要把我们编写的一个C程序源代码，转换成可以在硬件上运行的程序（可执行代码），需要进行编译和链接。过程图解如下：
本文讲解C语言编译过程中所做的工作，对我们理解头文件、库等的工作过程是有帮助的。而且清楚的了解编译链接过程还对我们在编程时定位错误，以及编程时尽量调动编译器的检测错误会有很大的帮助的。
编译 编译是读取源程序（字符流），对之进行词法和语法的分析，将高级语言指令转换为功能等效的汇编代码，源文件的编译过程包含预处理与编译优化两个主要阶段。
预处理 第一个阶段是预处理阶段，在正式的编译阶段之前进行。预处理阶段将根据已放置在文件中的预处理指令来修改源文件的内容。如#include指令就是一个预处理指令，它把头文件的内容添加到.cpp文件中。这个在编译之前修改源文件的方式提供了很大的灵活性，以适应不同的计算机和操作系统环境的限制。一个环境需要的代码跟另一个环境所需的代码可能有所不同，因为可用的硬件或操作系统是不同的。在许多情况下，可以把用于不同环境的代码放在同一个文件中，再在预处理阶段修改代码，使之适应当前的环境。
主要是以下几方面的处理：
 宏定义指令，如 #define a b  对于这种伪指令，预编译所要做的是将程序中的所有a用b替换，但作为字符串常量的 a则不被替换。还有 #undef，则将取消对某个宏的定义，使以后该串的出现不再被替换。
 条件编译指令，如#ifdef，#ifndef，#else，#elif，#endif等。  这些伪指令的引入使得程序员可以通过定义不同的宏来决定编译程序对哪些代码进行处理。预编译程序将根据有关的文件，将那些不必要的代码过滤掉。
 头文件包含指令，如#include &ldquo;FileName&quot;或者#include 等。  在头文件中一般用伪指令#define定义了大量的宏（最常见的是字符常量），同时包含有各种外部符号的声明。采用头文件的目的主要是为了使某些定义可以供多个不同的C源程序使用。因为在需要用到这些定义的C源程序中，只需加上一条#include语句即可，而不必再在此文件中将这些定义重复一遍。预编译程序将把头文件中的定义统统都加入到它所产生的输出文件中，以供编译程序对之进行处理。包含到c源程序中的头文件可以是系统提供的，这些头文件一般被放在 /usr/include目录下。在程序中#include它们要使用尖括号（&lt; &gt;）。另外开发人员也可以定义自己的头文件，这些文件一般与c源程序放在同一目录下，此时在#include中要用双引号（&quot;&quot;）。
 特殊符号，预编译程序可以识别一些特殊的符号。  例如在源程序中出现的LINE标识将被解释为当前行号（十进制数），FILE则被解释为当前被编译的C源程序的名称。预编译程序对于在源程序中出现的这些串将用合适的值进行替换。
预编译程序所完成的基本上是对源程序的“替代”工作。经过此种替代，生成一个没有宏定义、没有条件编译指令、没有特殊符号的输出文件。这个文件的含义同没有经过预处理的源文件是相同的，但内容有所不同。下一步，此输出文件将作为编译程序的输出而被翻译成为机器指令。
编译、优化 第二个阶段编译、优化阶段，经过预编译得到的输出文件中，只有常量；如数字、字符串、变量的定义，以及c语言的关键字，如main,if,else,for,while,{,}, +,-,*,\等等。
编译程序所要做的工作就是通过词法分析和语法分析，在确认所有的指令都符合语法规则之后，将其翻译成等价的中间代码表示或汇编代码。
优化处理是编译系统中一项比较艰深的技术。它涉及到的问题不仅同编译技术本身有关，而且同机器的硬件环境也有很大的关系。优化一部分是对中间代码的优化。这种优化不依赖于具体的计算机。另一种优化则主要针对目标代码的生成而进行的。
对于前一种优化，主要的工作是删除公共表达式、循环优化（代码外提、强度削弱、变换循环控制条件、已知量的合并等）、复写传播，以及无用赋值的删除，等等。
后一种类型的优化同机器的硬件结构密切相关，最主要的是考虑是如何充分利用机器的各个硬件寄存器存放的有关变量的值，以减少对于内存的访问次数。另外，如何根据机器硬件执行指令的特点（如流水线、RISC、CISC、VLIW等）而对指令进行一些调整使目标代码比较短，执行的效率比较高，也是一个重要的研究课题。
汇编 汇编实际上指把汇编语言代码翻译成目标机器指令的过程。对于被翻译系统处理的每一个C语言源程序，都将最终经过这一处理而得到相应的目标文件。目标文件中所存放的也就是与源程序等效的目标的机器语言代码。目标文件由段组成。通常一个目标文件中至少有两个段：
代码段：该段中所包含的主要是程序的指令。
该段一般是可读和可执行的，但一般却不可写。
数据段：主要存放程序中要用到的各种全局变量或静态的数据。一般数据段都是可读，可写，可执行的。
UNIX环境下主要有三种类型的目标文件：
 可重定位文件  其中包含有适合于其它目标文件链接来创建一个可执行的或者共享的目标文件的代码和数据。
  共享的目标文件 这种文件存放了适合于在两种上下文里链接的代码和数据。第一种是链接程序可把它与其它可重定位文件及共享的目标文件一起处理来创建另一个 目标文件；第二种是动态链接程序将它与另一个可执行文件及其它的共享目标文件结合到一起，创建一个进程映象。
  可执行文件
  它包含了一个可以被操作系统创建一个进程来执行之的文件。汇编程序生成的实际上是第一种类型的目标文件。对于后两种还需要其他的一些处理方能得到，这个就是链接程序的工作了。
链接过程 由汇编程序生成的目标文件并不能立即就被执行，其中可能还有许多没有解决的问题。
例如，某个源文件中的函数可能引用了另一个源文件中定义的某个符号（如变量或者函数调用等）；在程序中可能调用了某个库文件中的函数，等等。所有的这些问题，都需要经链接程序的处理方能得以解决。
链接程序的主要工作就是将有关的目标文件彼此相连接，也即将在一个文件中引用的符号同该符号在另外一个文件中的定义连接起来，使得所有的这些目标文件成为一个能够诶操作系统装入执行的统一整体。
根据开发人员指定的同库函数的链接方式的不同，链接处理可分为两种：
 静态链接  在这种链接方式下，函数的代码将从其所在的静态链接库中被拷贝到最终的可执行程序中。这样该程序在被执行时这些代码将被装入到该进程的虚拟地址空间中。静态链接库实际上是一个目标文件的集合，其中的每个文件含有库中的一个或者一组相关函数的代码。
 动态链接  在此种方式下，函数的代码被放到称作是动态链接库或共享对象的某个目标文件中。链接程序此时所做的，只是在最终的可执行程序中记录下共享对象的名字以及其它少量的登记信息。在此可执行文件被执行时，动态链接库的全部内容将被映射到运行时相应进程的虚地址空间。动态链接程序将根据可执行程序中记录的信息找到相应的函数代码。
对于可执行文件中的函数调用，可分别采用动态链接或静态链接的方法。使用动态链接能够使最终的可执行文件比较短小，并且当共享对象被多个进程使用时能节约一些内存，因为在内存中只需要保存一份此共享对象的代码。但并不是使用动态链接就一定比使用静态链接要优越。在某些情况下动态链接可能带来一些性能上损害。
]]></content>
  </entry>
  
  <entry>
    <title>STM32单片机开发中的RTOS</title>
    <url>/post/mcu/the-rtos-for-STM32-development.html</url>
    <categories><category>MCU</category>
    </categories>
    <tags>
      <tag>STM32</tag>
      <tag>RTSO</tag>
    </tags>
    <content type="html"><![CDATA[很多STM32单片机初学者都是从裸机开始的，裸机确实也能开发出好的产品。但是，作为一个嵌入式软件工程师，况且用的并不是51那种低端单片机，如果只会用裸机开发产品，那肯定是不够的。
要从裸机的思维转变到RTOS（Real Time Operating System）的思维，其实需要一个过程，而且开始的一段时间会很痛苦。但过一段时间理解了一些内容，能写一些Demo之后，你会发现其实RTOS也不难。
现在FreeRTOS在CubeMX工具中可以直接配置并使用，相当方便。
为什么需要RTOS 为什么我们需要RTOS？就像最开始学C编程时，老师告诉我们，指针很重要，那时你肯定有一个大的疑问，指针到底有什么好？
心里一直犯嘀咕着：不用指针不一样把程序编出来了？ 现在想想看C语言没了指针，是不是“寸步难行”呢。
回到正题，我们到底为什么需要RTOS？
一般的简单的嵌入式设备的编程思路是下面这样的：
main { {处理事物1}; {处理事物2}; {处理事物3}; ...... ...... {处理事物N}; } isr_server { {处理中断}; } 这是最常见的一种思路，对于简单的系统当然是够用了，但这样的系统实时性很差。
比如“事务1”如果是一个用户输入的检测，当用户输入时，如果程序正在处理事务1下面的那些事务，那么这次用户输入将失效，用户的体验是“这个按键不灵敏，这个机器很慢”，而我们如果把事务放到中断里去处理，虽然改善了实时性但会导致另外一个问题，有可能会引发中断丢失，这个后果有时候比“慢一点”更加严重和恶劣！
又比如事务2是一个只需要1s钟处理一次的任务，那么显然事务2会白白浪费CPU的时间。
改进思路 看到上面裸机开发的局限了吗？
这时，我们可能需要改进我们的编程思路，一般我们会尝试采用“时间片”的方式。这时候编程会变成下面的方式：
main { {事务1的时间片到了则处理事务1}; {事务2的时间片到了则处理事务2}; ...... ...... {事务N的时间片到了则处理事务N}; } time_isr_server { {判断每个事务的时间片是否到来，并进行标记}; } isr_server { {处理中断}; } 可以看到，这种改进后的思路，使得事务的执行时间得到控制，事务只在自己的时间片到来后，才会去执行。但这种方式仍然不能彻底解决“实时性”的问题，因为某个事务的时间片到来后，也不能立即就执行，必须等到当前事务的时间片用完，并且后面的事务时间片没到来，才有机会获得“执行时间”。
这时候我们需要继续改进思路,为了使得某个事务的时间片到来后能立即执行，我们需要在时钟中断里判断完时间片后，改变程序的返回位置，让程序不返回到刚刚被打断的位置，而从最新获得了时间片的事务处开始执行，这样就彻底解决了事务的实时问题。
我们在这个思路上，进行改进，我们需要在每次进入时钟中断前，保存CPU的当前状态和当前事务用到的一些数据，然后我们进入时钟中断进行时间片处理，若发现有新的更紧急的事务的时间片到来了，则我们改变中断的返回的地址，并在CPU中恢复这个更紧急的事务的现场，然后返回中断开始执行这个更紧急的事务。
使用RTOS的好处 上面那段话，对于初学者来说，可能有些不好理解。
事实上，这是因为要实现这个过程是有些复杂和麻烦的，这时候我们就需要找一个操作系统(OS)帮我们做这些事了，如果你能自己用代码实现这个过程，事实上你就在自己写操作系统了。
其实从这里也可也看出，操作系统的原理其实并不那么神秘，只是一些细节你很难做好。我们常见的RTOS基本都是这样的一个操作系统，它能帮你完成这些事情，而且是很优雅的帮你完成！
事实上，RTOS的用处远不止帮你完成这个“事务时间片的处理”，它还能帮你处理各种超时，进行内存管理，完成任务间的通信等。
有了RTOS，程序的层次也更加清晰，给系统添加功能也更方便，这一切在大型项目中越发的明显！
]]></content>
  </entry>
  
  <entry>
    <title>常见的电路保护元器件</title>
    <url>/post/hardware/common-circuit-protection-components.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>电路保护</tag>
    </tags>
    <content type="html"><![CDATA[电子电路很容易在过压、过流、浪涌等情况发生的时候损坏，随着技术的发展，电子电路的产品日益多样化和复杂化，而电路保护则变得尤为重要。电路保护元件也从简单的玻璃管保险丝，变得种类更多，防护性能更优越。
电路保护的意义 在各类电子产品中，设置过压保护和过流保护变得越来越重要，那么电路保护的意义到底是什么，今天就来跟大家聊一聊：
  由于如今电路板的集成度越来越高，板子的价格也跟着水涨船高，因此我们要加强保护。
  半导体器件，IC的工作电压有越来越低的趋势，而电路保护的目的则是降低能耗损失，减少发热现象，延长使用寿命。
  车载设备，由于使用环境的条件比一般电子产品更加恶劣，汽车行驶状况万变，汽车启动时产生很大的瞬间峰值电压等。因此，在为这些电子设备配套产品的电源适配器中，一般要使用过压保护元件。
  通信设备，通信场所对防雷浪涌有一定的要求，在这些设备中使用过压保护、过流保护元件就变得重要起来，它们是保证用户人身安全和通信正常的关键。
  大部分电子产品出现的故障，都是电子设备电路中出现的过压或者电路现象造成的，随着我们对电子设备质量的要求越来越高，电子电路保护也变得更加不容忽视。
  那么电路保护如此重要，常用的电路保护元件有哪些？今天就给大家介绍几种。
防雷器件 陶瓷气体放电管： 防雷器件中应用最广泛的是陶瓷气体放电管，之所以说陶瓷气体放电管是应用最广泛的防雷器件，是因为无论是直流电源的防雷还是各种信号的防雷，陶瓷气体放电管都能起到很好的防护作用。
其最大的特点是通流量大，级间电容小，绝缘电阻高，击穿电压可选范围大。
半导体放电管： 半导体放电管是一种过压保护器件，是利用晶闸管原理制成的，依靠PN结的击穿电流触发器件导通放电，可以流过很大的浪涌电流或脉冲电流。其击穿电压的范围，构成了过压保护的范围。
固体放电管使用时可直接跨接在被保护电路两端。具有精确导通、快速响应（响应时间ns级）、浪涌吸收能力较强、双向对称、可靠性高等特点。
玻璃放电管： 玻璃放电管（强效放电管、防雷管）是20世纪末新推出的防雷器件，它兼有陶瓷气体放电管和半导体过压保护器的优点：绝缘电阻高（≥10^8Ω）、极间电容小（≤0.8pF）、放电电流较大（最大达3 kA）、双向对称性、反应速度快（不存在冲击击穿的滞后现象）、性能稳定可靠、导通后电压较低。
此外还有直流击穿电压高（最高达5000V）、体积小、寿命长等优点。其缺点是直流击穿电压分散性较大（±20%）。
过压器件 压敏电阻： 压敏电阻也是一种用得最多的限压器件。利用压敏电阻的非线性特性，当过电压出现在压敏电阻的两极间，压敏电阻可以将电压钳位到一个相对固定的电压值，从而实现对后级电路的保护。
压敏电阻的响应时间为ns级，比空气放电管快，比TVS管稍慢一些，一般情况下用于电子电路的过电压保护其响应速度可以满足要求。压敏电阻的结电容一般在几百到几千pF的数量级范围，很多情况下不宜直接应用在高频信号线路的保护中，应用在交流电路的保护中时，因为其结电容较大会增加漏电流，在设计防护电路时需要充分考虑。压敏电阻的通流容量较大，但比气体放电管小。
贴片压敏电阻的作用：
贴片压敏电阻主要用于保护元件和电路，防止在电源供应、控制和信号线产生的ESD。
瞬态抑制二极管： 瞬态抑制器TVS二极管广泛应用于半导体及敏感器件的保护，通常用于二级保护。基本都会是用于在陶瓷气体放电管之后的二级保护，也有用户直接将其用于产品的一级保护。
其特点为反应速度快(为 ps 级) ，体积小 ，脉冲功率较大 ，箝位电压低等。其 10/1000μs波脉冲功率从400W ～30KW，脉冲峰值电流从 0.52A～544A ；击穿电压有从6.8V～550V的系列值，便于各种不同电压的电路使用。
过流器件 自恢复保险丝： 自恢复保险丝PPTC就是一种过流电子保护元件，采用高分子有机聚合物在高压、高温，硫化反应的条件下，搀加导电粒子材料后，经过特殊的工艺加工而成。自恢复保险丝（PPTC：高分子自恢复保险丝）是一种正温度系数聚合物热敏电阻，作过流保护用，可代替电流保险丝。
电路正常工作时它的阻值很小（压降很小），当电路出现过流使它温 度升高时，阻值急剧增大几个数量级，使电路中的电流减小到安全值以下，从而使后面的电路得到保护，过流消失后自动恢复为低阻值。
静电元件 ESD静电放电二极管： ESD静电放电二极管是一种过压、防静电保护元件，是为高速数据传输应用的I/O端口保护设计的器件。ESD静电二极管是用来避免电子设备中的敏感电路受到ESD(静电放电)的影响。
可提供非常低的电容，具有优异的传输线脉冲（TLP）测试，以及IEC6100-4-2测试能力，尤其是在多采样数高达1000之后，进而改善对敏感电子元件的保护。
电感的作用： 电磁的关系相信大家都清楚，电感的作用就是在电路刚开始的时候，一切还不稳定的时候，如果电感中有电流通过，就一定会产生一个与电流方向相反的感应电流(法拉第电磁感应定律)，等到电路运行了一段时间后，一切都稳定了，电流没有什么变化了，电磁感应也就不会产生电流，这时候就稳定了，不会出现突发性的变故，从而保证了电路的安全，就像水车，一开始由于阻力转动的比较慢，后来慢慢趋于平和。
磁珠的作用： 磁珠有很高的电阻率和磁导率，他等效于电阻和电感串联，但电阻值和电感值都随频率变化。他比普通的电感有更好的高频滤波特性，在高频时呈现阻性，所以能在相当宽的频率范围内保持较高的阻抗，从而提高调频滤波效果，在以太网芯片上用到过。
]]></content>
  </entry>
  
  <entry>
    <title>揭穿四个常见的AI/ML数据存储迷思</title>
    <url>/post/datacenter/busting-four-common-AI-ML-and-Data-Storage-Myths.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>AI</tag>
      <tag>ML</tag>
    </tags>
    <content type="html"><![CDATA[如今，消费者和商业用户在不知不觉中与人工智能和机器学习互动。从消费者的角度来看，这涉及从流媒体上观看喜爱的节目到随时叫车的车辆服务。从商业的角度来看，组织利用人工智能和机器学习来获得更好的洞察，支持其业务目标。
 AI  /ML是关于模式识别的。具体来说，它是指识别和理解实时模式以改进业务流程、经营业绩和人们的生活。根据2022年市场研究洞察报告，人工智能市场预计从2022年的3874.5亿美元增长到2029年的13943亿美元。
随着越来越多的组织采用这些深度学习技术，IT团队正在探索如何最好地理解如何以具有成本效益的方式构建和管理基础设施，以支持人工智能和机器学习为组织带来的机会，并支持其能够为未来的业务增长进行扩展。一个不容小觑的要素，而应该成为重点关注的焦点，是支持这些新兴应用所需的数据存储基础设施。
以下是需要揭穿的四个常见的AI/ML存储迷思。
AI/ML应用必须由高IOPS全闪存存储支持
为了“满足需求”，加速器需要在需要时随时提供数据。因此，这强调了AI/ML存储不仅仅关乎纯速度。全闪存存储系统具有令人印象深刻的高IOPS，但也可能耗尽您的IT预算。
与大多数AI/ML应用一样，加速器也有不同级别的性能。例如，目标识别应用中每个图像的计算时间足够长，以至于混合（HDD和SSD）系统可以作为全NVMe解决方案的可比解决方案，而价格要低得多。IT团队必须注意并平衡计算加速器、AI/ML工作负载和存储方案，以找到最佳解决方案。独立的基准报告，如MLPerf，可以在这方面提供帮助。
AI/ML完全依赖于GPU 在具有极高计算能力的现代GPU出现之前，今天使用的AI/ML应用和神经网络仅仅是一个有趣的概念而已。毫无疑问，加速器芯片对于AI/ML应用至关重要，但如果没有足够的存储和网络支持，它就毫无价值。
存储和网络被认为是“满足需求”的手段，它们确保在加速器完成当前数据集之前，下一组数据始终可用。因此，组织必须像仔细考虑GPU一样慎重选择存储和网络基础设施。每个元素必须平衡，以实现最佳结果：过多的存储性能或容量将被浪费，而过少则会导致昂贵的计算芯片闲置。
AI/ML可以有效利用专用的单一用途存储系统 当将AI/ML应用于其核心数据源时，组织能够从中获得最大价值。已经有银行采用这些技术进行欺诈检测，药品制造商可以更好地分析实验或生产的数据以加快药物开发。多家领先的零售商也正在将人工智能技术应用于其技术和业务基础设施的核心，以最好地满足客户的需求。许多企业不再将AI/ML视为试验性的边缘项目，而是作为业务的一部分和未来增长的催化剂。因此，这些应用最适合在公司核心的IT基础设施中使用专用存储系统。
分层降低AI/ML存储成本 分层存储是一种常见的策略，用于最大化存储资源并降低成本。将“热门”的关键任务和频繁访问的数据存储在昂贵且快速的存储介质上（例如SSD），而将很少访问或更新的“冷藏”归档数据存储在最便宜的存储设备上（例如磁带）。
由于不存在所谓的“冷藏”AI/ML数据，这种模型无法应用于这些类型的应用程序。
由于在每次训练运行中都会使用所有的AI/ML训练数据，将部分数据分层存储到不同的存储层将导致严重的性能问题。AI/ML存储解决方案必须将所有数据视为“热数据”，确保所有数据始终可用。
值得注意的是，AI/ML工作负载的准确性与可用的训练数据量成正比。这意味着存储基础设施必须能够在训练数据量扩大时无缝扩展。与存储分层相比，规模化的线性增长是这些环境的关键存储要求。
AI/ML的创新正在推动企业进行大规模的数字转型，从而实现更好的业务成果。如果使用和管理不当，它将影响组织的几乎所有方面，并不是以积极的方式。根据2022 Gartner Hype Cycle，许多技术预计在未来两到五年内实现主流应用，例如边缘人工智能、决策智能和深度学习。在组织踏上自己的数字化之旅时，不要将基础存储基础设施作为一个次要考虑因素，因为它在实现组织最大化AI/ML应用潜力的成功中扮演着关键角色。
]]></content>
  </entry>
  
  <entry>
    <title>盘点优秀PCB工程师的好习惯</title>
    <url>/post/hardware/the-good-habits-of-excellent-PCB-engineers.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>PCB</tag>
    </tags>
    <content type="html"><![CDATA[在有些人看来，PCB layout工程师的工作会有些枯燥无聊，每天对着板子成千上万条走线，各种各样的封装，重复着拉线的工作&hellip;
事实上，并没有看上去的那么简单！
设计人员需要在各种设计规则之间做出取舍，兼顾性能、工艺、成本等各方面，同时还要注意板子布局的合理整齐。
作为一名优秀的PCB layout工程师，好的工作习惯会使你的设计更合理、性能更好、生产更容易。
下面罗列了PCB layout工程师的7个好习惯，来看看你都占了几个吧！
学会会设计规则 其实现在不光高级的PCB设计软件需要设置布线规则，一些简单易用的PCB工具同样可以进行规则设置。
人脑毕竟不是机器，那就难免会有疏忽有失误。
所以把一些容易忽略的问题设置到规则里面，让电脑帮助我们检查，尽量避免犯一些低级错误。
另外，完善的规则设置能更好的规范后面的工作，所谓磨刀不误砍柴工，板子的规模越复杂规则设置的重要性越突出。
尽可能地执行DRC 尽管在PCB软件上运行DRC功能只需花费很短时间，但在更复杂的设计环境中，只要你在设计过程中始终执行检查便可节省大量时间，这是一个值得保持的好习惯。
每个布线决定都很关键，通过执行DRC可随时提示你那些最重要的布线。
画好原理图 很多工程师都觉得layout工作更重要一些，原理图就是为了生成网表方便PCB做检查用的。
其实，在后续电路调试过程中原理图的作用会更大一些，无论是查找问题还是和同事交流，还是原理图更直观更方便。
另外养成在原理图中做标注的习惯，把各部分电路在layout的时候要注意到的问题标注在原理图上，对自己或者对别人都是一个很好的提醒。
层次化原理图，把不同功能不同模块的电路分成不同的页，这样无论是读图还是以后重复使用都能明显的减少工作量。
优化PCB布局 心急的工程师画完原理图，把网表导入PCB后就迫不及待的把器件放好，开始拉线。
其实一个好的PCB布局能让你后面的拉线工作变得简单，让你的PCB工作的更好。
每一块板子都会有一个信号路径，PCB布局也应该尽量遵循这个信号路径，让信号在板子上可以顺畅的传输，人们都不喜欢走迷宫，信号也一样。
如果原理图是按照模块设计的，PCB也一样可以，按照不同的功能模块可以把板子划分为若干区域。
模拟数字分开，电源信号分开，发热器件和易感器件分开，体积较大的器件不要太靠近板边，注意射频信号的屏蔽等等……
多花一分的时间去优化PCB的布局，就能在拉线的时候节省更多的时间。
多为别人考虑 在进行PCB设计的时候，尽量多考虑一些最终使用者的需求。
比如，如果设计的是一块开发板，那么在进行PCB设计的时候就要考虑放置更多的丝印信息，这样在使用的时候会更方便，不用来回的查找原理图或者找设计人员支持了。
如果设计的是一个量产的产品，那么就要更多的考虑到生产线上会遇到的问题，同类型的器件尽量方向一致，器件间距是否合适，板子的工艺边宽度等等。
这些问题考虑的越早，越不会影响后面的设计，也可以减少后面支持的工作量和改板的次数。
看上去开始设计上用的时间增加了，实际上是减少了自己后续的工作量。
在板子空间允许的情况下，尽量放置更多的测试点，提高板子的可测性，这样在后续调试阶段同样能节省更多的时间，给发现问题提供更多的思路。
反复和客户沟通确认 作为一名优秀的PCB layout工程师，要学会和客户有效沟通。
Layout中一些重要的问题最好和客户反复沟通确认，比如封装的确认。
特别是含有正负极的，三极管，结构连接器的位置，这些将直接影响到后期板卡的安装定位。
细节决定成败 PCB设计是一个细致的工作，需要的就是细心和耐心。刚开始做设计的新手经常犯的错误就是一些细节错误。
器件管脚弄错了，器件封装用错了，管脚顺序画反了等等，有些可以通过飞线来解决，有些可能就让一块板子直接变成了废品。
画封装的时候多检查一遍，投板之前把封装打印出来和实际器件比一下，多看一眼，多检查一遍不是强迫症，只是让这些容易犯的低级错误尽量避免。
]]></content>
  </entry>
  
  <entry>
    <title>用协议分析仪直观理解USB传输的核心概念</title>
    <url>/post/hardware/understand-usb-transition-core-concept-by-using-protocol-analyze.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>USB</tag>
      <tag>Analyzer</tag>
    </tags>
    <content type="html"><![CDATA[本文内容是基于 USB 2.0  。在讲述USB传输前，有必要先了解下USB的几个基本概念。
几个基本概念 本文内容是基于USB 2.0。在讲述USB传输前，有必要先了解下USB的几个基本概念。
USB主机（Host） 每一次传输都是主机发起的，不管是发送数据还是接收数据。
USB设备（Device） 设备会响应主机的请求，每个设备的内部固件中都会存有设备描述符来表明设备的基本信息，比如厂家ID、产品ID、配置数目、接口和端点信息等。
USB枚举 主机识别到有设备接入后，会发起枚举，读取设备的描述符信息后配置设备。
USB配置（Configuration） 一个设备有一个或者多个配置，主机可以通知设备选用哪个配置，但一个设备当前只能有一种生效的配置。
USB接口（Interface） 简单地说，一个接口表示一种功能，比如说这个设备既有虚拟串口功能，也有虚拟网口功能，可能还有大容量存储（U盘）功能。当然一个功能并不都是严格对应一个接口，有些功能可能需要两个以上的接口，这里为了简化描述，暂不深究。
USB端点（Endpoint） 端点简称EP，一个接口配置有一个或者多个端点。端点是主机和设备之间通信的部件，数据是从某个端点发送出去或者接收进来的。端点是有方向的，方向是基于主机侧来定义的，OUT方向是指从主机传出，IN是指传入主机。
USB传输、USB事务与USB包 USB传输由主机发起，任何时刻整个USB体系内仅允许一个数据包在传输。
USB传输有4种类型，控制传输、中断传输、批量传输和同步传输，每种传输有不同的适用场景。
传输由事务构成，事务类型有SETUP事务、DATA IN事务、DATA OUT事务，SETUP事务主要往设备发送控制命令，DATA IN事务主要从设备读取数据，DATA OUT事务主要往设备发送数据。每个事务采用“令牌包”-“数据包”-“握手包”的三段式传输机制（同步传输没有握手包）。
包是USB总线上数据传输的最小单位，其中的PID字段指明了包的类型。令牌包有IN/OUT/SETUP/SOF四种。前三种“令牌包”都含有指定数据包去向或者来源的设备地址和端点，从而保证了只有一个设备对总线上的数据包/令牌包作出响应。“握手包”表示了传输的成功与否。“数据包”承载实际数据。
USB传输、事务、包是从不同层次上去说明一次数据交互的三个概念。一次传输包含多个事务，一个事务包含多个包。直接看一下USB协议分析仪的抓包示例，可能更清楚一些：
控制传输 控制传输是一种可靠的传输，所有USB设备都必须支持的一种传输方式，主机在枚举设备的过程中就是通过端点0进行控制传输。一次控制传输可分为三个阶段：
第一阶段为建立阶段，通过SETUP事务指定了此次控制传输的请求类型；
第二阶段为数据阶段，也有些请求没有数据阶段；
第三阶段为状态阶段，通过一次IN/OUT事务表明请求是否成功完成。
下图是通过USB协议分析仪抓取U盘的某次控制传输，对应的是枚举阶段获取设备描述符：
Control SETUP事务展开图：
Control IN事务展开图：
Control OUT事务展开图：
中断传输 中断传输是一种可靠的传输，主机通过固定的间隔对中断端点进行查询，如果设备有数据则往主机回送数据，否则回送NAK。同样的，如果主机要发送数据，设备没有准备好接收，也会回送NAK。
下图是通过USB协议分析仪抓取的USB camera的某次中断传输（control数据，非图像数据）：
Interrupt IN事务展开图：
批量传输 批量传输是一种可靠的传输，但延迟没有保证，它尽量利用可以利用的带宽来完成传输，适合数据量比较大的传输。高速批量端点的最大包长度为 512。
对于批量输出，如果设备收到的数据包正确，并有足够的空间保存数据，那么设备会返回ACK握手包或NYET握手包（只有高速模式才有NYET握手包，它表示本次数据接收成功，但是没有能力接收下一次传输）。如果设备收到的数据包正确，但是没有足够的空间保存数据：设备返回NAK握手包。主机收到NAK，延时一段时间后，再重新进行批量输出。
对于批量输入，如果设备没有准备好数据，则回送NAK，否则回送数据包。
批量传输由一个或者多个IN / OUT事务组成，下图是抓取的U盘某次批量传输：
Bulk OUT事务展开图：
同步传输 同步传输是一种实时的、不可靠的传输，不支持错误重发机制。高速同步端点的最大包长度为1024。
常规情况下，一个微帧内仅允许一次同步事务传输；但是，高速高带宽端点最多可以在一个微帧内进行三次同步事务传输，传输高达3072字节的数据，这个特性对数据量较大的设备比如USB camera的性能表现有很重要的影响。
下图是抓取的USB Camera的某次同步传输：
Isoch IN事务展开图：
USB包 USB包由SOP（包起始域）、SYNC（同步域）、Packet Content（包内容）、EOP（包结束域）四部分组成，其中SOP、SYNC、EOP为所有包共有的域，Packet Content由PID、地址、帧号、数据、CRC组成，注意这只是一个通用构成，不同类型的包，Packet Content的构成会有一些区别。
USB包按大类分为令牌包、数据包、握手包和帧首包，每个大类里面还有具体类型，比如令牌类有OUT/IN/SETUP等，数据类有DATA0/DATA1等，包的类型由Packet Content中的PID指定。这里不罗列所有类型的包，只抓取几个有代表性的包来看看。
下图是抓取的某个SETUP包的Packet Content：
下图是抓取的某个SOF包的Packet Content：
下图是抓取的某个DATA1包的Packet Content：
USB帧与微帧 USB帧与微帧属于USB传输中时间基准的概念。低速和全速下每个帧时长为1ms，高速下每个帧又分为8个微帧，每个微帧时长为125us。USB 2.0 规范上有详细描述，如下图所示。
在每一个帧（微帧）的起始点，会发送一个SOF包（帧起始包），帧起始包之后可以是输出（OUT）、输入（IN）、建立（SETUP）等令牌包，或者没有包。数据的传输在每个帧（微帧）内进行，如果没有数据要传输，则SOF包发送完毕之后，总线进入空闲状态。
]]></content>
  </entry>
  
  <entry>
    <title>efuse在soc中的重要作用</title>
    <url>/post/soc/the-important-role-of-efuse-in-soc.html</url>
    <categories><category>SOC</category>
    </categories>
    <tags>
      <tag>SOC</tag>
      <tag>Efuse</tag>
    </tags>
    <content type="html"><![CDATA[在现代 SoC 设计中，efuse 是一种非常重要的技术，可以用于保护 SoC 中的敏感信息，防止黑客攻击和未经授权的访问。本文将介绍什么是 efuse，它在 SoC 设计中的应用以及它的优缺点。
什么是 efuse? efuse 是一种可编程的存储器，用于存储 SoC 中的敏感信息，例如密钥、证书、序列号等。与传统的存储器不同，efuse 的编程是一次性的，一旦编程就无法更改。这种特性使得 efuse 非常适合用于存储需要保护的信息，因为它可以防止黑客通过修改存储器中的数据来破解 SoC 的安全性。
efuse 的工作原理是什么？ efuse 通常由两部分组成：编程器和熔丝。编程器用于将数据编程到熔丝中，而熔丝则用于存储数据。当编程器将数据编程到熔丝中时，它会改变熔丝中的物理结构，从而使其不可更改。一旦熔丝被编程，它将永久保存数据，即使 SoC 失去电源，熔丝中的数据也不会丢失。
在 SoC 设计中，efuse 通常被用于以下方面：
  存储加密密钥：在许多安全应用中，加密密钥是必须保护的信息。使用 efuse 可以确保密钥不会被黑客访问或修改。
  存储序列号：每个 SoC 都有一个唯一的序列号，用于标识 SoC。使用 efuse 可以确保序列号不会被修改或伪造。
  存储证书：在安全应用中，证书用于验证 SoC 的身份。使用 efuse 可以确保证书不会被黑客攻击或修改。
  存储配置信息：efuse 还可以用于存储 SoC 的配置信息，例如 CPU 频率、内存大小等。这些信息可以在 SoC 启动时被读取，并根据需要进行更改。
  尽管 efuse 在 SoC 设计中非常有用，但它也有一些缺点：
  一次性编程：由于 efuse 是一次性编程的，因此如果在编程过程中出现错误，则必须丢弃整个 SoC。
  成本：efuse 通常比传统的存储器更昂贵，因此可能会增加 SoC 的成本。
  安全性：尽管 efuse 可以保护 SoC 中的敏感信息，但它本身也可能受到攻击。例如，黑客可以使用高功率的激光器来擦除 efuse 中的数据，或者使用特殊的攻击技术来破解 efuse 的安全性。
  在现代 SoC 设计中，efuse 是一种非常重要的技术，可以保护 SoC 中的敏感信息，防止黑客攻击和未经授权的访问。尽管它有一些缺点，但它仍然是一个非常有用的工具，可以用于保护 SoC 的安全性。
]]></content>
  </entry>
  
  <entry>
    <title>认识电感器的重要作用与特性</title>
    <url>/post/hardware/understand-the-importance-and-feature-of-inductors.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>Inductor</tag>
    </tags>
    <content type="html"><![CDATA[电感器俗称电感，本质上是一个线圈，有空心线圈也有实心线圈，实心线圈有铁芯或者其它材料制成的芯，电感的单位是“H”，简称“亨”。此外，更小的单位是mH，uH，换算方式为1H=1000mH=1000000uH。
电感的常见作用 阻交通直 对于直流电，电感是相当于短路的；而对于交流电，电感是对其有阻碍作用的，交流电的频率越高，电感对它的阻碍作用越大。
变压器 对我们来说最熟悉的电感应用莫过于变压器了，如下图所示为变压器的电路符号。假如左侧线圈匝数为100，右侧匝数为50，如果左侧接220V交流电，那么右侧感应出来的电压为110V，即“匝数比=电压比”而电流却会截然相反；如果左侧流进1A电流，那么右侧会流出2A的电流，即“匝数比=电流的反比”，因为电感只会对电压、电流进行变化，而不能对功率进行变化，如果电压和电流都为正比显然是不合情理的。
RL低通滤波器 所谓低通滤波器是：低频信号可以通过，而高频信号不能通过，电路原理图如下图。输入信号如果是直流电，那么电感相当于一根导线；现在是短路，信号会经过电感，直接输出，而不经过电阻。如果我们逐渐升高电流的频率，由于电感对交流电有阻碍作用，通过电感的信号会慢慢变小，直到达到某一个频率，当高于这个频率之后的电流再也无法通过，这时候就形成了低通滤波器，这个频率就叫做截止频率，公式为 f=R/(2πL)。
RL高通滤波器 高通滤波器的道理和低通的类似，只不过电阻和电感的位置变了，如下图。如果是直流电，会经过电感流回去，这时候如果改变频率，当频率逐渐升高，由于电感对交流电的阻碍作用，当频率达到截止频率时，高频信号不经过电感，而直接把我们需要的高频信号输出。截止频率的计算也是 f=R/(2πL)。
以上列举了一些常用的电感应用，当然电感的作用远远不止这些，以上讲的都是基础，应用的时候考虑的远比以上所说的要多。
十种电感的特性 工字型电感 它的前身是挠线式贴片电感，工字型电感是它们的改良，挡板有效加强储能能力，改变EMI方向和大小，亦可降低RDC。它可以说是讯号通讯电感跟POWER电感的一种妥协。
贴片式的工字型电感主要用于几百kHz至一两MHz的较小型电源切换，如数字相机的LED升压、ADSL等较低频部份的讯号处理或POWER用途。它的Q值有20、30，做为讯号处理颇为适合。RDC比挠线式贴片电感低，作为POWER也是十分好用。当然，很大颗的工字型电感，那肯定是POWER用途了。
工字型电感最大的缺点仍是开磁路，有EMI的问题；另外，噪音的问题比挠线式贴片电感大。
色环电感 色环电感是最简单的棒形电感的加工，主要是用作讯号处理。本身跟棒形电感的特性没有很大的差别，只是多了一些固定物，和加上一些颜色方便分辨感值。因单价算是十分便宜，现时比较不注重体积，以及仍可用插件的电子产品，使用色环电感仍多。
空芯电感 空心电感主要是讯号处理用途，用作共振、接收、发射等。空气可应用在甚高频的产品，故此很多变异要求不太高的产品仍在使用。因为空气不是固定线圈的最佳材料，故此在要求越来越严格的产品趋势上，发展有限。
环形线圈电感 环形线圈电感，是电感理论中很理想的形状。闭磁路，很少EMI的问题，充分利用磁路，容易计算，几乎理论上的好处，全归环形线圈电感。可是，有一个最大的缺点，就是不好挠线，制程多用人工处理。
环形线圈电感最大量的，是用铁粉芯作材料跟树脂等混在一起，使得Air gap均匀分布在铁粉芯内部。
铁粉芯环形线圈电感的优点是环形，但缺点亦是环形。我前面曾说，使用者最喜欢的形状是方形，故此在妥协下环形线圈电感并不是最具优势。
贴片迭层高频电感 贴片迭层高频电感，其实就是空心电感。特性完全相同，不过因为容易固定，可以小型化。
贴片迭层高频电感跟空心电感比较，因为空气不是好的固定物，但空气的相对导磁率是一，在高频很好用，因此找一些相对导磁率是一，又是很好的固定物，那不是很好。
贴片迭层高频电感跟贴片挠线式高频电感的比较，贴片迭层高频电感的Q值不够高是最大的缺点,。
磁棒电感 磁棒电感是空心电感的加强，电感值跟导磁率成正比，塞磁性材料进空心线圈，电感值、Q值等都会大为增加。好处，就自己想象了。如果想不通，或者不想思考，要早点改行喔。磁棒电感是最简单、最基本的电感；30年到100年前，电感有什么应用，它就有什么应用，特性亦是如此。
SMD贴片功率电感 SMD贴片功率电感最主要是强调储能能力，以及LOSS要少。
穿心磁珠 穿心磁珠，就是阻抗器啦，电感是低通组件，可让低频通过，阻挡高频。
贴片磁珠 贴片磁珠就是穿心磁珠的下一代。
贴片高频变压器、插件高频变压器 高频变压器嘛，一般用于开关电源。
原文地址: 认识电感器的重要作用与特性  
]]></content>
  </entry>
  
  <entry>
    <title>常见总线：IIC、IIS、SPI、UART、JTAG、CAN、SDIO、GPIO</title>
    <url>/post/hardware/usual-bus-protocol-introduction.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>IIC</tag>
      <tag>IIS</tag>
      <tag>SPI</tag>
      <tag>UART</tag>
    </tags>
    <content type="html"><![CDATA[本文介绍了常见的总线：IIC、IIS、SPI、UART、JTAG、CAN、SDIO、GPIO。
IIC IIC(Inter－Integrated Circuit)总线是一种由PHILIPS公司开发的两线式串行总线，用于连接微控制器及其外围设备。I2C总线用两条线(SDA和SCL)在总线和装置之间传递信息，在微控制器和外部设备之间进行串行通讯或在主设备和从设备之间的双向数据传送。I2C是OD输出的，大部分I2C都是2线的（时钟和数据），一般用来传输控制信号。
IIS I2S（Inter-IC Sound Bus）是飞利浦公司为数字音频设备之间的音频数据传输而制定的一种总线标准。I2S有3个主要信号：1.串行时钟SCLK，也叫位时钟，即对应数字音频的每一位数据，SCLK有1个脉冲。2.帧时钟LRCK，用于切换左右声道的数据。LRCK为“1”表示正在传输的是左声道的数据，为“0”则表示正在传输的是右声道的数据。3.串行数据SDATA，就是用二进制补码表示的音频数据。有时为了使系统间能够更好地同步，还需要另外传输一个信号MCLK，称为主时钟，也叫系统时钟（Sys Clock）。
SPI SPI(Serial Peripheral Interface：串行外设接口);SPI是Motorola首先在其MC68HCXX系列处理器上定义的。SPI接口主要应用在EEPROM,FLASH,实时时钟,AD转换器,还有数字信号处理器和数字信号解码器之间。SPI接口是以主从方式工作的,这种模式通常有一个主器件和一个或多个从器件,其接口包括以下四种信号：（1）MOSI – 主器件数据输出,从器件数据输入 （2）MISO – 主器件数据输入,从器件数据输出 （3）SCLK – 时钟信号,由主器件产生（4）/SS – 从器件使能信号,由主器件控制。
UART UART(Universal Asynchronous Receiver Transmitter：通用异步收发器)。将由计算机内部传送过来的并行数据转换为输出的串行数据流。将计算机外部来的串行数据转换为字节，供计算机内部使用并行数据的器件使用。在输出的串行数据流中加入奇偶校验位，并对从外部接收的数据流进行奇偶校验。在输出数据流中加入启停标记，并从接收数据流中删除启停标记。处理由键盘或鼠标发出的中断信号（键盘和鼠标也是串行设备）。可以处理计算机与外部串行设备的同步管理问题。有一些比较高档的UART还提供输入输出数据的缓冲区。常用TXD，RXD，/RTS，/CTS。
JTAG JTAG (Joint Test Action Group 联合测试行动小组)是一种国际标准测试协议（IEEE1149.1兼容），主要用于芯片内部测试。标准的JTAG接口是4线：TMS、TCK、TDI、TDO，分别为模式选择、时钟、数据输入和数据输出线。测试复位信号(TRST,一般以低电平有效)一般作为可选的第五个端口信号。一个含有JTAGDebug接口模块的CPU，只要时钟正常，就可以通过JTAG接口访问CPU的内部寄存器和挂在CPU总线上的设备，如FLASH，RAM，内置模块的寄存器，象UART，Timers，GPIO等等的寄存器。
CAN CAN全称为“Controller Area Network”，即控制器局域网，是国际上应用最广泛的现场总线之一。最初，CAN被设计作为汽车环境中的微控制器通讯，在车载各电子控制装置ECU之 间交换信息，形成汽车电子控制网络。比如：发动机管理系统、变速箱控制器、仪表装备、电子主干系统中，均嵌入CAN控制装置。一个由CAN总线构成的单一网络中，理论上可以挂接无数个节点。实际应用中，节点数目受网络硬件的电气特性所限制。例如，当使用Philips P82C250作为CAN收发器时，同一网络中允许挂接110个节点。CAN 可提供高达1Mbit/s的数据传输速率，这使实时控制变得非常容易。另外，硬件的错误检定特性也增强了CAN的抗电磁干扰能力。
SDIO SDIO是SD型的扩展接口，除了可以接SD卡外，还可以接支持SDIO接口的设备，插口的用途不止是插存储卡。支持 SDIO接口的PDA，笔记本电脑等都可以连接象GPS接收器，Wi-Fi或蓝牙适配器，调制解调器，局域网适配器，条型码读取器，FM无线电，电视接收 器，射频身份认证读取器，或者数码相机等等采用SD标准接口的设备。
GPIO GPIO (General Purpose Input Output 通用输入/输出)或总线扩展器利用工业标准I²C、SMBus™或SPI™接口简化了I/O口的扩展。当微控制器或芯片组没有足够的I/O端口，或当系统 需要采用远端串行通信或控制时，GPIO产品能够提供额外的控制和监视功能。
每个GPIO端口可通过软件分别配置成输入或输出。
]]></content>
  </entry>
  
  <entry>
    <title>十年经验的大神谈如何学STM32嵌入式开发</title>
    <url>/post/mcu/how-to-learn-stm32-embedded-development.html</url>
    <categories><category>MCU</category>
    </categories>
    <tags>
      <tag>STM32</tag>
      <tag>MCU</tag>
    </tags>
    <content type="html"><![CDATA[本文描述了十年经验的大神谈如何学STM32嵌入式开发
理解嵌入式 从硬件上说 “嵌入”将基于CPU的处围器件，整合到CPU芯片内部，比如早期基于X86体系结构下的计算机，CPU只是有运算器和累加器的功能，一切芯片要造外部桥路来扩展实现，象串口之类的都是靠外部的16C550/2的串口控制器芯片实现，而目前的这种串口控制器芯片早已集成到CPU内部，还有PC机有显卡，而多数嵌入式处理器都带有LCD控制器，但其种意义上就相当于显卡。比较高端的ARM类Intel Xscale架构下的IXP网络处理器CPU内部集成PCI控制器，可配成支持4个PCI从设备或配成自身为CPI从设备;还集成3个NPE网络处理器引擎，其中两个对应于两个MAC地址， 可用于网关交换用，而另外一个NPE网络处理器引擎支持DSL，只要外面再加个PHY芯片即可以实现DSL上网功能。IXP系列最高主频可以达到 1.8G，支持2G内存，1G×10或10G×1的以太网口或Febre channel的光通道。IXP系列应该是目标基于ARM体系统结构下由intel进行整合后成Xscale内核的最高的处理器了。
从软件上说 嵌入就是在定制操作系统内核里将应用一并选入，编译后将内核下载到ROM中。而在定制操作系统内核时所选择的应用程序组 件就是完成了软件的“嵌入”，比如Win在内核定制时，会有相应选择，其中就是wordpad,PDF,MediaPlay等等选择，如果我们选择 了，在CE启动后，就可以在界面中找到这些东西，如果是以前PC上将的windows操作系统，多半的东西都需要我们得新再装。
把软件内核或应用文件系统等东西烧到嵌入式系统硬件平台中的ROM中就实现了一个真正的“嵌入”。
以上的定义是我在6、7年前给嵌入式系统下自话侧重于理解型的定义，书上的定义也有很多，但在“嵌入式”这个领域范围内，谁都不敢说自己的定义是十分确切的，包括那些专家学者们，历为毕竟嵌入式系统是计算机范畴下的一门综合性学科。
嵌入式系统的分层 嵌入式系统分为4层，硬件层、驱动层、操作系统层和应用层。
硬件层 硬件层是整个嵌入式系统的根本，如果现在单片机及接口这块很熟悉，并且能用C和汇编语言来编程的话，从嵌入式系统的硬件层走起来相对容易，硬件层也是驱动层的基础，一个优秀的驱动工程师是要能够看懂硬件的电路图和自行完成CPLD的逻辑设计的，同时还要对操作系统内核及其调度性相当的熟悉的。但硬件平台是基础，增值还要靠软件。
硬件层比较适合于，电子、通信、自动化、机电一体、信息工程类专业的人来搞，需要掌握的专业基础知识有，单片机原理及接口_技术、微机原理及接口_技术、C语言。
驱动层 驱动层这部分比较难，驱动工程师不仅要能看懂电路图还要能对操作系统内核十分的精通，以便其所写的驱动程序在系统调用时，不会独占操作系统时间片，而导致其它任务不能动行，不懂操作系统内核架构和实时调度性，没有良好的驱动编写风格，按大多数书上所说添加的驱动的方式，很多人都能做到，但可能连个初级的 驱动工程师的水平都达不到，这样所写的驱动在应用调用时就如同windows下我们打开一个程序运行后，再打开一个程序时，要不就是中断以前的程序，要不就是等上一会才能运行后来打开的程序。想做个好的驱动人员没有三、四年功底，操作系统内核不研究上几编，不是太容易成功的，但其工资在嵌入式系统四层中可 是最高的。
嵌入式的驱动层比较适合于电子、通信、自动化、机电一体、信息工程类专业尤其是计算机偏体系结构类专业的人来搞，除硬件层所具备的基础学科外，还要对数据结构与算法、操作系统原理、编译原理都要十分精通了解。
操作系统层 对于操作系统层目前可能只能说是简单的移植，而很少有人来自已写操作系统，或者写出缺胳膊少腿的操作系统来，这部分工作大都由驱动工程师来完成。操作系统是负责系统任务的调试、磁盘和文件的管理，而嵌入式系统的实时性十分重要。据说，XP操作系统是微软投入300人用两年时间才搞定的，总时工时是600人年，中科院软件所自己的女娲Hopen操作系统估计也得花遇几百人年才能搞定。因此这部分工作相对来讲没有太大意义。
应用层 应用层相对来讲较为容易的，如果会在windows下如何进行编程接口函数调用，到操作系统下只是编译和开发环 境有相应的变化而已。如果涉及Jave方面的编程也是如此的。嵌入式系统中涉及算法的由专业算法的人来处理的，不必归结到嵌入式系统范畴内。但如果涉及嵌 入式系统下面嵌入式数据库、基于嵌入式系统的网络编程和基于某此应用层面的协议应用开发(比如基于SIP、H.323、Astrisk)方面又较为复杂， 并且有难度了。
从哪里入手嵌入式系统 从硬件上入手ARM 一方面就是学习接口电路设计，另一方面就是学习汇编和C语言的板级编程。如果从软件上讲，就是要学习基于ARM处理器的操作系统层面 的驱动、移植了。这些对于初学都来说必须明确,要么从硬件着手开始学，要么从操作系统的熟悉到应用开始学，但不管学什么，只要不是纯的操作系统级以上基于 API的应用层的编程，硬件的寄存器类的东西还是要能看懂的，基于板级的汇编和C编程还是要会的。因此针对于嵌入式系统的硬件层和驱动程的人，ARM的接 口电路设计、ARM的C语言和汇编语言编程及调试开发环境还是需要掌握的。
因此对于初学者必然要把握住方向，自己学习嵌入式系统的目标是什么，自己要在那一层面上走。然后再着手学习较好，与ARM相关的嵌入式系统的较为实际的两个层面硬件层和驱动层，不管学好了那一层都会很有前途的。
从嵌入式系统的应用层如何入手 可能与ARM及其它体系相去较远，要着重研究基嵌入式操作系统的环境应用与相应开发工具链，比如WinCe操作系统下的EVC应用开发(与windows下的VC相类似)，如果想再有突破就往某些音视频类的协议上靠，比如VOIP领域的基于SIP或H.323协议的应用层开发，或是基于嵌入式网络数据库的开发等等。
对于初学者来讲，要量力而行，不要认为驱动层工资高就把它当成方向了，要结合自身特点，嵌入式系统四个层面上那个层面上来讲都是有高人存在，当然高人也对应 的高工资，我是做硬件层的，以前每月工资中个人所得税要被扣上近3千大元，当然我一方面充当工程师的角色，一方面充当主管及人物的角色，两个职位我一个人 干，但上班时间就那些。硬件这方面上可能与我PK的人很少了，才让我拿到那么多的工资。
芯片的选择 很多ARM初学者都希望有一套自己能用的系统，但他们住住会产生一种错误认识就是认为处理器版本越高、性能越高越好，就象很多人认为ARM9与ARM7好， 我想对于初学者在此方面以此入门还应该理智，开发系统的选择最终要看自己往嵌入式系统的那个方向上走，是做驱动开发还是应用，还是做嵌入式系统硬件层设计与板级测试。如果想从操作系统层面或应用层面上走，不管是驱动还是应用，那当然处理器性能越高越好了，但嵌入式系统这个东西自学，有十分大的困难，不是几个月或半年 或是一年二年能搞定的事。
在某种意义上请，ARM7与9的差别就是在某些功能指令集上丰富了些，主频提高一些而已，就比如286和386。对于用户来讲可能觉查不到什么，只能是感觉速度有些快而已。
ARM7比较适合于那些想从硬件层面上走的人，因为ARM7系列处理器内部带MMU的很少，而且比较好控制，就比如S3C44B0来讲，可以很容易将 Cache关了，而且内部接口寄存器很容易看明白，各种接口对于用硬件程序控制或AXD单步命令行指令都可以控制起来，基于51单片机的思想很容易能把他 搞懂，就当成个32位的单片机，从而消除很多51工程师想转为嵌入式系统硬件ARM开发工程师的困惑，从而不会被业界某此不是真正懂嵌入式烂公司带到操作 系统层面上去，让他们望而失畏，让业界更加缺少这方面的人才。
而嵌入式系统不管硬件设计还是软件驱动方面都是十分注重接口这部分的，选择平台还要考察一个处理器的外部资源，你接触外部资源越多，越熟悉他们那你以后就业成功的机率就越高，这就是招聘时 所说的有无“相关技能”，因为一个人不可能在短短几年内把所有的处理器都接触一遍，而招聘单位所用的处理器就可能是我们完全没有见过的，就拿台湾数十家小公司(市价几千万)的公司生产的ARM类处理器，也很好用，但这些东西通用性太差，用这些处理器的公司就只能招有相关工作经验的人了，那什么是相关工作经 验，在硬件上讲的是外围接口设计，在软件上讲是操作系统方面相关接口驱动及应用开发经验。我从业近十年，2000年ARM出现，我一天始做ARM7,然后 直接跑到了Xscale(这个板本在ARM10-11之间)，一做就是五年，招人面试都不下数百人，在这些方面还是深有体会的。
开发系统的选择，要看自己的未来从来目标方向、要看开发板接口资源、还要看业界的通用性。
成为高级嵌入式系统硬件工程师要具备的技能 就单纯信号来分为数字和模拟，模拟比较难搞，一般需要很长的经验积累，单单一个阻值或容值的精度不够就可能使信号偏差很大。因此年轻人搞的较少，随着技术的发展，出现了模拟电路数字化，比如手机的Modem射频模块，都采用成熟的套片，而当年国际上只有两家公司有此技术，自我感觉模拟功能不太强的人，不太适合搞这个，如果真能搞定到手机的射频模块，只要达到一般程度可能月薪都在15K以上。
一个优秀的硬件工程师应该能够在没有参考方案的前提下设计出一个在成本和性能上更加优秀的产品，靠现有的方案，也要进行适当的可行性裁剪，但不是胡乱的来，我遇到一个工程师把方案中的5V变1.8V的DC芯片，直接更换成LDO，有时就会把CPU烧上几个。
高级硬件件工程师技术技能都要具备那些东西哪，首先要掌握EDA设计的辅助工具类如Protel、ORCAD、PowperPCB、Maplux2、ISE、VDHL语言，要能用到这些工具画图画板做逻辑设计，再有就是接口设计审图能力，再者就是调试能力，如果能走到总体方案设计这块，那就基本上快成为资深工程师了。硬件是要靠经验，也要靠积累的，十年磨一剑，百年磨一针。
]]></content>
  </entry>
  
  <entry>
    <title>STM32怎么选型</title>
    <url>/post/mcu/how-to-select-stm32.html</url>
    <categories><category>MCU</category>
    </categories>
    <tags>
      <tag>STM32</tag>
      <tag>MCU</tag>
    </tags>
    <content type="html"><![CDATA[什么是 STM32 STM32，从字面上来理解，ST是意法半导体，M是Microelectronics的缩写，32表示32位，合起来理解，STM32就是指ST公司开发的32位微控制器。在如今的32位控制器当中，STM32可以说是最璀璨的新星，它受宠若娇，大受工程师和市场的青睐，无芯能出其右。
STM32属于一个微控制器，自带了各种常用通信接口，比如USART、I2C、SPI等，可接非常多的传感器，可以控制很多的设备。现实生活中，我们接触到的很多电器产品都有STM32的身影，比如智能手环，微型四轴飞行器，平衡车、移动POST机，智能电饭锅，3D打印机等等。下面我们以最近最为火爆的两个产品来讲解下，一个是手环，一个是飞行器。
现在无人机非常火热，高端的无人机用STM32做不来，但是小型的四轴飞行器用STM32还是绰绰有余的。
STM32 分类 STM32有很多系列，可以满足市场的各种需求，从内核上分有Cortex-M0、M3、M4和M7这几种，每个内核又大概分为主流、高性能和低功耗。具体如下表所示。
单纯从学习的角度出发，可以选择F1和F4，F1代表了基础型，基于Cortex-M3内核，主频为72MHZ，F4代表了高性能，基于Cortex-M4内核，主频180M。之于F1，F4（429系列以上）除了内核不同和主频的提升外，升级的明显特色就是带了LCD控制器和摄像头接口，支持SDRAM，这个区别在项目选型上会被优先考虑。但是从大学教学和用户初学来说，还是首选F1系列，目前在市场上资料最多，产品占有量最多的就是F1系列的STM32。
以STM32F103VET6来讲下STM32的命名方法，具体如下表所示。
更详细的命名方法说明，见下图。
选择合适的 MCU 了解了STM32的分类和命名方法之后，就可以根据项目的具体需求先大概选择哪类内核的MCU，普通应用，不需要接大屏幕的一般选择Cortex-M3内核的F1系列，如果要追求高性能，需要大量的数据运算，且需要外接RGB大屏幕的则选择Cortex-M4内核的F429系列。明确了大方向之后，接下来就是细分选型，先确定引脚，引脚多的功能就多，价格也贵，具体得根据实际项目中需要使用到什么功能，够用就好。确定好了引脚数目之后再选择FLASH大小，相同引脚数的MCU会有不同的FLASH大小可供选择，这个也是根据实际需要选择，程序大的就选择大点的FLASH，要是产品一量产，这些省下来的都是钱啊。有些月出货量以KK（百万数量级）为单位的产品，不仅是MCU，连电阻电容能少用就少用，更甚者连PCB的过孔的多少都有讲究。项目中的元器件的选型有很多学问。
]]></content>
  </entry>
  
  <entry>
    <title>你了解C 和 C++ 标准库吗</title>
    <url>/post/programming/do-you-understand-c-and-c-plus-plus-standard-library.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>C</tag>
      <tag>C++</tag>
    </tags>
    <content type="html"><![CDATA[本文简要介绍编写 C/C++ 应用程序的领域，标准库的作用以及它是如何在各种操作系统中实现的。
我已经接触 C++ 一段时间了，一开始就让我感到疑惑的是其内部结构：我所使用的内核函数和类从何而来？谁发明了它们？他们是打包在我系统中的某个地方吗？是否存在一份官方的 C++ 手册？
在本文中，我将通过从 C 和 C++ 语言的本质到实际实现来尝试回答这些问题。
C 和 C++ 是如何制订的 当我们谈论 C 和 C++ 时，实际上是指一组定义（程序）语言应该做些什么，如何表现，应该提供哪些功能的规则。C/C++ 的编译器为了处理 C/C++ 编写的源代码必须跟随着这些规则，并生成二进制应用程序。听起来非常接近于 HTML:浏览器遵循着一组指令，所以它们可以以明确的方式来渲染网页。
与 HTML 一样，C 和 C++ 的规则都是理论上的。国际标准化组织(ISO)的一大群人每年都会聚集几次来讨论和定义语言规则。没错，C 和 C++ 是标准化的东西。他们最终都会得到一本官方的叫标准的书，你可以从他们的网站中购买。随着语言的发展新的 papers（指官方的叫标准的书）会被发布，每一次都定义一个新的标准。这就是为什么我们会有不同的 C 和 C++ 版本的原因：C99, C11, C++03, C++11, C++14 等等，数字与出版/发布年份相符。
这些标准都市非常详细和有技术新的文档：我不会把它们当作手册。通常会分为两部分：
  C/C++ 的功能和特性；
  C/C++ 的 API&ndash; 开发人员可以用于他们的 C/C++ 程序的一个类、函数和宏的集合。它也被称为标准库。
  例如，这里有个来自于 C 标准库第一部分的摘选，它定义了 main 函数的结构：
 main 的定义，程序启动时调用的函数。  这是另外一个来自与同样标准的摘录，描述了 CAPI 的成员 &ndash;fmin 函数：
在 math.h 文件中定义 min 函数。  如你所见，几乎没涉及到代码。有人必须阅读标准并将其转换成计算机可以消化的东西。这是工作于编译器和（功能）实现上人们所做的：前者是一种可以读取和处理 C 和 C++ 源文件的工具，后者将标准库转换为代码。我们来深入了解一下。
C 标准库 C 标准库也称为 ISO C 库，是用于完成诸如输入/输出处理、字符串处理、内存管理、数学计算和许多其他操作系统服务等任务的宏、类型和函数的集合。它是在 C 标准中（例如 C11 标准）中定义的。其内容分布在不同的头文件中，比如上面我所提到的 math.h。
C++ 标准库 和 C 标准库的概念类似，但仅针对 C++。C++ 标准库是一组 C++ 模板类，它提供了通用的编程数据结构和函数，如链表、堆、数组、算法、迭代器和任何其他你可以想到的 C++ 组件。C++ 标准库也包含了 C 标准库，并在 C++ 标准中进行了定义（例如 C++ 11 标准）。
实现 C/C++ 标准库 我们从这里开始讨论真正的代码了。从事于标准库实现的开发者阅读官方的 ISO 规范并将其转化为代码。他们必须依赖其操作系统所提供的功能（读/写文件，分配内存，创建线程，&hellip;&hellip;所有这些被称为系统调用），因此每个平台都有其自己的标准库实现。有时它是系统内核的一部分，有时它是作为一个附加组件 - 编译器 - 必须单独下载。
GNU/Linux 版实现 GNU C 库，也称为 glibc, 是 C 标准库的 GNU 项目实现。并非所有的标准 C 函数都可以在 glibc 中找到：大多数数学函数实际上是在 libm 库中实现的，这是一个独立的库。
至今，glibc 是 Linux 上使用最广泛的 C 库。然而，在 90 年代期间，有一段时间里，glibc 有一个竞争对手称为 Linux libc（或者简称 libc），它是由 glibc 1.x 的一个分支产生的。在一段时间里，Linux libc 是许多 Linux 发行版中的标准 C 库。
经过多年的发展，glibc 竟然比 Linux libc 更具优势，并且所有使用它的 Linux 发行版都切换回了 glibc。所以，如果你在你的磁盘中找到一个名为 libc.so.6 的文件，请不要担心：它是现代版的 glibc。为了避免与之前的 Linux libc 版本混淆，版本号增加到了 6（他们无法将其命名为 glibc.so.6：所有Linux库都必须以 lib 前缀打头）。
另一方面，C++ 标准库的实现位于 libstdc++ 或 GNU 标准 C++ 库中。这是一个正在进行的在 GNU/Linux 上实现标准 C++ 库的项目。一般来说，所有常规的 Linux 发行版都默认使用 libstdc++。
Mac 和 iOS 版实现 在 Mac 和 iOS 上，C 标准库的实现是 libSystem 的一部分，libSystem 是位于 /usr/lib/libSystem.dylib 中的核心库。LibSystem 包含其他组件，如数学库、线程库和其他底层实用程序。
关于 C++ 标准库，在 OS X Mavericks（V10.9）之前的 Mac 上，libstdc++ 是默认选项。这在现代的基于 Linux 的系统上可以找到的同样的实现。自 OS X Mavericks 开始，Apple 切换到使用 libc++，这是 LLVM 项目——Mac 官方编译器框架——所引入的 GNU libstdc++ 标准库的替代。
IOS 开发者可以使用 iOS SDK（软件开发工具包）来访问标准库，它是一系列允许创建移动应用程序的工具。
Windows 版实现 在 Windows 上，标准库的实现一直严格限定在 Visual Studio 中，它是微软官方的编译器。他们通常称之为 C/C++ 运行时库（CRT），并且它涵盖了 c/c++ 二者的实现。
在最开始，CRT 被实现为 CRTDLL.DLL 库（我猜，当时没有可用的 C++ 标准库）。从 Windows 95 开始，Microsoft 开始将其迁移到 MSVCRT [版本号] .DLL（MSVCR20.DLL，MSVCR70.DLL 等）之上，据推测也包含 C++ 标准库。在 1997 年，他们决定将文件名简化为 MSVCRT.DLL，这不幸导致了令人讨厌的DLL混乱。这就是为什么从 Visual Studio 7.0 版开始，他们切换回每个版本使用单独的 DLL 了。
Visual Studio 2015 引入了深度的 CRT 重构。C/C++ 标准库的实现迁移到一个新库，Universal C 运行时库 (Universal CRT 或 UCRT)，编译为 UCRTBASE.DLL。UCRT 目前已经成为 Windows 组之一，从 Windows 10 开始作为操作系统的一部分提供。
Android 版实现 Bionic 是 Google 为其 Android 操作系统所编写的 C 标准库实现，它直接在底层使用。第三方开发者可以通过 Android 原生开发工具包（NDK）访问 Bionic，该工具集允许你使用 C 和 C++ 代码编写 Android 应用程序。
在 C++ 端, NDK 提供了很多版本的实现：
  libc++，从从 Lollipop 开始的官方安卓系统和现代 Mac 操作系统都将其作为 C++ 标准库使用。从 NDK 发布 17 版本开始，它将成为 NDK 中唯一可用的 C++ 标准库实现；
  gnustl，libstdc++ 的别名，这两者在 GNU/linux 是同一个库。这个库的已被弃用，它将在 NDK 发布 18 中删除；
  STLport，由 STLport 项目编写的 C++ 标准库的第三方实现，自 2008 年以来一直处于不活跃状态。与 gnustl 一样，STLport 将在 NDK 发布 18 中移除。
  我能使用不同版本的实现代码来替代默认实现吗？ 如果你正在使用资源非常有限的系统，则通常需要引用 C 标准库的不同实现。比如，uClibc-ng, musl libc 和 diet libc 等等，所有这些都适用于嵌入式 Linux 系统的开发，提供更小的二进制文件和更少的内存占用。
C++ 标准库也有不同的实现版本：Apache C++ 标准库，uSTL 以及 EASTL 等等。后面两个实际上仅关注模板部分，而不是完整的库，并且他们是在速度优先的情况下开发的。Apache 版本的库注重的是可移植性。
如果我们脱离了标准库怎么办？ 不使用标准库很简单：只要在你的程序中不引入它们的任何一个头文件，你的工作就完成了。然而，为了让这个操作更有意义一些，你需要通过一些提供的系统调用使用某种方法与操作系统互动。就像我之前说的，这就是标准库中的函数/方法在底层实现的时候所使用的。很可能你也会不得不调用这些方法来与硬件设备交互。
如果对你来说这听起来很让人激动，有些人已经开始在网上尝试在不导入标准库的情况下创建工作流程。因为你依赖于一个特定操作系统所提供的函数，这种方式会丧失可移植性。然而通过使用这种艰难的方式，肯会让你学到更多，而且让你更好的理解当你所做的事情，即使是在使用高级库的时候。
除了知识，当你在嵌入式操作系统上面工作的时候你不会想去引入标准库：因为代码不需要移植，在有限的内存中每个字节都很重要，这会让你更加精准的写代码。另一个使用背景就是 demoscene，在这里人们尽量有限的程序的二进制大小中去保留高质量的音视频——4K仍然不是最小值：一些 demoparties 使用 1K，256 字节，64 字节或者甚至 32 字节来竞争。在那里不允许使用标准库！
]]></content>
  </entry>
  
  <entry>
    <title>yum安装PHP5.4的亲身体验系统管理员</title>
    <url>/post/linux/how-to-install-PHP5.4-through-yum.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>linux</tag>
      <tag>CentOS</tag>
      <tag>yum</tag>
      <tag>PHP5.4</tag>
    </tags>
    <content type="html"><![CDATA[本文将以一位系统管理员的角度，分享在CentOS系统上使用yum安装PHP 5.4的亲身体验。通过8个方面的详细介绍嵌入式linux驱动程序设计从入门到精通，帮助读者顺利完成安装，并提供一些实用的技巧和注意事项。
安装yum 首先，我们需要确保系统已经安装了yum软件包管理器。通过命令行输入“yum”，如果出现相关信息，则说明已经安装成功。若未安装，可以通过搜索引擎找到相关教程进行安装。
配置yum源 为了能够顺利安装PHP 5.4，我们需要配置正确的yum源。可以选择官方源或其他可信赖的源。根据系统版本和架构选择对应的源，并将其配置到yum的配置文件中。
更新yum缓存 在安装之前，务必执行“yum makecache”命令来更新yum缓存。这样可以确保我们获取到最新的软件包列表，避免因为旧版本软件包而导致的问题。
安装PHP 5.4 通过执行“yum install php54”命令来安装PHP 5.4及其相关依赖包。在此过程中，可以根据需要选择其他附加模块进行安装。
配置PHP 安装完成后，我们需要进行一些基本的配置。可以编辑php.ini文件来修改一些常用的配置项，如时区、内存限制等。同时centos yum php 54，也可以根据实际需求加载或禁用一些扩展模块。
启动PHP 安装完成后linux命令ls，默认情况下，PHP会自动启动。可以通过命令“service php-fpm start”来手动启动PHP服务。如果需要设置开机自启动，可以执行“chkconfig php-fpm on”命令。
测试PHP 为了验证安装是否成功，我们可以创建一个简单的php文件，并在浏览器中访问该文件。如果能够正常显示phpinfo信息，则说明安装成功。
常见问题及解决方法 最后，我们还将介绍一些常见的安装问题及解决方法。例如，遇到依赖问题时应如何处理；如何切换不同版本的PHP等等。这些问题和解决方法将帮助读者更好地应对实际情况。
通过以上8个方面的介绍，相信读者已经对在CentOS系统上使用yum安装PHP 5.4有了初步的了解。希望本文能够为读者提供实用的指导和帮助centos yum php 54，让大家能够顺利完成相关操作，并在使用中发挥出PHP 5.4的优势。
]]></content>
  </entry>
  
  <entry>
    <title>数据中心的机架密度：何以见高峰</title>
    <url>/post/datacenter/data-center-rack-density-how-high-can-it-go.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Rack Density</tag>
      <tag>HPC</tag>
      <tag>AI</tag>
    </tags>
    <content type="html"><![CDATA[人工智能（ AI  ）和高性能计算（HPC）已将计算、存储容量和网络资源的需求推向了极限。更强大的芯片是推动机架密度不断提升的主要驱动力之一。仅仅十年前，平均处理器的功耗还不到100瓦，但如今它们的功耗已经达到了约500瓦。
随着电力在有限的空间内变得更加充裕，应用开发人员不断寻找利用这一潜力的方法，并迫切需要更多的算力。Hyperion Research的首席执行官Earl Joseph指出，HPC的增长主要源于更加复杂和苛刻的应用程序的涌现。
“人工智能、机器学习和深度学习每年以接近30%的速度增长，” Joseph表示。
现在让我们一起来探讨机架密度的演变以及它可能达到的高度。
饥不择食的需求 数据中心正以前所未有的数量进行规划、建设和投入运营。根据Uptime Institute的统计数据，预计到2025年，数据中心的电力占地面积将增加50%，而2019年至2025年的全球数据产量将增长500%。数据中心的兴建数量创下历史新高，计算密度远远超越以往。为了满足这种需求，数据中心必须在每个机架或每平方英尺的空间内提供更多的算力。
十多年前，根据Uptime Institute的数据，每个机架的平均功率密度仅在4-5千瓦左右。然而，到了2020年，这个数字已经飙升至每个机架8-10千瓦。值得注意的是，在美国进行的数据中心调查中，有三分之二的数据中心表示，它们已经在每个机架16-20千瓦的功率密度范围内迎来了峰值需求。而截止到2022年的最新数据则显示，有10%的数据中心报告每个机架的功率密度已达到20-29千瓦，7%的数据中心每个机架的功率密度达到30-39千瓦，3%的数据中心每个机架的功率密度达到40-49千瓦，还有5%的数据中心每个机架的功率密度高达50千瓦或更高。显然，现代应用程序和数据量正在推动机架密度达到前所未有的高水平。
巨头主导的推力 在过去的十年里，像亚马逊、Facebook、谷歌和微软等超大规模数据中心一直在密度增长方面发挥着引领作用。他们开创了更佳冷却和供电方法，同时在有限的空间内提供尽可能多的算力。一些数据中心的机架已经演化到每个机架50千瓦甚至更高的功率密度。
然而，有趣的是，这些超大规模数据中心现在更愿意将最高密度的机架留给其他玩家。虽然他们曾将整个行业推向新的密度水平，但如今更倾向于在每个机架约30千瓦左右的密度范围内稳健运营。由于他们拥有规模庞大和高效率的优势，他们能够满足大多数用户的需求。超大规模数据中心所追求的是优化。他们需要的是高度（但不过高）的机架密度，这种密度可扩展，并且要以有吸引力的价格提供。
专业高密度数据中心 针对高性能计算（HPC）和人工智能（AI）市场，出现了专业的高密度数据中心供应商，它们正在从意想不到的领域赢得业务。
“尽管过去，HPC主要是亿美元营业额的大企业和研究机构的专属领域，但如今，越来越多规模较小的企业正在将其用于获取竞争优势，” Oper8 Global的首席执行官Mike Andrea指出。
他指出，HPC应用程序的民主化已经从大型、经济充裕的组织扩展到规模较小的研发单位，以及航天、地震工程、三维建模、自动驾驶汽车仿真、各种AI应用案例、能源、石油和天然气生产、天气预测、数据分析、医疗保健和三维电影渲染等领域。对HPC的需求不断增加。
“延迟问题仍然是HPC的主要驱动因素，与此同时，数据中心必须支持每个机架超过100千瓦的高机架密度，” Andrea表示。他的公司正在与多个客户合作，这些客户要求每个机架的功率从80千瓦到200千瓦不等。
为了在这一市场竞争中脱颖而出，高密度数据中心必须位于其客户附近，以减小延迟。因此，这些专门提供高密度服务的数据中心很可能只会在一些特定地区找到市场，这些地区有一批对高密度要求非常苛刻的客户。
逐步拓展的HPC机架 另一个趋势是，数据中心开始通过部署仅有一个或两个高密度机架来服务不断增长的HPC市场。一些边缘和合作数据中心开始实施一个高密度HPC机架单元，包括两到十二个机架，或者将高密度HPC机架与更常规密度的机架组成的集群并置。这种策略有助于数据中心满足一两个客户的需求，同时无需大规模投资对整个数据中心进行彻底重新设计。
然而，即使只引入一个或两个机架，也需要大量的工作。除了新的服务器和支持设备外，还需要进行布线和其他改进。这还要求有足够的额外电力供应，并确保HPC机架可以获得足够的冷却。因此，高密度数据中心可能需要投资计算流体动力学（CFD）技术，以增强空气和冷却流动，以避免出现热点问题。
此外，它们需要采用先进的冷却技术，甚至可能需要某种形式的液体冷却来保持新机架的温度在可接受范围内。极高温度的机架可能会导致电力配电单元（PDU）发生故障，因为机架后部积聚了大量的热空气。特别是在超过35千瓦的机架上，这一问题尤为明显。
“当高功耗元件安装在有限的空间内时，采用液冷技术可能变得不可或缺，”戴尔科技高性能计算（HPC）和新兴工作负载高级工程总监Onur Celebioglu指出。
基于水冷的方案，如主动冷却机架门、采用冷板直接液冷和液体浸没冷却，正在HPC机架中变得越来越常见。但需要注意的是，采用液冷的HPC应用可能需要更宽更深的机柜，以容纳额外的电力供应和液体管道。这些成本，再加上需要对现有数据中心进行大规模重新配置的情况，可能会使一些人望而却步，不进入HPC市场。
未来的高密度 不久前，高密度机架被认为是10千瓦或更多。与现代的密度数字相比，这看起来相当微不足道。没有人知道密度能够达到多高。但在未来几年，让我们为一些惊人的数字做好准备吧。
“如今，高密度机架大约在40千瓦到125千瓦之间，而极高密度机架甚至可达到200千瓦甚至更高，” Andrea表示。
]]></content>
  </entry>
  
  <entry>
    <title>什么是Page Cache</title>
    <url>/post/linux/what-is-page-cache.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>page cache</tag>
    </tags>
    <content type="html"><![CDATA[什么是Page Cache Page Cache，翻译为页高速缓冲存储器。它是动态变化的，因为操作系统会将所有未直接分配给应用程序的物理内存都用于页面缓存。Page Cache是文件系统层级的缓存，用于缓存文件的页数据，属于内核管理的内存。从磁盘中读取到的内容是存储在page cache里的。
为什么需要Page Cache Page Cache机制的目的是为了减少IO，提升IO磁盘读写的效率。由于程序的时间局部性和空间局部性，读写过的文件在下次还可能再次读取，如果每次读写文件都去磁盘中获取，显然读写性能太差，因为磁盘的读写速率相对于内存来说，慢了不止一点点。
因为有Page Cache机制，所以我们可以发现读写一个文件第一次非常慢，但是第二次就会变得很快，这是因为第一次读写这个文件的时候，Linux内核已经把文件内容缓存到了内存中的Page Cache里面，第二次读写的时候，由于发现文件内容已经在内存中了，就直接从内存中读取了，这显然比从硬盘读取快很多。
Page Cache的机制是很复杂的，那我们可不可以不用Page Cache呢？
答案当然是可以的，我们可以在应用层实现自己的类似这种的Cache机制，比如MySQL的Buffer Pool，我们也可以在使用open打开文件时指定为Direct I/O来绕开Page Cache，所以说是否使用Page Cache还是由应用程序自己决定，Linux内核只是提供了这种机制，并非要求我们强制使用。
Linux中Page Cache含义的变化 在 Linux 的实现中，文件 Cache 分为两个层面，一是 Page Cache，另一个是 Buffer Cache（块缓存）。page cache用于缓存文件的页数据，大小通常为4K；Buffer cache用于缓存块设备（如磁盘）的块数据，大小通常为1K。
在Linux2.4版本的内核之前，page cache和buffer cache是完全分离的。但是块设备大多数是磁盘，磁盘上的数据又大多通过文件系统来组织，这种设计导致很多数据被缓存了两次，浪费内存空间。
所以在2.4版本内核之后，两块内存近似融合在了一起，如果一个文件的页加载到了page cache，那么buffer cache只需要维护块指向页的指针。
在2.6版本内核中，page cache和buffer cache进一步结合。每一个 Page Cache 包含若干 Buffer Cache。将文件一页一页缓存到page cache中，buffer cache里面的指针指向磁盘block。
2.6内核中的buffer cache和page cache在处理上是保持一致的，但是存在概念上的差别，page cache是针对文件的cache，buffer是针对磁盘块数据的cache，仅此而已。
Page Cache大小的计算 通过命令cat /proc/meminfo可以看到Linux内存管理统计相关的各项数据：
Page Cache的大小有如下计算公式：
Page Cache = Buffers + Cached + SwapCached = Active(file) + Inactive(file) + Shmem + SwapCached 先对等号左边的字段做一个说明：
Buffers 是对原始磁盘块的临时存储，也就是用来缓存磁盘的数据，一般不会特别大（20MB 左右），Buffers 既可以用作“将要写入磁盘数据的缓存”，也可以用作“从磁盘读取数据的缓存”。
Cached 是从磁盘读取文件的内存页缓存，但是不包括SwapCached，也就是用来缓存从文件读取的数据。Cached 既可以用作“从文件读取数据的页缓存”，也可以用作“写文件的页缓存”。
SwapCached 是在打开了 Swap 分区后，把 Inactive(anon)+Active(anon) 这两项里的匿 名页给交换到磁盘（swap out），然后再读入到内存（swap in）后分配的内存。由于读入到 内存后原来的 Swap File 还在，所以 SwapCached 也可以认为是 File-backed page，即属 于 Page Cache。这样做的目的也是为了减少 I/O。注意，SwapCached 只在 Swap 分区打开的情况下才会有，我这个环境是关闭掉swap的，所以SwapCached为0。
再来看右边的字段：
在 Page Cache 中，Active(file)+Inactive(file) 是 File-backed page（与文件对应的内存 页），是最需要关注的部分。因为我们平时用的 mmap() 内存映射方式和 buffered I/O 来消 耗的内存就属于这部分。Active和Inactive的区别在于内存空间中是否包含最近被使用过的数据。当物理内存不足，不得不释放正在使用的内存空间时，会优先释放Inactive的内存空间。Linux内核中使用LRU表来分别记录对应的这两类文件内存页。
Page Cache 中的 Shmem 是指匿名共享映射这种方式分配的内存 （free 命令中 shared 这一项），比如 tmpfs（临时文件系统）。
free命令看到的buff/cache又是什么 Page Cache的概念很容易跟free命令看到的buff/cache混淆，所以这里我们有必要区分一下。
free 命令中的 buff/cache 是由 Buffers、Cached 和 SReclaimable 这三项组成的，它强调的是内存的可回收性，也就是说，可以被回收的内存会 统计在这一项。它只是free命令为了统计内存可回收性人为将这三个值进行统计到一起的。
buff/cache的大小还是来源于/proc/meminfo中看到的内存统计信息，计算公式如下：
buff/cache = Buffers + Cached + SReclaimable Buffers和Cached的含义前面已经讲过了，我们来看看SReclaimable：
SReclaimable 是 Slab 的一部分，是可以被回收的，例如缓存，linux内核使用 Slab 机制，管理文件系统的目录项和索引节点的缓存。Slab 包括两部分，其中的可回收部分，是指可以被回收的内核内存，包括目录项（dentry） 和索引节点（ inode ）的缓存等，用 SReclaimable 记录；而不可回收部分，用 SUnreclaim 记录。
Linux 文件系统为每个文件都分配两个数据结构，索引节点（index node）和目录项（directory entry）。它们主要用来记录文件的元信息和目录结构。
索引节点，简称为 inode，用来记录文件的元数据，比如 inode 编号、文件大小、访问权限、修改日期、数据的位置等。索引节点和文件一一对应，它跟文件内容一样，都会被持久化存储到磁盘中。所以记住，索引节点同样占用磁盘空间。
目录项，简称为 dentry，用来记录文件的名字、索引节点指针以及与其他目录项的关联关系。多个关联的目录项，就构成了文件系统的目录结构。不过，不同于索引节点，目录项是由内核维护的一个内存数据结构，所以通常也被叫做目录项缓存。
换句话说，索引节点是每个文件的唯一标志，而目录项维护的正是文件系统的树状结构。目录项和索引节点的关系是多对一，你可以简单理解为，一个文件可以有多个别名。举个例子，通过硬链接为文件创建的别名，就会对应不同的目录项，不过这些目录项本质上还是链接同一个文件，所以，它们的索引节点相同。
目录项本身就是一个内存缓存，而索引节点则是存储在磁盘中的数据。在前面的 Buffers 和 Cached 原理中，我们提到过，为了协调慢速磁盘与快速内存和 CPU 的性能差异，文件内容会缓存到页缓存 Cache 中。其实，这些索引节点也会缓存到内存中，加速文件的访问。
可以通过下面这张图，理解一下目录项、索引节点以及文件数据的关系：
总结 内存页包括文件页和匿名页，内核缓存的磁盘数据（Buffer）和内核缓存的文件数据（Cache）都叫作文件页，包括page cache、slab中的dcache、icache、用户进程的可执行程序的代码段。
匿名页包括进程使用各种api（malloc,mmap,brk/sbrk）申请到的物理内存(这些api通常只是申请虚拟地址，真实的页分配发生在page fault中)，包括堆、栈，进程间通信中的共享内存，bss段，数据段，tmpfs的页。
文件页和匿名页两个内存的区别在于，物理内存的内容是否与物理磁盘上的文件相关联，文件页与物理磁盘的文件是有关联的，而匿名页没有。
可以看出来，Page Cache的大小等于内核磁盘数据和文件数据的缓存与匿名页通过Swap机制交换出去的内存大小之和，也等于活跃文件页、未活跃文件页、匿名共享映射内存与匿名页通过Swap机制交换出去的内存大小之和。
而free命令看到的buff/cache等于内核磁盘数据和文件数据的缓存与 Slab 机制中文件系统的目录项和索引节点的缓存的可回收部分之和，指的是可直接回收的内存大小。
所以也可以看出来，文件页的缓存，在内存不足时是可以直接回收的（或者是脏页先会写到磁盘再回收），而匿名页是程序动态申请的内存，是不能直接回收的，即使是匿名页通过Swap机制换出的内存，以后也是得再从磁盘换入的。
另外，对于上面Page Cache和buff/cache的计算公式，我们需要注意一下，在做比较的过程中，一定要考虑到这些数据是动态变化的，而且执行 命令本身也会带来内存开销，所以这个等式未必会严格相等。
]]></content>
  </entry>
  
  <entry>
    <title>Intel下代至强缓存暴增至448MB</title>
    <url>/post/datacenter/intel-next-xeon-Emerald-Rapids.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>EPYC</tag>
      <tag>Bergamo</tag>
      <tag>9754</tag>
    </tags>
    <content type="html"><![CDATA[Intel已经宣布，将在12月14日正式发布第五代可扩展至强Emerald Rapids，和酷睿Ultra同一天。
它虽然只是Sapphire Rapids四代至强的升级版，不如明年Intel 3工艺的Granite Rapids、Sierra Forest变化那么大(后者288个小核)，但升级亮点依然不少。
YuuKi_AnS放出了高端型号至强铂金8580的软件识别截图，证实为双芯片整合，60核心120线程，二级缓存每核心2MB、总计120MB，三级缓存多达300MB，合计420MB，比现在多了2.6倍。
更关键的是，向上还有64核心旗舰，三级缓存增加到320MB，再加上128MB二级缓存，合计就是448MB，比现在增加足足3倍。
当然，AMD EPYC那边更猛，96核心旗舰EPYC 9654 96MB二级缓存、384MB三级缓存，合计480MB，3D缓存加持的EPYC 9684X也又堆叠了768MB，合计1248MB。
AMD Zen5如果能把二级缓存翻倍到每核心2MB，总缓存量又是一次突飞猛进。
泄露消息显示，五代至强还会提升DDR5内存支持的频率，优化电源模式，集成Intel加速器引擎以提升能效，总体能效提升可达17％。
回到桌面上，Intel最初计划在Meteor Lake这一代产品上更换新的接口LGA1851，但因为新的Intel 4工艺不够成熟，性能上不去，所以只能用于主流和轻薄笔记本，而高端游戏本、桌面都由13代酷睿的升级版14代酷睿来撑场面。
事实上，Meteor Lake-S桌面版一度曾经做出样品，但最终被砍掉。
Meteor Lake-S桌面版样品
就在酷睿Ultra架构技术已经官宣、将于12月14日正式发布的时刻，Intel执行副总裁、客户端计算事业部总经理Michelle Johnston Holthaus却给出了惊人的说法。
他确认，Meteor Lake确实会有桌面版，将在2024年发布，架构和移动版是完全一致的！
从目前的情况看，Meteor Lake不可能突然变得足够高性能，达到i9-13900K这样的性能，毕竟已经可以基本确认，i9-14900K等首批六款K/KF型号将在10月17日发布，主流和低功耗版本的型号也已经流出。
所谓的Meteor Lake桌面版，极有可能类似当年Boardwell五代酷睿唯二的桌面版i7-5775C、i5-5675C，本质上仍然是移动版本，只不过改成了桌面封装接口，当然不可能是下一代LGA1851，而肯定是现在的LGA1700。
理论上，这样的产品型号不会多，也算是Meteor Lake最后的倔强了，但是也不排除完全可以满足65W主流市场需求，毕竟6+8核心用于i5、i3系列是足够了。
]]></content>
  </entry>
  
  <entry>
    <title>一个简单的MCU内存管理模块(附源码)</title>
    <url>/post/mcu/a-simple-mcu-memory-management-module.html</url>
    <categories><category>MCU</category>
    </categories>
    <tags>
      <tag>STM32</tag>
      <tag>Memory Management</tag>
    </tags>
    <content type="html"><![CDATA[现在非常多的的MCU性能都还不错，同时用户也会去扩展一些外部RAM，那么如何高效便捷的管理这些内存是一个重要话题。
今天给大家分享一份源码：基于无操作系统的STM32单片机开发，功能强大，可申请到地址空间连续的不同大小的内存空间，且用户接口简单，使用方便。
源码说明 源码包含memory.h 和 memory.c 两个文件（嵌入式C/C++代码的“标配”），其源码中包含重要的注释。
 memory.h文件 ：包含结构体等定义，函数API申明等； memory.c文件 ：是实现内存管理相关API函数的原型。  头文件memory.h 头文件是相关的定义和申请：
#ifndef __MEMORY_H__ #define __MEMORY_H__  #include &#34;stdio.h&#34;#include &#34;string.h&#34;#include &#34;includes.h&#34;//用户使用 typedef struct { void *addr; //申请到的内存的起始地址  uint32_t size; //申请到的内存的大小，按照块大小分配，大于等于申请大小  uint16_t tb; //申请表序号，申请内存时分配，释放内存时使用，用户不使用 }DMEM; //若返回空，则申请失败 DMEM *DynMemGet(uint32_t size); void DynMemPut(DMEM *pDmem); #endif //__MEMORY_H__ 这里的代码比较简单，也是常规的写法，重点是要理解结构体成员的含义。
源文件memory.c 源文件主要就是实现内存管理的函数，源码比较多，这里才分为三部分。
相关的定义 #include &#34;memory.h&#34; #define DMEM_BLOCK_SIZE 256 //内存块大小为256字节 #define DMEM_BLOCK_NUM 20 //内存块个数为20个 #define DMEM_TOTAL_SIZE (DMEM_BLOCK_SIZE*DMEM_BLOCK_NUM) //内存总大小  static uint8_t DMEMORY[DMEM_TOTAL_SIZE]; static DMEM_STATE DMEMS = {0}; typedef enum { DMEM_FREE = 0, DMEM_USED = 1, }DMEM_USED_ITEM; typedef struct { DMEM_USED_ITEM used; //使用状态  uint16_t blk_s; //起始块序号  uint16_t blk_num; //块个数 }DMEM_APPLY; typedef struct { DMEM_USED_ITEM tb_blk[DMEM_BLOCK_NUM]; DMEM tb_user[DMEM_BLOCK_NUM]; //用户申请内存信息  DMEM_APPLY tb_apply[DMEM_BLOCK_NUM]; //系统分配内存信息  uint16_t apply_num; //内存申请表占用数目  uint16_t blk_num; //内存块占用数目 }DMEM_STATE; 内存分配函数DynMemGet DMEM *DynMemGet(uint32_t size) { uint16_t loop = 0; uint16_t find = 0; uint16_t blk_num_want = 0; DMEM * user = NULL; DMEM_APPLY *apply = NULL; //申请内存大小不能为0  if(size == 0) { return NULL; } //申请内存不可超过总内存大小  if(size &gt; DMEM_TOTAL_SIZE) { return NULL; } //申请内存不可超过剩余内存大小  if(size &gt; (DMEM_BLOCK_NUM - DMEMS.blk_num) * DMEM_BLOCK_SIZE) { return NULL; } //申请表必须有空余  if(DMEMS.apply_num &gt;= DMEM_BLOCK_NUM) { return NULL; } //计算所需连续块的个数  blk_num_want = (size + DMEM_BLOCK_SIZE - 1) / DMEM_BLOCK_SIZE; //寻找申请表  for(loop = 0; loop &lt; DMEM_BLOCK_NUM; loop++) { if(DMEMS.tb_apply[loop].used == DMEM_FREE) { apply = &amp;DMEMS.tb_apply[loop]; //申请表已找到  user = &amp;DMEMS.tb_user[loop]; //用户表对应找到  user-&gt;tb = loop; //申请表编号记录  user-&gt;size = blk_num_want * DMEM_BLOCK_SIZE; //分配大小计算  break; } } //没有找到可用申请表，理论上是不会出现此现象的，申请表剩余已在上面校验  if(loop == DMEM_BLOCK_NUM) { return NULL; } //寻找连续内存块  for(loop = 0; loop &lt; DMEM_BLOCK_NUM; loop++) { if(DMEMS.tb_blk[loop] == DMEM_FREE) {//找到第一个空闲内存块  for(find = 1; (find &lt; blk_num_want) &amp;&amp; (loop + find &lt; DMEM_BLOCK_NUM); find ++) {//找到下一个空闲内存块  if(DMEMS.tb_blk[loop + find] != DMEM_FREE) {//发现已使用内存块  break; } } if(find &gt;= blk_num_want) {//寻找到的空闲内存块数目已经够用  user-&gt;addr = DMEMORY + loop * DMEM_BLOCK_SIZE; //计算申请到的内存的地址  apply-&gt;blk_s = loop; //记录申请到的内存块首序号  apply-&gt;blk_num = blk_num_want; //记录申请到的内存块数目  for(find = 0 ; find &lt; apply-&gt;blk_num; find++) { DMEMS.tb_blk[loop + find] = DMEM_USED; } apply-&gt;used = DMEM_USED; //标记申请表已使用  DMEMS.apply_num += 1; DMEMS.blk_num += blk_num_want; return user; } else {//寻找到的空闲内存块不够用，从下一个开始找  loop += find; } } } //搜索整个内存块，未找到大小适合的空间  return NULL; } 内存释放函数DynMemPut void DynMemPut(DMEM *user) { uint16_t loop = 0; //若参数为空，直接返回  if(NULL == user) { return; } //释放内存空间  for(loop = DMEMS.tb_apply[user-&gt;tb].blk_s; loop &lt; DMEMS.tb_apply[user-&gt;tb].blk_s + DMEMS.tb_apply[user-&gt;tb].blk_num; loop++) { DMEMS.tb_blk[loop] = DMEM_FREE; DMEMS.blk_num -= 1; } //释放申请表  DMEMS.tb_apply[user-&gt;tb].used = DMEM_FREE; DMEMS.apply_num -= 1; } 代码中包含注释，注释描述的比较清楚，也比较容易理解。
]]></content>
  </entry>
  
  <entry>
    <title>STM32之GPIO点亮LED</title>
    <url>/post/mcu/STM32-gpio-led.html</url>
    <categories><category>MCU</category>
    </categories>
    <tags>
      <tag>STM32</tag>
      <tag>GPIO</tag>
      <tag>LED</tag>
    </tags>
    <content type="html"><![CDATA[我们在基础部分讲了有关GPIO的方面，从这章开始我们进入模块的讲解，从最开始的LED灯到各种传感器模块进行。专栏预计25个章节。后续可能会不定时的增加。
本专栏芯片为STM32F429
对于工程的移植和新建这里不做讲解，对工程建立不懂得，请参考其他博客或者某些教程。
本章使用到的是GPIO的输出功能，GPIO的内容这里不在说明，不理解或者其他原因请参看: https://www.vxbus.com  
硬件设计 本实验以1个LED灯为例，本篇文章主要目的是理解GPIO输出。
LED的正极通过1KΩ与3.3v连接，LED的阴极以单片机的GPIOC的13号引脚相连接。根据电路图，当PC13位输出低电平时，LED灯亮，输出为高时LED熄灭（亮和灭就是两端的电压差，当LED两端同时都是高电平时，没有电压差）。
注：具体连接引脚请根据自己开发板上的电路原理图进行连接。
软件设计 编程步骤 使能GPIO时钟（也就是RCC，这步是非常重要的。具体在那个总线上，请参考数据手册，本专栏芯片为STM32F429）
设置对应于片上外设使用的GPIO工作模式
在应用程序中读取引脚状态、控制引脚输出状态或使用复用功能完成特定功能。
编程要点 使能GPIO时钟。调用函数RCC_AHB1PeriphClockCmd()。不同的外设调用的时钟使能函数可能不一样。
初始化GPIO模式。调用函数GPIO_Init()。
操作GPIO，设置引脚输出状态。调用函数GPIO_SetBits();或GPIO_ResetBits()或GPIO_ToggleBits()。
代码实现 static void LED_Config(void) { GPIO_InitTypeDef GPIO_InitStructure; //GPIO_InitStructure用于存放GPIO的参数  /*开启LED相关的GPIO外设时钟*/ 第一步 RCC_AHB1PeriphClockCmd (RCC_AHB1Periph_GPIOC, ENABLE); //使能GPIOC的时钟  /*选择要控制的GPIO引脚*/ GPIO_InitStructure.GPIO_Pin = GPIO_Pin_13; //设置引脚  *设置引脚模式为输出*/ GPIO_InitStructure.GPIO_Mode = GPIO_Mode_OUT; //设置模式  /*设置引脚速率为2MHz */ GPIO_InitStructure.GPIO_Speed = GPIO_Speed_2MHz; //设置I/O输出速度  /*设置引脚的输出类型为推挽输出*/ GPIO_InitStructure.GPIO_OType = GPIO_OType_PP; //设置输出类型  /*设置引脚为上拉模式*/ GPIO_InitStructure.GPIO_PuPd = GPIO_PuPd_UP; //设置上拉/下拉模式  /*调用库函数，使用上面配置的GPIO_InitStructure初始化GPIO*/ 第二步 GPIO_Init(GPIOC, &amp;GPIO_InitStructure); //根据参数初始化LED的GPIO  GPIO_WriteBit(GPIOC, GPIO_Pin_13, Bit_SET); //将LED默认状态设置为熄灭 } void InitLED(void) { LED_Config(); //配置LED的GPIO } * 函数名称：Contl_lLED * 函数功能：控制LED亮灭 * 输入参数：mode:1-点亮，0-熄灭 * 输出参数：void * 返 回 值：void * 创建日期： *********************************************************************************************************/ void Contl_lLED(u8 mode) { if(mode) { GPIO_WriteBit(GPIOC, GPIO_Pin_13, Bit_RESET); //点亮LED  } else { GPIO_WriteBit(GPIOC, GPIO_Pin_13, Bit_SET); //熄灭LED  } } void LED_ON_OF(u16 cnt) { static u16 time; //time  time++; //计数器的计数值加1  if(time &gt;= cnt) //计数器的计数值大于cnt  { time = 0; //重置计数器的计数值为0  //LED状态取反，实现LED闪烁  GPIO_WriteBit(GPIOC, GPIO_Pin_13, (BitAction)(1 - GPIO_ReadOutputDataBit(GPIOC, GPIO_Pin_13))); } } int main(void) { InitLED(); LED_ON_OF(300); } ]]></content>
  </entry>
  
  <entry>
    <title>简单分析STM32和51的区别</title>
    <url>/post/mcu/difference-between-smt32-and-c51.html</url>
    <categories><category>MCU</category>
    </categories>
    <tags>
      <tag>SMT32</tag>
      <tag>C51</tag>
    </tags>
    <content type="html"><![CDATA[分享本文，简单分析STM32与51单片机的区别与取舍之处。
单片微型计算机简称单片机，简单来说就是集CPU（运算、控制）、RAM（数据存储-内存）、ROM（程序存储）、输入输出设备（串口、并口等）和中断系统处于同一芯片的器件，在我们自己的个人电脑中，CPU、RAM、ROM、I/O这些都是单独的芯片，然后这些芯片被安装在一个主板上，这样就构成了我们的PC主板，进而组装成电脑，而单片机只是将这所有的集中在了一个芯片上而已。
51单片机 应用最广泛的8位单片机当然也是初学者们最容易上手学习的单片机，最早由Intel推出，由于其典型的结构和完善的总线专用寄存器的集中管理，众多的逻辑位操作功能及面向控制的丰富的指令系统，堪称为一代“经典”，为以后的其它单片机的发展奠定了基础。
51单片机特性 51单片机之所以成为经典，成为易上手的单片机主要有以下特点：
从内部的硬件到软件有一套完整的按位操作系统，称作位处理器，处理对象不是字或字节而是位。不但能对片内某些特殊功能寄存器的某位进行处理，如传送、置位、清零、测试等，还能进行位的逻辑运算，其功能十分完备，使用起来得心应手。
同时在片内RAM区间还特别开辟了一个双重功能的地址区间，使用极为灵活，这一功能无疑给使用者提供了极大的方便。
乘法和除法指令，这给编程也带来了便利。很多的八位单片机都不具备乘法功能，作乘法时还得编上一段子程序调用，十分不便。
51单片机缺点  AD、EEPROM等功能需要靠扩展，增加了硬件和软件负担 虽然I/O脚使用简单，但高电平时无输出能力，这也是51系列单片机的最大软肋 运行速度过慢，特别是双数据指针，如能改进能给编程带来很大的便利 51保护能力很差，很容易烧坏芯片  51单片机应用范围 目前在教学场合和对性能要求不高的场合大量被采用，使用最多的器件是8051、80C51。
STM32单片 由ST厂商推出的STM32系列单片机，行业的朋友都知道，这是一款性价比超高的系列单片机，应该没有之一，功能及其强大。
其基于专为要求高性能、低成本、低功耗的嵌入式应用专门设计的ARM Cortex-M内核，同时具有一流的外设：1μs的双12位ADC，4兆位/秒的UART，18兆位/秒的SPI等等，在功耗和集成度方面也有不俗的表现，当然和MSP430的功耗比起来是稍微逊色的一些，但这并不影响工程师们对它的热捧程度。
STM32单片机特性 由STM32简单的结构和易用的工具再配合其强大的功能在行业中赫赫有名，其强大的功能主要表现在：
 内核：ARM32位Cortex-M3CPU，最高工作频率72MHz，1.25DMIPS/MHz，单周期乘法和硬件除法 存储器：片上集成32-512KB的Flash存储器。6-64KB的SRAM存储器 时钟、复位和电源管理：2.0-3.6V的电源供电和I/O接口的驱动电压。POR、PDR和可编程的电压探测器（PVD）。4-16MHz的晶振。内嵌出厂前调校的8MHz RC振荡电路。内部40 kHz的RC振荡电路。用于CPU时钟的PLL。带校准用于RTC的32kHz的晶振 调试模式：串行调试（SWD）和JTAG接口。最多高达112个的快速I/O端口、最多多达11个定时器、最多多达13个通信接口。  STM32使用最多的器件：  STM32F103系列 STM32 L1系列 STM32W系列  51单片机和STM32单片机的区别 51单片机是对所有兼容Intel8031指令系统的单片机的统称，这一系列的单片机的始祖是Intel的8031单片机，后来随着flash ROM技术的发展，8031单片机取得了长足的进展成为了应用最广泛的8bit单片机之一，他的代表型号就是ATMEL公司的AT89系列。
STM32单片机则是ST（意法半导体）公司使用arm公司的cortex-M3为核心生产的32bit系列的单片机，他的内部资源（寄存器和外设功能）较8051、AVR和PIC都要多的多，基本上接近于计算机的CPU了，适用于手机、路由器等等。
]]></content>
  </entry>
  
  <entry>
    <title>CPU架构 C-states</title>
    <url>/post/datacenter/cpu-architecture-c-state.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>CPU</tag>
      <tag>C State</tag>
    </tags>
    <content type="html"><![CDATA[Power and Performance是使用CPU当中一个重要的课题，不管是芯片厂商，系统厂商，还是互联网大厂都会有专门的人员来做这方面的调优。
我们希望CPU可以做到，静若处子，动若狡兔。
当我需要你的时候，你就疯狂的运行，追求极致的性能，当我不需要你的时候，请你安静地休息，睡得越悄无声息越好。在之前的文章中，我们介绍了CPU的几种电源状态和性能状态，今天我们就来讲一讲其中的C-states。
C-states介绍 在CPU中，C-states（C状态）为软件提供了请求CPU进入低功耗状态的能力，C-states就是通过关闭core或其他逻辑单元来实现的。
如果CPU支持同时多线程（SMT：simultaneous multithreading ），那么单个CPU core可以支持多个软件线程。每个硬件线程都有自己的状态，并有机会请求不同的C-states。这些被称为线程C-states，以TCx表示（其中x是一个整数）。为了让一个core进入core C-state（以CCx表示），该core上的每个线程必须请求该状态或更深层次的状态。例如，对于支持两个线程的core，如果任何一个线程处于TC0状态，那么该core必须处于CC0状态。如果一个线程处于TC3状态，另一个线程处于TC6状态，那么该core将被允许进入CC3状态。线程C-states本身仅能节省少量的功耗，而core C-states可以显著地节省功耗。
同样的道理，还有package C-states，当一个CPU的所有core进入深度C-state时，那么整个CPU的package就可以进入这些状态。这些状态通常表示为PCx或PkgCx（其中x是一个整数）。通常来说，package C-states的编号与该package中的core状态相关联，不过也不一定是这样。例如，某些服务器处理器上的PC2状态是在该package中的所有core都处于CC3或CC6状态，但还会有一些条件限制使得CPU只能停留在PC2，而不是更深的休眠状态。
在实际应用中，C-states的运用对于节能和功耗管理至关重要。通过充分利用C-states，我们可以实现对CPU的动态功耗管理，根据实际使用情况灵活调整CPU的性能与功耗平衡。对于移动设备和笔记本电脑来说，C-states可以显著延长电池续航时间，提供更好的用户体验；对于服务器和数据中心来说，C-states可以降低能源消耗，提高整体系统效率。
然而，C-states的管理也存在一些挑战。合理的C-states策略需要综合考虑性能需求、功耗控制和响应速度。过于激进的C-states配置可能导致性能下降，而过于保守的配置则无法发挥节能潜力。
Thread C-states 软件可以在线程级别请求进入C-states。当一个线程进入线程C-state时，如果没有引发core C-state，那么通常不会产生可见的功耗节省效果，或者节省的功耗非常有限。在支持SMT的CPU上，线程C-states实际上是进入core C-states的一个中间步骤。而在不支持SMT的CPU上，线程C-states和core C-states实际上是相同的。
通过将线程级别的C-states和core级别的C-states结合起来管理，我们可以更精确地控制CPU的功耗和性能。在一些高性能应用场景中，我们可以让某些线程进入较浅的C-states，以保持高性能，而其他较空闲的线程则可以进入更深的C-states，以实现节能效果。这种精细的管理方式可以使得CPU在兼顾性能的同时，最大限度地降低功耗。
Core C-States 在C-states中，Core C-states的作用是确定core是处于开启还是关闭状态。在正常执行中，core通常处于C0状态，即活跃状态。当软件（通常是操作系统）指示逻辑处理器进入空闲状态时，它将进入一个C-state。
各种唤醒事件可能触发core重新开始执行代码（常见的例子是中断和定时器）。软件向CPU提供关于应该进入的状态的提示。MWAIT指令用于告诉CPU进入C-state，并包含有关所需状态的参数。然而，CPU的电源管理子系统有权执行其认为最佳的状态（这称为C-state降级 C-state demotion），也就是说将在外，君命有所不受。操作系统可以建议CPU休息，但是CPU表示我还可以继续肝！
Intel的不同代的CPU的C-state定义并没有硬性规定，但在跨多个产品系列中，这些定义基本保持一致。
]]></content>
  </entry>
  
  <entry>
    <title>SR-IOV技术简介</title>
    <url>/post/datacenter/introduction-of-SR-IOV-technology.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>SR-IO</tag>
    </tags>
    <content type="html"><![CDATA[虚拟化场景都会听到SR-IOV，究竟什么是SR-IOV呢，下面资料可参考，具体实现步骤请再硬件满足的情况下自行尝试。
基础概念 SR-IOV全称为Single Root I/O Virtualization（单根输入/输出虚拟化），是一种硬件加速的虚拟化技术，它允许多个虚拟机同时访问物理设备，从而提高虚拟机的性能和可靠性。SR-IOV技术是通过将单个物理设备划分为多个虚拟设备或虚拟端口（即一张物理网卡虚拟化成多个虚拟网卡给虚拟机(VM)用），为每个虚拟机提供独立的物理通道。这样，每个虚拟机可以直接访问独立的虚拟设备或虚拟端口，而无需在主机操作系统和虚拟化层之间进行上下文切换。
SR-IOV中有两个PCIe的function types：
  物理功能 (Physical Function, PF)：用于支持 SR-IOV 功能的 PCI 功能，每个PF都可以被物理主机发现和管理。PF 包含 SR-IOV 功能结构，用于管理 SR-IOV 功能。PF 拥有完全配置资源，可以用于配置或控制 PCIe 设备。进一步讲，借助物理主机上的PF驱动可以直接访问PF所有资源，并对所有VF并进行配置，比如：设置VF数量，并对其进行全局启动或停止。
  虚拟功能 (Virtual Function, VF)：PF虚拟出来的功能，VF 是一种轻量级 PCIe 功能，仅允许拥有用于其自身行为的配置资源。一个或者多个VF共享一个PF，其驱动装在虚拟机上，当VF分配给虚拟机以后，虚拟机就能像使用普通PCIe设备一样初始化和配置VF。如果PF代表的是一张物理网卡，那么VF则是一个虚拟机可以看见和使用的虚拟网卡。
  每个 SR-IOV 设备都可有一个物理功能 (Physical Function, PF)，并且每个 PF 最多可有 64,000 个与其关联的虚拟功能 (Virtual Function, VF)。PF 可以通过寄存器创建 VF，这些寄存器设计有专用于此目的的属性。只要在 PF 中启用了 SR-IOV，就可以通过 PF 的总线、设备和功能编号（路由 ID）访问各个 VF 的 PCI 配置空间。每个 VF 都具有一个 PCI 内存空间，用于映射其寄存器集。VF 设备驱动程序对寄存器集进行操作以启用其功能，并且显示为实际存在的 PCI 设备。创建 VF 后，可以直接将其指定给 IO 或各个应用程序（如裸机平台上的 Oracle Solaris Zones）。此功能使得虚拟功能可以共享物理设备，并在没有 CPU 和虚拟机管理程序软件开销的情况下执行 I/O，即可跳过中间的虚拟化堆栈（即VMM层），以达到近乎于纯物理环境的性能。
SR-IOV技术的优缺点及应用场景 SR-IOV的缺点：   高性能：SR-IOV技术可以降低虚拟机与物理设备之间的通信延迟，提高虚拟机的性能和响应速度。
  简化管理：SR-IOV技术可以使虚拟机直接访问物理设备，从而简化了虚拟化环境的管理。
  提高可靠性：SR-IOV技术可以将物理设备的错误隔离到虚拟机级别，从而提高了系统的可靠性。
  提高安全性：SR-IOV技术将物理设备划分成多个虚拟设备，可以使不同虚拟机之间的数据传输更加安全。
  SR-IOV的缺点： 使用了VFs的虚拟机不能在线迁移。
应用场景：SR-IOV技术在云计算、虚拟化、服务器应用等领域得到了广泛应用。在云计算数据中心中，SR-IOV技术可以提高虚拟机的网络和存储性能，允许灵活的分配和管理资源，从而降低了资源消耗和成本。在服务器应用中，SR-IOV技术可以支持高密度虚拟化、高性能计算和大规模分布式存储。
如何实现SR-IOV功能？ 前提首先SR-IOV需要软硬都支持才行：
  一台支持SR-IOV的主机或服务器（主板），SR-IOV功能在BIOS中已开启；
  一块支持SR-IOV且能安装于上述设备的网卡。
  步骤如下：确认测试的设备在BIOS打开VT-D及SRIOV支持选项；进入ESXi硬件配置页面，找到我们需要进行配置的SR-IOV网卡；两个网口，配置某一个网口为SR-IOV口。并设置虚拟VF的数量；保存配置之后，重新启动ESXi，就可以看到硬件上出现对应的虚拟网卡设备。这样子我们就完成了SR-IOV配置。
]]></content>
  </entry>
  
  <entry>
    <title>2023Q2全球WLAN市场：思科、HPE、华为位列前三</title>
    <url>/post/datacenter/worldwide-top-5-enterprise-wlan-companies.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>WLAN</tag>
    </tags>
    <content type="html"><![CDATA[根据IDC全球季度无线局域网跟踪报告发布的结果，WLAN企业细分市场在2023年第二季度的收入同比增长43.3%，达到30亿美元；WLAN消费细分市场同比下降14.0%。
Wi-Fi 6 和 Wi-Fi 6E 作为 WLAN 行业的最新标准，将继续推动企业细分市场的增长。与此同时，Wi-Fi 6E 将 Wi-Fi 的使用扩展到 6 GHz 频谱频段，增长势头持续强劲，23 年第二季度，收入环比增长 51.5%。2023 年第二季度，Wi-Fi 6 占消费市场收入的 51.6%。
IDC 企业网络研究经理Brandon Butler表示：“推动 WLAN 市场增长的一个重要因素仍然是组件短缺和供应链中断的缓解，这使得供应商从完成积压的产品订单中获得收入。同时，新 Wi-Fi 标准的采用也增加了市场动力。”
企业 WLAN 市场在全球范围内普遍取得了强劲的业绩。2023 年第二季度，美国市场同比增长 79.5%，加拿大市场同比增长 79.1%，拉丁美洲市场同比增长 49.6%。西欧市场同比增长 29.7%，中欧和东欧市场同比增长 42.0%；在中东和非洲地区，市场收入增长了51.0%；亚太地区（不包括日本和中国），市场上涨36.8%；中国市场下降了 12.5%，日本市场上涨了 14.6%。
企业市场份额 2023 年第二季度，思科企业 WLAN 收入同比增长 65.6%，达到 13 亿美元。截至本季度末，该公司的市场份额为43.5%。
HPE Aruba Networking 收入同比增长 55.3%，该季度市场份额为 16.2%。
华为企业WLAN收入同比增长26.6%，2023年第二季度的市场份额达到7.5%。
Ubiquiti 企业 WLAN 收入同比增长了 4.4%，该季度市场份额为 5.8%。
CommScope 企业 WLAN 收入同比增长 72.2%，市场份额为 4.8%。
Juniper Networks 企业 WLAN 收入同比增长 39.3%，市场份额为3.7%。
]]></content>
  </entry>
  
  <entry>
    <title>866.5亿！全球以太网交换机市场Top 3 出炉</title>
    <url>/post/datacenter/worldwide-top-3-ethernet-switch-companies.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Network</tag>
      <tag>Ethernet</tag>
      <tag>Top 3</tag>
      <tag>Revenue</tag>
    </tags>
    <content type="html"><![CDATA[据IDC报告显示，2023年第二季度全球以太网交换机市场收入为118亿美元（约866.5亿人民币），同比增长 38.4%。2023 年第二季度，全球企业和服务提供商 (SP) 路由器市场总收入达到 46 亿美元，同比增长 9.4%。
以太网交换机市场 2023 年上半年以太网交换机市场与 2022 年上半年相比增长了 35.2%。市场增长的主要推动力仍然是供应链问题的缓解。随着组件可用性的提高，供应商能够通过履行积压的产品订单来增长收入。
这一趋势在以太网交换市场的非数据中心领域尤其重要，2023 年第二季度非数据中心的收入同比增长 52.5%，端口出货量增长 16.6%。第二季度数据中心部分市场收入同比增长 21.7%，端口出货量下降 2.4%。
在超大规模企业和云提供商构建数据中心网络容量的推动下，高速以太网交换机市场在数据中心部分继续保持强劲增长：
 2023 年第二季度200/400 GbE 交换机的市场收入同比增长 61.9%。 100GbE 交换机收入增长 18.5%。 25/50 GbE 收入同比增长 54.2%。 ODM（原始设备制造商）直销仍然是数据中心细分市场的重要组成部分，占数据中心细分市场收入的12.6%，与2022年第二季度相比增长12.2%。  通常部署在企业园区和分支机构的低速交换机也表现出了优势：
 1GbE 交换机的收入同比增长 53.1%。 10GbE 交换机同比增长 18.1%。 2.5/5GbE 交换机（也称为多千兆以太网交换机）收入同比增长 157.5%。  从地域角度来看，以太网交换机市场在全球大部分地区都出现了增长：
 美洲第二季度市场收入同比增长 54.3%，其中美国增长 51.8%，拉丁美洲增长 88.3%。 欧洲市场同比增长49.1%，其中中东欧增长60.8%，西欧增长44.0%。 中东和非洲地区收入同比增长62.4%。 亚太地区市场增长7.7%，其中中国市场同比下降7.8%，日本市场增长18.9%。  路由器市场 2023 年第二季度，包括通信服务提供商和云服务提供商在内的服务提供商部分占市场总收入的 77.5%，同比增长 14.8%。企业部分的收入下降 6.1%%。
从地域角度来看：
 美洲的综合服务提供商和企业路由器市场合计增长了10.3%。 亚太地区的市场同比增长了3.0%。 欧洲、中东和非洲 (EMEA) 地区市场同比增长18.3%。  厂商排名 思科2023年第二季度以太网交换机收入同比增长55.3%，市场份额达到47.2%。思科的服务提供商和企业路由器合并收入在本季度增长了 18.1%，市场份额达到 35.9%。
Arista Networks 的以太网交换机收入在 2023 年第二季度同比增长 42.6%，市场份额为 10.4%。
华为第二季度以太网交换机收入增长17.7%，市场份额为9.0%。服务提供商和企业路由器收入合计增长了 10.8%，市场份额为 33.3%。
HPE 的以太网交换机收入在第二季度增长了78.8%，市场份额为7.1%。
H3C 的以太网交换机收入第二季度同比下降 10.9%，市场份额为 4.1%。在服务提供商和企业路由综合市场中，H3C 的收入下降了 10.0%，市场份额为 2.0%。
瞻博网络的以太网交换机收入在第二季度同比增长 35.2%，市场份额为 2.9%。瞻博网络第二季度的路由收入同比增长 2.5%，市场份额为 10.3%。
]]></content>
  </entry>
  
  <entry>
    <title>降低29%能耗，AMD EPYC吊打同行</title>
    <url>/post/datacenter/amd-epyc-helps-reducing-power-consumption.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>EPYC</tag>
      <tag>Bergamo</tag>
      <tag>9754</tag>
    </tags>
    <content type="html"><![CDATA[在碳减排的今天，数据中心因其高能耗密度而受到严格审查。据 Energy Information, Policy &amp; Technology LLC 估计，数据中心消耗了全球约 1% 电力。美国能源部计算得出，数据中心耗电量占美国用电总量 2%，数据中心成为碳减排的主要对象。
开放数据中心委员会 ODCC 测算数据显示， 2020年中国数据中心能耗总量为 939 亿千瓦时， 碳排放量为 6464 万吨。 预计2030 年中国数据中心能耗总量将达到 3800 亿千瓦时左右， 碳排放增长率将超过 300%。
数据中心的能耗大户 服务器作为 IT 基础设施中最基本的算力设备， 在数据中心硬件设备耗电量中所占比重最高，随着多样化业务场景在数据中心出现，服务器需承担的计算量越来越大，其能耗和碳排放也在成倍增长，同时给数据中心的维护难度和支出成本带来压力。
据中国信通院统计， 2021 年全国数据中心服务器数量 1900 万台， 耗电量达 1100 亿千瓦时， 每台服务器一年的碳排放量约 2600KG。
Energy Innovation认为，对大多数公司来说，减少数据中心能耗的更实用方法，是降低数据中心两大耗电主力(服务器和散热)的用电量，其中每一项都占美国数据中心总用电量的 43%。
为此，整机柜技术、液冷技术、高密度技术等相关技术层出不穷，其终极目的，是如何在计算密集型工作负载和低能耗运行之间找到平衡， 如何在满足技术需求的基础上尽可能降低碳排放。
解决核心的CPU能耗问题 如果我们把视线回收到服务器层面来看，作为核心部件，CPU在控制服务器整体能耗中起到了至关重要的作用。一方面服务器能耗70%来自于CPU，另外服务器CPU的能耗将极大影响其他散热、制冷等辅助设备。研究发现，服务器CPU能耗每降低1W，由此带来自身及其他相应辅助设备的总能耗将降低2.84W！
这意味着对服务器CPU的持续优化，将更有效减少服务器以及整体数据中心的能耗及相关碳排放。
我们看到，在日益强调碳排放的今天，数据中心将更加“精打细算”，会更在意CPU的能效表现，即单位算力性能下的能耗/碳排放表现，或者说尽可能降低单位工作负载下的CPU功耗。
通常情况下，降低单位工作负载功耗的最好办法，是增加CPU的核心密度。CPU的核心密度越高，性能越强，其处理的单位芯片工作负载量也会大幅增加。
Moor Insights &amp; Strategy 数据中心副总裁兼首席分析师 Matt Kimball 表示，提高数据中心能源效率的首要方法是减少服务器占用空间。实现这一目标的最有效方法是提高处理器的效率，使用更多的内核并缩小组件之间的空间。“如果能够提高服务器中 CPU 的利用率，这将对数据中心的整体功耗产生巨大影响，”他说。
AMD EPYC CPU出类拔萃的能效表现 Moor Insights &amp; Strategy 数据中心副总裁兼首席分析师 Matt Kimball特别举例强调，与其他类似x86处理器相比，数据中心运行2000个由第四代AMD EPYC 9654处理器驱动的虚拟机，服务器数量同比减少35%；而每年因此降低的能源消耗则达到29%。[1]
ODCC的研究表明，CPU 性能越好，从服务器算力碳效、即单位算力性能的碳排放量来看会越低。 在服务器使用周期为 5 年的情况下， 单位算力性能得分的碳排放量在 20-60KG 之间， 其中 AMD 服务器单位算力性能得分的碳排放量相对更低， 几乎都低于 30KG。
以 SPEC 得分 8,000 为例， 替换 21 台基于 Intel 至强 Gold 6342 服务器，可使用 16 台 Intel 至强® Platinum 8380 服务器或 11 台 AMD EPYC 7763服务器，服务器台数最多可减少 10 台，其使用周期内的碳排放量最多减少 43%， 相当于 8100 多棵树一年吸收的碳排放量（数据来自：ODCC数据中心算力碳效白皮书）。
在体现能效的SPECpower_ssj®2008测试中，AMD EPYC 9654 的表现是Intel® Xeon® Platinum 8490H的1.8倍，能效基准测试数据表明，AMD EPYC处理器拥有比其他处理器更好的能效；AMD内部数据显示，在提供2000个虚拟机的场景下，仅需11台基于AMD EPYC 9654 处理器的系统，就可替换17台基于Intel Xeon Platinum 8490H处理器系统——服务器数量节省35%、能耗降低29%、投资节省46%！[2]
当AMD EPYC 97x4 （产品代号Bergamo）推出后，这款具有128个Zen 4c内核的处理器将提供更好的能效表现，并进一步降低占地面积和能源消耗等TCO费用——AMD内部测试表明，客户只用15台基于Bergamo处理器的服务器，即可替换38台基于其他品牌顶级CPU的服务器，数量减少了61%，每年功耗降低66%，年均碳排放量接近120吨。 [3]
]]></content>
  </entry>
  
  <entry>
    <title>英伟达再度释放AI“炸弹”</title>
    <url>/post/news/nvidia-launch-gh200-grace-hopper.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>AI</tag>
      <tag>NVIDIA</tag>
      <tag>GH200 Grace Hopper</tag>
    </tags>
    <content type="html"><![CDATA[近日，在计算机图形学顶会SIGGRAPH 2023现场，英伟达再度释放深夜“炸弹”，大模型专用芯片迎来升级版本。
英伟达在会上发布了新一代GH200 Grace Hopper平台，该平台依托于搭载全球首款搭载HBM3e处理器的新型Grace Hopper超级芯片——GH200，专为处理大语言模型、推荐系统、矢量数据库等全球最复杂的生成式AI工作负载而构建。
据悉，GH200芯片将成为世界上第一个配备HBM3e（High Bandwidth Memory 3e）内存的GPU芯片。
与当前一代产品相比，最新版本的GH200超级芯片内存容量增加了3.5倍，带宽增加了3倍；相比最热门的H100芯片，其内存增加1.7倍，传输频宽增加1.5倍。
在当前生成式AI不断激增的需求下，GH200超级芯片的推出，进一步吹响了AI算力之战的号角。
性能更高的GH200芯片 据介绍，GH200 Grace Hopper平台的HBM3e内存比当前HBM3快50%，可提供总计10TB/s的带宽。这使得新平台能够运行比上一版本大3.5倍的模型，同时凭借快3倍的内存带宽提升性能。
同时，该平台采用双配置，包括一个拥有144个Arm Neoverse内核、8 petaflops的AI性能和282GB最新HBM3e内存技术的单个服务器。
英伟达创始人兼首席执行官黄仁勋表示：“为了满足对生成式 AI不断激增的需求，数据中心需要能够满足特定需求的加速计算平台。全新GH200 Grace Hopper超级芯片平台以出色的内存技术和带宽，提高了吞吐量，在不影响性能的情况下可连接多GPU以整合性能，并且具有可以轻松部署到整个数据中心的服务器设计。”
据英伟达公布信息，新平台可以通过 NVIDIA NVLink™ 与其他超级芯片连接，使它们能够协同工作，从而部署当下大型生成式AI模型。这种高速、一致性技术使GPU可以完全访问CPU 内存，在双配置中可提供总计1.2TB的快速内存。
值得注意的是，新平台采用的新款超级芯片GH200与此前发布的H100相比，二者使用同样的GPU，但GH200将同时配备高达141G的内存和72核ARM中央处理器，每秒5TB带宽，内存增加了1.7倍，带宽增加了1.5倍。
新平台和芯片的加持，也让大模型训练的成本得到有效降低。黄仁勋表示，一台服务器可以同时装载两个GH200超级芯片，大型语言模型的推理成本将会大幅降低。
据介绍，投资800万美元Grace Hopper，就相当于8800个价值1亿美元的x86 GPU，意味着成本降低12倍，能耗降低20倍。
英伟达称，GH200已于5月全面投产，基于GH200 Grace Hopper平台的新系统将于2024年第二季度交付。
不过一个关键的问题是，英伟达没有透露超级芯片GH200的价格，这对计算成本高昂的大模型来说尤为重要，H100系列目前售价约为4万美元。
为什么内存对大模型重要 事实上，GH200超级芯片本身并不是一个新产品，而是今年5月在中国台北Computex展上发布的GH200芯片的更新版。
英伟达超大规模和高性能计算副总裁兼总经理伊恩·巴克（Ian Buck）表示：“我们对这款新的GH200感到非常兴奋。HBM3e不仅增加了GPU的容量和内存量，而且速度也更快。”
但为什么GPU内存这么重要？
这是因为随着支撑生成式人工智能应用程序的基础AI模型尺寸的增加，为了能够在不连接独立芯片和系统的情况下运行，大模型需要更大的内存量，以避免性能下降。
拥有更大的内存允许模型保留在单个GPU上，并且不需要多个系统或多个GPU来运行，而额外的内存只会提高 GPU的性能。
目前即使使用英伟达最顶级的H100芯片，有些模型也必须在其他GPU中“分解”模型才能运行。
据英伟达介绍，最新版本GH200配备141GB的HBM3e内存，旨在处理“世界上最复杂的生成式人工智能工作负载，涵盖大型语言模型、推荐系统和矢量数据库”。
对AI领域的影响 伟达的GH200超级芯片和DGX GH200超级计算机是AI领域的重大突破，它们为大规模生成式AI工作负载提供了前所未有的性能和内存空间，使得训练千亿甚至万亿参数的巨型模型成为可能。
这些模型可以在自然语言处理、计算机视觉、推荐系统、图形分析等领域实现更高的精度和效率，为人类解决更复杂的问题提供了强大的工具。
在多位AI从业者看来，当前大模型的训练需求过于迫切，对性能的要求也很高，而GPU的适配和生态转移都需要很长时间，因此目前大家都优先选择英伟达，和其他厂商的测试验证也在进行中。
一场新的算力之战已经拉开帷幕，如果说算力是一个江湖，那么此刻英伟达就是一名绝世高手。
它身怀加速计算的绝技，尤其在AI战场上一骑绝尘，似乎每一次都能精准地踏在浪潮的节奏上。从游戏PC市场、到深度学习的崛起、到云计算的普及、再到生成式AI的降临，英伟达的技术所向披靡。
回头看，英伟达早已超越了GPU本身的概念，AI成为最大的标签，算力的绝世武功撑起了新的万亿帝国。
2022年，英伟达推出了多款重磅产品，分别是基于全新Hopper架构的H100 GPU、CPU和GPU的合体Grace Hopper、两个CPU组合的Grace CPU Superchip，CPU的产品在2023年上市。
其中，设计GPU新架构Hopper时，英伟达增添了一个Transformer引擎，专门为Transformer算法做了硬件优化，加快AI计算的效率。
一位国内芯片从业者直言：“H100出来，其实就是一个新时代了，Grace-Hopper再一个组合，加上高配的互联，完全不给活路，英伟达赢家通吃，AMD、Intel继续苦追。”
同时他也表示：“目前国内一些企业还是在盯着CNN做优化，英伟达已经有Transformer引擎，然后AIGC火热，恰好能做支持。这个眼光，只能佩服他们的科学家们对这个领域深刻的认识。”
一位学术界人士也分析道：“从H100上，包括专用的Transformer引擎以及对FP8格式的支持，可以看到计算硬件在向应用定制的方向前进。Grace CPU说明了整合异构计算系统的重要性。单纯的加速器优化和设计已经不能够满足现在对于计算系统的算力和能效比的要求，需要各个部分的协同优化和设计。”
他还表示，Grace CPU通过提高通信带宽和在CPU和GPU之间建立一致（coherent）的内存模型来解决运算中的瓶颈，这也和学界（近存计算，存内计算）与业界（CXL，CCI等等系统互联协议）一直在关注的方向是一致的。
总而言之，在GPU和CPU的各种排列组合中，英伟达又将算力提升到了新高度。正如黄仁勋所言：“我们正在重新发明计算机，加速计算和人工智能标志着计算正在被重新定义。”
黄仁勋在采访中还提到，数据中心需要用的CPU越来越少，不再是传统上购买数百万个CPU，而是转而购买数百万个GPU。换言之，在他看来，AI算力江湖已经是GPU的主场。
英伟达的野心 事实上，随着ChatGPT引发AI大模型需求热潮，作为加速计算领导者，英伟达今年以来股价累计涨幅已超过210%，近三个月内涨幅就达56%，过去7年股价增长超40倍，目前市值冲破1.1万亿美元。
公开数据显示，英伟达占据全球80%以上的GPU服务器市场份额，同时拥有全球91.4%的企业GPU市场份额。
据投资者服务公司穆迪今年5月份发布的一份研究报告，英伟达在未来几个季度将实现“无与伦比”的收入增长，其数据中心业务的收入将超过竞争对手英特尔和AMD的总和。
但摩根士丹利策略分析师斯坦利（Edward Stanley）在最新报告中称，根据历史背景，英伟达的股价飙升处于“后期”阶段，摩根士丹利认为这标志着 AI 行业的“泡沫”。
GPU持续紧缺下，如今英伟达产品价格已同比上涨超30%，英伟达A800单卡现货近13万元一颗，eBay上H100售价高达4.5万美元。
同时，OpenAI的GPT-4大模型需要至少2.5万张英伟达A100 GPU芯片，而该公司目前至少已拥有1000万颗GPU芯片。
正如黄仁勋常说的，“你GPU买得越多，你越省钱”。主要原因是新的GPU产品能显著提升加速计算，比CPU性能更强、算力更大、功耗更低。
但英伟达的布局还不止于此。
一个现实问题是，高性能的算力也意味着高昂的价格。大模型训练成本动辄成千上百万美元，并不是所有公司都能承受。
而英伟达同时提出了云服务的解决方案NVIDIA AI foundations，黄仁勋表示要做“AI界的台积电”。台积电大大降低了芯片设计公司生产门槛，英伟达也要做代工厂的角色，通过和大模型厂商、云厂商合作提供高性价比的云服务。
在帮助下游企业降低大模型训练成本的同时，英伟达还在逐步参与到上游的产业链升级中。今年，英伟达牵手台积电、ASML、新思，发布了计算光刻库cuLitho。
计算光刻是在芯片设计和制造领域的关键步骤，也是最大的计算负载之一。计算光刻库的技术突破就在于，可以通过部署有大量GPU的DGX AI计算系统对计算光刻进行加速，使其达到原有的基于CPU的计算速度的几十倍，同时降低计算过程的总能耗。
这将有助于晶圆厂缩短原型周期时间、提高产量、减少碳排放，为2nm及更先进的工艺奠定基础，并为曲线掩模、高数值孔径极紫外、亚原子级光刻胶模型等新技术节点所需的新型解决方案和创新技术提供更多可能性。
在多位产业界人士看来，虽然短期内不会影响到下游的应用方面，但是这些上游的研发和升级将长期影响产业的发展，累积形成代际差。
“英伟达在GPU架构的迭代上，一直都有属于自己的发展路径，这几年的发展，也让英伟达跃居AI算力芯片领域的领导者，也因为领先，所以英伟达会思考如何做更多元的布局与行业内的深度合作，这样更能了解行业的需求，比方和台积电等合作便是很好的例子”，某芯片行业专家表示。
当然，英特尔和AMD都已经吹响反攻的号角。
7月，英特尔面向中国市场推出了AI芯片Habana Gaudi 2；6月，AMD推出AI芯片Instinct MI 300X，两者都直接对标英伟达100系列。
目前，在数据中心市场，英伟达和Intel、AMD形成三足鼎立之势。但随着GH200的正式发布，Grace CPU正式登台争角，最应该感到如芒在背的应该是Intel、AMD。虽说大家都知道GH200迟早发布，但等真正发布了，还是有所触动。
围绕着算力的权力游戏还将继续。
引用地址 英伟达再度释放AI“炸弹”  
]]></content>
  </entry>
  
  <entry>
    <title>哪个Linux发行版可以替代Ubuntu</title>
    <url>/post/linux/which-linux-distribution-could-be-alternative-of-ubuntu.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Ubuntu</tag>
      <tag>Fedora</tag>
      <tag>Mint</tag>
      <tag>Debian</tag>
    </tags>
    <content type="html"><![CDATA[在Linux的世界里，Ubuntu是广受欢迎的一个发行版，那么究竟还有哪些Linux的发行版可以和Ubuntu媲美的呢，今天就让我们一起来看看。
Linux Mint 自带音视频解码器，开箱即用，Ubuntu还要下载编解码器才能播放
主要利用免费和开源的编程，对一些限制性的编程，如 MP3、DVD 和 Adobe Flash 进行了豁免。 Linux Mint 对独占编程的考虑很奇怪； 许多 Linux 流通自然排除了限制性编程，因为一些 Linux 拨款的共同目标是坚持自由和开源编程的模型。 它可以通过各种桌面环境进行浏览，包括默认的 Cinnamon 桌面、APT、MATE 和 KDE。
Linux Mint 伴随着大量引入的编程，其中包括 VLC 播放器、Firefox、LibreOffice 等。它允许组织端口利用其防火墙关闭，并且可以访问重做端口选择。 默认的 Linux Mint 桌面环境 CinnamonandMATE 支持多种语言。 通过利用适用于 Linux 的 Wine Windows 相似层编程或虚拟化编程（包括 VMware 或基于内核的虚拟机），它同样可以运行许多用于 Microsoft Windows 的项目。
Debian GNU / Linux 于1993年首次公布。它的创始人Ian Murdock的初始想法是在空闲时间创建一个由数百名志愿者开发的完全非商业项目。当时怀疑论者远远超过乐观主义者，似乎注定要夭折收尾，但实际情况却恰恰相反。 Debian不仅幸存下来，而且还在不到十年的时间里成为了最大的Linux发行版，也是有史以来创建的最大的协作软件项目！
Debian GNU / Linux的成功可以用下面的数字来说明。它由1000多名志愿者开发，它的软件库包含近50,000个二进制包（编译为8个处理器架构），有120个基于Debian的发行版和live CD。这些数字是任何其他基于Linux的操作系统无法比拟的。 Debian主要有三个主要分支（或四个，如果包括增加稳定性的“实验”分支）：“unstable”（也称为“sid”），“testing”和“stable ”。软件包和功能的逐步整合和稳定性，以及项目完善的质量控制机制，使得Debian获得了今天可用的最佳测试和无缺陷发行版之一的声誉。
Fedora Linux 是由 Red Hat 赞助的社区构建发行版，在被 IBM 收购之前，它是世界上最赚钱的开源公司。Red Hat 仍然是开源世界的巨头，为维护整个 Linux 生态系统所依赖的大部分软件和基础设施的开发人员付费。
Red Hat 并不直接开发 Fedora Linux，尽管该公司的一些员工是 Fedora 社区的成员。相反，Red Hat 使用 Fedora Linux 开发自己的独立产品 CentOS 和 Red Hat Enterprise Linux。这两个版本的 Linux 广泛用于企业界、学术机构或任何需要维护自己的服务器的人。
Fedora 提供易于学习的桌面，集成了大多数其他 Linux 发行版之前的最新功能，并内置了 SELinux 等安全工具。
Arch Linux 是在2002年由加拿大计算机科学专业毕业生Judd Vinet在2002年推出的，几年来，它一直是一个为中级和高级Linux用户设计的边缘项目。但是它“滚动更新”，只需要安装一次，然后保持一直更新，不要从头安装新的系统。这都要感谢其强大的包管理器和一个总是最新的软件库。因此，Arch Linux的“发行版”很少，而且现在只限于一个基本的安装光盘，只有在基本系统发生相当大的变化时，才会发行新的安装介质。
Arch Linux除了拥有备受推崇的“滚动发布”更新机制之外，还以其快速和强大的软件包管理器“Pacman”而闻名，能够从源代码安装软件包，并且由于其AUR基础架构，以及经过充分测试的软件包不断增加的软件库。其高度重视的文档，以及卓越的Arch Linux手册，使得一些高级Linux用户可以自行安装和定制分发。用户可以使用的强大工具意味着发行版可以无限定制到最细微的细节，并且没有两个安装可能是相同的。
引用于 哪个Linux发行版可以替代Ubuntu  
]]></content>
  </entry>
  
  <entry>
    <title>HPC需求高速增长推动224G以太网SerDes技术发展加速</title>
    <url>/post/datacenter/hpc-demand-growth-promotes-the-development-of-224G-Ethernet-SerDes-technology.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>HPC</tag>
      <tag>Network</tag>
      <tag>SerDes</tag>
    </tags>
    <content type="html"><![CDATA[数据，是现在网联社会的核心，数据量级的提升远超我们的想象。根据IDC的预测，到2025年全球的数据量将达到175 ZB。数据量的增长带动了对高带宽和网络速度这些新基础设施的需求。
算力网络的发展对骨干网和大型数据中心提出了更高的要求，如今200G/400G的以太网链路已经在加速部署，其超高带宽可完全满足各种带宽密集型应用的需求，并大大降低端口成本。800G和1.6T以太网链路也在加速来袭。
224G SerDes技术实现更高速率以太网 去年200G/400G 产品大规模放量，800G则进入导入阶段。以800G以太网为例，利用了两组现有400G以太网逻辑，并进行了一些修改，将数据分布在八个112Gbps物理通道上。那继续提升每个通道速率，达到224Gbps，则能够支持高达1.6T的链路。网络提速的下一个前沿趋势无疑是1.6T以太网。
人工智能、自动驾驶、高性能计算HPC和云计算这些快速增长的应用对数据网络提速的需求肉眼可见，网络速度必须足够快，才能在计算、网络和存储组件之间快速移动数据。但相对来说，算力增长的步伐是快于传输速度增长步伐的。
以太网高速接口的出现正是为了满足连接方面的需求，高性能SerDes也为每一代的标准实现了速度的翻倍，以太网速度的发展已经在尽力跟上脚步了。最新一代以太网标准将提供224G的数据速率，为1.6T以太网的发展奠定了基础。
224G以太网 SerDes技术驱动力 驱动224G以太网SerDes发展的因素，不妨以数据中心的角度来看，因为不论是在数据中心内部、数据中心之间还是数据中心与终端之间都有着大量的数据交换需求。
其中最主要的带宽需求在数据中心内部，向更高速度以太网连接的转变不仅可以节省电力，还可以节省面积，从而增加互连密度。
空间的节省也是很明显的，从早前的640G交换机到现在51.2T甚至102.4T交换机，每一代的端口数都在不断变化，现在已经有多达512个端口，如果不向更高速率的以太网SerDes发展，端口数量继续增多的交换芯片越来越难做，而且良率很低。
SerDes接口必须不断提高运转速度来顺应发展。在高密度的数据中心，224G以太网SerDes的应用将大大减少所需的线缆和交换机数量，既节省宝贵的空间又增加互连密度。
当然，224G真正部署起来却并没有那么简单。半导体封装技术、链路连接技术、通道技术都还在努力地跟上224G以太网SerDes发展的节奏，每一项技术欠缺都会增加链路的损耗，增加串扰的风险，根据相关厂商的评估，想要112G增加到224G，实现难度总体上升了5倍。
Light Counting调研认为，今年224G以太网SerDes会有3到5个Design开始，到2026年，224G将会迎来第一波部署热潮。IP Nest在对SerDes IP的调研中，同样认为224G以太网SerDes PHY将在今年开始加快发展速度。
目前，已经有一些厂商能够提供针对HPC和数据中心应用支持更高速度的MAC、PCS和PHY，同时适用于多种先进FinFET工艺，并在性能最大化的情况下提供卓越的BER。
224G以太网SerDes早期的一些应用主要会覆盖重定时器、交换机、拓展AI、光学模块、I/O芯片和FPGA上，成熟应用后，将延伸至更多数据需求领域，在各行各业充分出更高数据速率的优势。
小结 现代数据中心对更高数据速率的需求，对网络扁平化减少延迟的需求，大大推动了对更高带宽连接的需求。交换器SoC芯片尺寸正在达到最大限制也意味着需要更高的连接速率来支持更高的带宽要求。
这些切实需要解决的问题大大推动了对224G以太网SerDes连接的需求。随着224G以太网 SerDes技术在今后的成熟，高密度数据应用将会有完全不同的解决方案。
]]></content>
  </entry>
  
  <entry>
    <title>Linux 日志管理</title>
    <url>/post/linux/linux-system-log-management.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Log</tag>
    </tags>
    <content type="html"><![CDATA[Linux 日志管理是指对 Linux 系统中产生的各种日志文件进行收集、分析、备份、轮转和删除等操作，以便监控系统的运行状况，诊断和解决问题，提高系统的安全性和性能。
介绍 Linux 系统中有两种主要的日志服务，一种是传统的 rsyslog 服务，它是一个灵活的日志处理器，可以将日志信息发送到不同的目标，如文件、数据库、网络等。另一种是新添加的 systemd-journal 服务，它是一个二进制日志系统，可以存储更多的元数据，如时间戳、主机名、优先级等，并支持日志查询和过滤。
Linux 系统中的日志文件通常存放在 /var/log 目录下，不同的程序和服务会生成不同的日志文件，记录了各种类型的信息，如内核消息、用户登录事件、程序错误等。常见的日志文件及其存放内容如下：
   日志文件 存放内容     /var/log/message 内核消息及各种应用程序的公共日志信息，是 Red Hat Linux 中最常用的日志之一   /var/log/secure 与安全相关的日志信息   /var/log/maillog 与邮件相关的日志信息   /var/log/cron 与定时任务相关的日志信息   /var/log/dmesg 引导过程中的各种事件信息   /var/log/lastlog 每个用户最近一次登录信息   /var/log/wtmp 每个用户登录注销及系统启动和停机事件   /var/log/btmp 失败的、错误的登录尝试及验证事件    Linux 系统中的日志文件会定期进行轮转，即将旧的日志文件重命名并压缩，创建新的日志文件。这样可以避免日志文件占用过多的磁盘空间，并保留一定时间段内的日志记录。日志轮转由 logrotate 命令实现，它根据 /etc/logrotate.conf 和 /etc/logrotate.d/ 目录下的配置文件来执行轮转操作。这些配置文件可以指定轮转周期、轮转次数、轮转方式、轮转后执行的命令等。
 Linux 系统中有多种命令和工具可以用来查看和分析日志文件，例如： tail 命令：用于查看日志文件的最后几行，常用 -f 选项实时监控日志变化。 grep 命令：用于在日志文件中搜索特定的关键字或模式。 who 命令：用于查看当前登录到系统的用户信息。 last 命令：用于查看成功登录到系统的用户记录。 lastlog 命令：用于查看系统中所有用户最近一次登录信息。 lastb 命令：用于查看用户错误的登录列表。 Logcheck 工具：用于分析系统日志并报告异常或重要事件。 Logcheck 工具：用于分析系统日志并报告异常或重要事件。  相关软件包和配置文件 软件包 [root@Demo01 ~]# rpm -qa | grep rsyslog rsyslog-relp-8.24.0-57.el7_9.3.x86_64 rsyslog-libdbi-8.24.0-57.el7_9.3.x86_64 rsyslog-mmnormalize-8.24.0-57.el7_9.3.x86_64 rsyslog-mmjsonparse-8.24.0-57.el7_9.3.x86_64 rsyslog-crypto-8.24.0-57.el7_9.3.x86_64 rsyslog-gnutls-8.24.0-57.el7_9.3.x86_64 rsyslog-snmp-8.24.0-57.el7_9.3.x86_64 rsyslog-kafka-8.24.0-57.el7_9.3.x86_64 rsyslog-mysql-8.24.0-57.el7_9.3.x86_64 rsyslog-mmkubernetes-8.24.0-57.el7_9.3.x86_64 rsyslog-gssapi-8.24.0-57.el7_9.3.x86_64 rsyslog-mmaudit-8.24.0-57.el7_9.3.x86_64 rsyslog-mmsnmptrapd-8.24.0-57.el7_9.3.x86_64 rsyslog-8.24.0-57.el7_9.3.x86_64 rsyslog-pgsql-8.24.0-57.el7_9.3.x86_64 rsyslog-udpspoof-8.24.0-57.el7_9.3.x86_64 rsyslog-elasticsearch-8.24.0-57.el7_9.3.x86_64 rsyslog-doc-8.24.0-57.el7_9.3.noarch 配置文件 /etc/rsyslog.conf #主配置文件 /etc/rsyslog.d/*.conf #辅配置文件 /var/log/ #日志文件存放位置 /usr/sbin/rsyslogd #执行文件 /usr/lib64/rsyslog/ #模块路径 /usr/lib/systemd/system/rsyslog.service #服务单元 [root@Demo01 ~]# grep &#39;####&#39; /etc/rsyslog.conf #### MODULES ####定义模块 #### GLOBAL DIRECTIVES ####定义全局环境 #### RULES #### 定义规则 模块定义 module(load=&#34;imuxsock&#34; # 提供对本地系统日志的支持 SysSock.Use=&#34;off&#34;) # 关闭通过本地日志接口的信息接收功能，日志信息接收通过下面的imjournal模块 module(load=&#34;imjournal&#34; # 提供对systemd日志的访问 StateFile=&#34;imjournal.state&#34;) # 定义状态文件，rsyslog用于记录文件上传进度，避免日志内容混乱 全局环境设置 # 定义工作目录 global(workDirectory=&#34;/var/lib/rsyslog&#34;) # 使用默认的时间戳格式 module(load=&#34;builtin:omfile&#34; Template=&#34;RSYSLOG_TraditionalFileFormat&#34;) # 定义辅助配置文件位置 include(file=&#34;/etc/rsyslog.d/*.conf&#34; mode=&#34;optional&#34;) 规则设置 信息来源.安全级别 处理方式 信息来源 kern：内核相关的日志 user：用户相关的日志 mail：邮件相关的日志 daemon：系统服务相关的日志 lpr：打印相关的日志 cron：计划任务相关的日志 authpriv:认证相关的日志 news：新闻相关的日志 uucp：文件copy相关的日志 local0-local7：自定义相关的日志信息 *： 所有 安全级别 debug: 调试 info: 消息 notice: 注意 warn,warning: 警告 err,error: 错误 crit: 严重级别 alert: 需要立即修改该的信息 emerg,panic: 内核崩溃，系统接近崩溃 *：所有日志级别 none:没有任何级别，也就是不记录日志信息 表达形式 mail.err err+crit+alert+emerg mail.=err err mail.!err 除了err 处理方式 /PATH/FILENAME：将信息储存至 /PATH/FILENAME文件中。注意，如果要系统日志服务把信息储存到文件，该文件必须以 斜线（/） 开头的绝对路径命名之。 USERNAME：将信息送给已登录的用户。 @HOSTNAME：代表使用udp协议将信息转送到远端的日志服务器。 @@hostname：代表使用tcp协议将信息传送到远端的日志服务器 *：将信息传送给所有已登录的用户。 #### RULES #### ########## 日志设备.日志级别 ################## ######### 消息发送位置 ############### # Log all kernel messages to the console. # Logging much else clutters up the screen. #kern.* /dev/console # Log anything (except mail) of level info or higher. # Don&#39;t log private authentication messages! *.info;mail.none;authpriv.none;cron.none /var/log/messages # The authpriv file has restricted access. authpriv.* /var/log/secure # Log all the mail messages in one place. mail.* -/var/log/maillog # Log cron stuff cron.* /var/log/cron # Everybody gets emergency messages *.emerg :omusrmsg:* # Save news errors of level crit and higher in a special file. uucp,news.crit /var/log/spooler # Save boot messages also to boot.log local7.* /var/log/boot.log local2.info /tmp/test.log 日志设备    日志设备.级别 说明     auth -pam(linux中的认证机制) 产生的日志   authpriv -ssh,ftp 等登录信息的验证信息   cron 时间任务相关   kern -内核相关   lpr -打印   mail -邮件   mark(syslog) -rsyslog服务内部的信息,时间标识   news -新闻组   user -用户程序产生的相关信息   uucp -unix to unix copy unix主机之间的相关通信   local 1~7 自定义日志设备文件    日志级别    级别 说明     NONE 什么都不记录   EMERG (紧急) 导致主机系统不可用的情况   ALERT(警告) 必须马上采取解决措施   CRIT (严重) 比较严重的情况   ERR 运行时的错误   WARNING (提醒) 可能影响系统功能的事件   NOTICE(注意) 不会影响系统功能,但是值得注意   INFO 一般信息   DEBUG 调试信息    自下而上,信息记录的越来越少
常用的日志文件 /var/log/boot.log	#系统启动时的日志。 /var/log/dnf.* #dnf软件包管理器相关日志 /var/log/firewalld #防火墙日志 /var/log/lastlog #所有用户最后一次登录信息,需要使用lastlog命令查看 /var/log/maillog #电子邮件系统相关日志 /var/log/messages #整体的系统日志，具体记录范围取决于服务的配置文件 /var/log/wtmp	#记录当前登录和过去登录的用户信息，使用last命令查看 日志格式 [root@Demo01 ~]# tail -n 20 /var/log/messages Jul 1 05:05:41 LAMP dhclient[2115]: bound to 192.168.10.25 -- renewal in 736 seconds. Jul 1 05:05:41 LAMP systemd: Starting Network Manager Script Dispatcher Service... Jul 1 05:05:41 LAMP dbus[777]: [system] Successfully activated service &#39;org.freedesktop.nm_dispatcher&#39; Jul 1 05:05:41 LAMP systemd: Started Network Manager Script Dispatcher Service. DATE TIME HOSTNAME APP（NAME）[PID]: MESSAGES 每一个字段的意义如下说明： DATE：信息发生的日期。 TIME：信息发生的时间。 HOSTNAME：信息发生的主机。 APP：产生信息的软件。 NAME：软件的名称，或是软件组件（Component）的名称。可以省略。 PID：进程标识符 （Process ID）。可以省略。 MESSAGES：信息的内容。 Sep 15 09:03:59 ecs-t6-large-2-linux-20190824103606 systemd-logind: New session 314 of user root. 时间 主机名 子系统名 消息字段 ]]></content>
  </entry>
  
  <entry>
    <title>C++中fmt库的用法</title>
    <url>/post/programming/fmt-lib-usage-in-c-plus-plus.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>C++</tag>
      <tag>fmt</tag>
    </tags>
    <content type="html"><![CDATA[本来这篇文章应该接着介绍 memset 的，但是我暂时不想写那个主题。这次就写点新的东西。介绍一下 fmt 库。
介绍与安装 {FMT} 是一个开源格式库，按照作者的说法，是提供快速和安全的 替代 C 的 stdio 和 C++ 的iostreams。
项目地址：https://github.com/fmtlib/fmt
下载最新版的 release 文件即可，将 fmt-10.0.0/include/fmt/core.h include 进来就完成了。其实还可以用 CMake 的，但是今天介绍的重点在语法，所以我就直接单文件 include 了。
据说这个库进入了 C++ 20 当中，不清楚，毕竟我用的还是 C++ 11
用法 最简单的输出就是 print
#define FMT_HEADER_ONLY #include &#34;fmt-10.0.0/include/fmt/core.h&#34; int main() { fmt::print(&#34;Hello fmt!&#34;); } 这样其实和正常的 printf 没有什么区别？C++相对C的最大区别之一就是尽可能地追求typesafe，所以C++从一开始就在寻找替代printf系列函数的解决方案，于是从一开始就有了各种各样的ostream。typesafe是做到了，但是性能低下。三方库 fmt 就是想既解决 typesafe ，也想要一定的性能。
参数替换 类似于 printf 的 % 占位符输出，fmt 使用大括号替代参数。
fmt::print(&#34;Also {}&#34;,&#34;hello fmt!&#34;); 输出结果：
Also hello fmt! 其实这种语法有点类似于 Vue.js 的设计。据说是未来的趋势。。。
另一个有效的函数是 fmt::format，所以上面的输出也可以写成
std::string hello_buffer = fmt::format(&#34;Also {}&#34;,&#34;hello fmt!&#34;); fmt::print(&#34;{}&#34;,hello_buffer); 另外，和 cout 一样，fmt 输出对参数是无关的。
fmt::println(&#34;{}&#34;,4.12); 这样的也是可以正常输出的。
参数格式设计 在大括号内可以对格式进行处理，printf 也有类似的功能，但据说 fmt 的功能更加全面？而且 fmt 利用 format 可以直接处理字符串，这点是 printf 没有的。
但是网上针对这个库的文章很少。。。我也是第一次用，所以只能贴出我整理到的用法了。
大括号的参数分为前后两部分，分别为为位置参数（arg_id）和格式化参数（format_spec）。前者用来显式声明参数在字符串中的位置，后者对值进行一定的格式化转换。（这两个是我翻译的，毕竟官网也没有这俩的中文翻译。。。）
位置参数 官方对位置参数的解释是用正则表达式。。。
replacement_field :: = &#34;{&#34;[arg_id][&#34;:&#34; format_spec] &#34;}&#34; arg_id :: = integer | identifier integer :: = digit + digit :: = &#34;0&#34;...&#34;9&#34; identifier :: = id_start id_continue* id_start :: = &#34;a&#34;...&#34;z&#34; | &#34;A&#34;...&#34;Z&#34; | &#34;_&#34; id_continue :: = id_start | digit 简单解释下吧 从0开始，然后1、2、3，等等
fmt::println(&#34;{0} + {1} = {2}&#34;,2,3,5); //2 + 3 = 5 如果没有指定位置，默认从0往后排
fmt::println(&#34;{} + {} = {}&#34;,2,3,5); //2 + 3 = 5 参数位置与值是一一对应的，所以可以交换顺序
fmt::println(&#34;{1} + {2} = {0}&#34;,5,2,3); //2 + 3 = 5 如果你不想用数字，也可以用fmt::arg指定别名参数
fmt::print(&#34;Hello, {name}! The answer is {number}. Goodbye, {name}.&#34;, fmt::arg(&#34;name&#34;, &#34;World&#34;), fmt::arg(&#34;number&#34;, 42)); //Hello, World! The answer is 42. Goodbye, World. 格式化参数 可选的arg_id放在format_spec的后面，其后是冒号'：'。
]]></content>
  </entry>
  
  <entry>
    <title>浮点数比较问题</title>
    <url>/post/linux/floating-point-comparison-problem.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Linux</tag>
      <tag>Float Point</tag>
    </tags>
    <content type="html"><![CDATA[在C语言中，使用浮点型进行比较可能会出现误差，那么究竟是什么原因呢？
浮点比较出现误差的原因 在C语言中，使用浮点型进行比较可能会出现误差的原因主要有以下几点：
 浮点数舍入误差：由于计算机对浮点数的表示方式是有限的，无法精确表示所有实数。在进行浮点数运算时，会存在舍入误差，这可能导致两个看似相等的浮点数实际上是略微不同的。 浮点数精度限制：浮点数在计算机中以二进制表示，因此只能表示一定范围的实数。对于超出这个范围的数值，浮点数将近似表示，从而引入了误差。 运算顺序和优化：浮点数的运算顺序可能会影响最终结果，特别是当运算涉及到大量的累加操作时。此外，编译器的优化过程也可能对浮点数进行重排或合并，导致结果与预期不符。  解决此问题的方法 为了解决浮点数比较容易出错的问题，可以采取以下措施：
 使用误差范围比较：而不是直接比较两个浮点数是否相等，可以通过定义一个允许的误差范围（例如 epsilon），然后判断两个浮点数之间的差值是否小于该误差范围。 使用整数比较：将浮点数转换为整数的形式，然后进行比较。这种方法可以避免浮点数运算中的舍入误差。但需要注意整数溢出的问题。 避免直接比较浮点数：尽量避免直接使用 == 运算符来比较浮点数是否相等，而是使用其他方式，如比较它们的差值或比例，以减少精度误差的影响。 使用更高精度的数据类型：如果对于高精度的计算要求较高，可以考虑使用库函数提供的更高精度的数据类型，例如 long double 或者自定义的高精度计算库。 注意浮点数的运算顺序和优化：合理安排浮点数的运算顺序，避免大量累加操作的积累误差。同时，可以通过调整编译器的优化选项来控制浮点数运算的行为。  综上所述，使用浮点数进行比较时，要注意浮点数运算的特性，并选择合适的比较方法。根据具体需求，可以采用上述解决办法来减小误差并提高比较的准确性。
使用误差范围方法实现浮点比较的代码 在C语言中，可以使用以下方法来比较浮点数：
 使用差值的绝对值进行比较：  #include &lt;math.h&gt; int compare_float(float a, float b, float epsilon) { if (fabs(a - b) &lt;= epsilon) { return 0; // a 和 b 相等  } else if (a &lt; b) { return -1; // a 小于 b  } else { return 1; // a 大于 b  } } 在上面的示例中，compare_float 函数通过计算两个浮点数 a 和 b 的差的绝对值，并与给定的精度 epsilon 进行比较。如果差的绝对值小于等于 epsilon，则返回0表示两个浮点数相等，否则根据大小关系返回 -1 或 1。
使用 &lt;, &gt;, == 操作符进行比较：  int compare_float(float a, float b, float epsilon) { if (a + epsilon &lt; b) { return -1; // a 小于 b  } else if (a - epsilon &gt; b) { return 1; // a 大于 b  } else { return 0; // a 和 b 相等  } } 上述代码使用 &lt; 和 &gt; 操作符将带有一定误差范围的比较转换为数值的大小比较。
这些方法都需要在比较时指定一个误差范围 epsilon，你可以根据具体情况选择一个适当的值。
请注意，在使用这些方法比较浮点数时，由于浮点数的精度限制，结果可能受到舍入误差的影响。因此，仍然需要谨慎比较浮点数并注意误差范围的选择。
浮点误差范围 epsilon如何确定 确定浮点误差范围 epsilon 的值是一个复杂的问题，因为它取决于许多因素，例如应用的要求、数据的特征以及浮点数运算的性质。下面是几种常见的方法来选择合适的 epsilon 值：
 了解浮点数表示：首先，了解浮点数在计算机中的表示方式和精度限制是很重要的。浮点数由尾数和指数组成，在计算过程中可能会产生舍入误差。这样的了解有助于你理解浮点误差的本质，从而选择一个适当的 epsilon 值。 预估误差：根据你对浮点数操作的预期误差，选择一个合理的 epsilon 值。根据具体应用的需求，确定你对结果精度的要求。例如，如果你需要高精度计算，那么 epsilon 应该设置得更小；如果你可以容忍一定的近似误差，那么可以选择相对较大的 epsilon 值。 观察数据集：观察你的数据集中浮点数的典型差异和范围。根据数据的分布情况来选择一个合适的 epsilon 值。例如，如果数据集中的浮点数差异较小，那么可以选择较小的 epsilon 值。 考虑数值范围：浮点数在不同数量级上可能会有不同的精度要求。当比较较大或较小的浮点数时，可能需要相应地调整 epsilon 的值。例如，对于接近 0 的浮点数，可能需要选择较小的 epsilon 值来确保精度。 实验和测试：通过在你的具体应用场景中运行实验和测试，观察不同 epsilon 值下的结果差异，选择一个最合适的值。可以通过与预期结果进行比较，并评估 epsilon 值对应用的性能和准确性的影响。  需要注意的是，选择 epsilon 值是一个折衷问题，过小的值可能导致计算开销增加，而过大的值可能无法满足精度要求。根据具体情况，在实际应用中可以尝试不同的 epsilon 值，并评估其对结果的影响，以找到最适合你应用的值。
]]></content>
  </entry>
  
  <entry>
    <title>FPGA时序约束--实战篇</title>
    <url>/post/fpga/fpga-timing-constraints.html</url>
    <categories><category>FPGA</category>
    </categories>
    <tags>
      <tag>FPGA</tag>
      <tag>Xilinx</tag>
    </tags>
    <content type="html"><![CDATA[前面几篇文章介绍了“如何写时序约束”和“如何看懂时序约束报告”，这些知识点都是基础，可以知道设计的HDL代码不收敛的位置，但解决时序收敛问题更关键。
 FPGA  时序不收敛，会出现很多随机性问题，上板测试大概率各种跑飞，而且不好调试定位原因，所以在上板测试前，先优化时序，再上板。
今天我们就来唠一唠解决时序不收敛的问题，分享一些常用的解决办法和思路。
模块运行时钟频率 设计模块前，需明确模块运行的最大时钟频率。
不同时钟频率下，对应的时序约束最大延时是不一样的。
比如100MHz时钟下运行的HDL逻辑，比200MHz时钟下的HDL逻辑，支持的组合逻辑层数多。
根据最大时钟频率，来设计HDL代码的层级，时钟频率越高，插入寄存器要更多，增加流水线级数，减少过长的组合逻辑。
HDL代码  HDL代码风格  优先参考FPGA开发软件提供的HDL Template，比如Vivado的language template。
HDL代码逻辑优化  对于复杂的算法代码逻辑，需要结合FPGA并行计算和最小处理单位为bit的特性，对逻辑代码进行优化实现。
明确变量最大的数据位宽。
逻辑代码中，一些软件逻辑实现起来较复杂，尽量简化，删除掉一些不可能出现的情况。
这部分代码逻辑，可能需要重复迭代几遍实现才行。
组合逻辑层数  插入寄存器  将计算逻辑分成多个时钟周期实现，这是常用的时序优化方法，可以减少过多的组合逻辑层数，但会增加延时。
这里以一个多路输入求和计算为例
module sum( input clk, input [15:0] data_A, input [15:0] data_B, input [15:0] data_C, input [15:0] data_D, output [17:0] sum_o); always @(posedge clk) begin sum_o &lt;= data_A + data_B + data_C + data_D; end endmodule 增加寄存器后，改为
module sum( input clk, input [15:0] data_A, input [15:0] data_B, input [15:0] data_C, input [15:0] data_D, output [17:0] sum_o); reg [16:0] sum0, sum1; always @(posedge clk) begin sum0 &lt;= data_A + data_B; sum1 &lt;= data_C + data_D; end always @(posedge clk) begin sum_o &lt;= sum0 + sum1; end endmodule 逻辑展平设计  优化代码中优先级译码电路逻辑，主要出现在IF/ELSE结构语句中，这样逻辑结构被展平，路径延迟得以缩短。
IF ELSE结构语句存在明显的优先级，建议尽量用CASE语句来替代。
防止变量被优化  HDL综合布线软件会根据实际情况，自动优化代码逻辑，可能存在将多个不同寄存器变量合并成一个寄存器变量的情况。
对于不希望被优化的变量，可以在变量定义前，添加（* keep = &ldquo;ture&rdquo; *）
高扇出 高扇出问题，原因是一个寄存器驱动后级数超过了它本身的驱动能力，导致延迟时间过大，不满足时序。
 使用max_fanout  在变量定义前，可以添加(* max_fanout = n *)，来设置变量的最大扇出数，超过这个扇出数，综合软件会自动复制多份变量。
复位信号高扇出  复位信号是常见的高扇出问题，主要解决办法有：
 减少复位信号的使用，能使用使能信号控制的，就用使能信号。 对于大型模块，复位信号可以使用BUFG来驱动复位信号，可以增加复位信号的驱动能力  资源消耗 FPGA器件的整个工程资源消耗，不管是LUT还是BRAM等资源，建议不超过80%。
一旦资源消耗超过80%，在布线综合时，就出现布线资源不够，导致出现布线拥塞，从而出现了时序不收敛的情况。
布线拥塞也分为全局拥塞和局部拥塞，可能是高扇出信号过多，也可能是局部布线资源不够用，导致时序路径过长。
 优化代码逻辑，减少资源消耗。  在资源不够用的情况下，建议检查代码是否可优化，设置的RAM大小是否过大等等。
使用替代资源实现  在FPGA中实现RAM时，可以根据整个资源的使用情况，考虑使用Distributed RAM、URAM等资源来减少BRAM的消耗。
总结 本文分享了时序收敛的一些方法和思路，希望可以给大家带来一点启发。
]]></content>
  </entry>
  
  <entry>
    <title>ZYNQ 读写SD卡</title>
    <url>/post/fpga/read-and-write-sd-card-through-Xilinx-ZYNQ.html</url>
    <categories><category>FPGA</category>
    </categories>
    <tags>
      <tag>FPGA</tag>
      <tag>Xilinx</tag>
      <tag>ZYNQ</tag>
      <tag>SD</tag>
    </tags>
    <content type="html"><![CDATA[SD卡（Secure Digital Card）是一种常用的可移动存储设备，广泛应用于嵌入式系统中。它具有体积小、容量大、读写速度快、易于携带等特点，在数据存储和传输中发挥着重要的作用。
前言 本实验介绍如何使用Xilinx ZYNQ芯片在SD卡上读写文件。
Zynq芯片具有SD卡接口，通过该接口可以实现对SD卡的读写操作。SD卡接口通常是通过SPI（Serial Peripheral Interface）或SDIO（Secure Digital Input Output）协议实现的。SPI协议适用于低速的SD卡读写操作，而SDIO协议则适用于高速的读写操作。
FatFs库 本实验是通过调用FatFs库来对SD卡进行读写。
FatFs是一个开源的文件系统模块，提供了简单而灵活的接口，使嵌入式应用程序能够轻松地读取、写入和管理存储在FAT文件系统中的文件。
FatFs支持FAT12、FAT16和FAT32三种主要的FAT文件系统类型，这些类型广泛应用于各种存储介质，如磁盘、SD卡和USB闪存驱动器等。它被广泛用于嵌入式系统中。
FatFs的特点和功能包括：(1)轻量级和可移植性，它的核心代码非常精简，可以根据系统的需求进行裁剪和配置。它的接口设计独立于底层的存储介质和操作系统，因此可以轻松地移植到不同的平台上。(2)多种API函数，用于打开、关闭、读取、写入和管理文件以及目录操作。(3)支持长文件名。(4)可选的缓存机制，FatFs提供了可选的缓存机制，可以加快对存储介质的访问速度。(5) 可靠性和错误处理。
Vivado工程的编写 我们使用的开发板为ZedBoard。
  本实验使用的Vivado工程延用《ZYNQ 串口打印输出——FPGA Vitis篇》中使用的Vivado工程，大家可以查看该文章来了解Vivado工程的建立。
  修改Vivado工程，勾选SD 0外设。
  我们使用的开发板为ZedBoard，相关SD卡原理图如下图所示。  按照原理图的管脚连接方式，设置好MIO Configuration。
 保存设计，右键点击.bd文件，选择“Generate Output Products…”。
  右键点击.bd文件，选择“Create HDL Wrapper…”。弹出的窗口中选择“Let Vivado manage wrapper and auto-update”，点击“OK”。
  点击Vivado “Flow Navigator”一栏里的“Generate Bitstream”，等待Vivado生成好bit文件后，在菜单栏“File -&gt; Export -&gt; ExportHardware&hellip;”导出硬件信息(.xsa文件)，这里就包含了PS端的配置信息。该步骤如有疑问，可以参考以前的文章《ZYNQ串口打印输出——FPGA Vitis篇》。
  Vitis工程的编写  点击 Vivado 菜单“Tools-&gt; Launch Vitis IDE”，启动 Vitis。 新建 Vitis平台工程。Vitis工程的建立可以参考以前的文章《ZYNQ串口打印输出——FPGA Vitis篇》。 新建 Vitis应用工程，创建一个名为“SD_DEMO_SDK”的工程，工程模板可以选择“Hello World”。 在工程平台的Board Support Package Settings里勾选上xilffs文件系统。注意是“standalone alone”模式下。  工程主要包含两个文件：main.c、bmp.h。  bmp.h里包含了各种不同像素图片的数据头定义。 main.c里包含3个函数：
void genTestImage(u8 * imageSrc); void bmp_write(char * name, char *head_buf, char *data_buf); int main(void); 该工程示例是先通过genTestImage函数生成一张像素为640*480的彩条图片。然后再通过bmp_write函数将图片的数据头（*head_buf）以及图片数据本身（*data_buf）写入SD卡。
 编译工程，将工程下载到硬件板卡上，硬件板卡插上待写入数据的SD卡。
  程序运行完后，串口会打印“sd card write done!”信息。
  开发板断电，将SD卡插入到电脑上的读卡器。可以在文件游览器看到，此时SD卡上多了一个“COLOR.BMP”文件，打开该文件，可以发现正是我们用genTestImage函数生成一张像素为640*480的彩条图片。  实验小结 本实验介绍了如何通过调用FatFs库来加载SD卡、在SD卡上创建文件并写入文件。同理读取SD卡上文件是调用FatFs库的f_read函数。本工程的部分源码见附录A。该工程对应的完整源码可以在公众号输入ZYNQ_SD来获取工程的下载链接，工程采用的是Vivado2021.1版本。
附录 A
int main(void) { FRESULT rc; genTestImage(gImage_640x480); rc = f_mount(&amp;fatfs, &#34;0:/&#34;, 0); if (rc != FR_OK) { return 0 ; } bmp_write(&#34;color.bmp&#34;, (char *)&amp;BMODE_640x480, (char *)&amp;gImage_640x480) ; xil_printf(&#34;sd card write done! \n\r&#34;); return 0; } void genTestImage(u8 * imageSrc) { int total = 0; for (int idxRow = 0; idxRow &lt; 480; idxRow++) { for (int idxCol = 0; idxCol &lt; 640; idxCol++) { if ( idxCol &gt;= 0 &amp;&amp; idxCol &lt; 213) { *(imageSrc + (640*idxRow + idxCol)*3) = 0xFF; *(imageSrc + (640*idxRow + idxCol)*3 + 1) = 0x00; *(imageSrc + (640*idxRow + idxCol)*3 + 2) = 0x00; } else if ( idxCol &gt;= 213 &amp;&amp; idxCol &lt; 426) { *(imageSrc + (640*idxRow + idxCol)*3) = 0x00; *(imageSrc + (640*idxRow + idxCol)*3 + 1) = 0xFF; *(imageSrc + (640*idxRow + idxCol)*3 + 2) = 0x00; } else if ( idxCol &gt;= 426 &amp;&amp; idxCol &lt; 640) { *(imageSrc + (640*idxRow + idxCol)*3) = 0x00; *(imageSrc + (640*idxRow + idxCol)*3 + 1) = 0x00; *(imageSrc + (640*idxRow + idxCol)*3 + 2) = 0xFF; } total++; } } } void bmp_write(char * name, char *head_buf, char *data_buf) { short y,x; short Ximage; short Yimage; u32 iPixelAddr = 0; FRESULT res; unsigned int br; // File R/W count  memset(&amp;Write_line_buf, 0, 1920*3) ; res = f_open(&amp;fil, name, FA_CREATE_ALWAYS | FA_WRITE); if(res != FR_OK) { return ; } res = f_write(&amp;fil, head_buf, 54, &amp;br) ; if(res != FR_OK) { return ; } Ximage=(unsigned short)head_buf[19]*256+head_buf[18]; // bm_width  Yimage=(unsigned short)head_buf[23]*256+head_buf[22]; // bm_height  iPixelAddr = (Yimage-1)*Ximage*3 ; for(y = 0; y &lt; Yimage ; y++) { for(x = 0; x &lt; Ximage; x++) { Write_line_buf[x*3 + 0] = data_buf[x*3 + iPixelAddr + 0] ; Write_line_buf[x*3 + 1] = data_buf[x*3 + iPixelAddr + 1] ; Write_line_buf[x*3 + 2] = data_buf[x*3 + iPixelAddr + 2] ; } res = f_write(&amp;fil, Write_line_buf, Ximage*3, &amp;br) ; if(res != FR_OK) { return ; } iPixelAddr -= Ximage*3; } f_close(&amp;fil); } ]]></content>
  </entry>
  
  <entry>
    <title>手把手教你在嵌入式设备中使用SQLite3</title>
    <url>/post/linux/how-to-use-SQLite3-in-embedded-devices.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Linux</tag>
      <tag>SQLite3</tag>
    </tags>
    <content type="html"><![CDATA[数据库是用来存储和管理数据的专用软件，使得管理数据更加安全，方便和高效。数据库对数据的管理的基本单位是表(table)，在嵌入式linux中有时候它也需要用到数据库，听起来好难，其实就是几个函数，掌握了就好。
常见的数据库 大型数据库(大型机)Oracle(亿级)，中型数据库(分布式超大型)mysql(百万级)，轻型数据库(嵌入式设备)sqlite(万级)，访问数据库使用SQL语句，适用于所有的数据库。
安装SQLite3 有C环境就可以调用sqlite
直接用命令安装 sudo apt-get update sudo apt-get install sqlite3 直接编译源码 将源码拷贝到Ubuntu的非共享目录解压
解压命令：
tar zvxf sqlite-autoconf-3380500.tar.gz 配置
cd sqlite-snapshot-201708031550 ./configure --prefix=/home/gec/sqlite 编译
make 安装
make install SQLite的使用 新建数据库文件
sqlite3 数据库文件的路径 //打开/创建 //比如：sqlite3 first.db 基本操作命令 .exit/.quit -------- 退出数据库命令行 .help -------------- 帮助说明信息 .tables ------------ 查看当前数据库中所有的表 数据库访问的SQL语句 基本语法：
所有的SQL语句都以分号(;)结束 不区分大小写 新建表格 create table 表名(字段名1 字段类型1,字段名2 字段类型2,字段名3 字段类型3,...); 比如： //创建一个stutbl的表，表中有3个字段 //分别是整数类型的学号id,字符串类型的name和整数类型的age create table zhiguoxin(id int,name char[20],age int); //不存在则创建 create table if not exists zhiguoxin(id int,name char[20],age int); //如果希望表中某个字段的内容不重复,可以用unique修饰该字段 create table if not exists zhiguoxin(id int unique,name char[20],age int); 删除表格 drop table 表名; //drop table zhiguoxin; 往表格中插入数据 insert into 表名 values(字段值1,字段值2,字段值3,....); //字段值如果是字符串,必须用&#39;&#39;(单引号)括起来 比如： insert into zhiguoxin values(1001,&#39;刘尧&#39;,18); insert into zhiguoxin values(1002,&#39;聂衍文&#39;,19); insert into zhiguoxin values(1003,&#39;杨佳晨&#39;,20); insert into zhiguoxin values(1004,&#39;冯华阳&#39;,21); 完成插入之后，zhiguoxin 的表格内容如下：
id	name	age 1001	刘尧	18 1002	聂衍文	19 1003	杨佳晨	20 1004	冯华阳	21
查询表中的数据 //查询表中的所有数据
select * from 表名; //select * from zhiguoxin; 查看数据库 可以把first.db数据库文件拷贝至windows下，使用SQLite Developer打开即可看到。SQLite Developer下载地址
https://mydown.yesky.com/pcsoft/443425.html 按条件查找  使用where指定查询条件  select * from zhiguoxin where id=1003;//查询id值为1003的条目 select * from zhiguoxin where age&gt;=19 and age&lt;21; select * from zhiguoxin where age&gt;=19 or age&lt;21; 指定查询的字段  select id,name,age from zhiguoxin;//只查询id,name,age的字段 使用where+like实现模糊查询  select * from zhiguoxin where name like &#39;刘%&#39;;//查找名字以刘开头的条目 使用order by实现查询结果按某个字段的值升序/降序输出  select * from zhiguoxin order by age desc;//按年龄降序排序 select * from zhiguoxin order by id asc; //按id升序排序 删除表中的条目 delete from 表名 where 条件;//删除所有符合条件的条目 比如： delete from zhiguoxin where id=1001; 更新(修改)表中的条目 update 表名 set 字段名1=字段值1,字段名2=字段值2... where 条件;//修改符合条件的条目 比如： update zhiguoxin set age=100 where id=1002; SQLite中字段类型 数字：
int ------- 整型 smallint ---- 短整型 tinyint ----- 微型整数(0~255) bit --------- 0 or 1 float ------ 单精度浮点型 real ------- 双精度浮点型 字符串：
char ---------- 非unicode定长字符串 &lt; 8000 varchar ------- 非unicode变长字符串 &lt; 8000 text ---------- 非unicode变长字符串 &lt; 2^32-1 nchar ---------- unicode定长字符串 &lt; 8000 nvarchar ------- unicode变长字符串 &lt; 8000 ntext ---------- unicode变长字符串 &lt; 2^32-1 SQLite的C语言访问接口 sqlite本身自带C语言访问接口，在C语言的环境下可以直接使用，使用这些接口的代码需要 sqlite的源码编译进可执行程序 或者 编译时链接sqlite的库。
打开 sqlite3_open int sqlite3_open( const char *filename, /* 数据库的文件路径 */ sqlite3 **ppDb /* 输出参数：传出代表打开数据库的句柄 */ ); //成功返回SQLITE_OK,否则打开失败char ---------- 非unicode定长字符串 &lt; 8000 varchar ：非unicode变长字符串 &lt; 8000 text ：非unicode变长字符串 &lt; 2^32-1 nchar：unicode定长字符串 &lt; 8000 nvarchar : unicode变长字符串 &lt; 8000 ntext :unicode变长字符串 &lt; 2^32-1 关闭 sqlite3_close int sqlite3_close(sqlite3 *pDb); //传入要关闭的数据库的句柄 编译方法  直接编译源码  gcc sqlite3.c sqlite_test.c -pthread -ldl -o sqlite_test 链接sqlite3的动态库  gcc sqlite_test.c -pthread -ldl -lsqlite3 -L /home/gec/sqlite/lib -o sqlite_test //如果运行时找不到sqlite3的库，可以将编译出来的库文件拷贝到/usr/lib目录下(cp -r) 执行SQL语句的接口 sqlite3_exec int sqlite3_exec( sqlite3 *pDb, /* 打开的数据库的句柄 */ const char *sql, /* 要执行的SQL语句 */ int (*callback)(void *arg,int col,char **str,char **name), /* 回调函数,处理SQL语句执行返回的结果(查询),一条结果调用一次 arg - exec的第四个参数 col - 本条结果的字段数 str - 记录字段值的数组 name - 记录字段名的数组 回调函数必须返回SQLITE_OK */ void *arg, /* 传递给回调函数的第一个参数 */ char **errmsg /* 错误信息 */ ); //成功返回SQLITE_OK,否则执行失败 几个例子
//连接数据库 int Connection_Sqlite3DataBase() { rc = sqlite3_open(&#34;./face_database/face.db&#34;, &amp;db); if (rc != SQLITE_OK) { fprintf(stderr, &#34;Can&#39;t open database: %s\n&#34;, sqlite3_errmsg(db)); sqlite3_close(db); exit(1); } else printf(&#34;You have opened a sqlite3 database named bind.db successfully!\nCongratulation! Have fun!\n&#34;); return 0; } //将图片插入到数据库 void insert_face_data_toDataBase(const char *name, MByte *face_feature, MInt32 featureSize) { sqlite3_prepare(db, &#34;insert into face_data_table(name,face_feature,feature_size) values (?,?,?);&#34;, -1, &amp;stmt, NULL); sqlite3_bind_text(stmt, 1, name, strlen(name), NULL); sqlite3_bind_blob(stmt, 2, face_feature, featureSize, NULL); sqlite3_bind_int(stmt, 3, featureSize); sqlite3_step(stmt); } ]]></content>
  </entry>
  
  <entry>
    <title>Intel驱动开始默认搜集数据：NVIDIA强制、AMD良心</title>
    <url>/post/news/Intel-driver-starts-to-collect-data-by-default.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Intel</tag>
      <tag>AMD</tag>
      <tag>NVIDIA</tag>
    </tags>
    <content type="html"><![CDATA[Intel最近发布了101.4578 Beta测试版显卡驱动，重点支持Arc锐炫系列，但在常规更新之外，还有一个隐藏点。
在安装过程中，选择典型模式，会出现一个“Compute Improvement Program”(CIP)的新选项，字面意思就是“计算改进项目”，默认勾选。
它和所有类似的改进项目一样，都是用来搜集数据的。
按照Intel的解释，为了改进Arc显卡的性能、功能、使用体验，CIP项目会搜集用户的电脑使用习惯、系统信息、其他设备信息、访问网站(不包含URL链接)等。
其中，使用信息包括：显卡频率、驱动软件使用时长、系统内存占用量、笔记本电池续航时间、即插即用设备等等。
系统信息包括但不限于：设备制造商、CPU型号、内存与显示配置、操作系统版本、软件版本、区域和语言设置、位置与时区等等。
Intel还强调，绝对不会搜集用户姓名、地址、邮箱、手机号等敏感个人信息。
这些操作其实都属于厂商们的常规操作，只是默勾选，确实有点让人感觉不太舒服。
不过，Intel还不算过分的， NVIDIA  最狠，GeForce显卡驱动默认强制开启数据收集功能，而且不可关闭，自定义安装也不行。
AMD会在Adrenalin显卡驱动安装结束时给出选择，不管典型、仅驱动、自定义哪种安装方式，都可以选择开关数据搜集功能。
但不管怎么说，NVIDIA的卡现在就是牛啊，不管游戏卡还是加速卡。
尽管OpenAI CEO之前否认，但业界还是相信他们已经在训练GPT-5大模型，规模将是GPT-4的10倍以上，但这也意味着更烧钱，尤其是用于训练AI的显卡极为稀缺。
全球这么多搞AI大模型的，到底用了多少AI显卡是各家的秘密，很少公开准确数据，GPT-4猜测是在10000-25000张A100显卡上训练的，GPT-5所需的显卡还是迷，马斯克估计的是30000-50000张H100显卡，性能强得多。
除了OpenAI之外，其他公司对高性能AI显卡的需求也居高不下，Meta、谷歌、微软、苹果、特斯拉及马斯克自己都在搞各种AI，预测总需求高达43.2万张H100显卡，价值超过150亿美元。
这还只是国外网友分析的美国科技行业需求，实际上国内的需求不比美国少，即便只能购买特供版的A800、H800加速卡，但这没有妨碍国内公司投身AI大模型，不惜加价抢购AI显卡。
在当前的市场上，只有NVIDIA才能满足AI显卡的需求，AMD及Intel的AI显卡不仅性能、生态上存在问题，而且供货也跟不上，AMD的大杀器MI300X要到年底才能出货，2024年才能大批量上市。
归根到底，这波又是NVIDIA赚麻了，AI显卡的需求将持续到2024年底，这一年半中都是供不应求的情况，H100显卡售价25万元起步，加价的话就难说了，涨幅波动很大。
]]></content>
  </entry>
  
  <entry>
    <title>不让NVIDIA吃独食！AMD下一代Zen5大杀器在路上</title>
    <url>/post/news/AMD-next-generation-zen5-is-on-the-way.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>AMD</tag>
      <tag>Zen5</tag>
      <tag>AI</tag>
      <tag>Intel</tag>
      <tag>NVIDIA</tag>
    </tags>
    <content type="html"><![CDATA[AI硬件市场上，NVIDIA可谓呼风唤雨，旗下的A100、H100加速器炙手可热。
Intel、AMD也都在积极投入相关产品，前者主要是GPU Max系列，后者主要是Instinct MI系列。
不久前，AMD刚刚正式推出了MI300系列加速器，其中MX300X首次将Zen4 CPU、CDNA3 GPU架构合二为一，并集成多达128GB HBM3，MI300A则是纯GPU方案，配备192GB HBM3。
据说还有MI300C、MI300P两种版本，前者是纯CPU架构，后者则是MI300X的精简版，规模砍半。
按照规律，这一代产品发布了，下一代产品肯定已经在积极研发中了，但是能从CEO口中确认下一代的名字，还不多见。
AMD CEO苏姿丰近日表示，AMD持续在AI方面投资，包括下一代MI400系列加速，以及再下一代、再下一代。
苏姿丰还强调，AMD不但有极具竞争力的AI硬件路线图，还会在软件方面做出一些改变。
她没有透露更具体的细节，猜测可能终于要大幅革新AMD ROCm开发框架了，不然永远打不过NVIDIA CUDA。
不出意外的话，MI400系列应该会上Zen5 CPU、CDNA4 GPU两大新架构，既有CPU+GPU融合方案，也有纯GPU方案。
传闻称，AMD正在开发全新的XSwitch高速互连总线技术，对标NVIDIA NVLink，这对于大规模HPC、AI运算来说是至关重要的。
今年AI火爆全球，导致NVIDIA的AI显卡成为香饽饽，需求大涨之下价格也失控了，这几个月不时传出缺货涨价的消息，最夸张的说是H100这样的显卡涨到了50万一块，是原价的2倍。
NVIDIA的AI显卡加价抢购，还有个重要原因是就是市场上没有什么可替代的产品，之前几乎是NVIDIA一家独大，好在下半年AMD的MI300系列AI加速卡就要上市了。
为了跟NVIDIA抢市场，AMD这一波是准备充分了。
在日前的财报会议上，CEO苏姿丰提到他们正在增加AI相关的支出，并且制定了AI战略，在AI硬件芯片及软件开发上下功夫。
针对MI300芯片的产能担忧，苏姿丰提到尽管当前供应链依然吃紧，但他们已经包下了供应链的产能，包括AI芯片不可或缺的台积电CoWos封装及HBM显存等芯片产能。
AMD表示从芯片制造到封装，以及零部件等，MI300系列显卡这次已经确保了充足的产能，在2023年4季度到2024年会大幅扩产，确保满足客户需求。
]]></content>
  </entry>
  
  <entry>
    <title>Intel 4工艺太难了！酷睿Ultra终于突破5GHz</title>
    <url>/post/news/intel-core-ultra-finally-breaks-through-5G-HZ.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Intel</tag>
      <tag>Core</tag>
      <tag>Ultra</tag>
    </tags>
    <content type="html"><![CDATA[无论是14nm还是10nm，Intel这些年的新工艺都有一个通性：刚诞生的时候性能平平，高频率都上不去，只能用于笔记本移动端(分别对应5代酷睿、10代酷睿)，后期才不断成熟，比如到了13代酷睿就达到史无前例的6GHz。
接下来的Intel 4，也就是最初的7nm，也要重复这条路了。
代号Meteor Lake的酷睿Ultra将首发Intel 4工艺，第一代也是仅限笔记本移动端，而且前期样品只能达到4.8GHz的单核频率。
目前，酷睿Ultra已经进入冲刺阶段，也就是质量验证(QS)，和零售正式版相差无几。
据了解，酷睿Ultra QS样品运行在20-65W的热设计功耗范围内，最高配置6个P性能核、8个E能效核、2个LPE超低功耗能效核，组成16核心22线程。
根据快科技8月6日了解到的消息，酷睿Ultra 7系列现在已经可以做到单核5GHz，酷睿Ultra 9系列则超过了5GHz，后续虽然不可能再有天大突破，但仍有望继续提升一些。
作为对比，现有13代酷睿移动版的i9-13900H已经可以跑到5.4GHz，i7-1370P也能加速到5.2GHz。
酷睿Ultra系列要超越它们，频率难以企及，只能依靠新的架构和更高的IPC，难度不小。
核显方面，之前消息说样品频率已高达2.2GHz，比以往大幅提升，但暂无进一步消息。
酷睿Ultra的核显将会非常抢眼，升级到全新的Xe LPG架构，也就是Arc锐炫系列独立显卡Xe HPG高性能架构的低功耗版本，集成最多8个Xe核心，也就是128个EU执行单元/1024个流处理器，比增加1/3。
值得一提的是，酷睿Ultra系列会加大对AI技术的支持，有望增加一个VPU视觉计算单元，将AI芯片成为处理器的功能基础，性能据说在3T到7.1TFLOPS之间，是上代的10倍，能效很高。
Intel CEO基辛格就明确表态，AI将会融入到Intel构建的每一款产品中。
在此之前，Intel去年底推出的第四代至强可扩展处理器上就已经加入了AI单元，性能10倍提升，现在也是时候轮到消费级处理器升级AI单元了。
酷睿Ultra加强AI也跟微软的战略不谋而合，9月份的Win11 23H2中微软就会加入AI助手Copilot，而明年的Win12系统中更是把AI作为核心技术植入，要重塑所有体验。
此前泄露的消息显示，Intel的酷睿Ultra第一代产品Meteor Lake就会首发支持Win12系统，还有Wi-Fi 7新技术，亮点不少。
]]></content>
  </entry>
  
  <entry>
    <title>DDR的工作原理</title>
    <url>/post/hardware/ddr-working-principle.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>DDR</tag>
      <tag>Double Data Rate</tag>
      <tag>SDRAM</tag>
      <tag>DDR4</tag>
      <tag>DDR5</tag>
    </tags>
    <content type="html"><![CDATA[在本专栏中，我们将由浅入深地讲述如何利用FPGA开发DDR接口。首先，我们将回顾DDR协议的基本原理和工作机制；然后介绍FPGA开发DDR接口时需要用到的IP核及使用方法；随后，验证我们开发的接口功能是否正确；最后向大家介绍几种在 FPGA中实用的DDR接口应用方法。
前言 最近几期，我们将通过专栏的方式向大家介绍一个常用的高速接口——DDR接口。DDR（Double Data Rate）是一种常见的内存接口标准，它在计算机系统和其他电子设备中广泛应用，对于数据传输速度的提升起到了至关重要的作用。
DDR的发展历程 SDRAM从发展到现在已经经历了四代，分别是：第一代SDR SDRAM，第二代DDR SDRAM，第三代DDR2 SDRAM，第四代DDR3 SDRAM，现在已经发展到DDR5 SDRAM。
 DDR SDRAM是Double Data Rate Synchronous Dynamic Random Access Memory(双数据率同步动态随机存储器)的简称，为第二代SDRAM标准。其常见标准有DDR 266、DDR 333和DDR 400。其对于SDRAM，主要它允许在时钟脉冲的上升沿和下降沿传输数据，这样不需要提高时钟的频率就能实现双倍的SDRAM速度，例如DDR266内存与PC133 SDRAM内存相比，工作频率同样是133MHz，但在内存带宽上前者比后者高一倍。这种做法相当于把单车道更换为双车道，内存的数据传输性能自然可以翻倍。 DDR2(Double Data Rate 2)SDRAM是由JEDEC(电子设备工程联合委员会)开发的第三代SDRAM内存技术标准，1.8v工作电压，240线接口，提供了相较于DDR SDRAM更高的运行效能与更低的电压，同样采用在时钟的上升/下降延同时进行数据传输的基本方式，但拥有两倍于上一代DDR内存预读取能力(即4bit数据读预取能力)，其常见的频率规范有DDR2 400\533\667\800\1066\1333等，总线频率553MHz的DDR2内存只需133MHz的工作频率 DDR3 SDRAM相比起DDR2具备更低的工作电压(1.5v)，240线接口，支持8bit预读，只需133MHz的工作频率便可实现1066MHz的总线频率。其频率从800MHz起跳，常见频率有DDR3 800\1066\1333\1600\1866\2133等。DDR3是当前流行的内存标准，Intel酷睿i系列(如LGA1156处理器平台)、AMD AM3主板及处理器的平台都是其“支持者”。 DDR4相比DDR3最大的区别有三点：16bit预取机制（DDR3为8bit），同样内核频率下理论速度是DDR3的两倍；更可靠的传输规范，数据可靠性进一步提升；工作电压降为1.2V，更节能。  DDR到DDR5的主要变化，我们可以看到，为了配合整体行业对于性能，容量和省电的不断追求，规范的工作电压越来越低，芯片容量越来越大，IO的速率也越来越高。
除了电压，容量和IO的速率变化之外，上表还列出了Bank，Bank Group，Prefetch和Burst Length的演进，bank数越来越多，到DDR4出现bank group，prefetch也从2n增加到4n，8n。虽然我们说现在DDR4的最大速率是3200MT/s, 但是这是指的DDR4的IO频率，即DDR4和memroy controller之间的接口数据传输速率。那么DRAM是怎么实现用比较低的核心传输频率来满足日益高涨的高速IO传输速率的需求呢？这就是靠prefetch来实现的。
 从DDR开始到DDR3很好理解，Prefetch相当于DRAM core同时修了多条高速公路连到外面的IO口，来解决IO速率比内部核心速率快的问题，IO数据速率跟核心频率的倍数关系就是prefetch。 burst length的长度跟CPU的cache line大小有关。Burst length的长度有可能大于或者等于prefetch。但是如果prefetch的长度大于burst length的长度，就有可能造成数据浪费，因为CPU一次用不了那么多。所以从DDR3到DDR4，如果在保持DDR4内存data lane还是64的前提下，继续采用增加prefetch的方式来提高IO速率的话，一次prefetch取到的数据就会大于一个cache line的大小 （512bits），对于目前的CPU系统，反而会带来性能问题。 DDR4出现了Bank Group,这就是DDR4在不改变prefetch的情况下，能继续提升IO速率的秘密武器。DDR4利用Bank group的interleave，实现IO速率在DDR3基础上进一步提升。  内存原理 从外观上就可以看出来小张的内存条由很多内存颗粒组成。
从内存控制器到内存颗粒内部逻辑，笼统上讲从大到小为：channel＞DIMM＞rank＞chip＞bank＞row/column，如下图：
一个现实的例子是：
在这个例子中，一个i7 CPU支持两个Channel（双通道），每个Channel上可以插俩个DIMM，而每个DIMM由两个rank构成，8个chip组成一个rank。由于现在多数内存颗粒的位宽是8bit，而CPU带宽是64bit，所以经常是8个颗粒可以组成一个rank。所以小张的内存条2R X 8的意思是由2个rank组成，每个rank八个内存颗粒。由于整个内存是4GB，我们可以算出单个内存颗粒是256MB。
这次我们来看看rank和Chip里面有什么，如下图：
这是个DDR3一个Rank的示意图。我们把左边128MB Chip拆开来看，它是由8个Bank组成，每个Bank核心是个一个存储矩阵，就像一个大方格子阵。这个格子阵有很多列（Column）和很多行（Row），这样我们想存取某个格子，只需要告知是哪一行哪一列就行了，这也是为什么内存可以随机存取而硬盘等则是按块存取的原因。
实际上每个格子的存储宽度是内存颗粒（Chip）的位宽，在这里由8个Chip组成一个Rank，而CPU寻址宽度是64bit,所以64/8=8bit，即每个格子是1个字节。选择每个格子也不是简单的两组信号，是由一系列信号组成，以这个2GB DDR3为例：
其引脚按照功能可以分为7类：前3类为电源、地、配置
后4类为：控制信号、时钟信号、地址信号、数据信号
电源、地、配置信号的功能很简单，在此不赘述。DDR4中最重要的信号就是地址信号和数据信号。
如上DDR4芯片有20根地址线（17根Address、2根BA、1根BG），16根数据线。在搞清楚这些信号线的作用以及地址信号为何还有复用功能之前，我们先抛出1个问题。假如我们用20根地址线，16根数据线，设计一款DDR，我们能设计出的DDR寻址容量有多大？
Size（max）=(2^20) * 16 = 2048 KB =2 MB。 但是事实上，该DDR最大容量可以做到1GB，比传统的单线编码寻址容量大了整整512倍，它是如何做到的呢？答案很简单，分时复用。我们把DDR存储空间可以设计成如下样式：
首先将存储空间分成两个大块，分别为BANK GROUP0和BANK GROUP1，再用1根地址线（还剩19根），命名为BG，进行编码。若BG拉高选择BANK GROUP0，拉低选择BANK GROUP1。（当然你也可以划分成4个大块，用2根线进行编码）。
再将1个BANK GROUP区域分成4个BANK小区域，分别命名为BANK0、BANK1、BANK2、BANK3。然后我们挑出2根地址线（还剩余17根）命名为BA0和BA1，为4个小BANK进行地址编码。
此时，我们将DDR内存颗粒划分成了2个BANK GROUP，每个BANK GROUP又分成了4个BANK，共8个BANK区域，分配了3根地址线，分别命名为BG0，BA0，BA1。然后我们还剩余17根信号线，每个BANK又该怎么设计呢？这时候，就要用到分时复用的设计理念了。
剩下的17根线，第一次用来表示行地址，第二次用来表示列地址。即传输2次地址，再传输1次数据，寻址范围最多被扩展为2GB。虽然数据传输速度降低了一半，但是存储空间被扩展了很多倍。这就是改善空间。所以，剩下的17根地址线，留1根用来表示传输地址是否为行地址。过程如下：
 在第1次传输时，行地址选择使能，剩下16根地址线，可以表示行地址范围，可以轻松算出行地址范围为2^16=65536个=64K个。 在第2次传输时，行地址选择禁用，剩下16根地址线，留10根列地址线表示列地址范围，可以轻松表示的列地址范围为2^10=1024个=1K个，剩下6根用来表示读写状态/刷新状态/行使能、等等复用功能。 这样，我们可以把1个BANK划分成64K*1K个=64M个地址编号。 所以1个BANK可以分成64K行，每行1K列，每个存储单元16bit。  最后理一下： 每行可以存储1K *16bit=2KB。每行的存储的容量称为Page Size。
单个BANK共64K行，所以每个BANK存储容量为64K *2KB=128MB。
单个BANK GROUP共4个BANK，每个BANK GROUP存储容量为512MB。
单个DDR4芯片有2个BANK GROUP，故单个DDR4芯片的存储容量为1024MB=1GB。
至此，20根地址线和16根数据线全部分配完成，我们用正向设计的思维方式，为大家讲解了DDR4的存储原理以及接口定义和寻址方式。
但是细心的同学发现一个问题，对于每一个bank，按照正常的10位数据，那么col应该是1024，而现在是128，是什么原因呢？
那么问题又来了，为什么Column Address的寻址能力只有128呢？请继续看下图
在上图中，可以清晰地发现，10bits的Column Address只有7bits用于列地址译码，列地址0,1,2并没有用！列地址0,1,2,这3bits被用于什么功能了？或者是DDR的设计者脑残，故意浪费了这三个bits？在JESD79-3规范中有如下的这个表格：
可以发现，Column Address的A2，A1，A0三位被用于Burst Order功能，并且A3也被用于Burst Type功能。由于一般情况，我们采用的都是顺序读写模式（即{A2,A1,A0}={0,0,0}），所以此时的A3的取值并无直接影响，CA[2:0]的值决定了一次Burst sequence的读写地址顺序。
比如一次Burst Read的时候如果CA[2:0]=3’b001表示低三位从地址1开始读取，CA3=0的时候按顺序读取1，2，3，0，5，6，7，4，CA3=1的时候交错读取1，0，3，2，5，4，7，6。 对于Prefetch而言，正好是8N Prefetch，对于Burst而言对应BL8。BC4其实也是一次BL8的操作，只是丢弃了后一半的数据。
更形象地理解就是对于一个Bank里面的Memory Array，每个Memory Cell可以看作是一个Byte的集合体。CA[9:3]选中一行中的一个特定Byte，再由CA[2:0]选择从这个Byte的哪个位置开始操作。CA3既参与了列地址译码，也决定Burst是连续读取还是交错读取。Prefetch也决定了I/O Frequency和SDRAM Core Frequency之间的关系。
总结 本文主要是针对DDR的原理进行了学习，主要集中在硬件的组成原理，其中涉及到Channel &gt; DIMM &gt; Rank &gt; Chip &gt; Bank &gt; Row/Column，其组成如下图所示
下期预告 在DDR专栏的下期文章里，我们将介绍Xilinx平台下，开发对应DDR接口IP核的具体使用和设置方法。如果觉得我们原创或引用的文章写的还不错，帮忙点赞和推荐吧，谢谢您的关注。
]]></content>
  </entry>
  
  <entry>
    <title>多核同构SMP--调度算法分析</title>
    <url>/post/linux/smp-scheduler-algorithm.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>SMP</tag>
      <tag>Scheduler Algorithm</tag>
    </tags>
    <content type="html"><![CDATA[随着智能化产品的需求不断提高，慢慢的单芯片单核处理器已经不能满足我们的需求，于是就在一个芯片上集成两个或多个核心，进而转向了多核处理器的发展，多核处理器具有更高的计算密度和更强的并行处理能力，所以它也是大趋势。多核处理器从硬件的角度来区分，又分为同构和异构：
 多核同构处理器：一个处理器的多个核心的体系架构是一样的，如：T113 多核异构处理器：一个处理器中包含不同体系架构的核心，如：STM32MP157  多核处理器从软件的角度来区分，又分为SMP和AMP：
 SMP：又称对称多处理（Symmetric multiprocessing），只有一个操作系统（OS）实例上运行多个核心，一个OS同等的管理各个内核，为各个内核分配工作负载，系统中所有的内核平等地访问内存资源和外设资源。 AMP：又称非对称多处理（Asymmetric Multi-Processing），每个核心运行自己的OS或同一OS的独立实例，或者说不运行OS，如运行裸机，每个内核有自己独立的内存空间，也可以和其它内核共享部分内存空间，每个核心相对独立地运行不同的任务，但是有一个核心为主要核心，它负责控制其它核心以及整个系统的运 行，而其它核心负责“配合”主核心来完成特定的任务。  本篇文章围绕SMP展开讲解。
什么是SMP 对称多处理器结构 , 英文名称为 &ldquo;Symmetrical Multi-Processing&rdquo; , 简称SMP。SMP又称为UMA, 全称&quot;Uniform Memory Access&quot;, 中文名称&quot;统一内存访问架构&quot;。
在 &quot; 对称多处理器结构 &quot; 的 系统中 , 所有的处理器单元的地位都是平等的 , 一般指的是服务器设备上 , 运行的 多个 CPU , 没有 主次/从属 关系，都是平等的。
这些处理器共享所有的设备资源, 所有的资源对处理器单元具有相同的可访问性, 如: 内存, 总线等，多个CPU处理器共享相同的物理内存, 每个CPU访问相同的物理地址, 所消耗的时间是相同的;
SMP的优缺点 优点：避免了结构障碍, 其最大的特点是所有的资源共享。
缺点：SMP架构的系统, 扩展能力有限, 有瓶颈限制。如: 内存瓶颈限制, 每个CPU处理器必须通过相同的总线访问相同的内存资源, 如果CPU数量不断增加, 使用同一条总线, 就会导致内存访问冲突; 这样就降低了CPU的性能;
操作系统如何满足SMP  公平共享: CPU的负载, 需要公平地共享, 不能出现某个CPU空闲, 造成资源浪费。 可设置线程(进程)与CPU亲和性: 可以为某些类型的线程（进程）与指定的处理器设置亲和性, 可以针对性地匹配线程（进程）与处理器。 线程（进程）迁移: 可以将线程（进程）在不同的CPU处理器之间进行迁移 。  总结：操作系统的SMP对称多处理器结构调度，核心就是将线程（进程）迁移到合适的处理器上, 并且可以保持各个处理器的负载均衡。
SMP调度方式 作者总结SMP的调度算法可以分为三种：
线程（进程）默认核心0运行，可以指定亲和性：  当用户创建线程（进程）时，可以指定挂在到指定核心运行。当任务挂在到指定核心，那么该任务只能在该核心上运行。 当用户创建线程（进程）时，没有指定挂在到指定核心运行，线程（进程）默认挂在到核心0。该任务核心0上运行。  问题：  优点：可以规定某个核心专注的做某一件事或某一类事。 缺点：核心0的负载会很大，它需要调度其他核心不调度的任务。  适用场景：  项目需要指定核心专一处理某一件事情的时候，可以使用这种调度算法  线程（进程）默认均分到不同核心，可以指定亲和性。  当用户创建线程（进程）时，可以指定挂在到指定核心运行。当任务挂在到指定核心，那么该任务只能在该核心上运行。 当用户创建线程（进程）时，没有指定挂在到指定核心运行，系统会判断每个核心的任务数，将该任务放在任务数最少的核心中。  问题：  优点：将任务平分给每个核心，每个核心的负载会相对均衡。 缺点：可能存在某个核心分配的任务都是比较轻的，某个核心分配的任务比较重。导致核心中的任务比较轻的，会更加容易进入空闲状态，核心中的任务比较重的，可能会一直处于忙碌状态，这样也会导致每个核心的负载不均衡。  适用场景：  项目中，所有的任务的复杂程度都差不多，可以均分到每个核心上，这样可以提高系统性能。  线程（进程）根据核心负载获取任务调度，可以指定亲和性。  当用户创建线程（进程）时，可以指定挂在到指定核心运行。当任务挂在到指定核心，那么该任务只能在该核心上运行。 当用户创建线程（进程）时，没有指定挂在到指定核心运行，将该任务挂载一个总任务队列中，当某个核心调度空闲时，就从总任务队列中获取一个任务运行。运行完毕之后归还给总任务队列。  问题：  优点：根据每个核心的负载，均分整个系统的任务调度，提供了每个核心的利用率。 缺点：调度算法比较复杂  适用场景：  项目中不需要关心任务的具体运行到那个核心。  总结  上述的调度算法，只有第三种满足：①公平共享；②可设置线程(进程)与CPU亲和性；③线程（进程）迁移。 调度算法，第一种和第二种，只满足三个条件的某一部分。 不用的调度适用于不同的场景，需要根据实际的需求选择相应的调度算法。 ]]></content>
  </entry>
  
  <entry>
    <title>Napatech如何将服务器转变为网络安全设备</title>
    <url>/post/datacenter/napatech-turned-servers-into-cybersecurity-appliances.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Intel</tag>
      <tag>Smart NIC</tag>
    </tags>
    <content type="html"><![CDATA[目前，几乎所有企业、政府机构、非营利实体及其它各类型组织都面临着网络安全挑战，随着网络罪犯不断提高作案能力，这类挑战将日益加剧。
网络安全现状 病毒、木马、蠕虫和其它恶意软件数量的增加，分布式拒绝服务 (DDoS) 攻击的蔓延，物联网设备与互联网的融合，以及数据中心线路速度的不断增长（从 1 Gbps 增长至 10 Gbps、25 Gbps 乃至 40 Gbps），各种趋势进一步加剧了网络安全挑战。
开源网络安全软件的缺点 在汹涌的数据海啸中，我们需要使用专业的软硬件确保网络安全性。网络安全呈现双轨并进的发展态势，其中既包括根据网络安全需求开发的自定义软硬件（较为昂贵），也包括开源网络安全软件，这些开源安全软件包括：
 Zeek（前身为 Bro）：Zeek 入侵检测系统 (IDS) 框架是一款非常强大的网络监控工具，可捕捉有关网络连接的数百个元数据字段。这些元数据可为网络流量提供无与伦比的可视性，帮助我们识别异常行为，如可疑或威胁活动。 Suricata：Suricata 是一款成熟的开源网络威胁检测引擎，具备实时网络入侵检测、内联入侵防御 (IPS)、网络安全监控 (NSM) 和捕捉数据包离线处理等功能。 Snort：Snort 是一款开源网络 IPS 工具，可在 IP 网络上执行实时流量分析和数据包记录。该工具可实施协议分析、内容搜索/匹配，并可用于检测各种攻击与探头，包括缓冲区溢出、秘密端口扫描、CGI 攻击、服务器信息块 (SMB) 探头及操作系统指纹识别尝试等。 ntop n2disk 和 nProbe Cento：ntop n2disk 和 ntop nProbe Cento 分别是网络流量记录器和高速流量分析探头，支持 1/10/100 Gbps 以太网连接。  相比定制化的网络安全系统，开源网络安全软件具有更低的成本，但基于 CPU 的服务器无力应对流量增长。使用开源网络安全软件检测实时数据流时，每台基于 CPU 的服务器最高可达到约 15 Gbps 的速率。但是在数据中心，网络安全系统所产生的负载往往会远高于这个数值。
这时候用户往往使用多台基于 CPU 的网络安全服务器处理较大负载，他们通常利用负载均衡器将入站流量分为大小适当的数据流，并将这些数据流分配至各个网络安全服务器。分别通过开源网络安全软件进行处理。在这种方式中，由于服务器节点增加、采购负载均衡器等原因，会推高网络安全系统的整体成本。
英特尔® 可编程加速卡加速网络应用 借助一款基于 FPGA 的加速器卡，即采用英特尔® Arria® 10 GX FPGA 的英特尔® 可编程加速卡 (PAC)，Napatech 获得了一种中间方案，从而提升了开源网络安全应用的性能，实现了加速。
此外，采用英特尔® Arria® 10 GX FPGA 的英特尔® PAC 基于 FPGA 的通用型加速技术还可帮助 Napatech 加速其他网络应用，包括：
 TRex：TRex 是一款开源状态和无状态流量生成器，基于数据平面开发套件 (DPDK)。通过对真实流量模板的预处理和使用，TRex 可实现智能重播，生成第 4 层到第 7 层流量。 Wireshark：Wireshark 是一款广泛使用的网络协议分析器，可提供网络活动的微观视图。Wireshark 是实际上（通常也是法定的）协议分析标准，被众多企业、非营利机构、政府机构和教育机构广泛使用。  英特尔® PAC 中的英特尔® Arria® 10 FPGA 可加速关键网络安全与其它网络应用，支持具备适当配置的服务器全速处理 40-Gbps 流量，且不会遗漏任何数据包。最新的加速统计数据如下：
 Suricata – 加速 4 倍 n2disk – 加速 3 倍 TRex – 加速 4 倍 Wireshark – 加速 7 倍  采用英特尔® Arria® 10 GX FPGA 的英特尔® 可编程加速卡 (PAC)
面向采用英特尔® Arria® 10 GX FPGA 的英特尔® 可编程加速卡的 Napatech Link™ Capture Software 将英特尔® 加速器卡转变为 SmartNIC，对上述开源应用实施多种加速网络安全和其他网络功能，数据中心运营商可根据特定要求选择所需的网络安全应用。
]]></content>
  </entry>
  
  <entry>
    <title>如何做个PCB电动机</title>
    <url>/post/hardware/how-to-make-a-pcb-motor.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>PCB</tag>
      <tag>Motor</tag>
    </tags>
    <content type="html"><![CDATA[电动机印制电路板的每一层都有一组线圈，它们堆叠在一起并互相连接以形成连续的迹线。
我一开始只是想做一架非常小的无人机。但我很快意识到，在我的设计中，有一个制约因素，那就是马达的体积和重量。即使是小电动机也仍然是分立的装置，需要连接到所有其他电子元件和结构元件上去。所以我开始想知道是否有办法合并这些元件并减轻一些质量。
我的灵感来自于一些无线电系统如何使用由印制电路板（PCB）上的铜迹线制成的天线。我可以使用类似的东西来制造足够强大的磁场来驱动电动机吗？我决定看看是否可以使用由PCB迹线制成的电磁线圈来制造轴向磁通电动机。在轴向磁通电动机中，形成电动机定子的电磁线圈平行于圆盘形转子安装。永磁体嵌入转子的圆盘中。用交流电驱动定子线圈使转子旋转。
第一个挑战是确保我能够创造出足够的磁通量来转动转子。设计一个扁平的螺旋线圈迹线并让电流流过它是很简单的，但是我将我的电动机的直径限制在16毫米，以使整个电动机的直径可与最小的成品无刷电动机的相媲美。16毫米意味着我只能在转子圆盘的下面总共安装6个线圈，每个螺旋上安装大约10匝。十匝不足以产生足够大的磁场，但是如今很容易制作出多层的PCB。通过打印成堆叠的线圈（四层的每一层上都有线圈），我可以让每一线圈获得40匝，足以转动一个转子。
随着设计的向前推进，一个更大的问题出现了。为了保持电动机的旋转，必须使转子和定子之间动态变化的磁场同步。在由交流电驱动的典型电动机中，由于桥接起定子和转子的电刷的排列，这种同步自然就产生了。在无刷电动机中，需要的是实现反馈系统的控制电路。
左图：完成的四层印制电路板。
中图：对这些线圈施加脉冲，驱动3D打印出来的带有嵌入式永磁体的转子。
右图：虽然没有传统的无刷电动机那么强大，但PCB更便宜、更轻。
在我以前制造的一个无刷电动机驱动器中，我测量了作为反馈来控制速度的反电动势。反电动势产生的原因是旋转的电动机就像一个小发电机，在定子线圈中产生与用于驱动电动机的电压相反的电压。对反电动势进行感应，可以提供有关转子旋转方式的反馈信息，并让控制电路使线圈同步。但在我的PCB电动机中，反电动势太弱而无法使用。为此，我安装了霍尔效应传感器，它可以直接测量磁场的变化以测量转子及其永磁体在传感器上方旋转的速度。随后这些信息被输入到电动机控制电路中。
为了制造转子本身，我转向了3D打印。起初，我制作了一个转子，我安装在一个单独的金属轴上，但后来我开始将卡扣轴作为转子的一个组成部分进行打印。这将物理组件简化为了只有转子、四个永磁体、一个轴承以及提供线圈和结构支撑的PCB。
我很快就得到了我的第一台电动机。测试表明它能产生0.9克厘米的静态扭矩。这不足以满足我最初的制造一个集成进无人机的电动机的目标，但我意识到这个电动机仍然可以用来推动小型廉价的机器人轮子上用轮子沿着地面前进，所以我坚持进行研究（电动机通常是机器人身上最昂贵的部件之一）。这一印制电动机可以在3.5到7伏的电压下工作，尽管它在较高的电压下会明显升温。在5 V时，其工作温度为70°C，这仍然是可控的。它吸收大约250毫安电流。
目前，我一直在努力增加电动机的扭矩。通过在定子线圈的背面添加铁氧体片来包含线圈的磁场线，我几乎可以使扭矩倍增。我还在研究设计具有不同绕组配置和更多定子线圈的其他原型。此外，我一直在努力使用相同的技术来构建一个PCB电动推杆，它可以驱动一个3d打印出来的滑块在一排12个线圈上滑动。而且，我正在测试一个柔性PCB原型，它使用相同的印制线圈来执行电磁驱动。我的目标是——即使我还不能制造出能飞上天空的小无人机——开始制造具有比现有机器人更小更简单的机械构造的机器人。
]]></content>
  </entry>
  
  <entry>
    <title>ARM服务器在信创市场能突围吗</title>
    <url>/post/datacenter/arm-server-in-information-technology-industry.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>ARM</tag>
      <tag>Server</tag>
    </tags>
    <content type="html"><![CDATA[近几年随着中美脱钩加速，在信息技术领域频频遇到美国卡脖子情况。为保障技术自主可控，信创产业必要性愈发凸显。信创作为一项国家战略，发展信创是为了解决本质安全的问题。本质安全也就是说，把它变成中国自己可掌控、可研究、可发展、可生产的。
而信创包含IT基础设施，基础软件、应用软件、信息安全等领域，其中CPU芯片作为服务器、PC整机等IT基础设施底座支撑，CPU设计技术的自主可控格外重要。当前国内六家主流CPU厂商鲲鹏、飞腾、兆芯、龙芯、海光、申威纷纷发力，从国外引进各种不同技术架构如 ARM  、 X86  、MIPS等，共同推进信国产CPU发展。
引进各种不同的架构路线的初衷，是希望充分发挥试错作用，借市场和用户的选择，找到国产CPU最正确的技术发展路线。而在几年的技术发展后，或是龙芯LoongArch，或是国产X86，技术积累和产品迭代都做得不错；只是发现ARM架构，貌似无法扛起自主可控的重任。
ARM服务器难以成为主流 ARM作为精简指令集，其优势在功耗比，主要应用在移动和嵌入式市场。早在2008年，ARM公司在研究ARM架构是否可以用于服务器，为数据中心提供算力支持，AMD、惠普、博通、高通等美国企业都曾发布过ARM架构的服务器芯片，但全都折戟。
其主要原因在于ARM芯片依靠多核心堆砌，单核性能孱弱，总体比不上x86架构芯片性能。后来在2018年，高通推出ARM的高效服务器芯片Centriq 2400，性能比英特尔 Purley铂金8160高出7%，但高通的ARM CPU还是没有厂商愿意使用，最后无疾而终。
业界终于认识到，ARM架构生态不健全才是最大的问题。那为什么鲲鹏和飞腾还要引进ARM架构？原因在于ARM授权模式。ARM公司不涉及芯片生产，只对外销售IP核授权，国内厂商拿到授权的公版架构后，开发设计门槛低，容易造出量产芯片。
目前国产Arm架构CPU服务器大体分为华为的鲲鹏系和中电子的飞腾系。鲲鹏系主要是以华为自家TaiShan系列服务器为核心，只向服务器合作厂商（如神州数码、宝德等）出售鲲鹏主板而非鲲鹏芯片。而飞腾系聚焦于国产芯片研发，自身并不设计生产服务器，直接向服务器合作厂商（如长城、浪潮、联想、宝德、同方、五舟等）出售服务器芯片。
当前国内ARM服务器受制于ARM芯片性能、生态原因发展较为有限。鲲鹏系服务器，因为只供货搭载鲲鹏CPU的主板，导致神州、宝德等服务器产品性能同质化严重，另外华为还有自家TaiShan系列，导致鲲鹏下游厂商出货情况基本受华为控制。
而飞腾系服务器，据众多金融行业IT从业者反映，在资源使用率达到 70%-80% 左右就出现问题，可能跟相关产品的适配不兼容有关联。因为服务器领域应用最广泛的还是X86架构，生最为健全，很多情况下ARM服务器需要重新编译适配，才能使用X86软件应用。
虽然近两年在鲲鹏和飞腾的大力推动下，国内ARM生态发展迅速，但目前国内市占仅有7%，长远看也难以成为主流选择。
桌面应用生态有限 相对于ARM芯片在服务器领域还能做到小部分应用，国产ARM芯片在PC电脑的应用更为惨淡，目前主要为飞腾PKS体系内，长城有一些ARM芯片电脑。主要原因还是性能与生态孱弱，低性价比导致在商业市场竞争力较弱。
PC电脑文字处理、图片编辑等日常办公用途，CPU单核性能更为重要。以长城（Great Wall）TN140A2笔记本电脑为例，搭载8核的飞腾D-2000芯片，最高主频仅有2.3GHz，单核性能不足Intel同期产品的1/4，8个核心性能仅相当于Intel CPU的两个核心。而21年的龙芯3A5000的主频为2.5GHz，单核性能测试较飞腾D-2000也高出不少。
除了性能较差很难支持桌面办公的需要，ARM电脑的生态也是个大问题。主流的Windows生态都是基于X86开发，此前因为Arm在PC领域的表现不佳，让很多应用开发者的积极性不够高。原生应用数量严重不足，这些都会影响到用户的基础体验。如果靠编译器移植到ARM电脑，则会损失大量性能，让本就不充足的CPU性能雪上加霜。
在性能和生态孱弱的情况下，国产ARM PC售价却不低，基本价格都在9000元以上，售价比肩苹果MacBook，且售后质保仅有一年。
综上原因，ARM PC所占份额很小，仅在某些特定领域有些应用。
迁移成本高昂，后续维护困难 从用户角度出发，对于国产CPU及终端产品，生态覆盖面广、处理性能好，并结合自身应用系统对国产芯片的适配情况，迁移改造工作量小的产品才是最佳选择。
但无论是ARM服务器还是电脑，使用成本不低。
一方面是ARM服务器，因跟最主流的x86生态相互不兼容，如果用户需要从x86平台切换到ARM平台，迁移工作量大，改造时间长。并且因为各行各业部署环境不相同，若遇到适配错误的情况，也很难从服务器厂商获得帮助。整个过程费心费力不说，迁移工作还会导致业务停摆，其中造成的损失难以避免。另一方面，办公使用ARM电脑，其操作逻辑与主流电脑并不相同，用户需要花费很多精力从新学习使用，况且因生态不健全，很多软件也用不了。
就这些用户痛点，ARM厂商会在前期承诺免费开发移植调优，附加条件为全套系统全部采购ARM架构产品。这对用户意味着，这套强封闭和强捆绑手段，后期若是出现问题，将面临着高昂的维修维护费用。
用户与其重新适应ARM架构，不如从一开始选择自主程度更高、性能更好的龙芯产品。
技术迭代之路已被掐断 除了ARM产品性能、生态的本身问题外，ARM架构不适合信创产业的最根本原因在于，国外已经禁止出口ARM架构后续架构，国产ARM芯片停止迭代。ARM公司多次制裁中国企业，已经证明ARM芯片自主可控是伪命题。
并且ARM公司今年开始赴美IPO，获得美国资本之后，对国内制裁将更顺理成章，后续架构授权必然遥遥无期。在此情况下，国产ARM芯片性能逐渐落后，生态脱钩加速，对于用户业务的影响将一步步凸显出来。
因此对于国内信创产业， ARM架构在延续性和自主性上都有亟待解决的问题，很难担负自主可控的重任。
在国外制裁之下，国产ARM芯片未来何去何从？已投入的资源如何止损？这些问题都需要ARM厂商们再做考量。
]]></content>
  </entry>
  
  <entry>
    <title>无惧SSD压力 8月起机械硬盘要涨价</title>
    <url>/post/datacenter/prices-of-hard-drive-will-raise-in-auguest.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Storage</tag>
      <tag>SSD</tag>
      <tag>HDD</tag>
      <tag>Solidigm</tag>
      <tag>P5336</tag>
    </tags>
    <content type="html"><![CDATA[在装机硬盘选择上，现在大家几乎默认选择SSD硬盘，系统盘没人选HDD机械硬盘了，然而对市场来说，HDD硬盘厂商似乎并不担心被SSD侵蚀份额，8月份还会上调价格。
来自硬盘渠道商的消息显示，8月份起硬盘厂商计划提价，最近零售版的行货硬盘价格已经上涨了。
目前的低价机械硬盘主要是来自OEM厂商之前的库存，所以市面上还是能买到一些低价仓库盘。
目前机械硬盘市场上主要是西数、希捷及东芝三大厂商，前两年占据主要份额，具体涨价是哪家开始的不得而知，但双方应该有默契，在总销量一直下滑的情况下，涨价是改善业绩的主要手段了。
考虑到消费级市场上，SSD硬盘已经成为必选，因此这次的涨价对大部分个人用户来说影响不大，主要影响企业级客户，毕竟大容量存储上，机械硬盘还是不可少的。
前几天Solidigm推出了61.44TB的SSD硬盘，这个容量远超当前的机械硬盘，后者最大容量还在26TB级别，30TB的尚未正式出货。
不过超大容量的SSD价格也很贵，单位价格下依然是机械硬盘占优，因此企业级也不可能完全淘汰机械硬盘，至少8TB到20TB以上的市场中，机械硬盘还会长期存在。
存储行业确实在底部了，一些公司业绩已经在证实。
全球第二大存储芯片制造商韩国SK海力士给出的报告称，第二季度净亏损2.98万亿韩元（约合23亿美元），上年同期为净利润2.88万亿韩元。
该公司表示，第二季度营业亏损2.88万亿韩元，而上年同期为盈利4.19万亿韩元。营收下降47.1%，至7.3万亿韩元。
这是SK海力士连续第三个季度出现亏损，原因是需求疲软导致整体存储芯片价格持续低迷。
但第二季度亏损比第一季度3.4万亿韩元的创纪录亏损有所收窄。该公司表示，内存芯片市场正开始从深度低迷中复苏。
该公司在一份声明中表示，尽管消费者需求持续疲软，但“生成式人工智能市场的扩张”迅速推高了“对人工智能服务器内存的需求”。
“因此，高端产品(如HBM3和DDR5)的销售增加，导致第二季度营收环比增长44%，而营业亏损环比缩小了15%。”
行业人士表示，随着存储行业需求的增加，一些存储产品也即将迎来大涨价时代，这会缓解存储厂商的业绩。
]]></content>
  </entry>
  
  <entry>
    <title>ARMv8架构u-boot启动流程详细分析</title>
    <url>/post/mcu/uboot-boot-process-depth-analysis-based-on-ARMv8.html</url>
    <categories><category>MCU</category>
    </categories>
    <tags>
      <tag>ARM</tag>
      <tag>ARMv8</tag>
      <tag>uboot</tag>
    </tags>
    <content type="html"><![CDATA[本文基于 armv8 架构来对 u-boot 进行启动流程分析，u-boot版本为2022-01。
概述 首先引用wiki上的简介：u-boot 是一个主要用于嵌入式系统的引导加载程序，可以支持多种不同的计算机系统结构。
u-boot最先是由德国DENX软件中心团队开发，后续众多有志于开放源码bootloader移植工作的嵌入式开发人员将各个不同系列嵌入式处理器的移植工作不断展开和深入，以支持了更多的嵌入式操作系统的装载与引导。
选择u-boot的理由： 开放源码；支持多种嵌入式操作系统内核的引导，如Linux、NetBSD, VxWorks, QNX, RTEMS, ARTOS, LynxOS, android；支持多个处理器系列，如PowerPC、ARM、x86、MIPS；
较高的可靠性和稳定性；高度灵活的功能设置，适合U-Boot调试、操作系统不同引导要求、产品发布等；
丰富的设备驱动源码，如串口、以太网、SDRAM、FLASH、LCD、NVRAM、EEPROM、RTC、键盘等；
较为丰富的开发调试文档与强大的网络技术支持；
基于以上理由本篇文章对现在主流的armv8架构的u-boot启动流程进行详细分析，以便所有人快速学习和理解u-boot的工作流程。
armv8 u-boot的启动 先看arm官网提供的一张图：
上图详细概括了arm官方推荐的armv8的启动层次结构：
官方将启动分为了BL1，BL2，BL31，BL32，BL33阶段，根据顺序，芯片启动后首先执行BL1阶段代码，接着验签启动BL2，BL2根据具体设计启动BL31或者BL33，BL32只有在有BL31时才可能会存在并被验签加载启动。
armv8分为Secure World和Non-Secure World（Normal World），四种异常级别从高到低分别为EL3，EL2，EL1，EL0。
Secure World就是可以执行可信的firmware和app，比如密码支付，指纹识别等一系列依赖安全保证的服务。Non-Secure World就是我们常见的u-boot，linux，qnx等裸机程序或者操作系统。
EL3具有最高管理权限，负责安全监测和安全模式切换。EL2主要提供了对虚拟化的支持。EL1是一个特权模式，能够执行一些特权指令，用于运行各类操作系统，在安全模式则是可信任OS。EL0是无特权模式，所有APP应用都在EL0。
上图中的BL1，BL2，BL31，BL32，BL33分别对应如下功能：
BL1：是一切信任的根，一般就是固化在ROM中的一段启动加载代码，用于引导bl2，并对bl2进行验签保证可信任执行；
BL2：一般是在flash中的一段可信安全启动代码，它的可信建立在bl1对它的验证，主要完成一些平台相关的初始化，比如对ddr的初始化等，并在完成初始化后寻找BL31或者BL33进行执行；如果找到了BL31则不会继续调用BL33，如果没有BL31则BL33必须有；
BL31：BL31不像BL1和BL2是一次性运行的，它作为最后一道可信任固件存在，在系统运行时通过smc指令陷入EL3调用系统安全服务或者在Secure World和Non-Secure World之间进行切换；在完成BL31初始化后会去寻找BL32或者BL33进行验签后加载执行；
BL32：OPTee OS + 安全app，它是一个可信安全的OS运行在EL1并在EL0启动可信任APP（上述的指纹验证等app），并在Trust OS运行完成后通过smc指令返回BL31，BL31切换到Non-Seucre World继续执行BL33；
BL33：非安全固件，也就是我们常见的UEFI firmware或者u-boot也可能是直接启动Linux kernel；
启动BL1，BL2，BL31，BL32则是一个完整的ATF信任链建立流程（ARM Trusted Firmware），像常见的PSCI（Power State Coordination Interface）功能则是在ATF的BL31上实现；
最后一张图完整展示整个调用流程：
BL2根据是否存在BL31和BL32可选择性的加载不同firmware；
综上所述，可知u-boot是一个运行在非安全世界的bootloader，负责加载各类操作系统，并提供丰富的驱动接口；并根据是否存在安全固件还可以进行不同的boot流程，如下。
u-boot，u-boot-spl，u-boot-tpl的关系：对于一般嵌入式而言只需要一个u-boot作为bootloader即可，但是在小内存，或者有atf的情况下还可以有spl，tpl；
spl：Secondary Program Loader，二级加载器
tpl：Tertiary Program Loader，三级加载器
出现spl和tpl的原因最开始是因为系统sram太小，rom无法在ddr未初始化的情况下一次性把所有代码从flash，emmc，usb等搬运到sram中执行，也或者是flash太小，无法完整放下整个u-boot来进行片上执行。
所以u-boot又定义了spl和tpl，spl和tpl走u-boot完全相同的boot流程，不过在spl和tpl中大多数驱动和功能被去除了，根据需要只保留一部分spl和tpl需要的功能，通过CONFIG_SPL_BUILD和CONFIG_TPL_BUILD控制；
一般只用spl就足够了，spl完成ddr初始化，并完成一些外设驱动初始化，比如usb，emmc，以此从其他外围设备加载u-boot，但是如果对于小系统spl还是太大了，则可以继续加入tpl，tpl只做ddr等的特定初始化保证代码体积极小，以此再次从指定位置加载spl，spl再去加载u-boot。
从目前来看，spl可以取代上图中bl2的位置，或者bl1，根据具体厂商实现来决定，有一些芯片厂商会将spl固化在rom中，使其具有从emmc，usb等设备加载u-boot或者其他固件的能力。
当然在有atf的情况下可以由atf加载u-boot，或者由spl加载atf，atf再去加载u-boot。甚至在快速启动的系统中可以直接由spl启动加载linux等操作系统而跳过启动u-boot；在上图中arm官方只是给出了一个建议的启动信任链，具体实现都需要芯片厂商来决定；
后续分析启动流程中会在具有SPL和TPL的地方拓展它们的分叉执行路径尽量把SPL和TPL的功能也一并分析；
u-boot源码整体结构和一些编译配置方式 编译配置方式 u-boot使用了同Linux一样的编译配置方式，即使用kbuild系统来管理整体代码的配置和编译，通过defconfig来定制各种不同厂商的芯片bootloader二进制程序。编译只需要注意通过环境变量或者命令行参数的方式引入一个交叉编译工具即可：
CROSS_COMPILE：定义交叉编译工具链，可以是aarch64-linux-gnu-，arm-none-eabi-或者ppc-linux-gnu-等等；u-boot有几个配置是需要由对应board配置的。
SYS_ARCH，SYS_CPU，SYS_SOC，SYS_BOARD，SYS_VENDOR，SYS_CONFIG_NAME；一般在board/vendor/board/Kconfig中可全部定义，部分SYS_CPU，SYS_SOC也可以在arch/xxx/Kconfig中定义，根据这几个配置即可确定使用的cpu架构，厂商，板级信息，soc信息。
Makefile会自动根据上述信息进入对应目录组织编译规则，一般如果没有自己对应的这些board信息，需要自己在对应目录建立这些Kconfig和在configs中建立defconfig。
在configs目录中保存了uboot中所有支持的board配置，比如要使用rk3399的evb板的配置信息使用如下方式即可编译出来：
make CROSS_COMPILE=aarch64-linux-gnu- evb-rk3399_defconfig make 如果没有对应的defconfig可以找一个与自己板级信息类似的defconfig生成一个.config，再通过menuconfig来完成自己board的配置，并最后通过savedefconfig保存为自己board的defconfig：
make CROSS_COMPILE=aarch64-linux-gnu- evb-rk3399_defconfig make menuconfig make savedefconfig cp defconfig configs/my_defconfig 下面是evb rk3399的定义：
CONFIG_SYS_ARCH=&#34;arm&#34; CONFIG_SYS_CPU=&#34;armv8&#34; CONFIG_SYS_SOC=&#34;rk3399&#34; CONFIG_SYS_VENDOR=&#34;rockchip&#34; CONFIG_SYS_BOARD=&#34;evb_rk3399&#34; CONFIG_SYS_CONFIG_NAME=&#34;evb_rk3399&#34; 根据CONFIG_SYS_BOARD的定义还会为每个源文件自动包含include/configs/xxxx.h头文件，evb rk3399则是include/configs/evb_rk3399.h头文件。这个头文件可在其中定义board的一些关键配置，系统的ram大小，环境变量的起始地址和大小，GIC基地址，时钟频率，是否开启看门狗等定义，可根据具体需求来定义。
u-boot使用Kconfig和include/configs/xxx.h来灵活的确定u-boot编译流程及最终生成的文件。比如当定义CONFIG_SYS_CPU为&quot;armv8&quot;，CONFIG_SYS_ARCH为&quot;arm&quot;时，即确定了目标架构为armv8会自动根据Makefile进入对应目录进行编译链接。
u-boot源码结构 这里只对一些常用的目录进行说明：
 arch：各种架构的启动初始化流程代码，链接脚本等均在此目录对应的架构中存放； board：包含了大部分厂商的board初始化代码，基本平台化相关的代码都在对应的board目录中，早期的一些board代码在arch/xxx/xxx-mach中，现在基本不会放在arch目录下面了； cmd：包含了大量实用的u-boot命令的实现，比如md，cp，cmp，tftp，fastboot，ext4load等命令的实现，我们也可以在此处添加自己实现的命令； common：包含了u-boot的核心初始化代码，包括board_f，board_r，spl等一系列代码； configs：包含了所有board的配置文件，可直接使用； drivers：大量驱动代码的存放处； dts：编译生成dtb，内嵌dtb到u-boot的编译规则定义目录； env：环境变量功能实现代码； fs：文件系统读写功能的实现，里面包含了各类文件系统的实现； include：所有公用头文件的存放路径； lib：大量通用功能实现，提供给各个模块使用； net：网络相关功能的实现； scripts：编译，配置文件的脚本文件存放处； tools：测试和实用工具的实现，比如mkimage的实现代码在此处；  u-boot armv8链接脚本 在进行源码分析之前，首先看看u-boot的链接脚本，通过链接脚本可以从整体了解一个u-boot的组成，并且可以在启动分析中知道某些逻辑是在完成什么工作。
在armv8中，u-boot使用arch/arm/cpu/armv8/u-boot.lds进行链接。
u-boot-spl和u-boot-tpl使用arch/arm/cpu/armv8/u-boot-spl.lds进行链接，因为每个board的情况可能不同，所以u-boot可以通过Kconfig来自定义u-boot-spl.lds和u-boot-tpl.lds。
u-boot.lds /* SPDX-License-Identifier: GPL-2.0+ */ /* * (C) Copyright 2013 * David Feng &lt;fenghua@phytium.com.cn&gt; * * (C) Copyright 2002 * Gary Jennejohn, DENX Software Engineering, &lt;garyj@denx.de&gt; */ #include &lt;config.h&gt;#include &lt;asm/psci.h&gt; OUTPUT_FORMAT(&#34;elf64-littleaarch64&#34;, &#34;elf64-littleaarch64&#34;, &#34;elf64-littleaarch64&#34;) OUTPUT_ARCH(aarch64) ENTRY(_start) -------------------------------------------------------------------- (1) /* *（1）首先定义了二进制程序的输出格式为&#34;elf64-littleaarch64&#34;， * 架构是&#34;aarch64&#34;，程序入口为&#34;_start&#34;符号； */ SECTIONS { #ifdef CONFIG_ARMV8_SECURE_BASE -------------------------------------------------- (2) /* *（2）ARMV8_SECURE_BASE是u-boot对PSCI的支持，在定义时可以将PSCI的文本段， * 数据段，堆栈段重定向到指定的内存，而不是内嵌到u-boot中。 * 不过一般厂商实现会使用atf方式使其与bootloader分离，这个功能不常用； */ /DISCARD/ : { *(.rela._secure*) } #endif  . = 0x00000000; -------------------------------------------------------------- (3) /* *（3）定义了程序链接的基地址，默认是0，通过配置CONFIG_SYS_TEXT_BASE可修改 * 这个默认值。 */ . = ALIGN(8); .text : { *(.__image_copy_start) --------------------------------------------------- (4) /* *（4）__image_copy_start和__image_copy_end用于定义需要重定向的段， * u-boot是一个分为重定向前初始化和重定向后初始化的bootloader， * 所以此处会定义在完成重定向前初始化后需要搬运到ddr中数据的起始地址和结束地址； * * 大多数时候u-boot是运行在受限的sram或者只读的flash上， * u-boot为了启动流程统一会在ddr未初始化和重定位之前不去访问全局变量， * 但是又为了保证u-boot能够正常读写全局变量，内存，调用各类驱动能力， * 所以u-boot将启动初始化分为了两个部分，重定向前初始化board_f和 * 重定向后初始化 board_r，在重定向之前完成一些必要初始化， * 包括可能的ddr初始化，然后通过__image_copy_start和__image_copy_end * 将u-boot搬运到ddr中，并在ddr中进行重定向后初始化，这个时候的u-boot就可以 * 正常访问全局变量等信息了。 * * 如果想要在board_f过程中读写一些全局变量信息该怎么办呢？ * u-boot通过定义global_data（gd）来完成此功能， * 后续在分析到时会详细讲解实现方式。 */ CPUDIR/start.o (.text*) -------------------------------------------------- (5) /* *（5）定义了链接程序的头部文本段，armv8就是 * arch/arm/cpu/armv8/start.S， * start.S中所有文本段将会链接到此段中并且段入口符号就是_start； */ } /* This needs to come before *(.text*) */ .efi_runtime : { ------------------------------------------------------------ (6) /* *（6）在定义了efi运行时相关支持时才会出现使用的段，一般不用关心； */ __efi_runtime_start = .; *(.text.efi_runtime*) *(.rodata.efi_runtime*) *(.data.efi_runtime*) __efi_runtime_stop = .; } .text_rest : ---------------------------------------------------------------- (7) /* *（7）除了start.o，其他的所有文本段将会链接到此段中； */ { *(.text*) } #ifdef CONFIG_ARMV8_PSCI -------------------------------------------------------- (8) /* *（8）同（2），是PSCI相关功能的支持，一般不会使用； */ .__secure_start : #ifndef CONFIG_ARMV8_SECURE_BASE  ALIGN(CONSTANT(COMMONPAGESIZE)) #endif  { KEEP(*(.__secure_start)) } #ifndef CONFIG_ARMV8_SECURE_BASE #define CONFIG_ARMV8_SECURE_BASE #define __ARMV8_PSCI_STACK_IN_RAM #endif  .secure_text CONFIG_ARMV8_SECURE_BASE : AT(ADDR(.__secure_start) + SIZEOF(.__secure_start)) { *(._secure.text) . = ALIGN(8); __secure_svc_tbl_start = .; KEEP(*(._secure_svc_tbl_entries)) __secure_svc_tbl_end = .; } .secure_data : AT(LOADADDR(.secure_text) + SIZEOF(.secure_text)) { *(._secure.data) } .secure_stack ALIGN(ADDR(.secure_data) + SIZEOF(.secure_data), CONSTANT(COMMONPAGESIZE)) (NOLOAD) : #ifdef __ARMV8_PSCI_STACK_IN_RAM  AT(ADDR(.secure_stack)) #else  AT(LOADADDR(.secure_data) + SIZEOF(.secure_data)) #endif  { KEEP(*(.__secure_stack_start)) . = . + CONFIG_ARMV8_PSCI_NR_CPUS * ARM_PSCI_STACK_SIZE; . = ALIGN(CONSTANT(COMMONPAGESIZE)); KEEP(*(.__secure_stack_end)) } #ifndef __ARMV8_PSCI_STACK_IN_RAM  . = LOADADDR(.secure_stack); #endif  .__secure_end : AT(ADDR(.__secure_end)) { KEEP(*(.__secure_end)) LONG(0x1d1071c); /* Must output something to reset LMA */ } #endif  . = ALIGN(8); .rodata : { *(SORT_BY_ALIGNMENT(SORT_BY_NAME(.rodata*))) } ------------------- (9) /* *（9）所有仅读数据将会在这个段中对齐排序存放好； */ . = ALIGN(8); .data : { -------------------------------------------------------------------- (10) /* *（10）所有数据段将会链接到此段中； */ *(.data*) } . = ALIGN(8); . = .; . = ALIGN(8); .u_boot_list : { ------------------------------------------------------------- (11) /* *（11）u_boot_list段定义了系统中当前支持的所有命令和设备驱动，此段把散落在各个文件中 * 通过U_BOOT_CMD的一系列拓展宏定义的命令和U_BOOT_DRIVER的拓展宏定义的设备驱动收集到一起， * 并按照名字排序存放，以便后续在命令行快速检索到命令并执行和检测注册的设备和设备树匹配 * probe设备驱动初始化；（设备驱动的probe只在定义了dm模块化驱动时有效） */ KEEP(*(SORT(.u_boot_list*))); } . = ALIGN(8); .efi_runtime_rel : { __efi_runtime_rel_start = .; *(.rel*.efi_runtime) *(.rel*.efi_runtime.*) __efi_runtime_rel_stop = .; } . = ALIGN(8); .image_copy_end : { *(.__image_copy_end) } . = ALIGN(8); .rel_dyn_start : -------------------------------------------------------- (12) /* *（12）一般u-boot运行时是根据定义的基地址开始执行，如果加载地址和链接地址 * 不一致则会出现不能执行u-boot的问题。通过一个 * 配置CONFIG_POSITION_INDEPENDENT即可打开地址无关功能， * 此选项会在链接u-boot时添加-PIE参数。此参数会在u-boot ELF文件中 * 生成rela*段，u-boot通过读取此段中表的相对地址值与实际运行时地址值 * 依次遍历进行修复当前所有需要重定向地址，使其可以实现地址无关运行； * 即无论链接基地址如何定义，u-boot也可以在任意ram地址 * 运行（一般需要满足最低4K或者64K地址对齐）； * * 注意此功能只能在sram上实现，因为此功能会在运行时修改文本段数据段中的地址， * 如果此时运行在片上flash，则不能写flash，导致功能失效无法实现地址无关； */ { *(.__rel_dyn_start) } .rela.dyn : { *(.rela*) } .rel_dyn_end : { *(.__rel_dyn_end) } _end = .; . = ALIGN(8); .bss_start : { -------------------------------------------------------- (13) /* *（13）众所周知的bbs段； */ KEEP(*(.__bss_start)); } .bss : { *(.bss*) . = ALIGN(8); } .bss_end : { KEEP(*(.__bss_end)); } /DISCARD/ : { *(.dynsym) } -------------------------------------------- (14) /* *（14）一些在链接时无用需要丢弃的段； */ /DISCARD/ : { *(.dynstr*) } /DISCARD/ : { *(.dynamic*) } /DISCARD/ : { *(.plt*) } /DISCARD/ : { *(.interp*) } /DISCARD/ : { *(.gnu*) } #ifdef CONFIG_LINUX_KERNEL_IMAGE_HEADER ----------------------------------- (15) /* *（15）在efi加载时会很有用，主要在u-boot的二进制头部添加了一些头部信息， * 包括大小端，数据段文本段大小等，以便于efi相关的加载器读取信息， * 此头部信息来自于Linux arm64的Image的头部信息；该头部也不属于u-boot的 * 一部分只是被附加上去的； */ #include &#34;linux-kernel-image-header-vars.h&#34;#endif } u-boot-spl.lds 此链接脚本是标准的spl链接脚本，还包含了u_boot_list段，如果对应自己board不需要命令行或者模块化驱动设备，只作为一个加载器则可以自定义更简略的链接脚本。
/* SPDX-License-Identifier: GPL-2.0+ */ /* * (C) Copyright 2013 * David Feng &lt;fenghua@phytium.com.cn&gt; * * (C) Copyright 2002 * Gary Jennejohn, DENX Software Engineering, &lt;garyj@denx.de&gt; * * (C) Copyright 2010 * Texas Instruments, &lt;www.ti.com&gt; * Aneesh V &lt;aneesh@ti.com&gt; */ MEMORY { .sram : ORIGIN = IMAGE_TEXT_BASE, ---------------------------------------- (1) /* *（1）\&gt;XXX 的形式可以将指定段放入XXX规定的内存中；一般u-boot-spl只有 * 很小的可运行内存块，所以spl中会舍去大量不需要用的段只保留关键的 * 文本段数据段等，并且通过&gt;.sram的形式将不在ddr初始化前用到的段定义到sdram中， * 后续只需在完成ddr初始化后将这些段搬运到ddr中即可，而不需要额外的 * 地址修复逻辑，如下：有一个sram 0x18000-0x19000， * 一个sdram 0x80000000 - 0x90000000， * 那么通过&gt;.sram方式则map文件可能如下： * 0x18000 stext * ... * 0x18100 sdata * ... * 0x80000000 sbss * ... */ LENGTH = IMAGE_MAX_SIZE } MEMORY { .sdram : ORIGIN = CONFIG_SPL_BSS_START_ADDR, LENGTH = CONFIG_SPL_BSS_MAX_SIZE } OUTPUT_FORMAT(&#34;elf64-littleaarch64&#34;, &#34;elf64-littleaarch64&#34;, &#34;elf64-littleaarch64&#34;) OUTPUT_ARCH(aarch64) ENTRY(_start) -------------------------------------------------------------------- (2) /* *（2）同u-boot.lds一致，共用一套逻辑入口_start； */ SECTIONS { .text : { . = ALIGN(8); *(.__image_copy_start) -------------------------------------------------- (3) /* *（3）同样的，如果spl需要重定向则会使用此段定义，大多数情况下spl中会用上重定向； */ CPUDIR/start.o (.text*) *(.text*) } &gt;.sram .rodata : { . = ALIGN(8); *(SORT_BY_ALIGNMENT(SORT_BY_NAME(.rodata*))) } &gt;.sram .data : { . = ALIGN(8); *(.data*) } &gt;.sram #ifdef CONFIG_SPL_RECOVER_DATA_SECTION ---------------------------------------- (4) /* *（4）SPL_RECOVER_DATA_SECTION段用于保存数据段数据， * 一些board在初始化时修改data段数据，并在后续某个阶段 * 从此段中恢复data的原始数据； */ .data_save : { *(.__data_save_start) . = SIZEOF(.data); *(.__data_save_end) } &gt;.sram #endif  .u_boot_list : { . = ALIGN(8); KEEP(*(SORT(.u_boot_list*))); } &gt;.sram .image_copy_end : { . = ALIGN(8); *(.__image_copy_end) } &gt;.sram .end : { . = ALIGN(8); *(.__end) } &gt;.sram _image_binary_end = .; .bss_start (NOLOAD) : { . = ALIGN(8); KEEP(*(.__bss_start)); } &gt;.sdram -------------------------------------------------------------- (5) /* *（5）将bss段数据定义到&gt;.sdram中，即可在初始化ddr后直接对此段地址清零 * 即可使用全局未初始化变量，并且不会带来副作用。 */ .bss (NOLOAD) : { *(.bss*) . = ALIGN(8); } &gt;.sdram .bss_end (NOLOAD) : { KEEP(*(.__bss_end)); } &gt;.sdram /DISCARD/ : { *(.rela*) } /DISCARD/ : { *(.dynsym) } /DISCARD/ : { *(.dynstr*) } /DISCARD/ : { *(.dynamic*) } /DISCARD/ : { *(.plt*) } /DISCARD/ : { *(.interp*) } /DISCARD/ : { *(.gnu*) } } 从上述的链接脚本可以看出，armv8的u-boot的启动是从arch/arm/cpu/armv8/start.S中的_start开始的，并在后续初始化中调用了很多链接脚本中定义的地址符号表。
]]></content>
  </entry>
  
  <entry>
    <title>通俗易懂讲讲 通信原理</title>
    <url>/post/hardware/easy-to-understand-communication-principle.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>Communication Principle</tag>
    </tags>
    <content type="html"><![CDATA[学了通信原理这门课，一开始觉得很难，这里用我自己的学习过程以及对通信系统的了解来说明这些技术的应用。
上面是我画的认为比较完整的通信系统的简单流程图，对此我做一翻解释。
首先日常生活中的信号总是模拟的，我们把这些信号通过滤波等处理，得到带限的信号，这里以基带信号singnal为例子,signal 经过采样保持电路，我们就得到PAM信号，如图
这样的信号就是离散信号了。
离散信号经过量化归属到个档次的幅度中比如我们有2V,4V,6,V,8V四个档次的归类，并且规定1V~3V之间的PAM离散信号就归类到2V的档次中去,一次类推，通过比较给每个PAM信号进行归类，这就是量化。
之后将量化了的信号进行编码，编码是一种认为规定的过程比如我们规定2V用00表示，4V用01表示，6V用10表示，而8V用11来表示,这样就把阶梯信号和二进制信号有了一种对应关系，顺着这种对应关系，我们可以得到刚才量化了的信号的二进制代码，这就是PCM编码得到了可以在存储器中存储的数字信号。
以上从模拟到数字信号的一种转变就是我们常说的A/D转换。至于我们平时要求的转换比特率的求法可以从它的转换过程得出计算方法。一个PAM信号对应一个档次，而一个档次对应几个比特的数字是在编码中体现的，例子中就是一个档次对应两个比特，假设这种对应关系是1对N个比特，对模拟信号的采样率是F,也就是1秒钟有F个PAM信号，这F个PAM信号就要被转换成FN个比特，所以比特率就是FN了。
对于完成转换的数字信号，我们如何处理呢？有的是被放进存储器中存储了，有的是到CPU中进行计算，加密等处理了。
通常为了达到通信目的，我们就要将数字信号传递并且转换成模拟信号，毕竟在生活中模拟信号才是我们可以识别的。
所以我们从存储器中读取数字信号，这些信号是基带信号，不容易传输，经过数字调制系统就可以转换成高频信号而被发送设备以各种形式比如微波，光信号传播出去。发送这些高频信号的速度关系到发送的比特率注意与前面的转换的比特率有不同。假如整个发送端可以发送四中波形A,B,C,D，它们可以分别表示发送了00，01，10，11信号，那么我们就说发送一个符号（即波形）就是发送了两个比特了。由此得到符号率与比特率的关系B=N*D.D是符号率baud/s, B是比特率bit/s, N表示一个符号与N个比特对应。
接收设备将这些信号转换成电信号，通过解调器，就可以还原基带信号，同样可以将它们放进存储器存储，这可以理解成网络视频在我们的电脑上的缓存。缓存中的信号通过解码器，也就是与编码器功能相反的器件将数字序列转换成各种量化的台阶（档次）信号。
最后将台阶信号进行填充恢复，我们就又可以原来的输入的模拟波形了，由此我们完成一次通信。
如果模拟信号不需要数字化，那么我们可以进行模拟调制，同样可以发送出去，这个过程要简单很多。
当然，这里所讲的只是我们学习中所涉及的一些概念，完整的通信系统还有更多要考虑的，这只是我觉得通信过程的关键的骨架问题。
还有几个概念是对它们的理解和总结，希望可以和大家分享。
二进制比特率与信息量中的比特率。 因为我们假定二进制信号是等概率发生的，也就是P=0.5，而信息量的定义是这样的I=-log2(p)bit,通过此式，我们可以计算发送的一个二进制符号的信息量I=-log2(0.5)bit=1 bit,所以我们通常说一个0或者1就是一个比特了。
方波的带宽问题。 由上图我们可以注意到，一个持续时间为T的方波，它的频谱是一个SINC函数，零点带宽是1/T，即时间的倒数。当然，方波的带宽是无限大的，因此这样的波形在现实中是很难实现的，我们只能给方波提供一定的带宽，就是说得到的肯定只能是经过了过滤的波形。
在这里我们可以联系到吉布斯现象。我们可以这样理解：频率越大，就说明变化越快，而方波的转折点处就是一个极快的变化也就是有频带的高频部分构成，而经过带限的滤波之后，高频被滤去，得到的波形在转折点处就变化慢下来，于是在需要变化快的地方（如方波的转折点）变化慢，由此产生吉布斯现象。
升余弦滚降滤波器。 我们知道升余弦滚降滤波器是防止码间串扰而设计的。码间串扰是指各个时间点上发送的符号并非准确的方波，而是在规定的时间内仍有余波，于是对下一个时刻发送的符号产生影响，最后可能因为影响的叠加效果而使后果严重，得到相反的采样结果。注意我们这里讲的码间串扰都是发生在基带频率上的。因此升余弦滚降滤波器也是在基带上的应用。
下图是升余弦滚降滤波器的原理图，上半部分是滤波器的频谱相应图，下半部分是滤波结果在时间域上的波形图。
我们可以这样思考，发送的基带波形是在一定的带限内的，假如说要求发送的符号率是D，那么图下半部分中可知1/2f0=1/D，所以f0=1/(2D), 或者说D=2 f0,由下半图我们可以看出我们发送的符号的频率是2* f0，这串符号在频谱上的表示（上半图）是个带宽为f0的信号，这个就是采样定理中说的当波形用SINC函数来表示时，符号率是该波形的带宽的两倍，也就是升余弦滚降滤波器在r=0的时候的特性。
当然，我们这里表示的只是发送一个符号的波形的带宽，但是我们可以这样想象，一个系统在任何时候发送符号是使用的带宽f0都是固定的，在1时间段内发送的波形的带宽在f0以内，那么我们完全有理由相信在2时间段内发送的波形的带宽必然在f0以内，所以这样可以理解多个符号组成的波形的带宽是在f0以内的。
从下半图我们可以看到，随着r的增加，符号波形在一个周期段以外的衰减就会加快，这里我们就可以看到它对码间串扰的影响会减小，这个就是升余弦滚降滤波器的作用，但是我们必须清楚的看到，符号率是不变的2* f0,而系统的绝对带宽在增加。根据升余弦滚降滤波器的定义我们得到这样一个关系D=2* f0/(1+r)。从以上的分析过程我们可以认为1/2*f0就是发送的数字信号的周期，也就是对于同样周期的信号我们需要不同的带宽，这个带宽就是发送的数字信号的带宽，而与原始的模拟波的带宽无关。
调制的一些想法。 在学习调制的过程中，我一直搞不清什么是调制信号，什么是载波。最后总算明白，原来（一般来讲）调制就是将低频信号（调制信号）携带的信息在另外一个高频的信号（载波）上表现出来，表现的方法可以是改变载波的幅度或者相位或者频率等。当我们看到调制完成的波形是，发现它与载波有不同的幅度或者相位或者频率，从这里的变化我们极可以判断处调制信号有那些信息。载波就是用来携带低频信号要表达的意思的高频信号。之所以用高频是因为在一般情况下高频信号便于传输。
以上是我在学习通信原理中觉得关键要明白的只是点，这样知识才可以融会贯通。
]]></content>
  </entry>
  
  <entry>
    <title>C语言while(1) 和 for ( ; ; )的区别</title>
    <url>/post/programming/difference-while-1-and-for-loop.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>C</tag>
      <tag>Embedded</tag>
    </tags>
    <content type="html"><![CDATA[while(1) 和 for(;;) 它们不都是无限循环吗，作用应该一样啊，它们到底有什么区别？
要回答这个问题，其实你各自编写一段while(1) 和 for(;;)的代码，编译对比一下代码大小和汇编文件，你就大概知道了。
while(1)和for(;;)语法表达 这里先说一下while(1)和for(;;)语法表达式。
while语法表达 while (表达式) { 语句 } 其中：
 表达式：是循环条件 语句：为循环体。  while语句的语义是：计算表达式的值，当值为真(非0)时， 执行循环体语句。其执行过程可用下图表示：
for语法表达 for(表达式1; 表达式2; 表达式3) { 语句 } 它的执行过程如下：
先求解表达式1 求解表达式2 若其值为真（非0），则执行for语句中指定的内嵌语句，然后执行下面第3）步； 若其值为假（0），则结束循环，转到第5）步。
求解表达式3 转回上面第2）步继续执行。 循环结束，执行for语句下面的一个语句。 执行过程可用下图表示：
while(1)和for(;;)异同点 这里先说一下结论，然后再验证验证结论。
相同点 作用和效果都一样：都是实现无限循环的功能。
不同点 while(1)：其中括号里面是一个条件，程序会判断真假。而括号里面的“1”永远是一个“真值”。
其中，每一次循环，编译器都要判断常量1是不是等于零。
for(;;)：这两个;;空语句，编译器一般会优化掉的，直接进入死循环。
根据上面的描述，你可能会觉得：while(1) 比 for(;;) 要做更多事，汇编代码更多，代码量也更大。
但事实是这样吗？下面验证一下。
验证while(1)和for(;;)差异 我们编写分别两个文件for.c和while.c，然后分别生成汇编代码，看下情况。
源代码 while.c：
for.c：
生成汇编 我们这里使用gcc编译器生成汇编，执行命令如下：
while汇编代码：
for汇编代码：
你会发现，除了文件名不同，其余都相同。
当然，这里额外说一下，不同代码、不同编译器，以及不同优化等级，可能最终结果有所差异。
]]></content>
  </entry>
  
  <entry>
    <title>Intel全线涨价！12/13/14代、酷睿Ultra无一例外</title>
    <url>/post/datacenter/intel-raises-prices-across-the-board.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Core</tag>
      <tag>Alder Lake</tag>
      <tag>Raptor Lake</tag>
    </tags>
    <content type="html"><![CDATA[据报道，Intel即将对旗下处理器全线涨价，现有的和即将发布的，无一例外。
权威德国硬件媒体PCGH从两个不同的德国供应商处获悉，他们都收到了Intel的通知信，包括在库、打折的处理器，都会涨价，这让他们感到非常意外。
已经在售的Alder Lake 12代酷睿、Raptor Lake 13代酷睿，即将推出的Raptor Lake Refresh 14代处理器、Meteor Lake一代酷睿Ultra处理器，都在涨价范围之内。
不过，Intel并没有明确涨价的市场区域。
德国供应商和德国媒体都怀疑，这次涨价有可能是区域性的，甚至仅限德国，原因就是Intel在德国建厂需要钱，需要大量的钱。
根据Intel IDM 2.0战略，他们将投资上千亿美元在美国、欧洲兴建多座晶圆厂，包括德国马格德堡，最初计划投资170亿欧元，后来一路飙升到300亿欧元。
Intel要求德国政府补贴100亿欧元，自己投资200亿欧元。
就在近日，德国政府基本确定拨款200亿欧元支持国内的半导体制造业，其中Intel就拿走一半。
如果Intel真的是因为在德国投资建厂就全线涨价，德国消费者就真的有点欲哭无泪了。
涨价就算了，关键是新品进步也不大。
Raptor Lake Refresh 14代酷睿只会是13代的升级版，不同型号增加核心、提升频率、扩大缓存，大概率还会拉高内存频率支持。
不过，最初的曝料看起来很美好，后来却被发现并非如此，i5系列让人失望之后i3系列也翻车了。
早先说法称，14代酷睿i3系列包括i3-14300、i3-143100/F，从4核心升级为6核心，而且是全系列唯一仅有大核心而没有小核心的。
但是根据最新消息，14代酷睿i3系列依然是4核心，而且目前只能确认有i3-14100/F，三级缓存12MB，主频最高4.7GHz，相比13代、12代分别提高200MHz、600MHz。
i3-14300是否还有暂时无法完全确认，13代就缺失了这个档位，12代则是4.4GHz频率，这一代如果有的话怎么也得做到4.9GHz。
对于14代酷睿i5系列，最初的说法是i5-14600系列都升级为8+8 16核心、i5-14500/14400系列都升级为6+8 14核心。
但后来也变了，现在也得到确认：i5-14600/14500系列停留在6+8 14核心，三级缓存都是24MB，最高频率5.2GHz、5.0GHz。
i5-14400系列停留在6+4 10核心，三级缓存20MB，最高频率4.7GHz。
再往上就好说了，酷睿i7系列8+8核心升级为8+12核心、三级缓存33MB，其中i7-14700/F最高频率5.4GHz。
酷睿i9系列自然就是8+16核心，i9-14900K/KF可以加速到6.0GHz，i9-14900/F可以加速到5.8GHz。
传说还有i9-14900KS，频率达到史无前例的6.2GHz，但暂无进一步消息，可能要看能不能挑到足够多超好体质的芯片了。
]]></content>
  </entry>
  
  <entry>
    <title>网络安全技术的演进</title>
    <url>/post/datacenter/the-evolution-of-network-security.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Network</tag>
    </tags>
    <content type="html"><![CDATA[自诞生以来， 网络安全  领域经历了重大变革，本文回顾了网络安全的历史，重点介绍了其中几个重要里程碑，包括防火墙、VPN、下一代防火墙（NGFW）、零信任模型等技术的发展。此外，文中还讨论了诸如人工智能驱动的安全性和安全访问服务边缘（SASE）等趋势。最后，探讨了通用人工智能（GenAI）对网络安全的影响。
1980年代：防火墙的兴起 在网络技术发展的早期，安全措施还处于初级阶段，主要侧重于物理安全和简单的访问控制。随着网络的扩展，对更强大的安全措施的需求变得愈发明显。
包过滤防火墙：第一代防火墙主要侧重于数据包过滤。第一个防火墙系统称为“包过滤器”，由数字设备公司 (DEC) 于 20 世纪 80 年代末推出。这种最初的防火墙实现是在网络层操作的，通过决定是否允许或阻止特定的数据包来筛选网络流量。数据包的处理依据一套预设的规则进行，这些规则包括源IP地址、目标IP地址、第四层头部中的源端口和目标端口以及协议字段等标准。对于每个数据包的处理动作是允许或者拒绝其通过。这些规则构成了包过滤器的访问控制策略，并需要手动维护并推送到系统中。
这种基本的防火墙是在通用 CPU 上运行的软件中实现的。尽管它缺乏状态检查或深度包分析的能力，但它开启了防火墙演进的大门，为后续几代更高级和复杂的防火墙技术铺平了道路。随着时间的推移，包过滤防火墙继续进化，支持更复杂的规则集，如 IP 地址和端口号范围的最长前缀匹配。从2000年代初开始，这些防火墙也从CPU中的软件实现转变为ASIC中的原生实现。
1990 年代：状态检测防火墙 上世纪90年代初，状态检测防火墙的出现标志着防火墙技术的一次重大进步。它们能够追踪活跃连接的状态，并根据流量的上下文做出过滤决策，例如判断一个数据包是否属于现有连接的一部分还是一个新的连接请求。第一个商业化的状态检查防火墙是Check Point公司的FireWall-1，于1993年发布。该公司创造了“状态检查”这个术语。
状态防火墙维护状态表会记录正在进行的连接的详细信息，如源和目标IP地址、端口以及连接/会话状态（例如已建立、监听、关闭等）。这使得它们能够根据现有的连接信息做出明智的决策，决定是否允许数据包通过，确保只有与活动连接匹配的数据包才能通过防火墙。这种追踪连接状态的能力有助于防御各种类型的攻击所带来的未授权访问。
想象一下，一个攻击者试图劫持一个TCP连接以访问防火墙后面的Web服务器。使用无状态包过滤防火墙时，每个数据包都是独立检查的，仅基于源/目标IP地址和端口。无状态防火墙无法区分合法流量和攻击者伪造的数据包试图劫持已建立的连接。
相比之下，当攻击者的伪造数据包到达状态检查防火墙时，由于这些数据包并非源自与任何已跟踪连接相关的真实源IP地址，因此它们不会在状态表中有匹配的条目，从而被视为异常。防火墙随后会丢弃这些恶意数据包，防止劫持尝试的发生。
另一个例子是 SYN 泛洪攻击，攻击者向服务器发送许多 SYN 数据包（用于建立 TCP 连接的数据包），但不响应服务器发送的SYN-ACK数据包中的最终ACK。这会导致服务器上有许多半开的连接，消耗服务器资源，并阻止合法连接的建立。状态检查防火墙通过拦截TCP握手过程来缓解这种情况，在接收到每个SYN数据包时生成一个特殊的响应（SYN cookie）给客户端。当客户端发送ACK时，防火墙就会与服务器建立会话。防火墙还会对来自每个IP地址的SYN数据包进行速率限制，以减少泛洪攻击。
因此，状态检查防火墙通过理解网络连接的状态提供了更好的安全性。通过维护全面的状态表，它们可以区分合法流量与攻击，如IP欺骗、端口扫描或未授权连接。
随着网络流量的增长，会话查找的硬件加速和多核处理的会话设置使这些防火墙能够处理更高的流量和更复杂、更大的状态表。负载均衡和主动-被动故障切换也被引入到这些带有状态检查防火墙的系统中，以确保持续运行和可靠性。
1990 年代：入侵检测和入侵防御系统 (IDS/IPS) 入侵检测系统（IDS）的概念出现是为了应对防火墙的局限性。IDS能够检查数据包的内容（有效载荷）并检测可疑活动，从而提供额外的安全层。IDS非常依赖基于签名的检测方法，这种方法会在数据包的内容中搜索与数据库中任何签名匹配的情况。这些签名可能是有效载荷或数据包头部中的字节序列、文件哈希值，以及与恶意软件服务器之间的命令与控制通信模式。基于签名的检测可以准确地识别已知威胁，但它无法检测新的未知威胁（零日攻击），因为这些威胁与现有的签名不匹配。其有效性取决于定期更新签名数据库以包括新出现的威胁。
后来，入侵防御系统（IPS）被引入，不仅能够检测威胁，而且还能根据配置自动阻止或减轻威胁，这样可以减少响应时间和潜在损害。
1990 年代：应用代理防火墙 1990 年代中期，出现了应用代理防火墙，通常也称为代理防火墙。这些防火墙比状态检测防火墙更进一步，不允许通信直接穿过受保护的环境。它们在客户端与目标网络中的服务器之间建立代理连接，通过这个连接路由流量，避免两者之间直接相连。这些防火墙结合了状态检查防火墙的功能与应用层（OSI模型中的第7层）的检查和过滤能力。它们能够理解和执行基于特定应用协议及其有效载荷的规则。这使得应用代理防火墙能够：
 进行深度包检查和内容过滤：应用代理防火墙可以在应用层执行深度包检查，分析网络流量的实际内容和有效载荷。这使它们能够更有效地检测和阻止恶意内容、恶意软件及未授权活动。 细粒度的访问控制和用户身份验证：它们可以根据用户身份、角色和认证来实施细粒度的访问控制政策。 改善网络隔离和分割：代理防火墙作为内部网络与互联网之间的中介，阻止直接连接。这种隔离使得攻击者更难直接访问内部资源。 IP地址屏蔽和匿名性：作为中介，代理防火墙隐藏内部客户端的真实IP地址，提供匿名性。这使得攻击者更难以识别和针对网络中的特定设备。 安全缓存和性能优化：许多代理防火墙实现缓存机制来本地存储经常访问的Web内容。这可以提高性能，同时实现内容检查。  简而言之，应用代理防火墙的代理架构、深度包检查能力、缓存和身份验证集成相比传统的状态检查防火墙提供了一层额外的安全保障。这些防火墙还受益于深度包检查和其他特性的硬件加速。
到了2010年代后期，在云原生时代，基于云的应用防火墙提供了与传统防火墙类似的能力，但它们设计用于保护云原生应用和服务。这些基于云的防火墙适应了容器环境中东西向流量的检查和安全，确保在微服务内部强制执行安全策略。
1990 年代中期：VPN 和安全远程访问的兴起 虚拟专用网络 (VPN) 技术在安全性较低的网络（例如公共互联网）上创建了安全加密的连接。这种安全连接通常称为“隧道”，允许用户像直接连接到私有安全网络一样发送和接收数据。最早的VPN技术可以追溯到微软开发的PPTP（Point-to-Point Tunneling Protocol）协议。PPTP允许用户通过调制解调器拨号，并通过IP/Ethernet创建隧道，安全地连接到企业网络。
随着时间的推移，IPsec和SSL VPN成为通过公共网络创建安全连接的标准协议。
 SSL VPN使用SSL或其后继者传输层安全（TLS）协议，在互联网上创建安全且加密的连接。这项技术使用户能够通过标准的Web浏览器安全地远程访问私有网络和资源。 IPsec VPN使用IPsec协议套件建立安全且加密的连接。这种VPN通常用于站点到站点的连接。例如，一个组织可以通过IPsec VPN在其两个分支机构之间建立连接。  企业广泛采用了VPN技术，以便让在家办公或外出工作的员工能够安全地远程访问。然而，VPN可能会引入延迟并降低连接速度，这是因为加密带来的额外开销以及流量需要通过VPN服务器进行额外的路由。2000年代后期，防火墙设备中IPsec加密/解密和隧道终止的原生硬件实现帮助在一定程度上缓解了性能问题。
2000年代初期：统一威胁管理 (UTM) 到2003年，产生的数据量超过了以往所有信息的总和。这种数据爆炸也增加了网络攻击的风险。UTM设备在2000年代初期应运而生，它将多种安全功能（如防火墙、IDS/IPS、防病毒和内容过滤）集成到一个单一的设备中。
2000 年代中期：下一代防火墙 (NGFW) 2000年代中期，出现了高级持续性威胁（APT）。这些复杂的威胁需要新的防御策略和技术。频繁的零日漏洞利用（以前未见过的威胁）和其他威胁也导致了下一代防火墙（NGFW）硬件设备的出现。
NGFW是防火墙的一种高级形式，它包含了上述所有功能，即数据包过滤、状态/应用防火墙、IDS/IPS、NAT、VPN等，以极高的性能提供增强的安全功能和对网络流量的更精细控制。
下一代防火墙的主要功能  应用识别和控制：NGFW能够识别并控制应用程序，无论这些应用程序使用的端口、协议或IP地址如何。这意味着它们可以根据应用程序本身而非仅仅是网络级别的属性来执行策略。 身份感知：NGFW 可以与身份管理系统（例如 Active Directory）集成，根据用户身份而非 IP 地址应用策略。这允许更精细的访问控制和审计。 深度数据包检测：NGFW 执行深度数据包检测来分析数据包内容，从而检测并阻止隐藏在数据有效负载中的威胁。如 IDS 部分所述，这种检测通常是基于签名的。检查每一个数据包可能会占用大量资源，并且由于性能限制，在高吞吐量网络中并不总是可行。管理员通常会配置安全策略，根据应用程序、用户、源/目标IP地址和端口等因素指定哪些流量应该被检查。这些策略决定了是否需要对特定类型的流量进行更深入的内容检查。这种检查通常在完成状态检查和包过滤之后进行，这样也可以减轻深度包检查引擎的负担。 策略执行：基于应用、用户和内容执行策略的能力使 NGFW 能够应用更精确的安全控制。这种精细度有助于最大限度地降低未经授权的访问和数据泄露的风险。 SSL/TLS 检查：如果允许，NGFW 可以检查加密流量（SSL 代理）以检测可能隐藏在 SSL/TLS 会话中的威胁。这一点至关重要，因为大部分互联网流量都是加密的，如果不检查流量，威胁很容易逃避检测。 威胁情报集成：NGFW 通常与威胁情报服务集成，提供有关新兴威胁的最新信息。这使防火墙能够动态更新其威胁数据库并有效阻止已知的恶意流量。 自动响应：NGFW 可以通过隔离受感染的设备、阻止恶意流量和向管理员发出警报来自动响应检测到的威胁。它们还集成了沙箱沙盒等高级恶意软件防护功能，有助于快速有效地减轻攻击的影响。 分段和微分段：NGFW 支持网络分段，将网络划分为更小、隔离的分段（区域），以限制威胁的传播。微分段更进一步，在网络内创建高度精细的安全区域，从而增强安全性。 网络功能：许多 NGFW 还支持 L2/L3 转发功能。通过集成路由和交换功能，NGFW 可以减少对单独设备的需求，从而简化网络设计和管理。将功能整合到单个设备中可降低硬件成本并降低网络基础设施的复杂性。  因此，通过将多种安全和网络功能整合到单个设备中，NGFW提供了针对各种威胁的全面保护。它们还提供了对网络流量和用户活动更好的可见性，使管理员能够洞察潜在的安全问题并应用有效的控制措施。
大多数NGFW都有专门的硬件（ASICs/FPGA）和高性能CPU来处理高流量，同时尽量减少延迟，确保安全性不会影响网络性能。Palo Alto Networks 被广泛认为是第一家引入NGFW概念和技术的公司。目前，包括Fortinet、瞻博网络和思科在内的许多公司都提供NGFW硬件设备。
NGFW 的局限性 此前，NGFW设备提供的边界安全性非常有效，但在当今的数字化环境中，网络流量高度分散，不再局限于明确界定的边界内，NGFW有效性被削弱。
组织使用多个云服务提供商来托管他们的应用程序和数据，形成了分布式的基础设施。将流量通过集中式的NGFW设备进行路由可能会导致效率低下和远程用户的性能瓶颈。
随着软件即服务（SaaS）的采用，关键业务应用驻留在云端并通过互联网访问。此外，现代网络架构，如SD-WAN，通常将用户流量直接路由到互联网，而不是回传到企业的中心数据中心。
鉴于网络流量分布的性质，需要新的安全方法来补充传统的边界安全。
2010 年代：云/虚拟 NGFW 随着云计算的兴起，保护云环境变得至关重要。云或虚拟防火墙是一种基于软件的 NGFW，可在云环境中提供防火墙功能。它提供与硬件 NGFW 设备类似的功能，但针对云和虚拟环境的独特挑战进行了量身定制。
云防火墙在企业托管的云平台、基础设施、应用程序和数据资产周围创建虚拟安全屏障。这些通常由第三方安全供应商以服务形式提供。云防火墙与企业中的NGFW设备相辅相成，提供多层次的安全策略。
2010 年代后期：SASE 安全访问服务边缘（SASE）是由Gartner在2019年定义的一种网络架构概念。它代表了广域网（WAN）和网络安全功能的融合。在SASE之前，传统的安全解决方案涉及管理、扩展和部署等单点解决方案/工具。SASE通过将这些工具融合到一个单一平台上简化了这一过程。SASE的基础组件包括软件定义广域网（SD-WAN）、安全Web网关（SWG）、云访问安全代理（CASB）、防火墙即服务（FWaaS）和零信任网络访问（ZTNA）。
ZTNA提供了一种比传统VPN更为强大的方法，并且正在许多企业中迅速取代VPN。原因显而易见。VPN 在用户通过身份验证后以隐式信任为前提运行。用户在连接时通常可以广泛访问内部网络，如果他们的凭据被泄露，风险就会增加。ZTNA 在应用程序级别实施细粒度、基于上下文的访问控制。用户被授予所需的最低访问限度，从而减少了未授权访问和在网络内横向移动的风险。
因此，SASE利用上述提到的云原生技术从分布式云位置提供网络和安全服务，以确保低延迟、高性能和可扩展性。单一管理控制台整合了网络和安全策略，简化了管理并提供了全面的可视性。SASE强调身份为主要参数，确保访问决策基于用户身份、设备状态和实时上下文。它可以轻松扩展以满足不断增长的需求，而无需额外的硬件。它通过消除对多个单点解决方案的需求并利用云规模经济，降低了运营支出。
如果企业出于遗留原因（拥有大量已安装的单点解决方案）或为了避免供应商锁定，不想部署SASE，他们可以选择继续使用针对ZTNA、SWG（安全Web网关）和其他云安全解决方案的单点解决方案。
2020 年代：网络安全中的AI/ML 在这个时代，AI/ML在增强安全态势以及威胁检测和响应机制的有效性方面发挥着关键作用。然而，广泛采用AI/ML也面临着一些挑战。实时的AI/ML处理需要大量的计算资源，这可能是昂贵的并且会影响网络性能。网络威胁不断演变，需要不断改进AI/ML模型以保持其有效性并减少误报。不完整或质量差的数据可能导致模型不准确和遗漏威胁。过多的误报可能会压垮安全团队，并在AI/ML自动阻止流量的情况下干扰流量。
最后，模型本身可能成为安全攻击的目标。用于模型训练的数据可能会被污染或篡改。组织使用各种技术来防范这种情况。最基本的是实施严格的访问控制，仅限授权人员访问AI/ML模型及其训练数据。对抗性训练，即在训练过程中定期引入对抗性示例，可用于使模型抵抗被污染的数据。使用多个模型交叉验证预测结果也有助于提高准确性。
2020 年代：5G 网络安全 5G网络的部署带来了新的安全问题，主要是由于连接设备数量的增加，这增加了攻击者潜在的入口点。管理这些终端，尤其是其中许多是具有有限安全能力的IoT设备，是一项挑战。5G支持网络切片，即在单一物理基础设施上创建多个虚拟网络。然而，这也增加了复杂性和配置错误的可能性。一个切片中的安全漏洞可能会影响到其他切片，导致广泛的安全问题。在过渡期间，5G网络必须与现有的4G和其他遗留系统互操作。确保安全互操作性的同时保持向后兼容性可能会引入漏洞。
5G网络的复杂性和规模意味着它们需要更先进的威胁检测和响应能力。实施基于AI/ML的实时监控以进行威胁检测，并确保对安全事件的快速响应是至关重要的。
未来：网络安全的 GenAI/LLM GenAI 有可能改变人工智能在网络安全中的使用方式。大模型擅长理解文本的上下文，通过大量历史网络安全数据对LLM进行微调，可以让模型从模式和趋势中学习，并能比传统的AI/ML模型更精确地识别未来的威胁。
通过对供应商文档进行微调的LLM可以帮助安全专业人员更快地学习新的安全工具。GenAI能够从多个来源进行快速、精确的数据分析，并生成自然语言形式的事件和威胁评估摘要，从而提高生产力。这样一来，人们就可以专注于战略性和复杂的挑战，而将重复性的任务，如日志分析、威胁追踪和事件响应等工作，委托给GenAI来完成。
总结 网络安全领域的演进一直伴随着对新兴威胁的持续创新与适应。从最初的包过滤防火墙到当今复杂的下一代防火墙(NGFW)、SASE框架以及人工智能驱动的安全解决方案，这一领域经历了显著的进步。然而，随着genAI/LLM等新技术的出现，威胁环境也在不断变化。总的来说，唯有不断创新才能始终领先一步！
]]></content>
  </entry>
  
  <entry>
    <title>如何同时运行多个 Linux 命令</title>
    <url>/post/linux/how-to-run-multiple-linux-commands-simultaneously.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Multiple Commands</tag>
    </tags>
    <content type="html"><![CDATA[了解如何在 Linux 中同时执行多个命令可以显著提高您的效率和生产力。本文将指导您通过各种方式在单行中运行多个 Linux 命令，甚至如何自动化重复的任务。
理解基础知识 深入了解高级技巧之前，您应该熟悉命令行或终端，这是 Linux 的强大工具。在这里，您可以通过输入一系列命令来执行任务。虽然一开始可能会觉得令人生畏，但学会使用它可以打开一个提高效率和生产力的新世界。
连续运行命令 如果您想连续运行多个命令，即在前一个命令完成后运行下一个命令，请使用分号（;）。例如，command1 ; command2 ; command3 将执行 command1，等待它完成，然后执行 command2，以此类推。
并行执行命令 要同时运行或并行运行命令，请使用和号（&amp;）。但请记住，使用和号会将进程发送到后台，允许下一个命令立即启动。例如，command1 &amp; command2 同时执行 command1 和 command2。
使用逻辑运算符 您还可以使用逻辑运算符（&amp;&amp; 和 ||）根据前一个命令的成功或失败来运行命令。’&amp;&amp;’ 运算符将在前一个命令成功时执行下一个命令。例如，command1 &amp;&amp; command2 仅在 command1 成功时才执行 command2。相反，’||’ 运算符仅在前一个命令失败时才执行下一个命令。
分组命令 如果您有一组要按特定顺序执行的命令，可以使用括号。例如，(command1 ; command2) &amp; command3 将同时运行 command1 和 command2，但仅在两个命令都完成后才启动 command3。
利用命令行管道 管道是一种非常有用的工具，可以将一个命令的输出传递为另一个命令的输入。您可以使用垂直线（|）来实现这一点。例如，command1 | command2 将 command1 的输出作为 command2 的输入传递。
自动化重复任务 如果您经常执行特定的一组命令，可以编写一个简单的 bash 脚本来自动化该过程。您只需要在文本文件中编写命令并将其保存为 .sh 扩展名即可。例如，您可以创建一个名为 ‘myscript.sh’ 的文件并编写：
#!/bin/bash command1 command2 command3 然后，运行 chmod +x myscript.sh 使脚本可执行，并使用 ./myscript.sh 执行它。
总结 掌握同时执行多个 Linux 命令的艺术可以节省大量时间，极大地增强您的工作流程。通过理解分号、和号、逻辑运算符、括号、管道和 bash 脚本，您将能够以更高效、更强大的方式让终端为您工作。
原文地址： 如何同时运行多个 Linux 命令  
]]></content>
  </entry>
  
  <entry>
    <title>关于超级以太网，大佬有话要说</title>
    <url>/post/datacenter/the-boss-has-something-to-say-about-super-ethernet.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Ethernet</tag>
    </tags>
    <content type="html"><![CDATA[关于超级以太网，大佬有话要说
来自行业分析师引述 &ldquo;由于系统互连功能的弱点，许多HPC和AI用户发现很难从他们的系统中获得全部性能。用户也很难集成和学习多个新的或不同的解决方案。看到这群令人印象深刻的领先公司的共同努力，创建一个新的通用的更高性能互连解决方案，这令人兴奋。HPC和AI领域的买家具有非常苛刻的工作负载，Ultra Ethernet Consortium（UEC）可以极大地帮助提高互操作性、性能和功能。我们期待在不久的将来看到一套新产品进入市场。&quot;——Dr. Earl Joseph, CEO of Hyperion Research.
“AI/ML和HPC的业务用例正在继续扩大，越来越多的公司希望利用可扩展的计算来获得竞争优势，无论是在自己的计算设施中还是在云中。如今，没有标准的、供应商中立的数据中心网络解决方案专注于并行应用程序的大规模性能。由于大多数数据中心都是基于以太网的，因此拥有由UEC驱动的可扩展解决方案将使可扩展性更加直接和可访问。参与UEC的公司能够开发一致的以太网解决方案，从单一连接到最大的超级计算机和超大规模数据中心。”——Addison Snell, CEO of Intersect360 Research.
“我敢说，关于用于基础设施的最佳网络的讨论一直在进行，以支持生成式AI的大型语言模型的训练和推理。一些公司已经转向基于以太网的网络，更喜欢其易于安装和使用。UEC计划将成为AI社区的一个受欢迎的补充。”——Karl Freund, Founder and Principal Analyst at Cambrian-AI Research.
UEC创始成员语录 “高度计算密集型工作负载（例如 AI 培训、机器学习以及 HPC 模拟和建模）需要可扩展且经济高效的行业解决方案，并将互操作性作为重中之重。为了创建基于以太网的开放式架构，以满足现代数据中心工作负载不断变化的需求，我们将作为创始成员加入超级以太网联盟。AMD 在支持开放行业标准方面有着悠久的历史，我们很自豪今天能继续与 UEC 一起走这条路。&quot;——Robert Hormuth, corporate vice president, Architecture and Strategy, Data Center Solutions Group, AMD.
“Arista Networks 很高兴参与 UEC，支持以太网向更多用例的演进，作为 HPC 和 AI/ML 工作负载的无处不在的传输。”——Hugh Holbrook, Group Vice President, Software Engineering for Arista Networks.
“凭借其无与伦比的生态系统、极高的灵活性和高性能，以太网已成为几乎所有类型数据网络的首选结构。Broadcom长期以来一直是以太网技术的支持者，推动网络堆栈各个方面的创新。我们很高兴能与许多云和网络行业巨头合作，推动以太网满足下一代人工智能和HPC网络的需求。“——Ram Velaga, senior vice president and general manager, Core Switching Group, Broadcom.
“我们正处于几乎每个行业大规模转型的开始。AI / ML将从根本上改变我们做所有事情的内容，时间和方式。为了实现这种转变，行业需要改进未来网络的构建方式。思科支持UEC的目标，即识别和标准化优化，这将使部署AI/ML基础设施的客户受益。”——Rakesh Chopra, Cisco Fellow, Common Hardware Group, Cisco.
“HPC 市场一直是开发高速互连的关键驱动力。随着 AI/ML/DL 密集型和大规模工作负载的出现，市场正在向创建包含互操作性、成本效益和真正高性能的新标准的方向趋同。我们很自豪也很热心成为超级以太网联盟（UEC）的创始成员之一，该联盟旨在通过基于以太网的通信协议和软件堆栈来应对这些挑战。我们相信UEC将提供强劲的结果，以满足市场需求和要求。”——Eric Eppe, Group VP, HPC/AI/Quantum Portfolio &amp; Strategy for Eviden at Atos Group.
“生成式 AI 工作负载将要求我们构建网络以实现超级计算规模和性能。超级以太网联盟的重要性在于开发一个开放、可扩展且经济高效的基于以太网的通信堆栈，以支持这些高性能工作负载高效运行。以太网的普遍性和互操作性将为客户提供选择，以及处理各种数据密集型工作负载的性能，包括模拟以及AI模型的训练和调整。随着人工智能模型的数据和规模的持续增长，高度并行化的计算成为性能、可靠性和可持续性的重要组成部分。”—— Justin Hotard, executive vice president and general manager, HPC &amp; AI, at Hewlett Packard Enterprise.
”人工智能、机器学习和大规模高性能工作负载的计算和网络性能需求是永不满足的。该行业需要开放式解决方案来满足这些需求，以实现专有解决方案的选择和自由。英特尔很荣幸成为超级以太网联盟 （UEC） 的创始成员，该联盟将通过更新和优化的基于以太网的高性能、可扩展和开放的网络解决方案和通信堆栈迎来未来的计算基础设施。“——Jeff McVeigh, corporate vice president &amp; general manager of the Super Compute Group at Intel.
“下一代人工智能系统需要前所未有的规模和性能。Meta 致力于构建高性能以太网结构和技术的开放生态系统，以实现下一个计算时代。”——Alexis Björlin, Vice President of Infrastructure, AI Systems and Accelerated Platforms at Meta.
“下一个计算时代的特点将是人工智能和人工智能优化的基础设施取得突破性进展，Microsoft致力于使组织能够利用Azure的强大功能推动一切可能。联手开发一套通用标准，以增强超大规模人工智能和高性能计算工作负载的以太网，将有助于实现现在和未来的持续创新。”——Steve Scott, Corporate Vice President of Azure Hardware Architecture at Microsoft.
]]></content>
  </entry>
  
  <entry>
    <title>英伟达的芯片版图</title>
    <url>/post/datacenter/nvidia-chip-layout.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Nvidia</tag>
      <tag>GPU</tag>
      <tag>CUDA</tag>
    </tags>
    <content type="html"><![CDATA[当英伟达市值站上万亿美元之巅时，全球最神秘资本大鳄罗斯柴尔德家族却传出减持英伟达股票的消息，股神巴菲特也作出了相同选择。
这一最新投资动向似乎传递出一个信号：支撑英伟达万亿美元市值大厦的支柱，或许没有大家想象得那般牢固。站在芯片市值之颠的英伟达，其实少了一块芯片“拼图”。
“三剑客”的芯片版图 在AI时代，仅凭借GPU或CPU，恐怕很难胜任高算力需求的大模型训练与推理。作为英伟达强有力的竞争对手，英特尔和AMD明显意识到了这点，不仅加快开发对标英伟达GPU的直接竞品，更是纷纷出手布局整个芯片版图。
各企业加紧研发AI芯片 在GPU领域，英特尔其实是一位“老玩家”。1998年2月12日，英特尔发布了旗下首款独立GPU产品——英特尔i740 AGP显卡，但是i740的各项性能表现远不及预期，无法与同期英伟达的RIVA TNT同台竞技，故而在一年后含恨退市。而于2009年发布Larrabee也并未收获良好的市场表现，最终英特尔不得不终止该计划。虽在独立GPU上屡屡受挫，但自i740开始的技术积累和CPU的强大底蕴使得英特尔摸索出集成显卡的发展路径。即便如此，英特尔也仍未放弃对独立GPU的探索。2017年，英特尔推出Xe架构，这成为几年后英特尔开发GPU的基石架构。
英特尔首款GPU i740
英特尔对芯片的追逐不仅仅只有CPU和GPU。2015年，英特尔以167亿美元高调收购元老级FPGA厂商Altera，以求在芯片设计上弥补自身芯片灵活性不足的短板。2018年，英特尔首次推出了负责神经网络处理的处理器IPU。在同年的英特尔架构日上，英特尔向业界提出了XPU异构愿景：一个由标量、矢量、矩阵、空间组成的SVMS架构——分别对应了CPU、GPU、IPU和FPGA，可进行多种异构组合。
AMD公布第四代EPYC家族处理器一系列更新
AMD在GPU上也马不停蹄。AMD的GPU发展历程以2006年7月收购图形处理公司ATI为分水岭。被收购前的ATI凭借Radeon系列的自身性能与英伟达直接竞争；而收购ATI后的AMD，则利用高性价比战略抢占中端市场，并在此后不断同英伟达的GPU进行对抗和拉扯。
2022年2月，AMD斥资498亿美元完成对FPGA厂商赛灵思的收购，以加强在数据中心业务的布局；同年4月，AMD宣布以19亿美元收购DPU芯片厂商Pensando，继续扩大数据中心业务。今年6月13日在美国旧金山的发布会上，CEO苏姿丰发布了多款面向数据中心领域的产品，AMD的“CPU+GPU+FPGA+DPU”的芯片版图也搭建完成，并逐渐成熟。
面对挑战者的层层加码，英伟达不敢有丝毫怠慢。
英伟达Grace Hopper使用Grace CPU
英伟达深知自己能成为当今AI芯片的领头羊，依靠的绝不仅是CUDA平台这一条护城河。英伟达在手握GPU和软件生态上的先发优势的同时，也积极向CPU探索。虽然英伟达对Arm的收购计划因反垄断原因以失败告终，但其在CPU上布局的脚步仍未停止。早在2021年，英伟达便已经发布了名为Grace的处理器，而Grace Hopper于今年在ISC上亮相。这款包含了Hopper架构和Grace CPU的超级处理器，彰显了英伟达对CPU的野心。
另外，2019年，英伟达以69亿美元的价格收购以色列网络芯片公司Mellanox，并于同年推出BlueField-2 DPU，自此拉开DPU高速发展的序幕，并实现了“CPU+GPU+DPU”三芯布局。
对比三家的芯片版图，不难发现无论英特尔的XPU封装，还是AMD的异构集成，均把FPGA作为重要砝码。相反，英伟达在FPGA的部署上则稍显犹豫。业界不禁发出疑问：FPGA有多重要？它会是目前正坐在AI王座上的英伟达所忽略遗漏的那一块拼图吗？
英伟达没有FPGA FPGA这块拼图，看似市场体量不大，但其实非常重要。与GPU相比，FPGA作为可编程逻辑列阵，具备更低能耗、更强的灵活度和可编辑性，因而具备较短的设计周期，更适合算法快速迭代、应用场景不断拓展的AI时代。
各类芯片功能对比 进入AI时代，FPGA广泛应用于云计算、网络，如5G通信和边缘、端的垂直市场。整体而言，高性能计算与云服务提供商是FPGA扮演的两大重要角色。
英特尔FPGA产品图
“目前，AI在市场上仍处于早期萌芽状态，很多客户都在探索AI在边缘及嵌入式市场中究竟能够为大家带来什么。而FPGA的内在价值恰恰是可编程性和灵活性，可以赋予AI更高的灵活性。当客户在边缘领域运行AI相关工作负载时，FPGA能够为其特定工作负载进行优化，这一点至关重要。”英特尔数据中心与人工智能集团副总裁兼可编程解决方案事业部产品营销总经理Deepali Trehan对《中国电子报》记者表示。
正是意识到了FPGA未来可期，英特尔和AMD纷纷展开收购FPGA厂商的行动。而被两大巨头收入囊中的Altera和赛灵思，一度占据80%以上的市场份额。完成收购后，英特尔、AMD在业务、营收规模、对下游的议价权等方面，会获得更大主动权。
AMD收购FPGA企业赛灵思
AMD AECG 有线与无线事业部高级总监 Gilles Garcia对《中国电子报》记者表示：“此前赛灵思在通信领域一直处于领先地位。AMD对赛灵思的收购使得AMD处理器与赛灵思原有产品在5G领域实现强强联合，在7大5G无线设备制造商中，AMD现已部署6家。”
当前市场上已难有堪比Altera和赛灵思的优质标的。由于错失了通过收购抢占市场的先手棋，英伟达目前几乎不可能再通过收购来补充FPGA这块拼图。
总体而言，相对于较早布局FPGA的英特尔和AMD，没有FPGA产品线的英伟达在技术解决方案的多样性和灵活性上略逊一筹。
毕竟，高性能计算芯片不止GPU这一处理架构，“CPU+FPGA”的方案同样能满足AI的算力需求。FPGA芯片作为可编程芯片，可以针对特定功能进行扩展，在AI模型构建第二阶段具有一定的发挥空间。通过与CPU结合，FPGA能够实现深度学习功能，两者共同应用于深度学习模型。
既然已经错过了收购的最佳时期，那么自研FPGA或许是另一条路。记者注意到，英伟达近日在求职网站上发布了招聘FPGA工程师的招聘信息。对于FPGA，英伟达或许已经暗中行动。
但是，英伟达研发FPGA的道路并非坦途，短期来看性价比仍然较低。
“FPGA研发的难点在于工具链和工艺，需要大量人力、物力、财力。”澎峰科技联合创始人兼首席运营官王军辉对《中国电子报》记者表示，行业通常将CPU、GPU、DSP（已经提的越来越少了）和FPGA这四类计算硬件，从计算能效、计算密度、计算性能、易用性等几个维度去进行对比，看它们适合于什么市场。在大算力、大模型时代，英伟达的“GPU+CUDA”方向可作为一个选择。
]]></content>
  </entry>
  
  <entry>
    <title>聊聊闪存存储的延迟可预测性</title>
    <url>/post/datacenter/predictability-on-unpredictable-flash-storage-with-a-light-neural-network.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Storage</tag>
      <tag>SSD</tag>
      <tag>LinnOS</tag>
    </tags>
    <content type="html"><![CDATA[如何利用神经网络预测闪存尾端延迟的发生
如何利用神经网络预测闪存尾端延迟的发生 由于用户对低且稳定的延迟（微秒级）的需求越来越大，人们对SSD的百分比延迟越来越关心，即SSD有99%的概率可以提供低且稳定的延迟，但有1%的概率产生几倍于正常情况的延迟，而这1%的高延迟被称为尾端延迟。尾端延迟有什么影响？如何降低尾端延迟的影响？如何在存储环境下利用神经网络？这些疑问，本文将一一解答。
尾端延迟与Hedged Request 百分比延迟 也许你已经查了维基百科中”百分比延迟“的定义，但我想对大多数人而言有点晦涩难懂，下面我将举一个简单的例子以帮助你理解。
首先，我们先列举出一系列收集到的延迟：
23，20，21，20，23，20，45，21，25，25
对它们排序：
20，20，20，21，21，23，23，25，25，45
接下来可以选择前x%的延迟，例如假设我们想要得到50th百分比延迟，则选择前5个延迟：
20，20，20，21，21
然后选择这一组延迟中最大的那个——即21——就是这一组延迟中的50th百分比延迟（也可以写作p50），同理，p90是25。
尾端延迟 尾端延迟就是百分比延迟中末尾的（通常p99之后）那些延迟。看起来尾端延迟占比并不多，但当系统处理的请求达到10^6个数量级，可能足足有104个请求处理延迟远高于正常情况——你不会想成为那不幸的1%，对吗？
分析SSD的内部行为后，本文作者认为尾端延迟的产生源自SSD内部日益复杂的内部活动，如垃圾回收、负载均衡等，和用户请求的冲突。为了降低尾端延迟或者降低尾端延迟的影响，业界提出的方案分为两大类：
 白盒子方案  此时SSD内部的行为可知，通过改进SSD内部架构来降低尾端延迟。这种方式无疑是直接而强有效的，但是不利于推广到市场。
 灰盒子方案  此时不需要修改SSD的内部架构，但是需要修改上层的软件栈。
 黑盒子方案  以各种预测为代表，既不需要修改上层软件栈，也不需要修改SSD内部架构，是目前最流行的解决方案。其中一个经典的方案是Hedged Request，它的原理和应用环境将在下文中介绍。
Hedged Request 为了保证数据安全、实现负载均衡，现代的存储系统通常存在一定冗余，而多个不同的SSD的内部行为同时和用户请求产生冲突的概率非常低。基于这样的思考，Hedged Request将一个请求发给一个SSD后，若等待请求完成的时间超过了阈值，则重发请求到另一个可用的SSD。如下图所示：
然而，传统Hedged Request中，快SSD需要等待一段时间（等待慢SSD处理的时间超过阈值）后才能处理请求，对于微秒级SSD而言，这个等待时间是致命的。如果可以学习SSD的特征，预测将要变慢的SSD而及时将请求重发到快SSD中，则可以节约出等待的时间，从而降低闪存组的尾端延迟——这就是LinnOS完成的工作，如下图所示，用户发送请求后，若经过LinnOS网络预测得知该SSD将变慢，则提前告知用户重发请求，随后请求将被送到下一个SSD，减少了Hedged Request中的等待时间。
LinnOS的三大挑战 设计LinnOS存在三大挑战，接下来将一一阐述。
 对用户输出什么结果？  需要输出具体的延迟（如120μs）吗？虽然这样更灵活，但是一方面，对用户而言，120μs或者125μs其实区别不大，另一方面，如此精确的输出意味着准确率低，并不划算。那么如果输出一个延迟范围，如80~100μs、100~120μs呢？此时准确率稍高了些，但不够（仅60%-70%），处于区间交界处的延迟往往预测不准确。回顾Hedged Request的原理，其实对用户而言，知道SSD是”快“或者”慢“就足够了！所以LinnOS使用简单的二分类模型。
 使用什么信息进行预测？  看起来一系列信息都和SSD快或慢有关：读写请求？请求的块内偏移？长期的写入历史？然而，作者发现这些请求都对提高精确度没有明显帮助。首先，由于当前SSD常有内置写缓存，写之后的读延迟常常没有明显提高，更为常见的其实是数据从缓存”冲“（flush）入SSD后，读延迟会更高。其次，一组I/O请求会通过条带均匀地写入各个通道或者芯片，它们写入同一个芯片的概率很低，所以块内偏移这个特征其实并不重要。最后，GC或者flush通常发生时间短，短期写入历史足矣预测。
因此，可以使用SSD当前I/O队列长度来预测SSD快或者慢：一个直观的感受是，当I/O队列较长时，SSD处理通常比较慢。但是这样并不能体现SSD的内部活动的发生，因此额外增加了历史四条请求进入SSD时的队列长度和完成请求的时间。若某个请求进入SSD时队列短而完成请求的时间长，意味着SSD内部行为可能和用户请求冲突了。
 如何最小化预测错误的影响？  作者分析发现，若将一个快的SSD预测为慢的从而错误地重发了，将带来微秒级延迟，而若将一个慢的SSD预测为快，将带来毫秒级延迟，比第一种情况严重许多，所以作者在训练时对第二种情况施加了更加严重的惩罚以减少它们的发生。此外，还补充了hedged request以减少预测失败的损失。
 实验结果与总结  作者上层使用了不同的软件产生负载，底层使用同构的消费级SSD阵列或者异构企业级SSD阵列测试它们的表现，以读延迟为指标展示结果。总共比较了7种不同的方案：
 Base：无优化 Clone：同时发送两份请求，选择先返回的SSD的结果返回给用户 Hedge95：等待p95之后重发请求 Hedge IP（inflection point）：和上一个相比，使用针对负载优化后的等待时间 HeurSim：队列较长时重发请求 HeurAdv：队列较长、且考虑历史信息（和LinnOS一样）后决定重发请求 LinnOS-Raw：没有hedged补偿的LinnOS LinnOS+HL：最终的LinnOS方案  实验结果如下图：
]]></content>
  </entry>
  
  <entry>
    <title>状态机的三种实现方法</title>
    <url>/post/programming/three-implementation-methods-of-state-machine.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>State Machine</tag>
    </tags>
    <content type="html"><![CDATA[这次我们一起来学习C语言实现状态机的三种方法解析。
状态机的实现无非就是 3 个要素：状态、事件、响应。转换成具体的行为就 3 句话。
 发生了什么事？ 现在系统处在什么状态？ 在这样的状态下发生了这样的事，系统要干什么？  用 C 语言实现状态机主要有 3 种方法：switch—case 法、表格驱动法、函数指针法。
switch—case 法 状态用 switch—case 组织起来， 将事件也用switch—case 组织起来， 然后让其中一个 switch—case 整体插入到另一个 switch—case 的每一个 case 项中 。
「程序清单 List4 ：」 switch (StateVal) { case S0: switch (EvntID) { case E1: action_S0_E1(); /*S0 状态下 E1 事件的响应 */ StateVal = new state value; /*状态迁移，不迁移则没有此行 */ break; case E2: action_S0_E2(); /*S0 状态下 E2 事件的响应 */ StateVal = new state value; break; ... ... case Em: action_S0_Em(); /*S0 状态下 Em 事件的响应 */ StateVal = new state value; break; default: break; } break; case S1: ... ... break; ... ... case Sn: ... ... break; default: break; } 上面的伪代码示例只是通用的情况，实际应用远没有这么复杂。虽然一个系统中事件可能有很多种，但在实际应用中，许多事件可能对某个状态是没有意义的。
例如在程序清单 List4中，如果 E2、······ Em 对处在 S0 状态下的系统没有意义，那么在 S0 的 case 下有关事件E2、······ Em 的代码根本没有必要写,状态 S0 只需要考虑事件 E1 的处理就行了。
既然是两个 switch—case 之间的嵌套， 那么就有一个谁嵌套谁的问题， 所以说 switch—case法有两种写法：状态嵌套事件和事件嵌套状态。这两种写法都可以， 各有利弊， 至于到底选用哪种方式就留给设计人员根据具体情况自行决断吧。
关于 switch—case 法还有最后一点要说明， 因为 switch—case 的原理是从上到下挨个比较，越靠后，查找耗费的时间就越长，所以要注意状态和事件在各自的 switch 语句中的安排顺序，不推荐程序清单 List4 那样按顺序号排布的方式。出现频率高或者实时性要求高的状态和事件的位置应该尽量靠前。
表格驱动法 如果说 switch—case 法是线性的，那么表格驱动法则是平面的。表格驱动法的实质就是将状态和事件之间的关系固化到一张二维表格里，把事件当做纵轴，把状态当做横轴，交点[Sn , Em]则是系统在 Sn 状态下对事件 Em 的响应。
如图 4， 我把表格中的 Node_SnEm 叫做状态机节点， 状态机节点 Node_SnEm 是系统在 Sn状态下对事件 Em 的响应。这里所说的响应包含两个方面：输出动作和状态迁移。状态机节点一般是一个类似程序清单 List5 中的结构体变量 。
struct fsm_node { void (*fpAction)(void* pEvnt); INT8U u8NxtStat; }; 程序清单 List5 中的这个结构体有两个成员：fpAction 和 u8NxtStat。fpAction 是一个函数指针， 指向一个形式为 void func(void * pEvnt)的函数， func 这个函数是对状态转移中动作序列的标准化封装。
也就是说， 状态机在状态迁移的时候， 不管输出多少个动作、操作多少个变量、调用多少个函数，这些行为统统放到函数 func 中去做。
把动作封装好了之后，再把封装函数 func 的地址交给函数指针 fpAction，这样，想要输出动作，只需要调用函数指针 fpAction 就行了。
再看看上面的 func 函数，会发现函数有一个形参 pEvnt，这是一个类型为 void * 的指针， 在程序实际运行时指向一个能存储事件的变量，通过这个指针我们就能获知关于事件的全部信息，这个形参是很有必要的。事件一般包括两个属性：事件的类型和事件的内容。
例如一次按键事件，我们不仅要知道这是一个按键事件，还要知道按下的到底是哪个键。事件的类型和状态机当前的状态可以让我们在图 4 的表格中迅速定位，确定该调用哪个动作封装函数， 但是动作封装函数要正确响应事件还需要知道事件的内容是什么， 这也就是形参pEvnt 的意义。
由于事件的多样性，存储事件内容的数据格式不一定一样，所以就把 pEvnt 定义成了 void * 型，以增加灵活性。有关 fpAction 的最后一个问题：如果事件 Em 对状态 Sn 没有意义，那么状态机节点Node_SnEm 中的 fpAction 该怎么办？我的答案是：那就让它指向一个空函数呗！前面不是说过么，什么也不干也叫响应。
u8NxtStat 存储的是状态机的一个状态值。我们知道， 状态机响应事件要输出动作， 也就是调用函数指针 fpAction 所指向的那个封装函数， 函数调用完毕后程序返回主调函数， 状态机对事件的响应就算结束了， 下一步就要考虑状态迁移的问题了。
可能要保持本状态不变， 也可能要迁移到一个新的状态，该如何抉择呢？u8NxtStat 存储的状态就是状态机想要的答案！
图 4 的这张表格反映在 C 语言代码里就是一个二维数组，第 1 维就是状态机的状态，第 2维就是统一分类的事件，而数组的元素则是程序清单 List5 中的结构体常量。如果程序中使用表格驱动法，还需要注意一些特别的事项。要将状态当做表格的横轴，那么就要求状态值集合必须满足以下条件：
(1) 该集合是一个递增的等差整数数列
(2) 该数列初值为 0
(3) 该数列等差值为 1
“事件” 作为纵轴，其特点和要求与用来做横轴的“状态” 完全一致。在 C 语言提供的数据类型中， 没有比枚举更符合以上要求的可选项了， 极力推荐将状态集合和事件类型集合做成枚举常量。表格驱动法的优点：调用接口统一 ，定位快速。
表格驱动法屏蔽了不同状态下处理各个事件的差异性，因此可以将处理过程中的共性部分提炼出来，做成标准统一的框架式代码，形成统一的调用接口。根据程序清单 List5 中的状态机节点结构体，做成的框架代码如程序清单 List6 所示。
表格驱动法查找目标实际上就是一次二维数组的寻址操作，所以它的平均效率要远高于switch—case 法。
###「程序清单 List6 ：」
extern struct fsm_node g_arFsmDrvTbl[][]; /*状态机驱动表格*/ INT8U u8CurStat = 0; /*状态暂存*/ INT8U u8EvntTyp = 0; /*事件类型暂存*/ void* pEvnt = NULL; /*事件变量地址暂存*/ struct fsm_node stNodeTmp = {NULL, 0}; /*状态机节点暂存*/ u8CurStat = get_cur_state(); /*读取当前状态*/ u8EvntTyp = get_cur_evnt_typ(); /*读取当前触发事件类型*/ pEvnt = (void*)get_cur_evnt_ptr(); /*读取事件变量地址*/ stNodeTmp = g_arFsmDrvTbl[u8CurStat ][u8EvntTyp ];/*定位状态机节点*/ stNodeTmp.fpAction(pEvnt ); /*动作响应*/ set_cur_state(stNodeTmp.u8NxtStat); /*状态迁移*/ ..... 表格驱动法好则好矣，但用它写出来的程序还有点儿小问题，我们先来看看按照表格驱动法写出来的程序有什么特点 。
前面说过，表格驱动法可以把状态机调度的部分做成标准统一的框架代码，这个框架适用性极强， 不管用状态机来实现什么样的应用， 框架代码都不需要做改动， 我们只需要根据实际应用场合规划好状态转换图，然后将图中的各个要素(状态、事件、动作、迁移，有关“条件”要素一会儿再说)用代码实现就行了，我把这部分代码称作应用代码。
在应用代码的.c 文件中， 你会看到一个声明为 const 的二维数组， 也就是图 4 所示的状态驱动表格， 还会看到许多彼此之间毫无关联的函数， 也就是前面提到的动作封装函数。这样的一份代码， 如果手头上没有一张状态转换图， 让谁看了也会一头雾水， 这样的格式直接带来了代码可读性差的问题。
如果我们想给状态机再添加一个状态，反映到代码上就是给驱动表格再加一列内容，同时也要新添加若干个动作封装函数。如果驱动表格很大， 做这些工作是很费事儿的， 而且容易出错。如果不小心在数组中填错了位置， 那么程序跑起来就和设计者的意图南辕北辙了，
远没有在 switch—case 法中改动来得方便、安全。上面说的只是小瑕疵， 其实最让我不爽的是表格驱动法不能使用Extended State Machine(对这个词组还有印象吧？)！Extended State Machine 的最大特点就是状态机响应事件之前先判断条件，根据判定结果选择执行哪些动作，转向哪个状态。
也就是说，系统在状态 Sn 下发生了事件 Em 后，转向的状态不一定是唯一的，这种灵活性是 Extended State Machine 的最有价值的优点。
回过头来看看程序清单 List5 中给出的状态机节点结构体，如果系统在状态 Sn 下发生了事件 Em， 状态机执行完 fpAction 所给出的动作响应之后， 必须转到 u8NxtStat 指定的状态。
表格驱动法的这个特性直接杜绝了 Extended State Machine 在表格驱动法中应用的可能性， 所以表格驱动法的代码实现中不存在“条件” 这个状态机要素。ESM，你是如此的优秀，我怎么舍得抛弃你 ？！
再看图 4 所示的表格驱动法示例图，如果我们把表格中的代表事件的纵轴去掉，只留下代表状态的横轴，将一列合并成一格，前文提到的问题是不是能得到解决呢？不错！这就是失传江湖多年的《葵花宝典》 ——阉割版表格驱动法 ！！
阉割版表格驱动法，又名压缩表格驱动法，一维状态表格与事件 switch—case 的合体。压缩表格驱动法使用了一维数组作为驱动表格，数组的下标即是状态机的各个状态。
表格中的元素叫做压缩状态机节点， 节点的主要内容还是一个指向动作封装函数的函数指针， 只不过这个动作封装函数不是为某个特定事件准备的， 而是对所有的事件都有效的。
节点中不再强制指定状态机输出动作完毕后所转向的状态， 而是让动作封装函数返回一个状态， 并把这个状态作为状态机新的状态。
压缩表格驱动法的这个特点， 完美的解决了 Extended State Machine 不能在表格驱动法中使用的问题 。
程序清单 List7 中的示例代码包含了压缩状态机节点结构体和状态机调用的框架代码。
「程序清单 List7：」 struct fsm_node /*压缩状态机节点结构体*/ { INT8U (*fpAction)(void* pEvnt); /*事件处理函数指针*/ INT8U u8StatChk; /*状态校验*/ }; ...... u8CurStat = get_cur_state(); /*读取当前状态*/ ...... if(stNodeTmp.u8StatChk == u8CurStat ) { u8CurStat = stNodeTmp.fpAction(pEvnt ); /*事件处理*/ set_cur_state(u8CurStat ); /*状态迁移*/ } else { state_crash(u8CurStat ); /*非法状态处理*/ } ..... 对照程序清单 List5，就会发现程序清单 List7 中 struct fsm_node 结构体的改动之处。首先， fpAction 所指向函数的函数形式变了，动作封装函数 func 的模样成了这样的了：
INT8U func(void * pEvnt); 现在的动作封装函数 func 是要返回类型为 INT8U 的返回值的，这个返回值就是状态机要转向的状态， 也就是说， 压缩表格驱动法中的状态机节点不负责状态机新状态的确定， 而把这项任务交给了动作封装函数 func， func 返回哪个状态， 状态机就转向哪个状态。
新状态由原来的常量变成了现在的变量，自然要灵活许多。上面说到现在的动作封装函数 func 要对当前发生的所有的事件都要负责， 那么 func 怎么会知道到底是哪个事件触发了它呢？看一下 func 的形参 void * pEvnt 。
在程序清单 List5 中我们提到过，这个形参是用来向动作封装函数传递事件内容的，但是从前文的叙述中我们知道， pEvnt 所指向的内存包含了事件的所有信息， 包括事件类型和事件内容 ， 所以通过形参 pEvnt ， 动作封装函数 func 照样可以知道事件的类型。
程序清单 List7 中 struct fsm_node 结构体还有一个成员 u8StatChk ， 这里面存储的是状态机 的一个状态，干什么用的呢？玩 C 语言数组的人都知道，要严防数组寻址越界。
要知道，压缩表格驱动法的驱动表格是一个以状态值为下标的一维数组， 数组元素里面最重要的部分就是一个个动作封装函数的地址。
函数地址在单片机看来无非就是一段二进制数据， 和内存中其它的二进制数据没什么两样，不管程序往单片机 PC 寄存器里塞什么值，单片机都没意见。假设程序由于某种意外而改动了存储状态机当前状态的变量，使变量值变成了一个非法状态。
再发生事件时， 程序就会用这个非法的状态值在驱动表格中寻址， 这时候就会发生内存泄露，程序拿泄露内存中的未知数据当函数地址跳转，不跑飞才怪！
为了防止这种现象的发生， 压缩状态机节点结构体中又添加了成员 u8StatChk 。u8StatChk中存储的是压缩状态机节点在一维驱动表格的位置， 例如某节点是表格中的第 7 个元素， 那么这个节点的成员 u8StatChk 值就是 6。
看一下程序清单 List7 中的框架代码示例， 程序在引用函数指针 fpAction 之前， 先检查当前状态和当前节点成员 u8CurStat 的值是否一致，一致则认为状态合法，事件正常响应，如果不一致，则认为当前状态非法，转至意外处理，最大限度保证程序运行的安全。当然，如果泄露内存中的数据恰好和 u8CurStat 一致，那么这种方法真的就回天乏力了。
还有一个方法也可以防止状态机跑飞，如果状态变量是枚举，那么框架代码就可以获知状态值的最大值， 在调用动作封装函数之前判断一下当前状态值是否在合法的范围之内， 同样能保证状态机的安全运行。
压缩表格驱动法中动作封装函数的定义形式我们已经知道了，函数里面到底是什么样子的呢？程序清单 List8 是一个标准的示例。
「程序清单List8：」 INT8U action_S0(void* pEvnt) { INT8U u8NxtStat = 0; INT8U u8EvntTyp = get_evnt_typ(pEvnt); switch(u8EvntTyp ) { case E1: action_S0_E1(); /*事件 E1 的动作响应*/ u8NxtStat = new state value; /*状态迁移，不迁移也必须有本行*/ break; ...... case Em: action_S0_Em(); /*事件 Em 的动作响应*/ u8NxtStat = new state value; /*状态迁移，不迁移也必须有本行*/ break; default: ; /*不相关事件处理*/ break; } return u8NxtStat ; /*返回新状态*/ } 从程序清单 List8 可以看出， 动作封装函数其实就是事件 switch—case 的具体实现。函数根据形参 pEvnt 获知事件类型， 并根据事件类型选择动作响应， 确定状态机迁移状态， 最后将新的状态作为执行结果返回给框架代码。
有了这样的动作封装函数， Extended State Machine 的应用就可以完全不受限制了！到此，有关压缩表格驱动法的介绍就结束了。
个人认为压缩表格驱动法是相当优秀的，它既有表格驱动法的简洁、高效、标准，又有 switch—case 法的直白、灵活、多变，相互取长补短，相得益彰。
函数指针法 上面说过，用 C 语言实现状态机主要有 3 种方法(switch—case 法、表格驱动法、函数指针法)， 其中函数指针法是最难理解的， 它的实质就是把动作封装函数的函数地址作为状态来看待。不过，有了之前压缩表格驱动法的铺垫，函数指针法就变得好理解了，因为两者本质上是相同的。
压缩表格驱动法的实质就是一个整数值(状态机的一个状态)到一个函数地址(动作封装函数)的一对一映射， 压缩表格驱动法的驱动表格就是全部映射关系的直接载体。在驱动表格中通过状态值就能找到函数地址，通过函数地址同样能反向找到状态值。
我们用一个全局的整型变量来记录状态值，然后再查驱动表格找函数地址，那干脆直接用一个全局的函数指针来记录状态得了，还费那劳什子劲干吗？！这就是函数指针法的前世今生。
用函数指针法写出来的动作封装函数和程序清单 List8 的示例函数是很相近的， 只不过函数的返回值不再是整型的状态值， 而是下一个动作封装函数的函数地址， 函数返回后， 框架代码再把这个函数地址存储到全局函数指针变量中。
相比压缩表格驱动法，在函数指针法中状态机的安全运行是个大问题，我们很难找出一种机制来检查全局函数指针变量中的函数地址是不是合法值。如果放任不管， 一旦函数指针变量中的数据被篡改，程序跑飞几乎就不可避免了。
小节 有关状态机的东西说了那么多，相信大家都已经感受到了这种工具的优越性，状态机真的是太好用了！其实我们至始至终讲的都是有限状态机(Finite State Machine 现在知道为什么前面的代码中老是有 fsm 这个缩写了吧！)， 还有一种比有限状态机更 NB 更复杂的状态机， 那就是层次状态机(Hierarchical State Machine 一般简写为 HSM)。
通俗的说，系统中只存在一个状态机的叫做有限状态机，同时存在多个状态机的叫做层次状态机(其实这样解释层次状态机有些不严谨， 并行状态机也有多个状态机， 但层次状态机各个状态机之间是上下级关系，而并行状态机各个状态机之间是平级关系)。
层次状态机是一种父状态机包含子状态机的多状态机结构，里面包含了许多与面向对象相似的思想， 所以它的功能也要比有限状态机更加强大， 当一个问题用有限状态机解决起来有些吃力的时候， 就需要层次状态机出马了。
层次状态机理论我理解得也不透彻， 就不在这里班门弄斧了，大家可以找一些有关状态机理论的专业书籍来读一读。要掌握状态机编程，理解状态机(主要指有限状态机)只是第一步，也是最简单的一步，更重要的技能是能用状态机这个工具去分析解剖实际问题：划分状态、 提取事件、 确定转换关系、规定动作等等，形成一张完整的状态转换图，最后还要对转换图进行优化，达到最佳。
把实际问题变成了状态转换图， 工作的一大半就算完成了， 这个是具有架构师气质的任务，剩下的问题就是按照状态图编程写代码了，这个是具有代码工特色的工作。
]]></content>
  </entry>
  
  <entry>
    <title>Linux系统内核概述</title>
    <url>/post/linux/linux-share-file-folder-and-time-synchronization.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>NFS</tag>
      <tag>NTP</tag>
    </tags>
    <content type="html"><![CDATA[本文讲解了主从服务器创建共享目录以及主从服务器实现时钟同步。
目标：主从服务器创建共享目录。 安装并启动NFS 安装：yum install -y nfs-utils systemctl enable rpcbind #网络服务,负责在客户端和服务端之间建立联系。 systemctl enable nfs-server systemctl start rpcbind systemctl start nfs-server NFS服务端创建共享目录 所有服务器均创建目录：mkdir -p /opt/gao/test/file 修改配置文件：vi /etc/exports /opt/gao/test/file/ *(insecure,rw,sync,no_root_squash) :wq 重启NFS服务:systemctl restart nfs-server 查看配置路径键入：exportfs NFS客户端配置 查看主服务器可挂载目录：showmount -e 主服务器IP 修改文件：vi /etc/fsta 主服务器IP:/opt/gao/test/file /opt/gao/test/file/ nfs defaults,_netdev 0 0 :wq 挂载目录：mount -t nfs 主服务器IP:/opt/gao/test/file /opt/gao/test/file/ 测试连通性 任意一台服务器的/opt/gao/test/file目录下创建文件，去其他服务相同目录查看。
目标：主从服务器实现时钟同步。 部署NTP服务 查看ntp和ntpdate安装情况：rpm -qa | grep ntp 安装ntp：yum -y install ntp 安装ntpdate：yum -y install ntpdate 设置时区：cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 配置NTP服务端 vim /etc/ntp.conf #注释以下内容 restrict default nomodify notrap nopeer noquery restrict ::1 # 新增下面内容 # 备注eg:191.169.6.156服务端所在网段为191.169.6.0 restrict default kod nomodify notrap nopeer noquery restrict -6 default kod nomodify notrap nopeer noquery restrict -6 ::1 restrict 191.169.6.0(服务端所在网段) mask 服务端子网掩码 server 127.127.1.0 prefer fudge 127.127.1.0 stratum 10 #执行命令 启动服务：service ntpd start 开机启动：chkconfig ntpd on 查看同步状态：ntpstat 配置NTP客户端 vim /etc/ntp.conf # 注释下面内容： restrict default nomodify notrap nopeer noquery restrict ::1 #新增下面内容： restrict default kod nomodify notrap nopeer noquery restrict -6 default kod nomodify notrap nopeer noquery restrict -6 ::1 server 191.169.6.156 preferservice ntpd stop chkconfig ntpd off 配置定时同步时间 crontab -e 输入：*/2 * * * * /usr/sbin/ntpdate NTP服务端IP &gt;&gt; /dev/null 硬件时钟与系统时钟同步：hwclock -w 各服务器键入date验证：date 原文连接： Linux共享目录和时钟同步  
]]></content>
  </entry>
  
  <entry>
    <title>find使用xargs和exec执行效率比较</title>
    <url>/post/linux/comparison-of-execution-efficiency-between-find-xargs-and-exec.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Find</tag>
      <tag>xargs</tag>
      <tag>exec</tag>
    </tags>
    <content type="html"><![CDATA[在linux上测试在使用find命令进行批量操作时，xargs和exec两种方式那个效率更高。
目的 在linux上测试在使用find命令进行批量操作时，xargs和exec两种方式那个效率更高。
实验 实验准备 批量创建一批文件，用来做测试使用
# 创建100个文件 [root@localhost a]# touch file{1..100} [root@localhost a]# ls file1 file13 file18 file22 file27 file31 file36 file40 file45 file5 file54 file59 file63 file68 file72 file77 file81 file86 file90 file95 file10 file14 file19 file23 file28 file32 file37 file41 file46 file50 file55 file6 file64 file69 file73 file78 file82 file87 file91 file96 file100 file15 file2 file24 file29 file33 file38 file42 file47 file51 file56 file60 file65 file7 file74 file79 file83 file88 file92 file97 file11 file16 file20 file25 file3 file34 file39 file43 file48 file52 file57 file61 file66 file70 file75 file8 file84 file89 file93 file98 file12 file17 file21 file26 file30 file35 file4 file44 file49 file53 file58 file62 file67 file71 file76 file80 file85 file9 file94 file99 # 修改前50个文件的文件属主和属组为jin [root@localhost a]# chown jin:jin file{1..50} [root@localhost a]# ll |head -10 total 0 -rw-r--r--. 1 jin jin 0 Jul 19 11:02 file1 -rw-r--r--. 1 jin jin 0 Jul 19 11:02 file10 -rw-r--r--. 1 root root 0 Jul 19 11:02 file100 -rw-r--r--. 1 jin jin 0 Jul 19 11:02 file11 -rw-r--r--. 1 jin jin 0 Jul 19 11:02 file12 -rw-r--r--. 1 jin jin 0 Jul 19 11:02 file13 -rw-r--r--. 1 jin jin 0 Jul 19 11:02 file14 -rw-r--r--. 1 jin jin 0 Jul 19 11:02 file15 -rw-r--r--. 1 jin jin 0 Jul 19 11:02 file16 time命令 time命令用来统计一个命令执行使用的时间
语法：time + 执行的命令
实例：
[root@localhost a]# time ls file1 file13 file18 file22 file27 file31 file36 file40 file45 file5 file54 file59 file63 file68 file72 file77 file81 file86 file90 file95 file10 file14 file19 file23 file28 file32 file37 file41 file46 file50 file55 file6 file64 file69 file73 file78 file82 file87 file91 file96 file100 file15 file2 file24 file29 file33 file38 file42 file47 file51 file56 file60 file65 file7 file74 file79 file83 file88 file92 file97 file11 file16 file20 file25 file3 file34 file39 file43 file48 file52 file57 file61 file66 file70 file75 file8 file84 file89 file93 file98 file12 file17 file21 file26 file30 file35 file4 file44 file49 file53 file58 file62 file67 file71 file76 file80 file85 file9 file94 file99 real 0m0.006s user 0m0.002s sys 0m0.004s xargs测试 #删除当前目录下文件属主为jin的文件 [root@localhost a]# time find ./ -user jin |xargs rm -rf  real 0m0.006s user 0m0.001s sys 0m0.005s [root@localhost a]# ls file100 file53 file56 file59 file62 file65 file68 file71 file74 file77 file80 file83 file86 file89 file92 file95 file98 file51 file54 file57 file60 file63 file66 file69 file72 file75 file78 file81 file84 file87 file90 file93 file96 file99 file52 file55 file58 file61 file64 file67 file70 file73 file76 file79 file82 file85 file88 file91 file94 file97 可以看到使用管道配置xargs查找并删除文件属主为jin的文件一共用时0.006s,前50个文件已经被删除
exec测试 #创建file1...file50文件 [root@localhost a]# touch file{1..50} [root@localhost a]# ls file1 file13 file18 file22 file27 file31 file36 file40 file45 file5 file54 file59 file63 file68 file72 file77 file81 file86 file90 file95 file10 file14 file19 file23 file28 file32 file37 file41 file46 file50 file55 file6 file64 file69 file73 file78 file82 file87 file91 file96 file100 file15 file2 file24 file29 file33 file38 file42 file47 file51 file56 file60 file65 file7 file74 file79 file83 file88 file92 file97 file11 file16 file20 file25 file3 file34 file39 file43 file48 file52 file57 file61 file66 file70 file75 file8 file84 file89 file93 file98 file12 file17 file21 file26 file30 file35 file4 file44 file49 file53 file58 file62 file67 file71 file76 file80 file85 file9 file94 file99 #修改file1...file50这50个文件的属主和属组为jin [root@localhost a]# chown jin:jin file{1..50} [root@localhost a]# time find ./ -user jin |wc -l 50 real 0m0.032s user 0m0.002s sys 0m0.003s #使用-exec删除匹配到到文件 [root@localhost a]# time find /root/a -user jin -exec rm -rf {} \; real 0m0.057s user 0m0.018s sys 0m0.032s [root@localhost a]# ls file100 file53 file56 file59 file62 file65 file68 file71 file74 file77 file80 file83 file86 file89 file92 file95 file98 file51 file54 file57 file60 file63 file66 file69 file72 file75 file78 file81 file84 file87 file90 file93 file96 file99 file52 file55 file58 file61 file64 file67 file70 file73 file76 file79 file82 file85 file88 file91 file94 file97 可以看到使用exec查找并删除文件属主为jin的文件一共用时0.057s,前50个文件已经被删除.
结论 在使用find查找文件并准备做批量操作时，xargs的效率比exec的效率更高效
彩蛋 使用rm命令直接删除效率更高
[root@localhost a]# touch file{1..50} [root@localhost a]# ls file1 file13 file18 file22 file27 file31 file36 file40 file45 file5 file54 file59 file63 file68 file72 file77 file81 file86 file90 file95 file10 file14 file19 file23 file28 file32 file37 file41 file46 file50 file55 file6 file64 file69 file73 file78 file82 file87 file91 file96 file100 file15 file2 file24 file29 file33 file38 file42 file47 file51 file56 file60 file65 file7 file74 file79 file83 file88 file92 file97 file11 file16 file20 file25 file3 file34 file39 file43 file48 file52 file57 file61 file66 file70 file75 file8 file84 file89 file93 file98 file12 file17 file21 file26 file30 file35 file4 file44 file49 file53 file58 file62 file67 file71 file76 file80 file85 file9 file94 file99 [root@localhost a]# time rm -rf file{1..50} real 0m0.003s user 0m0.001s sys 0m0.002s [root@localhost a]# ls file100 file53 file56 file59 file62 file65 file68 file71 file74 file77 file80 file83 file86 file89 file92 file95 file98 file51 file54 file57 file60 file63 file66 file69 file72 file75 file78 file81 file84 file87 file90 file93 file96 file99 file52 file55 file58 file61 file64 file67 file70 file73 file76 file79 file82 file85 file88 file91 file94 file97 ]]></content>
  </entry>
  
  <entry>
    <title>DDRC中的RAS：Parity & ECC</title>
    <url>/post/datacenter/DDRC-RAS-Parity-and-ECC.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>DDRC</tag>
      <tag>RAS</tag>
      <tag>Parity & ECC</tag>
    </tags>
    <content type="html"><![CDATA[DDRC 中常见的RAS功能有两种：一种为Parity（奇偶）校验，一种为ECC（纠错码）校验。
什么是RAS？ DDR subsystem会因为设计bug或传输过程中的干扰导致bit翻转出现数据错误，其中设计缺陷导致的通常认为是硬错误，而系统噪声或者其他原因导致DRAM阵列翻转的则认为是软错误。硬错误是永久性的错误，而软错误则是短暂性的。为了能够在运行中纠正软错误，保障Memory subsystem的稳定运行，DDRC（DDR Controller）需要具有先进的RAS功能（可靠性，可用性，可维护性功能）。如果没有 RAS 功能，系统很可能会因为内存错误而崩溃。RAS 功能允许系统在出现可纠正的错误时继续运行，同时记录不可纠正错误的详细信息，以便将来进行调试。DDRC 中常见的RAS功能有两种：一种为Parity（奇偶）校验，一种为ECC（纠错码）校验。
Parity Check 奇偶校验是最简单的一种校验码，用于检测数据传输过程中是否发生错误，但是parity check只能用于检测1bit error，如果碰上2bit error，则无法检测，而且也无法区分出错是在data上出错，还是校验位出错。其次parity check也无法修复出错的数据。Parity check分为Odd parity check（奇校验）和Even parity check（偶校验）。
 奇校验：原始数据+校验位 总共有奇数个1 偶校验：原始数据+校验位 总共有偶数个1  Image DDRC中通常会有一根单独的信号用于传输Parity bit, 并且Pairty Check方式一般可以通过寄存器配置。
ECC Check DDRC会针对数据产生ECC bits，并将ECC bits存储到DRAM存储器中, 可对DRAM发送的数据进行1bit ECC error纠错或者2bit ECC error检错。
ECC生成校验数据步骤一般为：
  DDRC接收到写数据，在DDRC内部产生ECC bits，然后将写数据和产生的ECC bits一起写入颗粒
  DDRC收到读请求，DDRC从DRAM读取数据和对应ECC bits，然后DDRC通过读回数据重新产生ECC bits，然后和从DRAM读回的ECC数据进行比较。如果两者匹配，则认为该传输没有Error，如果不match，则开始检测是否为1bit ECC error或者2bit ECC error，如果为1bit ECC Error，则纠正单bit错误，然后继续传输。如果为2bit ECC error，直接通过中断上报。一般针对1bit ECC error，也可以设计一个阈值，例如一定时间内出现多少1bit ECC error, 也会上报中断。
  ECC bit根据存储在DRAM中的位置可以分为两种类型：side-band ECC 或者 inline ECC。在 side-band ECC 中，ECC 数据存储在单独的 DRAM 上；在inline ECC 中，ECC 数据与实际数据一起存储在同一个 DRAM 上。
Side-band ECC Side-band ECC一般运用在标准DDR内存（DDR4，DDR5）或者HBM中。ECC bits会作为side-band数据同实际数据一起发送到内存。例如64位数据，会增加8个数据位宽用于传输ECC bits。因此，一般企业级DDR4 ECC DIMM会有72bit位宽，并且DIMM会有两个额外x4 DRAM或者一个x8DRAM用于ECC bits存储。而对于HBM2E，则是通过复用DM（data mask）信号作为ECC bits传输，同样HBM中也有额外的区域用于存储ECC bits。因为Side-band ECC中，DDRC会同时写入或者读取ECC bits和实际数据，这种场景下不需要额外的WR或者RD command，因此和inline ECC方案相比，会有更高的Efficiency。
Side-band ECC的WR 和 RD 操作流程：
Inline ECC inline ECC常见于LPDDR或者GDDR中。通常LPDDR或者GDDR为固定信道宽度(GDDR6为x16), 因此side-band因为需要额外的side-band信号，这需要额外的昂贵花销。例如，对于 16 位数据宽度，需要为 7 位或 8 位 ECC 位宽的 inline ECC 额外分配 16 位 LPDDR 信道。此外，7 或 8 位 ECC 数据字段仅部分填充了 16 位额外的通路，导致存储效率低下，还给地址命令信道带来额外负载，可能会对性能有所影响。
Inline ECC 中的控制器不需要额外的信道来存储 ECC，而是将 ECC 数据存储在存储实际数据的同一 DRAM 信道中。因此，内存信道的总体数据宽度与实际数据宽度相同。同side-band一样，假设64bit Data则对应8bit数据，由于ECC bits和Data存储在同一个DRAM，因此需要1/9的DRAM容量作为ECC bits区域，其余8/9则用于存放数据。
当 ECC 数据未与读写数据一起发送时，控制器为 ECC 数据生成单独的开销 WR 和 RD 命令。因此，实际数据的每条 WR 和 RD 命令都伴有一条 ECC 数据的开销 WR 和 RD 命令。高性能控制器通过在一条 ECC WR 命令中封装几个连续地址的 ECC 数据，以此来降低此类 ECC 命令的损失。同样，控制器在一条 ECC RD 命令中读取内存发出的若干连续地址的 ECC 数据，并且可以将读出的 ECC 数据，应用于该连续地址产生的实际数据。因此traffic越有序，inline ECC导致的Efficiency Loss越小。假设GDDR中我们通过AXI发送AwLen=7，AwSize=5的一笔AXI transaction，那么一次发送2^5 * 8 = 256Byte的数据，并且产生32Byte的ECC bits。根据GDDR6的Spec，一个WR CMD能够写入32Byte的数据，那么正好9个cmd 能够完成256B data+32B ECC bits的写入，此时inline ECC导致的速率损失最小。因此可以计算得到inline ECC enable之后最大Efficiency为disable inline ECC的8/9 = 88.89%。如果不够64bit，即narrow transfer小于8Byte的场景，则还需要通过先读回DRAM内的数据，和narrow transfer data拼成64bit 计算 ECC bits，然后再写回DRAM，这种场景下inline ECC带来的效率损失会进一步扩大。
Inline ECC 的 WR 和 RD 操作流程：
]]></content>
  </entry>
  
  <entry>
    <title>61.44TB！全球第一SSD诞生：QLC闪存70年写不死</title>
    <url>/post/datacenter/the-birth-of-the-world-first-SSD-of-61.44TB.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Storage</tag>
      <tag>SSD</tag>
      <tag>Solidigm</tag>
      <tag>P5336</tag>
    </tags>
    <content type="html"><![CDATA[Solidigm公司近日发布了新一代旗舰企业级SSD D5-P5336，容量起步就有7.68TB，最高更是做到了史无前例的61.44TB，是此前纪录的整整两倍，1U机箱就可以达成2PB的存储空间！
Solidigm D5-P5336提供了多种形态、容量规格，首发出货的是E1.L，最高容量30.72TB。
今年晚些时候增加U.2并升级E1.L，最高容量升至61.44TB，明年还会扩大到E3.S，最高容量30.72TB。
它采用了QLC闪存，192层堆叠，企业级品质。
按照Solidigm此前给出的数据，最大写入寿命可达65PBW，五年质保期内平均每天可以写入最多35.6TB，甚至能够坚持使用长达70年都不会坏。
性能方面，目前官方只给出了首发的E1.L 15.36TB，支持PCIe 4.0 x4，顺序读写最高7GB/s、3.1GB/s，4K随机读取最高1005K IOPS，16KB随机写入最高35K IOPS，写入寿命14.11PBW，平均故障间隔时间200万小时。
Solidigm表示，相比于全TLC闪存阵列，D5-P5336可节省17％的TCO成本，机架占地面积减少1.9倍，供电和散热能耗减少1.25倍，对比全SAS硬盘阵列、混合阵列的优势更大。
如果61.44TB还是不够用，也有办法，一块不行就多块！
Apex Storage曾经发布过一款SSD扩展卡“X21”，双卡并排，可以挂载21块M.2 SSD，单个最大容量8TB，合计最多168TB，而凭借100条PCIe 4.0系统通道，顺序读写速度可达31GB/s。
现在，Apex Storage又发布了一款“X16”，支持16块SSD，看起来弱了一些，但其实更强悍了。
它只用了一块PCB，正反面都可以挂载8块M.2 SSD，每块还是最大8TB，合计容量最多128TB。
PCIe 4.0通道依然有84条，读写速度仍旧能达到31GB/s，也支持完整的UEFI、安全启动。
正面中间布置了一个主动风扇和一些散热片，为PLX桥接控制器散热，同时让整体体积只有单插槽厚度，非常迷你。
X21扩展卡的价格为2800美元(约合人民币2.0万元)，X16则是1800美元(约合人民币1.3万元)。
当然，SSD你得另外单独购买。
今年PCIe 5.0硬盘终于迎来了一波爆发，多家厂商的旗舰SSD都换成PCIe 5.0的了，主控也多是群联E26，速度冲上14GB/s，然而享受超高速度不是没代价，最近有多款硬盘热到崩溃了。
出现问题的SSD此前主要是Corsair MP700，现在有用户报告Seagate FireCuda 540、Gigabyte Aorus Gen5 10000和Adata Legend 970等PCIe 5.0硬盘也会有问题，就是使用过程中会突然崩溃、关机。
问题的根源并不负责，这些SSD设计之初是搭配散热器使用的，但是如果是没有散热器的情况下，PCIe 5.0的高性能会导致大量热量产生，SSD的保护机制甚至做不到过热降频，就直接停止工作了。
解决方法也不复杂，已有厂商承诺发布固件升级，群联最新的22.1固件引入了链路状态热调节功能，降低了PCIe接口速度，也就是从5.0降至4.0甚至3.0，这样就能降低PCIe物理层温度，SSD不用热到崩溃、关机。
当然，这样做就是变相降低了性能，PCIe 5.0变成PCIe 4.0、3.0而已。
彻底解决办法就是上散热器，不要在没有散热器的情况下使用PCIe 5.0硬盘，甚至传统的被动散热都不够用了，最好是上那种带有小风扇的主动散热。
如果风扇还不够用，还有厂商推出了SSD水冷散热器，之前大家还嘲笑厂商，现在应该理解厂商的一片苦心了吧。
]]></content>
  </entry>
  
  <entry>
    <title>PCB线路设计制作术语</title>
    <url>/post/hardware/terminology-of-PCB-circuit-design-and-fabrication.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>PCB</tag>
      <tag>Layout</tag>
    </tags>
    <content type="html"><![CDATA[本文讲解了PCB线路设计制作术语。
1、Annular Ring 孔环
指绕接通孔壁外平贴在板面上的铜环而言。在内层板上此孔环常以十字桥与外面大地相连，且更常当成线路的端点或过站。在外层板上除了当成线路的过站之外，也可当成零件脚插焊用的焊垫。与此字同义的尚有 Pad(配圈)、 Land (独立点)等。
2、Artwork 底片
在电路板工业中，此字常指的是黑白底片而言。至于棕色的“偶氮片”(Diazo Film)则另用 Phototool 以名之。PCB 所用的底片可分为“原始底片”Master Artwork 以及翻照后的“工作底片”Working Artwork 等。
3、Basic Grid 基本方格
指电路板在设计时，其导体布局定位所着落的纵横格子。早期的格距为 100 mil，目前由于细线密线的盛行，基本格距已再缩小到 50 mil。
4、Blind Via Hole 盲导孔
指复杂的多层板中，部份导通孔因只需某几层之互连，故刻意不完全钻透，若其中有一孔口是连接在外层板的孔环上，这种如杯状死胡同的特殊孔，称之为“盲孔”(Blind Hole)。
5、Block Diagram 电路系统块图
将组装板及所需的各种零组件，在设计图上以正方或长方形的空框加以框出， 且用各种电性符号，对其各框的关系逐一联络，使组成有系统的架构图。
6、Bomb Sight 弹标
原指轰炸机投弹的瞄准幕。PCB 在底片制作时，为对准起见也在各角落设置这种上下两层对准用的靶标，其更精确之正式名称应叫做Photographers'　Target。
7、Break-away panel 可断开板
指许多面积较小的电路板，为了在下游装配线上的插件、放件、焊接等作业的方便起见，在 PCB 制程中，特将之并合在一个大板上，以进行各种加工。完工时再以跳刀方式，在各独立小板之间进行局部切外形(Routing)断开，但却保留足够强度的数枚“连片”(Tie Bar 或Break-away Tab)，且在连片与板边间再连钻几个小孔；或上下各切 V 形槽口，以利组装制程完毕后，还能将各板折断分开。这种小板子联合组装方式，将来会愈来愈多， IC卡即是一例。
8、Buried Via Hole 埋导孔
指多层板之局部导通孔，当其埋在多层板内部层间成为“内通孔”，且未与外层板“连通”者，称为埋导孔或简称埋孔。
9、Bus Bar 汇电杆
多指电镀槽上的阴极或阳极杆本身，或其连接之电缆而言。另在“制程中”的电路板，其金手指外缘接近板边处，原有一条连通用的导线(镀金操作时须被遮盖)，再另以一小窄片(皆为节省金量故需尽量减小其面积)与各手指相连，此种导电用的连线亦称 Bus Bar。而在各单独手指与 Bus Bar 相连之小片则称Shooting Bar。在板子完成切外形时，二者都会一并切掉。
10、CAD电脑辅助设计
Computer Aided Design，是利用特殊软体及硬体，对电路板以数位化进行布局(Layout)，并以光学绘图机将数位资料转制成原始底片。此种 CAD对电路板的制前工程，远比人工方式更为精确及方便。
11、Center-to-Center Spacing 中心间距
指板面上任何两导体其中心到中心的标示距离(Nominal Distance)而言。若连续排列的各导体，而各自宽度及间距又都相同时(如金手指的排列)，则此“中心到中心的间距”又称为节距(Pitch)。
12、Clearance 余地、余隙、空环
指多层板之各内层上，若不欲其导体面与通孔之孔壁连通时，则可将通孔周围的铜箔蚀掉而形成空环，特称为“空环”。又外层板面上所印的绿漆与各孔环之间的距离也称为 Clearance 。不过由于目前板面线路密度愈渐提高，使得这种绿漆原有的余地也被紧逼到几近于无了。
13、Component Hole 零件孔
指板子上零件脚插装的通孔，这种脚孔的孔径平均在 40 mil 左右。现在SMT盛行之后，大孔径的插孔已逐渐减少，只剩下少数连接器的金针孔还需要插焊，其余多数 SMD 零件都已改采表面粘装了。
14、Component Side 组件面
早期在电路板全采通孔插装的时代，零件一定是要装在板子的正面，故又称其正面为“组件面”。板子的反面因只供波焊的锡波通过，故又称为“焊锡面”(Soldering Side) 。目前 SMT 的板类两面都要粘装零件，故已无所谓“组件面“或“焊锡面”了，只能称为正面或反面。通常正面会印有该电子机器的制造厂商名称，而电路板制造厂的 UL 代字与生产日期，则可加在板子的反面。
15、Conductor Spacing 导体间距
指电路板面的某一导体，自其边缘到另一最近导体的边缘，其间所涵盖绝缘底材面的跨距，即谓之导体间距，或俗称为间距。又，Conductor 是电路板上各种形式金属导体的泛称。
16、Contact Area 接触电阻
在电路板上是专指金手指与连接器之接触点，当电流通过时所呈现的电阻之谓。为了减少金属表面氧化物的生成，通常阳性的金手指部份，及连接器的阴性卡夹子皆需镀以金属，以抑抵其“接载电阻”的发生。其他电器品的插头挤入插座中，或导针与其接座间也都有接触电阻存在。
17、Corner Mark 板角标记
电路板底片上，常在四个角落处留下特殊的标记做为板子的实际边界。若将此等标记的内缘连线，即为完工板轮廓外围(Contour)的界线。
18、Counterboring 定深扩孔，埋头孔
电路板可用螺丝锁紧固定在机器中，这种匹配的非导通孔(NPTH)，其孔口须做可容纳螺帽的“扩孔”，使整个螺丝能沉入埋入板面内，以减少在外表所造成的妨碍。
19、Crosshatching 十字交叉区
电路板面上某些大面积导体区，为了与板面及绿漆之间都得到更好的附着力起见，常将感部份铜面转掉，而留下多条纵横交叉的十字线，如网球拍的结构一样，如此将可化解掉大面积铜箔，因热膨胀而存在的浮离危机。其蚀刻所得十字图形称为 Crosshatch，而这种改善的做法则称为 Crosshatching。
20、Countersinking 锥型扩孔，喇叭孔
是另一种锁紧用的螺丝孔，多用在木工家俱上，较少出现精密电子工业中。
21、Crossection Area 截面积
电路板上线路截面积的大小，会直接影响其载流能力，故设计时即应首先列入见，常将感部份铜面转掉，而留下多条纵横交叉的十字线，如网球拍的结构一样，如此将可化解掉大面积铜箔，因热膨胀而存在的浮离危机。其蚀刻所得十字图形称为 Crosshatch，而这种改善的做法则称为 Crosshatching。
22、Current-Carrying Capability 载流能力
指板子上的导线，在指定的情况下能够连续通过最大的电流强度(安培)，而尚不致引起电路板在电性及机械性质上的劣化 (Degradation)，此最大电流的安培数，即为该线路的“载流能力”。
23、Datum Reference 基准参考
在 PCB 制造及检验的过程中，为了能将底片图形在板面上得以正确定位起见,特选定某一点、线,或孔面做为其图形的基准参考，称为 Datum Point，Datum Line，或称 Datum Level(Plane)，亦称 Datum Hole。
24、Dummy Land 假焊垫
组装时为了牵就既有零件的高度，某些零件肚子下的板面需加以垫高，使点胶能拥有更好的接着力，一般可利用电路板的蚀刻技术，刻意在该处留下不接脚不通电而只做垫高用的“假铜垫”，谓之 Dummy Land。不过有时板面上因设计不良，会出现大面积无铜层的底材面，分布着少许的通孔或线路。为了避免该等独立导体在镀铜时过度的电流集中，而发生各种缺失起见，也可增加一些无功能的假垫或假线，在电镀时分摊掉一些电流，让少许独立导体的电流密度不至太高，这些铜面亦称为 Dummy Conductors。
25、Edge Spacing板边空地
指由板边到距其“最近导体线路”之间的空地，此段空地的目的是在避免因导体太靠近板边，而可能与机器其他部份发生短路的问题，美国UL之安全认证，对此项目特别讲究。一般板材之白边分层等缺点不可渗入此“边地”宽度的一半。
26、Edge-Board contact板边金手指
是整片板子对外联络的出口，通常多设板边上下对称的两面上，可插接于所匹配的板边连接器中。
27、Fan Out Wiring／Fan in Wiring扇出布线／扇入布线
指QFP四周焊垫所引出的线路与通孔等导体，使焊妥零件能与电路板完成互连的工作。由于矩形焊垫排列非常紧密，故其对外联络必须利用矩垫方圈内或矩垫方圈外的空地，以扇形方式布线，谓之“扇出”或“扇入”。更轻薄短小的密集PCB，可在外层多安置一些焊垫以承接较多零件，而将互连所需的布线藏到下一层去。其不同层次间焊垫与引线的衔接，是以垫内的盲孔直接连通，无须再做扇出扇入式布线，目前许多高功能小型无线电话的手机板，即采此种新式的叠层与布线法。
28、Fiducial Mark 光学靶标，基准讯号
在板面上为了下游组装，方便其视觉辅助系统作业起见，常大型IC于板面组装位置各焊垫外缘的空地上，在其右上及左下各加一个三角的“光学靶标”，以协助放置机进行光学定位，便是一例。而PCB制程为了底片与板面在方位上的对准，也常加有两枚以上的基准记号。
29、Fillet内圆填角
指两平面或两直线，在其垂直交点处所补填的弧形物而言。在电路板中常指零件引脚之焊点，或板面T形或L形线路其交点等之内圆填补，以增强该处的机械强及电流流通的方便。
30、Film 底片
指已有线路图形的软片而言。通常厚度有7mil及4mil两种，其感光的药膜有黑白的卤化银，及棕色或其他颜色的偶氮化合物，此词亦称为Artwork。
31、Fine Line 细线
按目前的技术水准，孔间四条线或平均线宽在5～6mil以下者，称为细线。
32、Fine Pitch密脚距，密线距，密垫距
凡脚距(Lead Pitch)等于或小于 0.635mm(25mil)者，称为密距。
33、Finger 手指（板边连续排列接点）
在电路板上为能使整片组装板的功能得以对外联络起见，可采用板边“阳式“的镀金连续接点，以插夹在另一系统”阴式“连续的承接器上，使能达到系统间相互连通的目的。Finger的正式名称是“Edge-Board Contact&quot;。
34、Finishing 终饰、终修
指各种制成品在外观上的最后修饰或修整工作，使产品更具美观、保护，及质感的目的。Metal Finishing特指金属零件或制品，其外表上为加强防蚀功能及观而特别加做的处理层而言，如各种电镀层、阳极处理皮膜、有机物或无机物之涂装等，皆属之。
35、Form-to-List 布线说明清单
是一种指示各种布线体系的书面说明清单。
36、Gerber Date，Gerber File 格博档案
是美商 Gerber 公司专为电路板面线路图形与孔位，所发展一系列完整的软体档案。设计者或买板子的公司，可将某一料号的全部图形资料转变成 Gerber File(正式学名是“RS 274 格式”)，经由Modem 直接传送到 PCB 制造者手中，然后从其自备的 CAM 中输出，再配合雷射绘图机(Laser Plotter)的运作下，而得到钻孔、测试、线路底片、绿漆底片…甚至下游组装等具体作业资料，使得 PCB制造者可立即从事打样或生产，节省许多沟通及等待的时间。此种电路板“制前工程”各种资料的电脑软体，目前全球业界中皆以 Gerber File为标准作业。此外尚有 IPC-D-350D 另一套软体的开发，但目前仍未见广用。
37、Grid 标准格
指电路板布线图形时的基本经纬方格而言，早期长宽格距各为 100 mil，那是以“积体电路”(IC)引脚的脚距为参考而定的，目前密集组装已使得此种Grid 再逼近到 50 mil 甚至 25 mil。座落在格子交点上则称为 On Grid。
38、Ground Plane Clearance 接地空环
“积体电路器”不管是传统 IC 或是 VLSI ，其接地脚或电压脚，与其接地层(GND)或接电压层(Vcc)的脚孔接通后，再以“一字桥”或“十字桥”与外面的大铜面进行互连。至于穿层而过完全不接大铜面的通孔，则必须取消任何桥梁而与外地隔绝。又为了避免因受热而变形起见，通孔与大铜面之间必须留出膨胀所需的伸缩空环(Clearance Ring，即图中之白环)。因而可从已知引脚所接连的层次，即可判断出到底是 GND 或 Vcc 了。一般通孔制作若各站管理不善的话，将会发生“粉红圈”，但此种粉红圈只应出现在空环(Clearance Ring)以内的孔环(Annular Ring)上，而不应该越过空环任凭其渗透到大地上，那样就太过份了。
39、Ground plane(or Earth Plane) 接地层
是属于多层板内层的一种板面，通常多层板的一层线路层，需要搭配一层大铜面的接地层，以当成众多零件公共回路的接地、遮蔽(Shielding) 、以及散热(Heatsinking)之用。以传统 TTL 逻辑双排脚的 IC 为例，从其正面(背面)观看时，以其一端之缺口记号朝上，其左边即为第一只脚(通常在第一脚旁的本体也会打上一个小凹陷或白点作为识别)，按顺序数到该排的最后一脚即为“接地脚”。再按反时针方向数到另一排最后一脚，就是要接电压层(Power Plane)的引脚。
40、Hole Density 孔数密度
指板子在单位面积中所钻的孔数而言。
41、Indexing Hole 基准孔、参考孔
指电路板于制造中在板角或板边先行钻出某些工具孔，以当成其他影像转移、钻孔、或切外形，以及压合制程的基本参考点，称为 Indexing Hole。其他尚有 Indexing Edge、Slot、Notch 等类似术语。
42、Inspection Overlay 套检底片
是采用半透明的线路阴片或阳片(如 Diazo之棕片、绿片或蓝片等)，可用以套准在板面上做为对照目检的工具，此法可用于“首批试产品”(First Article)之目检用途。
43、Key 钥槽，电键
前者在电路板上是指金手指区某一位置的开槽缺口，目的是为了与另一具阴性连接器得以匹配，在插接时不致弄反的一种防呆设计，称为 Keying Slot。后者是指有弹簧接点的密封触控式按键，可做为电讯的快速接通及跳开之用。
44、Land 孔环焊垫、表面(方型)焊垫
早期尚未推出 SMT 之前，传统零件以其脚插孔焊接时，其外层板面的孔环,除须做为导电互连之中继站(Terminal)外，尚可与引脚形成强固的锥形焊点。后来表面粘装盛行,所改采的板面方型焊垫亦称为 Land。此字似可译为“焊环”或“配圈”或“焊垫”，但若译成“兰岛”或“鸡眼”则未免太离谱了。
45、Landless Hole 无环通孔
指某些密集组装的板子，由于板面需布置许多线路及粘装零件的方型焊垫，所剩的空地已经很少。有时对已不再用于外层接线或插焊，如仅做为层间导电用的导孔(Via Hole)时，则可将其孔环去掉，而挪出更多的空间用以布线，此种只有内层孔环而无外层孔环的通孔，特称为 Landless Hole。
46、Laser Photogenerator(LPG)，Laser Photoplotter 雷射曝光机
直接用雷射的单束平行光再配合电脑的操控，用以曝制生产 PCB 的原始底片(Master Artwork)，以代替早期用手工制作的原始大型贴片(Tape-up),及再缩制而成的原始底片。此种原始底片的运送非常麻烦,一旦因温湿度发生变化,则会导致成品板尺寸的差异，精密板子的品质必将大受影响。如今已可自客户处直接取得磁碟资料，配合雷射之扫瞄曝光即可得到精良的底片，对电路板的生产及品质都大有助益。
47、Lay Out 布线、布局
指电路板在设计时，各层次中各零件的安排，以及导线的走向、通孔的位置等整体的布局称为 Lay Out。
48、Layer to Layer Spacing 层间距离
是指多层板两铜箔导体层之间的距离，或指绝缘介质的厚度而言。通常为了消除板面相邻线路所产生的杂讯起见，其层次间距中的介质要愈薄愈好，使所感应产生的杂讯得以导入接地层之中。但如何避免因介质太薄而引发的漏电，及保持必须的平坦度，则又是另两项不易克服的难题。
49、Master Drawing 主图
是指电路板制造上各种规格的主要参考，也记载板子各部尺寸及特殊的要求，即俗称的“蓝图”，是品检的重要依据。所谓一切都要“照图施工”，除非在授权者签字认可的进一步资料(或电报或传真等)中可更改主图外，主图的权威规定是不容回避的。其优先度(Priority)虽比订单及特别资料要低，但却比各种成文的“规范”(Specs)及习惯做法都要重要。
50、Metal Halide Lamp 金属卤素灯
碘是卤素中的一种，碘在高温下容易由固体直接“升华”成为气体。在以钨丝发光体的白炽灯泡内，若将碘充入其中，则在高温中会形成碘气。此种碘气能够捕捉已蒸发的钨原子而起化学反应，将令钨原子再重行沉落回聚到钨丝上，如此将可大幅减少钨丝的消耗，而增加灯泡的寿命。并且还可加强其电流效率而增强亮度。一般多用于汽车的前灯、摄影、制片与晒版感光等所需之光源。这种碘气白炽灯也是一种不连续光谱的光源，其能量多集中在紫外区的 410～430 nm 的光谱带中,如同汞气灯一样，也不能随意加以开关。但却可在不工作时改用较低的能量，维持暂时不灭的休工状态，以备下次再使用时，将可得到瞬间的立即反应。
51、Mil 英丝
是一种微小的长度单位，即千分之一英吋【0.001 in】之谓。电路板工业中常用以表达“厚度”。此字在机械业界原译为“英丝”或简称为“丝”，且亦行之有年，系最基本的行话。不过一些早期美商“安培电子”的PCB从业人员，不明就里也未加深究，竟将之与另一公制微长度单位的“条”(即10微米) 混为一谈。流传至今已使得大部份业界甚至下游组装业界，在二十年的以讹传讹下，早已根深蒂固积非成是即使想改正也很不容易了。最让人不解的是，连金手指镀金层厚度的微吋(m-in)，也不分青红皂白一律称之为“条”，实乃莫名其妙之极。反而大陆的PCB界都还用法正确。此外若三个字母全大写成MIL时，则为“美军”Military的简写，常用于美军规范(如MIL -P-13949H,或MIL-P-55110D)与美军标准(如MIL-STD-202 等)之书面或口语中。
52、Minimum Electrical Spacing 电性间距下限，最窄电性间距
指两导体之间，在某一规定电压下，欲避免其间介质发生崩溃(Break down) ，或欲防止发生电晕(Corona)起见，其最起码应具有的距离谓之“下限间距”。
53、Mounting Hole 安装孔
为电路板上一种无导电功能的独立大孔，系将组装板锁牢在机体架构上而用的。这种做为机械用途的孔，称为“安装孔”。此词也指将较重的零件以螺丝锁在板子上用的机械孔而言。
54、Mounting Hole组装孔，机装孔
是用螺丝或其他金属扣件，将组装板锁牢固定在机器底座或外壳的工具孔，为直径 160mil左右的大孔。此种组装孔早期均采两面大型孔环与孔铜壁之PTH，后为防止孔壁在波焊中沾锡而影响螺丝穿过起见，新式设计特将大孔改成“非镀通孔”(在PTH之前予以遮盖或镀铜之后再钻二次孔) ，而于周围环宽上另做数个小型通孔以强化孔环在板面的固着强度。由于NPTH十分麻烦，近来SMT板上也有将大孔只改回PTH者，其两面孔环多半不相同，常将焊接面的大环取消而改成几个独立的小环，或改成马蹄形不完整的大环，或扩充面积成异形大铜面，兼做为接地之用。
55、Negative 负片，钻尖第一面外缘变窄
是指各种底片上(如黑白软片、棕色软片及玻璃底片等)，导体线路的图案是以透明区呈现，而无导体之基材部份则呈现为暗区(即软片上的黑色或棕色部份) ，以阻止紫外光的透过。此种底片谓之负片。又，此字亦指钻头之钻尖，其两个第一面外缘因不当重磨而变窄的情形。
56、Non-Circular Land 非圆形孔环焊垫
早期电路板上的零件皆以通孔插装为主，在填孔焊锡后完成互连(Interconnection)的功能。某些体积较大或重量较重的零件，为使在板面上的焊接强度更好起见，刻意将其孔外之环形焊垫变大，以强化焊环的附着力，及形成较大的锥状焊点。此种大号的焊垫在单面板上尤为常见。
57、Pad Master圆垫底片
是早期客户供应的各原始底片之一种，指仅有“孔位”的黑白“正片”。其中每一个黑色圆垫中心都有小点留白，是做为“程式打带机”寻找准确孔位之用。该Pad Master完成孔位程式带制作之后，还要将每一圆垫中心的留白点，以人工方式予以涂黑再翻成负片，即成为绿漆底片。如今设计者已将板子上各种所需的“诸元与尺度”都做成Gerber File的磁片，直接输入到CAM及雷射绘图机中，即可得到所需的底片，不但节省人力而且品质也大幅提升。附图即为新式Pad Master底片的一角，是两枚大型IC所接插座的孔位。
58、Pad焊垫，圆垫
此字在电路板最原始的意思，是指零件引脚在板子上的焊接基地。早期通孔插装时代，系表示外层板面上的孔环。1985年后的SMT时代，此字亦指板面上的方形焊垫。不过此字亦常被引伸到其他相关的方面，如内层板面上尚未钻孔成为孔环的各圆点或小圆盘，业界也通常叫做Pad；此字可与Land通用。
59、Panel制程板
是指在各站制程中所流通的待制板。其一片Panel中可能含有好几片“成品板“(Board)。此等”制程板“的大小，在每站中也不一定相同，如压合站之Panel板面可能很，大但为了适应钻孔机的每一钻轴作业起见，只好裁成一半或四分之一的Panel Size。当成品板的面积很小时，其每一Panel中则可排入多片的Board。通常Panel Size愈大则生产愈经济。
60、Pattern板面图形
常指电路板面的导体图形或非导体图形而言，当然对底片或蓝图上的线路图案，也可称为Pattern。
61、Photographic Film感光成像之底片
是指电路板上线路图案的原始载体，也就是俗称的“底片”(Art Work)。常用的有Mylar式软片及玻璃板之硬片。其遮光图案的薄膜材质，有黑色的卤化银(Silver halid)及棕色的偶氮化合物(Diazo)。前者几乎可挡住各种光线，后者只能挡住550nm以下的紫外光。而波长在550nm以上的可见光，对干膜已经不会发生感光作用，故其工作区可采用黄光照明，比起卤化银黑白底片只能在暗红光下作业，的确要方便得多了。
62、Photoplotter， Plotter光学绘图机
是以移动性多股单束光之曝光法，代替传统固定点状光源之瞬间全面性曝光法。在数位化及电脑辅助之设计下，PCB设计者可将原始之孔环、焊垫、布线及尺寸等精密资料，输入电脑在Gerber File系统下，收纳于一片磁片之内。电路板生产者得到磁片后，即可利用CAM及光学绘图机的运作而得到尺寸精准的底片，免于运送中造成底片的变形。由于普通光源式的Photoplotter缺点甚多，故已遭淘汰。现在业界已一律使用雷射光源做为绘图机。已成为商品者有平台式(Flat Bed)、内圆筒式(Inner drum)、外圆筒式(Outer Drum)，及单独区域式等不同成像方式的机种。其等亦各有优缺点，是现代PCB厂必备的工具。也可用于其他感光成像的工作领域，如LCD、PCM等工业中。
63、Phototool底片
一般多指偶氮棕片(Diazo film)，可在黄色照明下工作，比起只能在红光下工作的黑白卤化银底片要方便一些。
64、Pin接脚，插梢，插针
指电路板孔中所插装的镀锡零件脚，或镀金之插针等。可做为机械支持及导电互连用处，是早期电路板插孔组装的媒介物。其纵横之间距(Pitch)早期大多公定为100 mil，以做为电路板及各种零件制造的依据。
65、Pitch跨距，脚距，垫距，线距
Pitch纯粹是指板面两“单元”中心间之远近距离，PCB业美式表达常用mil pitch，即指两焊垫中心线间的跨距mil而言。 Pitch与Spacing不同，后者通常是指两导体间的“隔离板面”，是面积而非长度。
66、Plotting标绘
以机械方式将X、Y之众多座标数据在平面座标系统中，描绘成实际线路图的作业过程，便称为Plot或Plotting。目前底片的制作已放弃早期的徒手贴图(Tape up)，而改用“光学绘图”方式完成底片，不但节省人力，而且品质更好。
67、Polarizing Slot偏槽
指板边金手指区的开槽，一般故意将开槽的位置放偏，以避免因左右对称而可能插反，此种为确保正确插接而加开的方向槽，亦称为Keying Slot。
68、Process Camera制程用照像机
是做底片(Artwork)放大、缩小，或从贴片(Tape up)直接照像而得到底片的专用相机。其组成有三大件直立于可移动的轨道上且彼此平行，即图中右端的原始贴片或母片架、镜头，以及左端待成像的子片架等。这是早期生产底片的方式，目前已进步到数位化，自客户取得的磁碟，经由电脑软体及电射绘图机的工作下，即可直接得到原始底片，已无须再用到照相机了。
69、Production Master生产底片
指1：1可直接用以生电路板的原寸底片而言，至于各项诸元的尺寸与公差，则须另列于主图上 (Master Drawing亦即蓝图)。
70、Reference Dimension参考尺度
参考尺寸仅供参考资料用的尺度，因未设公差故不能当成正式施工及品检的根据。
71、Reference Edge参考边缘
指板边板角上某导体之一个边缘，可做为全板尺寸的量测参考用，有时也指某一特殊鉴别记号而言。
72、Register Mark对准用标记
指底片上或板面上，各边框或各角落所设定的特殊标记，用以检查本层或各层之间的对准情形，图示者即为两种常用的对准标记。其中同心圆形者可在多层板每层的板边或板角处，依序摆设不同直径的圆环，等压合后只要检查所“扫出”(即铣出)立体同心圆之套准情形，即可判断其层间对准度的好坏。
73、Registration对准度
电路板面各种导体之实际位置，与原始底片或原始设计之原定位置，其两者之间逼近的程度，谓之“ Registration”。大陆业界译为“重合度”。“对准度”可指某一板面的导体与其底片之对准程度；或指多层板之“层间对准度” (Layer to Layer Registration)，皆为PCB的重要品质。
74、Revision修正版，改订版
指规范或产品设计之修正版本或版次，通常是在其代号之后加上大写的英文字母做为修订顺序之表示。
75、Schemetic Diagram电路概略图
利用各种符号、电性连接、零件外形等，所画成的系统线路布局概要图。
76、Secondary Side第二面
此即电路板早期原有术语之“焊锡面” (Solder Side)。因早期在插孔焊接零件时，所有零件都装在第一面 (或称Component Side;组件面)，第二面则只做为波焊接触用途，故称为焊锡面。待近年来因SMT表面粘装兴起，其正反两面都装有很多零件，故不宜再续称为焊锡面，而以“第二面”较恰当。
77、Slot, Slotting槽口，开槽
指 PCB板边或板内某处，为配合组装之需求，而须进行“开槽”以做为匹配，谓之槽口。在金手指板边者，也称为“偏槽”或“定位槽”(Polarising Slot or Locating Slot)，是故意开偏以避免金手指阴式接头的插反。
78、Solder Dam锡堤
指焊点周围由绿漆厚度所形成的堤岸，可防止高温中熔锡流动所造成之短路，通常以干膜式的防焊膜较易形成 Solder Dam。
79、Solder Plug锡塞，锡柱
指在波焊中涌入镀通孔内的焊锡，冷却后即留在孔中成为导体的一部份，称为“锡塞”。若孔中已有插接的零件脚时，则锡塞还具有“焊接点”的功用。至于目前一般不再用于插接，而只做互连目的之 PTH，则多已改成直径在20 mil以下的小孔，称之为导通孔 (Via Hole)。此等小孔的两端都已盖满或塞满绿漆，阻止助焊剂及熔锡的进入，这种导通孔当然就不会再有 Solder Plug了。
80、Solder Side焊锡面
早期电路板组装完全以通孔插装为主流，板子正面(即零组件面)常用来插装零件，其布线多按“板横”方向排列。板子反面则用以配合引脚通过波焊机的锡波，故称为“焊接面”，此面线路常按“板长”方向布线，以顺从锡波之流动。此词之其他称呼尚有 Secondary Side， Far Side等。
81、Spacing间距
指两平行导体间其绝缘空地之宽度而言，通常将“间距”与“线路”二者合称为“线对”(Line Pair)。
82、Span跨距
指两特殊目标点之间所涵盖的宽度，或某一目标点与参考点之间的距离。
83、Spur底片图形边缘突出
指底片上的透明区或黑暗区的线路图形，当其边缘解像不良发生模糊不清时，常出现不当的突出点，称为Spur。
84、Step and Repeat逐次重复曝光
面积很小的电路板为了生产方便起见，在底片制作阶段常将同一图案重复排列成较大的底片。系使用一种特殊的 Step and Repect 式曝光机，将同一小型图案逐次局部曝光再并连成为一个大底片，再用以进行量产。
85、Supported Hole(金属)支助通孔
指正常的镀通孔(PTH)，即具有金属孔壁的钻孔。一般都省略前面的“支持性”字眼。原义是指可导电及提供引脚焊接用途的通孔。
86、Tab接点，金手指
在电路板上是指板边系列接点的金手指而言，为一种非正式的说法。
87、Tape Up Master原始手贴片
早期电路板之底片，并非使用 CAD/CAM及雷射绘图机所制作，而是采各种专用的黑色“贴件” (如线路、圆垫、金手指等尺寸齐全之专用品，以Bishop之产品最为广用)，在方眼纸上以手工贴成最原始的“贴片”(Tape Up Master)，再用照相机缩照成第一代的原始底片 (Master Artwork)。十余年前日本有许多电路板的手贴片工作，即以空运来往台湾寻求代工。近年来由于电脑的发达与精准，早已取代手工的做法了。
88、Terminal Clearance端子空环，端子让环
在内层板之接地(Ground)或电压(Power)两层大铜面上，当“镀通孔”欲从内层板中穿过而又不欲连接时，则可先将孔位处的铜面蚀掉，而留出较大的圆形空地，则当PTH铜孔壁完成时，其外围自然会出现一围“空环”。另外在外层板面上加印绿漆时，各待焊之孔环周围也要让出“环状空地”，避免绿漆沾污焊环甚至进孔。这两种“空环”也可称为“Terminal Clearance”。
89、Terminal端子
广义上所说的“端子”，是指做为电性连接的各种装置或零件。电路板上的狭义用法是指内外层的各种孔环(Annumlar Ring)而言。同义词尚有Pad、Land、Terminal Area、 Terminal Pad、 Solder Pad等。
90、Thermal Relief散热式镂空
不管是在内外层板上的大铜面，其连续完整的面积皆不可过大，以免板子在高温中(如焊接)，因板材与铜皮之间膨胀系数的差异而造成板翘、浮离，或起泡等毛病。一般可在大铜面上采“网球拍”式的镂空，以减少热冲击。此词亦称为 Halfonning或Crosshatching等。UL规定在其认证的“黄卡”中，需要登载在板子上最大铜面的直径，即是一种安全的考虑。
91、Thermal Via导热孔，散热孔
是分布在高功率(如5W以上)大型零件 (如CPU或其他驱动IC)腹底板面上的通孔，此等通孔不具导电互连功能只做散热用途。有时还会与较大的铜面连接，以增加直接散热的效果。此等散热孔对 Z方向热应力具有舒缓的作用。精密Daught Card的 8 层小板，或在某些BGA双面板上，就常有这种格点排列的散热孔，与两面镀金的“散热座”等设计。
92、Throwing Power分布力
当电镀进行时，因处在阴极的工作物受其外形的影响，造成“原始电流分布” (Primary Current Distribution)的高低不均，而出现镀层厚度的差异。此时口在槽液中添加各种有机助剂(如光泽剂、整平剂、润湿剂等)，使阴极表面原有之高电流区域，在各种有机物的影响下，对原本快速增厚的镀层有一种减缓作用，从而得以拉近与低电流区域在镀厚上的差异。这种槽液中有机添加剂对阴极镀厚分布的改善能力，称为槽液的“分布力”，是一种需高度配合的复杂实验结果。当湿式电解制程为阳极处理时，则此“分布力”一词也适用于挂在阳极的工作物。如铝件的阳极处理，就是常见的例子。
93、Thermo-Via导热孔
指电路板上之大型 IC 等高功率零件，在工作中会发生多量的热能，组装板必须要将此额外的热量予以排散，以免损及该电子设备的寿命。其中一种简单的散热方法，就是利用表面粘装大型 IC的底座板材空地，刻意另加制作PTH，将大型IC所发的热，直接引至板子背面的大铜面上，以进行散热。此种专用于传热而不导电的通孔，称为 Thermo-Via。
94、Tie Bar分流条
在电路板工业中是指板面经蚀刻得到独立线路后，若还需再做进一步电镀时，须预先加设导电的路径才能继续进行。例如于金手指区的铜面上，再进行镀镍镀金时，只能靠特别留下来的 Bus Bar( 汇流条)及 Tie Bar 去接通来自阴极杆的电流。此临时导电用的两种“工具线路”，在板子完工后均将自板边予以切除。
95、Tooling Feature工具标的物
是指电路板在各种制作及组制过程中，用以定位、对准、参考之各种标志物。如工具孔、参考点、裁切点、参考线、定位孔、定位槽、对准记号等，总称为“工具用标的物”。
96、Trace线路、导线
指电路板上一般导线或线路而言，通常并不包括通孔、大地，焊垫及孔环等。原文中当成“线路”用的术语尚有Track、Line、 Line Run、Conductor等。
97、Trim Line裁切线
电路板成品的外围，在切外型时所应遵循的边界线称为 Trim Line。
98、True Position真位
指电路板孔位或板面各种标的物(Feature)，其等在设计上所坐落的理论位置，称为真位。但由于各种图形转移以及机械加工制程等，不免都隐藏着误差公差，不可能每片都很准确。当板子在完工时，只要“标的”仍处于真位所要求圆面积的半径公差范围内(True Position Tolerance)，而不影响组装及终端功能时，则其品质即可允收。
99、Unsupported Hole非镀通孔
指不做导通或插装零件用途，又无镀铜孔壁之钻孔而言，通常此等NPTH孔径多半很大，如 125 mil之锁螺丝孔即是。
100、Via Hole导通孔
指电路板上只做为导电互连用途，而不再插焊零件脚之 PTH 而言。此等导通孔有贯穿全板的“全通导孔”(Through Via Hole)、有只接通至板面而未全部贯穿的“盲导孔”(Blind Via Hole)、有不与板面接通却埋藏在板材内部之“埋通孔”(Buried Via Hole)等。此等复杂的局部通孔，是以逐次连续压合法(Sequential Lamination) 所制作完成的。此词也常简称为“Via”。
101、Voltage Plane Clearance电压层的空环
当镀通孔须穿过多层板之内藏电压层，而不欲与之接触时，可在电压层的铜面上先行蚀刻出圆形空地，压合后再于此稍大的空地上钻出较小的孔，并继续完成PTH。此时其管状孔铜壁与电压层大铜面之间，即有一圈空环存在而得以绝缘，称之为Clearance。
102、Voltage Plane电压层
是指电路板上驱动各种零件工作所需的电压，可藉由板面一种公共铜导体区予以供给，或多层板中以一个层次做为电压层，如四层板的两内层之一就是电压层(如5V或 12V)，一般以Vcc符号表示。另一层是接地层 (Groung Plane)。通常多层板的电压层除供给零件所需的电压外，也兼做散热 (Heat Sinking)与屏障(Shielding)之功能。
103、Wiring Pattern布线图形
指电路板设计上之“布线”图形，与Circuitry Pattern、 Line Run Pattern等同义。
104、Working Master工作母片
指比例为1：1大小，能用于电路板生产的底片，并可直接再翻制成生产线上的实际使用的底片，这种原始底片称之 Working Master。CAD Computer Aided Design ; 电脑辅助设计CAE Computer Aided Engineering ; 电脑辅助工程CAM Computer Aided Manufacturing ; 电脑辅助制造MLB Multilayer Board ; 多层板 (指PCB的多层板)PCB Printed Circuit Board; 印刷电路板(亦做PWB，其中间为Wiring，不过目前已更简称为 Printed Board。大陆术语为“印制电路板”)SMOBC Solder Mask Over Bare Copper ; 绿漆直接印于裸铜板上 (即喷锡板)。
]]></content>
  </entry>
  
  <entry>
    <title>Linux性能优化</title>
    <url>/post/linux/linux-performance-optimization.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>linux</tag>
      <tag>performance</tag>
    </tags>
    <content type="html"><![CDATA[本文详细讲解了Linux性能优化的全景
Linux性能优化 性能优化 性能指标 高并发和响应快对应着性能优化的两个核心指标：吞吐和延时
 应用负载角度：直接影响了产品终端的用户体验 系统资源角度：资源使用率、饱和度等  性能问题的本质就是系统资源已经到达瓶颈，但请求的处理还不够快，无法支撑更多的请求。 性能分析实际上就是找出应用或系统的瓶颈，设法去避免或缓解它们。
 选择指标评估应用程序和系统性能 为应用程序和系统设置性能目标 进行性能基准测试 性能分析定位瓶颈 性能监控和告警  对于不同的性能问题要选取不同的性能分析工具。 下面是常用的Linux Performance Tools以及对应分析的性能问题类型。
到底应该怎么理解“平均负载” **平均负载：**单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是平均活跃进程数。它和我们传统意义上理解的CPU使用率并没有直接关系。
其中不可中断进程是正处于内核态关键流程中的进程（如常见的等待设备的I/O响应）。不可中断状态实际上是系统对进程和硬件设备的一种保护机制。
平均负载多少时合理 实际生产环境中将系统的平均负载监控起来，根据历史数据判断负载的变化趋势。当负载存在明显升高趋势时，及时进行分析和调查。 当然也可以当设置阈值（如当平均负载高于CPU数量的70%时）
现实工作中我们会经常混淆平均负载和CPU使用率的概念，其实两者并不完全对等：
 CPU密集型进程，大量CPU使用会导致平均负载升高，此时两者一致 I/O密集型进程，等待I/O也会导致平均负载升高，此时CPU使用率并不一定高 大量等待CPU的进程调度会导致平均负载升高，此时CPU使用率也会比较高  平均负载高时可能是CPU密集型进程导致，也可能是I/O繁忙导致。具体分析时可以结合mpstat/pidstat工具辅助分析负载来源
CPU CPU上下文切换(上) CPU上下文切换，就是把前一个任务的CPU上下文（CPU寄存器和PC）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的位置，运行新任务。其中，保存下来的上下文会存储在系统内核中，待任务重新调度执行时再加载，保证原来的任务状态不受影响。
按照任务类型，CPU上下文切换分为：
 进程上下文切换 线程上下文切换 中断上下文切换  进程上下文切换 Linux进程按照等级权限将进程的运行空间分为内核空间和用户空间。从用户态向内核态转变时需要通过系统调用来完成。
一次系统调用过程其实进行了两次CPU上下文切换：
 CPU寄存器中用户态的指令位置先保存起来，CPU寄存器更新为内核态指令的位置，跳转到内核态运行内核任务； 系统调用结束后，CPU寄存器恢复原来保存的用户态数据，再切换到用户空间继续运行。  系统调用过程中并不会涉及虚拟内存等进程用户态资源，也不会切换进程。和传统意义上的进程上下文切换不同。因此系统调用通常称为特权模式切换。
进程是由内核管理和调度的，进程上下文切换只能发生在内核态。 因此相比系统调用来说，在保存当前进程的内核状态和CPU寄存器之前，需要先把该进程的虚拟内存，栈保存下来。再加载新进程的内核态后，还要刷新进程的虚拟内存和用户栈。
进程只有在调度到CPU上运行时才需要切换上下文，有以下几种场景： CPU时间片轮流分配，系统资源不足导致进程挂起，进程通过sleep函数主动挂起，高优先级进程抢占时间片，硬件中断时CPU上的进程被挂起转而执行内核中的中断服务。
线程上下文切换 线程上下文切换分为两种：
 前后线程同属于一个进程，切换时虚拟内存资源不变，只需要切换线程的私有数据，寄存器等； 前后线程属于不同进程，与进程上下文切换相同。  同进程的线程切换消耗资源较少，这也是多线程的优势。
中断上下文切换 中断上下文切换并不涉及到进程的用户态，因此中断上下文只包括内核态中断服务程序执行所必须的状态（CPU寄存器，内核堆栈，硬件中断参数等）。
中断处理优先级比进程高，所以中断上下文切换和进程上下文切换不会同时发生
CPU上下文切换(下) 通过vmstat可以查看系统总体的上下文切换情况
vmstat 5 #每隔5s输出一组数据 procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 103388 145412 511056 0 0 18 60 1 1 2 1 96 0 0 0 0 0 103388 145412 511076 0 0 0 2 450 1176 1 1 99 0 0 0 0 0 103388 145412 511076 0 0 0 8 429 1135 1 1 98 0 0 0 0 0 103388 145412 511076 0 0 0 0 431 1132 1 1 98 0 0 0 0 0 103388 145412 511076 0 0 0 10 467 1195 1 1 98 0 0 1 0 0 103388 145412 511076 0 0 0 2 426 1139 1 0 99 0 0 4 0 0 95184 145412 511108 0 0 0 74 500 1228 4 1 94 0 0 0 0 0 103512 145416 511076 0 0 0 455 723 1573 12 3 83 2 0  cs （context switch） 每秒上下文切换次数 in （interrupt） 每秒中断次数 r （runnning or runnable）就绪队列的长度，正在运行和等待CPU的进程数 b （Blocked） 处于不可中断睡眠状态的进程数  要查看每个进程的详细情况，需要使用pidstat来查看每个进程上下文切换情况
pidstat -w 5 14时51分16秒 UID PID cswch/s nvcswch/s Command 14时51分21秒 0 1 0.80 0.00 systemd 14时51分21秒 0 6 1.40 0.00 ksoftirqd/0 14时51分21秒 0 9 32.67 0.00 rcu_sched 14时51分21秒 0 11 0.40 0.00 watchdog/0 14时51分21秒 0 32 0.20 0.00 khugepaged 14时51分21秒 0 271 0.20 0.00 jbd2/vda1-8 14时51分21秒 0 1332 0.20 0.00 argusagent 14时51分21秒 0 5265 10.02 0.00 AliSecGuard 14时51分21秒 0 7439 7.82 0.00 kworker/0:2 14时51分21秒 0 7906 0.20 0.00 pidstat 14时51分21秒 0 8346 0.20 0.00 sshd 14时51分21秒 0 20654 9.82 0.00 AliYunDun 14时51分21秒 0 25766 0.20 0.00 kworker/u2:1 14时51分21秒 0 28603 1.00 0.00 python3  cswch 每秒自愿上下文切换次数 （进程无法获取所需资源导致的上下文切换） nvcswch 每秒非自愿上下文切换次数 （时间片轮流等系统强制调度）  vmstat 1 1 #首先获取空闲系统的上下文切换次数 sysbench --threads=10 --max-time=300 threads run #模拟多线程切换问题 vmstat 1 1 #新终端观察上下文切换情况 此时发现cs数据明显升高，同时观察其他指标： r列： 远超系统CPU个数，说明存在大量CPU竞争 us和sy列： sy列占比80%，说明CPU主要被内核占用 in列： 中断次数明显上升，说明中断处理也是潜在问题 说明运行/等待CPU的进程过多，导致大量的上下文切换，上下文切换导致系统的CPU占用率高
pidstat -w -u 1 #查看到底哪个进程导致的问题 从结果中看出是sysbench导致CPU使用率过高，但是pidstat输出的上下文次数加起来也并不多。分析sysbench模拟的是线程的切换，因此需要在pidstat后加-t参数查看线程指标。
另外对于中断次数过多，我们可以通过/proc/interrupts文件读取
watch -d cat /proc/interrupts 发现次数变化速度最快的是重调度中断（RES），该中断用来唤醒空闲状态的CPU来调度新的任务运行。分析还是因为过多任务的调度问题，和上下文切换分析一致。
某个应用的CPU使用率达到100%，怎么办？ Linux作为多任务操作系统，将CPU时间划分为很短的时间片，通过调度器轮流分配给各个任务使用。为了维护CPU时间，Linux通过事先定义的节拍率，触发时间中断，并使用全局变了jiffies记录开机以来的节拍数。时间中断发生一次该值+1.
CPU使用率，除了空闲时间以外的其他时间占总CPU时间的百分比。可以通过/proc/stat中的数据来计算出CPU使用率。因为/proc/stat时开机以来的节拍数累加值，计算出来的是开机以来的平均CPU使用率，一般意义不大。可以间隔取一段时间的两次值作差来计算该段时间内的平均CPU使用率。 性能分析工具给出的都是间隔一段时间的平均CPU使用率，要注意间隔时间的设置。
CPU使用率可以通过top 或 ps来查看。分析进程的CPU问题可以通过perf，它以性能事件采样为基础，不仅可以分析系统的各种事件和内核性能，还可以用来分析指定应用程序的性能问题。
perf top / perf record / perf report （-g 开启调用关系的采样）
sudo docker run --name nginx -p 10000:80 -itd feisky/nginx sudo docker run --name phpfpm -itd --network container:nginx feisky/php-fpm ab -c 10 -n 100 http://XXX.XXX.XXX.XXX:10000/ #测试Nginx服务性能 发现此时每秒可承受请求给长少，此时将测试的请求数从100增加到10000。 在另外一个终端运行top查看每个CPU的使用率。发现系统中几个php-fpm进程导致CPU使用率骤升。
接着用perf来分析具体是php-fpm中哪个函数导致该问题。
perf top -g -p XXXX #对某一个php-fpm进程进行分析 发现其中sqrt和add_function占用CPU过多， 此时查看源码找到原来是sqrt中在发布前没有删除测试代码段，存在一个百万次的循环导致。 将该无用代码删除后发现nginx负载能力明显提升
系统的CPU使用率很高，为什么找不到高CPU的应用？ sudo docker run --name nginx -p 10000:80 -itd feisky/nginx:sp sudo docker run --name phpfpm -itd --network container:nginx feisky/php-fpm:sp ab -c 100 -n 1000 http://XXX.XXX.XXX.XXX:10000/ #并发100个请求测试 实验结果中每秒请求数依旧不高，我们将并发请求数降为5后，nginx负载能力依旧很低。
此时用top和pidstat发现系统CPU使用率过高，但是并没有发现CPU使用率高的进程。
出现这种情况一般时我们分析时遗漏的什么信息，重新运行top命令并观察一会。发现就绪队列中处于Running状态的进行过多，超过了我们的并发请求次数5. 再仔细查看进程运行数据，发现nginx和php-fpm都处于sleep状态，真正处于运行的却是几个stress进程。
下一步就利用pidstat分析这几个stress进程，发现没有任何输出。用ps aux交叉验证发现依旧不存在该进程。说明不是工具的问题。再top查看发现stress进程的进程号变化了，此时有可能时以下两种原因导致：
 进程不停的崩溃重启（如段错误/配置错误等），此时进程退出后可能又被监控系统重启； 短时进程导致，即其他应用内部通过exec调用的外面命令，这些命令一般只运行很短时间就结束，很难用top这种间隔较长的工具来发现  可以通过pstree来查找 stress的父进程，找出调用关系。
pstree | grep stress 发现是php-fpm调用的该子进程，此时去查看源码可以看出每个请求都会调用一个stress命令来模拟I/O压力。 之前top显示的结果是CPU使用率升高，是否真的是由该stress命令导致的，还需要继续分析。 代码中给每个请求加了verbose=1的参数后可以查看stress命令的输出，在中断测试该命令结果显示stress命令运行时存在因权限问题导致的文件创建失败的bug。
此时依旧只是猜测，下一步继续通过perf工具来分析。性能报告显示确实时stress占用了大量的CPU，通过修复权限问题来优化解决即可.
系统中出现大量不可中断进程和僵尸进程怎么办？ 进程状态  R Running/Runnable，表示进程在CPU的就绪队列中，正在运行或者等待运行； D Disk Sleep，不可中断状态睡眠，一般表示进程正在跟硬件交互，并且交互过程中不允许被其他进程中断； Z Zombie，僵尸进程，表示进程实际上已经结束，但是父进程还没有回收它的资源； S Interruptible Sleep，可中断睡眠状态，表示进程因为等待某个事件而被系统挂起，当等待事件发生则会被唤醒并进入R状态； I Idle，空闲状态，用在不可中断睡眠的内核线程上。 该状态不会导致平均负载升高； T Stop/Traced，表示进程处于暂停或跟踪状态（SIGSTOP/SIGCONT， GDB调试）； X Dead，进程已经消亡，不会在top/ps中看到。  对于不可中断状态，一般都是在很短时间内结束，可忽略。但是如果系统或硬件发生故障，进程可能会保持不可中断状态很久，甚至系统中出现大量不可中断状态，此时需注意是否出现了I/O性能问题。
僵尸进程一般多进程应用容易遇到，父进程来不及处理子进程状态时子进程就提前退出，此时子进程就变成了僵尸进程。大量的僵尸进程会用尽PID进程号，导致新进程无法建立。
磁盘O_DIRECT问题 sudo docker run --privileged --name=app -itd feisky/app:iowait ps aux | grep &#39;/app&#39; 可以看到此时有多个app进程运行，状态分别时Ss+和D+。其中后面s表示进程是一个会话的领导进程，+号表示前台进程组。
其中进程组表示一组相互关联的进程，子进程是父进程所在组的组员。 会话指共享同一个控制终端的一个或多个进程组。
用top查看系统资源发现：1）平均负载在逐渐增加，且1分钟内平均负载达到了CPU个数，说明系统可能已经有了性能瓶颈；2）僵尸进程比较多且在不停增加；3）us和sys CPU使用率都不高，iowait却比较高；4）每个进程CPU使用率也不高，但有两个进程处于D状态，可能在等待IO。
分析目前数据可知：iowait过高导致系统平均负载升高，僵尸进程不断增长说明有程序没能正确清理子进程资源。
用dstat来分析，因为它可以同时查看CPU和I/O两种资源的使用情况，便于对比分析。
dstat 1 10 #间隔1秒输出10组数据 可以看到当wai（iowait）升高时磁盘请求read都会很大，说明iowait的升高和磁盘的读请求有关。接下来分析到底时哪个进程在读磁盘。
之前top查看的处于D状态的进程号，用pidstat -d -p XXX 展示进程的I/O统计数据。发现处于D状态的进程都没有任何读写操作。 在用pidstat -d 查看所有进程的I/O统计数据，看到app进程在进行磁盘读操作，每秒读取32MB的数据。进程访问磁盘必须使用系统调用处于内核态，接下来重点就是找到app进程的系统调用。
sudo strace -p XXX #对app进程调用进行跟踪 报错没有权限，因为已经时root权限了。所以遇到这种情况，首先要检查进程状态是否正常。 ps命令查找该进程已经处于Z状态，即僵尸进程。
这种情况下top pidstat之类的工具无法给出更多的信息，此时像第5篇一样，用perf record -d和perf report进行分析，查看app进程调用栈。
看到app确实在通过系统调用sys_read()读取数据，并且从new_sync_read和blkdev_direct_IO看出进程时进行直接读操作，请求直接从磁盘读，没有通过缓存导致iowait升高。
通过层层分析后，root cause是app内部进行了磁盘的直接I/O。然后定位到具体代码位置进行优化即可。
僵尸进程 上述优化后iowait显著下降，但是僵尸进程数量仍旧在增加。首先要定位僵尸进程的父进程，通过pstree -aps XXX，打印出该僵尸进程的调用树，发现父进程就是app进程。
查看app代码，看看子进程结束的处理是否正确（是否调用wait()/waitpid(),有没有注册SIGCHILD信号的处理函数等）。
碰到iowait升高时，先用dstat pidstat等工具确认是否存在磁盘I/O问题，再找是哪些进程导致I/O，不能用strace直接分析进程调用时可以通过perf工具分析。
对于僵尸问题，用pstree找到父进程，然后看源码检查子进程结束的处理逻辑即可。
CPU性能指标   CPU使用率
 用户CPU使用率, 包括用户态(user)和低优先级用户态(nice). 该指标过高说明应用程序比较繁忙. 系统CPU使用率, CPU在内核态运行的时间百分比(不含中断). 该指标高说明内核比较繁忙. 等待I/O的CPU使用率, iowait, 该指标高说明系统与硬件设备I/O交互时间比较长. 软/硬中断CPU使用率, 该指标高说明系统中发生大量中断. steal CPU / guest CPU, 表示虚拟机占用的CPU百分比.    平均负载
理想情况下平均负载等于逻辑CPU个数,表示每个CPU都被充分利用. 若大于则说明系统负载较重.
  进程上下文切换
包括无法获取资源的自愿切换和系统强制调度时的非自愿切换. 上下文切换本身是保证Linux正常运行的一项核心功能. 过多的切换则会将原本运行进程的CPU时间消耗在寄存器,内核占及虚拟内存等数据保存和恢复上
  CPU缓存命中率
CPU缓存的复用情况,命中率越高性能越好. 其中L1/L2常用在单核,L3则用在多核中
  性能工具  平均负载案例  先用uptime查看系统平均负载 判断负载在升高后再用mpstat和pidstat分别查看每个CPU和每个进程CPU使用情况.找出导致平均负载较高的进程.   上下文切换案例  先用vmstat查看系统上下文切换和中断次数 再用pidstat观察进程的自愿和非自愿上下文切换情况 最后通过pidstat观察线程的上下文切换情况   进程CPU使用率高案例  先用top查看系统和进程的CPU使用情况,定位到进程 再用perf top观察进程调用链,定位到具体函数   系统CPU使用率高案例  先用top查看系统和进程的CPU使用情况,top/pidstat都无法找到CPU使用率高的进程 重新审视top输出 从CPU使用率不高,但是处于Running状态的进程入手 perf record/report发现短时进程导致 (execsnoop工具)   不可中断和僵尸进程案例  先用top观察iowait升高,发现大量不可中断和僵尸进程 strace无法跟踪进程系统调用 perf分析调用链发现根源来自磁盘直接I/O   软中断案例  top观察系统软中断CPU使用率高 查看/proc/softirqs找到变化速率较快的几种软中断 sar命令发现是网络小包问题 tcpdump找出网络帧的类型和来源, 确定SYN FLOOD攻击导致    根据不同的性能指标来找合适的工具:
在生产环境中往往开发者没有权限安装新的工具包,只能最大化利用好系统中已经安装好的工具. 因此要了解一些主流工具能够提供哪些指标分析.
先运行几个支持指标较多的工具, 如top/vmstat/pidstat,根据它们的输出可以得出是哪种类型的性能问题. 定位到进程后再用strace/perf分析调用情况进一步分析. 如果是软中断导致用/proc/softirqs
CPU优化   应用程序优化
 编译器优化: 编译阶段开启优化选项, 如gcc -O2 算法优化 异步处理: 避免程序因为等待某个资源而一直阻塞,提升程序的并发处理能力. (将轮询替换为事件通知) 多线程代替多进程: 减少上下文切换成本 善用缓存: 加快程序处理速度    系统优化
 CPU绑定: 将进程绑定要1个/多个CPU上,提高CPU缓存命中率,减少CPU调度带来的上下文切换 CPU独占: CPU亲和性机制来分配进程 优先级调整:使用nice适当降低非核心应用的优先级 为进程设置资源显示: cgroups设置使用上限,防止由某个应用自身问题耗尽系统资源 NUMA优化: CPU尽可能访问本地内存 中断负载均衡: irpbalance,将中断处理过程自动负载均衡到各个CPU上    TPS、QPS、系统吞吐量的区别和理解
  QPS (Queries Per Second)每秒查询率,一台服务器每秒能够响应的查询次数.
  TPS (Transactions Per Second)每秒事务数,软件测试的结果.
  用户请求服务器
  服务器内部处理
  服务器返回给客户
QPS类似TPS,但是对于一个页面的访问形成一个TPS,但是一次页面请求可能包含多次对服务器的请求,可能计入多次QPS
    系统吞吐量, 包括几个重要参数:
  QPS(TPS)
  并发数
  响应时间
QPS(TPS)=并发数/平均相应时间
      内存 Linux内存是怎么工作的 内存映射 大多数计算机用的主存都是动态随机访问内存(DRAM)，只有内核才可以直接访问物理内存。Linux内核给每个进程提供了一个独立的虚拟地址空间，并且这个地址空间是连续的。这样进程就可以很方便的访问内存(虚拟内存)。
虚拟地址空间的内部分为内核空间和用户空间两部分，不同字长的处理器地址空间的范围不同。32位系统内核空间占用1G，用户空间占3G。 64位系统内核空间和用户空间都是128T，分别占内存空间的最高和最低处，中间部分为未定义。
并不是所有的虚拟内存都会分配物理内存，只有实际使用的才会。分配后的物理内存通过内存映射管理。为了完成内存映射，内核为每个进程都维护了一个页表，记录虚拟地址和物理地址的映射关系。页表实际存储在CPU的内存管理单元MMU中，处理器可以直接通过硬件找出要访问的内存。
当进程访问的虚拟地址在页表中查不到时，系统会产生一个缺页异常，进入内核空间分配物理内存，更新进程页表，再返回用户空间恢复进程的运行。
MMU以页为单位管理内存，页大小4KB。为了解决页表项过多问题Linux提供了多级页表和HugePage的机制。
虚拟内存空间分布 用户空间内存从低到高是五种不同的内存段：
 只读段 代码和常量等 数据段 全局变量等 堆 动态分配的内存，从低地址开始向上增长 文件映射 动态库、共享内存等，从高地址开始向下增长 栈 包括局部变量和函数调用的上下文等，栈的大小是固定的。一般8MB  内存分配与回收 分配 malloc对应到系统调用上有两种实现方式：
 brk() 针对小块内存(&lt;128K)，通过移动堆顶位置来分配。内存释放后不立即归还内存，而是被缓存起来。 **mmap()**针对大块内存(&gt;128K)，直接用内存映射来分配，即在文件映射段找一块空闲内存分配。  前者的缓存可以减少缺页异常的发生，提高内存访问效率。但是由于内存没有归还系统，在内存工作繁忙时，频繁的内存分配/释放会造成内存碎片。
后者在释放时直接归还系统，所以每次mmap都会发生缺页异常。在内存工作繁忙时，频繁内存分配会导致大量缺页异常，使内核管理负担增加。
上述两种调用并没有真正分配内存，这些内存只有在首次访问时，才通过缺页异常进入内核中，由内核来分配
回收 内存紧张时，系统通过以下方式来回收内存：
  回收缓存： LRU算法回收最近最少使用的内存页面；
  回收不常访问内存： 把不常用的内存通过交换分区写入磁盘
  杀死进程： OOM内核保护机制 （进程消耗内存越大oom_score越大，占用CPU越多oom_score越小，可以通过/proc手动调整oom_adj）
echo -16 &gt; /proc/$(pidof XXX)/oom_adj   如何查看内存使用情况 free来查看整个系统的内存使用情况
top/ps来查看某个进程的内存使用情况
 VIRT 进程的虚拟内存大小 RES 常驻内存的大小，即进程实际使用的物理内存大小，不包括swap和共享内存 SHR 共享内存大小，与其他进程共享的内存，加载的动态链接库以及程序代码段 %MEM 进程使用物理内存占系统总内存的百分比  怎样理解内存中的Buffer和Cache？ buffer是对磁盘数据的缓存，cache是对文件数据的缓存，它们既会用在读请求也会用在写请求中
如何利用系统缓存优化程序的运行效率 缓存命中率 缓存命中率是指直接通过缓存获取数据的请求次数，占所有请求次数的百分比。命中率越高说明缓存带来的收益越高，应用程序的性能也就越好。
安装bcc包后可以通过cachestat和cachetop来监测缓存的读写命中情况。
安装pcstat后可以查看文件在内存中的缓存大小以及缓存比例
#首先安装Go export GOPATH=~/go export PATH=~/go/bin:$PATH go get golang.org/x/sys/unix go ge github.com/tobert/pcstat/pcstat dd缓存加速 dd if=/dev/sda1 of=file bs=1M count=512 #生产一个512MB的临时文件 echo 3 &gt; /proc/sys/vm/drop_caches #清理缓存 pcstat file #确定刚才生成文件不在系统缓存中，此时cached和percent都是0 cachetop 5 dd if=file of=/dev/null bs=1M #测试文件读取速度 #此时文件读取性能为30+MB/s，查看cachetop结果发现并不是所有的读都落在磁盘上，读缓存命中率只有50%。 dd if=file of=/dev/null bs=1M #重复上述读文件测试 #此时文件读取性能为4+GB/s，读缓存命中率为100% pcstat file #查看文件file的缓存情况，100%全部缓存 O_DIRECT选项绕过系统缓存 cachetop 5 sudo docker run --privileged --name=app -itd feisky/app:io-direct sudo docker logs app #确认案例启动成功 #实验结果表明每读32MB数据都要花0.9s，且cachetop输出中显示1024次缓存全部命中 但是凭感觉可知如果缓存命中读速度不应如此慢，读次数时1024，页大小为4K，五秒的时间内读取了1024*4KB数据，即每秒0.8MB，和结果中32MB相差较大。说明该案例没有充分利用缓存，怀疑系统调用设置了直接I/O标志绕过系统缓存。因此接下来观察系统调用.
strace -p $(pgrep app) #strace 结果可以看到openat打开磁盘分区/dev/sdb1，传入参数为O_RDONLY|O_DIRECT 这就解释了为什么读32MB数据那么慢，直接从磁盘读写肯定远远慢于缓存。找出问题后我们再看案例的源代码发现flags中指定了直接IO标志。删除该选项后重跑，验证性能变化。
内存泄漏，如何定位和处理？ 对应用程序来说，动态内存的分配和回收是核心又复杂的一个逻辑功能模块。管理内存的过程中会发生各种各样的“事故”：
 没正确回收分配的内存，导致了泄漏 访问的是已分配内存边界外的地址，导致程序异常退出  内存的分配与回收 虚拟内存分布从低到高分别是只读段，数据段，堆，内存映射段，栈五部分。其中会导致内存泄漏的是：
 堆： 由应用程序自己来分配和管理，除非程序退出这些堆内存不会被系统自动释放。 内存映射段：包括动态链接库和共享内存，其中共享内存由程序自动分配和管理  内存泄漏的危害比较大，这些忘记释放的内存，不仅应用程序自己不能访问，系统也不能把它们再次分配给其他应用。 内存泄漏不断累积甚至会耗尽系统内存.
如何检测内存泄漏 预先安装systat，docker，bcc
sudo docker run --name=app -itd feisky/app:mem-leak sudo docker logs app vmstat 3 可以看到free在不断下降，buffer和cache基本保持不变。说明系统的内存一致在升高。但并不能说明存在内存泄漏。此时可以通过memleak工具来跟踪系统或进程的内存分配/释放请求
/usr/share/bcc/tools/memleak -a -p $(pidof app) 从memleak输出可以看到，应用在不停地分配内存，并且这些分配的地址并没有被回收。通过调用栈看到是fibonacci函数分配的内存没有释放。定位到源码后查看源码来修复增加内存释放函数即可.
为什么系统的Swap变高 系统内存资源紧张时通过内存回收和OOM杀死进程来解决。其中可回收内存包括：
 缓存/缓冲区，属于可回收资源，在文件管理中通常叫做文件页  被应用程序修改过暂时没写入磁盘的数据(脏页)，要先写入磁盘然后才能内存释放  在应用程序中通过fsync将脏页同步到磁盘 交给系统，内核线程pdflush负责这些脏页的刷新     内存映射获取的文件映射页，也可以被释放掉，下次访问时从文件重新读取  对于程序自动分配的堆内存，也就是我们在内存管理中的匿名页，虽然这些内存不能直接释放，但是Linux提供了Swap机制将不常访问的内存写入到磁盘来释放内存，再次访问时从磁盘读取到内存即可。
Swap原理 Swap本质就是把一块磁盘空间或者一个本地文件当作内存来使用，包括换入和换出两个过程：
 换出： 将进程暂时不用的内存数据存储到磁盘中，并释放这些内存 换入： 进程再次访问内存时，将它们从磁盘读到内存中  Linux如何衡量内存资源是否紧张？
  直接内存回收 新的大块内存分配请求，但剩余内存不足。此时系统会回收一部分内存；
  kswapd0 内核线程定期回收内存。为了衡量内存使用情况，定义了pages_min,pages_low,pages_high三个阈值，并根据其来进行内存的回收操作。
  剩余内存 &lt; pages_min，进程可用内存耗尽了，只有内核才可以分配内存
  pages_min &lt; 剩余内存 &lt; pages_low,内存压力较大，kswapd0执行内存回收，直到剩余内存 &gt; pages_high
  pages_low &lt; 剩余内存 &lt; pages_high，内存有一定压力，但可以满足新内存请求
  剩余内存 &gt; pages_high，说明剩余内存较多，无内存压力
pages_low = pages_min 5 / 4 pages_high = pages_min 3 / 2
    NUMA 与 SWAP 很多情况下系统剩余内存较多，但SWAP依旧升高，这是由于处理器的NUMA架构。
在NUMA架构下多个处理器划分到不同的Node，每个Node都拥有自己的本地内存空间。在分析内存的使用时应该针对每个Node单独分析
numactl --hardware #查看处理器在Node的分布情况，以及每个Node的内存使用情况 内存三个阈值可以通过/proc/zoneinfo来查看，该文件中还包括活跃和非活跃的匿名页/文件页数。
当某个Node内存不足时，系统可以从其他Node寻找空闲资源，也可以从本地内存中回收内存。 通过/proc/sys/vm/zone_raclaim_mode来调整。
 0表示既可以从其他Node寻找空闲资源，也可以从本地回收内存 1，2，4表示只回收本地内存，2表示可以会回脏数据回收内存，4表示可以用Swap方式回收内存。  swappiness 在实际回收过程中Linux根据/proc/sys/vm/swapiness选项来调整使用Swap的积极程度，从0-100，数值越大越积极使用Swap，即更倾向于回收匿名页；数值越小越消极使用Swap，即更倾向于回收文件页。
注意：这只是调整Swap积极程度的权重，即使设置为0，当剩余内存+文件页小于页高阈值时，还是会发生Swap。
Swap升高时如何定位分析 free #首先通过free查看swap使用情况，若swap=0表示未配置Swap #先创建并开启swap fallocate -l 8G /mnt/swapfile chmod 600 /mnt/swapfile mkswap /mnt/swapfile swapon /mnt/swapfile free #再次执行free确保Swap配置成功 dd if=/dev/sda1 of=/dev/null bs=1G count=2048 #模拟大文件读取 sar -r -S 1 #查看内存各个指标变化 -r内存 -S swap #根据结果可以看出，%memused在不断增长，剩余内存kbmemfress不断减少，缓冲区kbbuffers不断增大，由此可知剩余内存不断分配给了缓冲区 #一段时间之后，剩余内存很小，而缓冲区占用了大部分内存。此时Swap使用之间增大，缓冲区和剩余内存只在小范围波动 停下sar命令 cachetop5 #观察缓存 #可以看到dd进程读写只有50%的命中率，未命中数为4w+页，说明正式dd进程导致缓冲区使用升高 watch -d grep -A 15 ‘Normal’ /proc/zoneinfo #观察内存指标变化 #发现升级内存在一个小范围不停的波动，低于页低阈值时会突然增大到一个大于页高阈值的值 说明剩余内存和缓冲区的波动变化正是由于内存回收和缓存再次分配的循环往复。有时候Swap用的多，有时候缓冲区波动更多。此时查看swappiness值为60，是一个相对中和的配置，系统会根据实际运行情况来选去合适的回收类型.
如何“快准狠”找到系统内存存在的问题 内存性能指标 系统内存指标
 已用内存/剩余内存 共享内存 （tmpfs实现） 可用内存： 包括剩余内存和可回收内存 缓存：磁盘读取文件的页缓存，slab分配器中的可回收部分 缓冲区： 原始磁盘块的临时存储，缓存将要写入磁盘的数据  进程内存指标
 虚拟内存： 5大部分 常驻内存： 进程实际使用的物理内存，不包括Swap和共享内存 共享内存： 与其他进程共享的内存，以及动态链接库和程序的代码段 Swap内存： 通过Swap换出到磁盘的内存  缺页异常
 可以直接从物理内存中分配，次缺页异常 需要磁盘IO介入(如Swap)，主缺页异常。 此时内存访问会慢很多  内存性能工具 根据不同的性能指标来找合适的工具:
内存分析工具包含的性能指标:
如何迅速分析内存的性能瓶颈 通常先运行几个覆盖面比较大的性能工具，如free，top，vmstat，pidstat等
 先用free和top查看系统整体内存使用情况 再用vmstat和pidstat，查看一段时间的趋势，从而判断内存问题的类型 最后进行详细分析，比如内存分配分析，缓存/缓冲区分析，具体进程的内存使用分析等  常见的优化思路：
 最好禁止Swap，若必须开启则尽量降低swappiness的值 减少内存的动态分配，如可以用内存池，HugePage等 尽量使用缓存和缓冲区来访问数据。如用堆栈明确声明内存空间来存储需要缓存的数据，或者用Redis外部缓存组件来优化数据的访问 cgroups等方式来限制进程的内存使用情况，确保系统内存不被异常进程耗尽 /proc/pid/oom_adj调整核心应用的oom_score，保证即使内存紧张核心应用也不会被OOM杀死  vmstat使用详解 vmstat命令是最常见的Linux/Unix监控工具，可以展现给定时间间隔的服务器的状态值,包括服务器的CPU使用率，内存使用，虚拟内存交换情况,IO读写情况。可以看到整个机器的CPU,内存,IO的使用情况，而不是单单看到各个进程的CPU使用率和内存使用率(使用场景不一样)。
vmstat 2 procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 1379064 282244 11537528 0 0 3 104 0 0 3 0 97 0 0 0 0 0 1372716 282244 11537544 0 0 0 24 4893 8947 1 0 98 0 0 0 0 0 1373404 282248 11537544 0 0 0 96 5105 9278 2 0 98 0 0 0 0 0 1374168 282248 11537556 0 0 0 0 5001 9208 1 0 99 0 0 0 0 0 1376948 282248 11537564 0 0 0 80 5176 9388 2 0 98 0 0 0 0 0 1379356 282256 11537580 0 0 0 202 5474 9519 2 0 98 0 0 1 0 0 1368376 282256 11543696 0 0 0 0 5894 8940 12 0 88 0 0 1 0 0 1371936 282256 11539240 0 0 0 10554 6176 9481 14 1 85 1 0 1 0 0 1366184 282260 11542292 0 0 0 7456 6102 9983 7 1 91 0 0 1 0 0 1353040 282260 11556176 0 0 0 16924 7233 9578 18 1 80 1 0 0 0 0 1359432 282260 11549124 0 0 0 12576 5495 9271 7 0 92 1 0 0 0 0 1361744 282264 11549132 0 0 0 58 8606 15079 4 2 95 0 0 1 0 0 1367120 282264 11549140 0 0 0 2 5716 9205 8 0 92 0 0 0 0 0 1346580 282264 11562644 0 0 0 70 6416 9944 12 0 88 0 0 0 0 0 1359164 282264 11550108 0 0 0 2922 4941 8969 3 0 97 0 0 1 0 0 1353992 282264 11557044 0 0 0 0 6023 8917 15 0 84 0 0 # 结果说明 - r 表示运行队列(就是说多少个进程真的分配到CPU)，我测试的服务器目前CPU比较空闲，没什么程序在跑，当这个值超过了CPU数目，就会出现CPU瓶颈了。这个也和top的负载有关系，一般负载超过了3就比较高，超过了5就高，超过了10就不正常了，服务器的状态很危险。top的负载类似每秒的运行队列。如果运行队列过大，表示你的CPU很繁忙，一般会造成CPU使用率很高。 - b 表示阻塞的进程,这个不多说，进程阻塞，大家懂的。 - swpd 虚拟内存已使用的大小，如果大于0，表示你的机器物理内存不足了，如果不是程序内存泄露的原因，那么你该升级内存了或者把耗内存的任务迁移到其他机器。 - free 空闲的物理内存的大小，我的机器内存总共8G，剩余3415M。 - buff Linux/Unix系统是用来存储，目录里面有什么内容，权限等的缓存，我本机大概占用300多M - cache cache直接用来记忆我们打开的文件,给文件做缓冲，我本机大概占用300多M(这里是Linux/Unix的聪明之处，把空闲的物理内存的一部分拿来做文件和目录的缓存，是为了提高 程序执行的性能，当程序使用内存时，buffer/cached会很快地被使用。) - si 每秒从磁盘读入虚拟内存的大小，如果这个值大于0，表示物理内存不够用或者内存泄露了，要查找耗内存进程解决掉。我的机器内存充裕，一切正常。 - so 每秒虚拟内存写入磁盘的大小，如果这个值大于0，同上。 - bi 块设备每秒接收的块数量，这里的块设备是指系统上所有的磁盘和其他块设备，默认块大小是1024byte，我本机上没什么IO操作，所以一直是0，但是我曾在处理拷贝大量数据(2-3T)的机器上看过可以达到140000/s，磁盘写入速度差不多140M每秒 - bo 块设备每秒发送的块数量，例如我们读取文件，bo就要大于0。bi和bo一般都要接近0，不然就是IO过于频繁，需要调整。 - in 每秒CPU的中断次数，包括时间中断 - cs 每秒上下文切换次数，例如我们调用系统函数，就要进行上下文切换，线程的切换，也要进程上下文切换，这个值要越小越好，太大了，要考虑调低线程或者进程的数目,例如在apache和nginx这种web服务器中，我们一般做性能测试时会进行几千并发甚至几万并发的测试，选择web服务器的进程可以由进程或者线程的峰值一直下调，压测，直到cs到一个比较小的值，这个进程和线程数就是比较合适的值了。系统调用也是，每次调用系统函数，我们的代码就会进入内核空间，导致上下文切换，这个是很耗资源，也要尽量避免频繁调用系统函数。上下文切换次数过多表示你的CPU大部分浪费在上下文切换，导致CPU干正经事的时间少了，CPU没有充分利用，是不可取的。 - us 用户CPU时间，我曾经在一个做加密解密很频繁的服务器上，可以看到us接近100,r运行队列达到80(机器在做压力测试，性能表现不佳)。 - sy 系统CPU时间，如果太高，表示系统调用时间长，例如是IO操作频繁。 - id 空闲CPU时间，一般来说，id + us + sy = 100,一般我认为id是空闲CPU使用率，us是用户CPU使用率，sy是系统CPU使用率。 - wt 等待IO CPU时间 pidstat 使用详解 pidstat主要用于监控全部或指定进程占用系统资源的情况,如CPU,内存、设备IO、任务切换、线程等。
使用方法：
 pidstat –d interval times 统计各个进程的IO使用情况 pidstat –u interval times 统计各个进程的CPU统计信息 pidstat –r interval times 统计各个进程的内存使用信息 pidstat -w interval times 统计各个进程的上下文切换 p PID 指定PID  1、统计IO使用情况
pidstat -d 1 10 03:02:02 PM UID PID kB_rd/s kB_wr/s kB_ccwr/s Command 03:02:03 PM 0 816 0.00 918.81 0.00 jbd2/vda1-8 03:02:03 PM 0 1007 0.00 3.96 0.00 AliYunDun 03:02:03 PM 997 7326 0.00 1904.95 918.81 java 03:02:03 PM 997 8539 0.00 3.96 0.00 java 03:02:03 PM 0 16066 0.00 35.64 0.00 cmagent 03:02:03 PM UID PID kB_rd/s kB_wr/s kB_ccwr/s Command 03:02:04 PM 0 816 0.00 1924.00 0.00 jbd2/vda1-8 03:02:04 PM 997 7326 0.00 11156.00 1888.00 java 03:02:04 PM 997 8539 0.00 4.00 0.00 java  UID PID kB_rd/s: 每秒进程从磁盘读取的数据量 KB 单位 read from disk each second KB kB_wr/s: 每秒进程向磁盘写的数据量 KB 单位 write to disk each second KB kB_ccwr/s: 每秒进程向磁盘写入，但是被取消的数据量，This may occur when the task truncates some dirty pagecache. iodelay: Block I/O delay, measured in clock ticks Command: 进程名 task name  2、统计CPU使用情况
# 统计CPU pidstat -u 1 10 03:03:33 PM UID PID %usr %system %guest %CPU CPU Command 03:03:34 PM 0 2321 3.96 0.00 0.00 3.96 0 ansible 03:03:34 PM 0 7110 0.00 0.99 0.00 0.99 4 pidstat 03:03:34 PM 997 8539 0.99 0.00 0.00 0.99 5 java 03:03:34 PM 984 15517 0.99 0.00 0.00 0.99 5 java 03:03:34 PM 0 24406 0.99 0.00 0.00 0.99 5 java 03:03:34 PM 0 32158 3.96 0.00 0.00 3.96 2 ansible  UID PID %usr: 进程在用户空间占用 cpu 的百分比 %system: 进程在内核空间占用 CPU 百分比 %guest: 进程在虚拟机占用 CPU 百分比 %wait: 进程等待运行的百分比 %CPU: 进程占用 CPU 百分比 CPU: 处理进程的 CPU 编号 Command: 进程名  3、统计内存使用情况
# 统计内存 pidstat -r 1 10 Average: UID PID minflt/s majflt/s VSZ RSS %MEM Command Average: 0 1 0.20 0.00 191256 3064 0.01 systemd Average: 0 1007 1.30 0.00 143256 22720 0.07 AliYunDun Average: 0 6642 0.10 0.00 6301904 107680 0.33 java Average: 997 7326 10.89 0.00 13468904 8395848 26.04 java Average: 0 7795 348.15 0.00 108376 1233 0.00 pidstat Average: 997 8539 0.50 0.00 8242256 2062228 6.40 java Average: 987 9518 0.20 0.00 6300944 1242924 3.85 java Average: 0 10280 3.70 0.00 807372 8344 0.03 aliyun-service Average: 984 15517 0.40 0.00 6386464 1464572 4.54 java Average: 0 16066 236.46 0.00 2678332 71020 0.22 cmagent Average: 995 20955 0.30 0.00 6312520 1408040 4.37 java Average: 995 20956 0.20 0.00 6093764 1505028 4.67 java Average: 0 23936 0.10 0.00 5302416 110804 0.34 java Average: 0 24406 0.70 0.00 10211672 2361304 7.32 java Average: 0 26870 1.40 0.00 1470212 36084 0.11 promtail  UID PID Minflt/s : 每秒次缺页错误次数 （minor page faults），虚拟内存地址映射成物理内存地址产生的 page fault 次数 Majflt/s : 每秒主缺页错误次数 (major page faults), 虚拟内存地址映射成物理内存地址时，相应 page 在 swap 中 VSZ virtual memory usage : 该进程使用的虚拟内存 KB 单位 RSS : 该进程使用的物理内存 KB 单位 %MEM : 内存使用率 Command : 该进程的命令 task name  4、查看具体进程使用情况
pidstat -T ALL -r -p 20955 1 10 03:12:16 PM UID PID minflt/s majflt/s VSZ RSS %MEM Command 03:12:17 PM 995 20955 0.00 0.00 6312520 1408040 4.37 java 03:12:16 PM UID PID minflt-nr majflt-nr Command 03:12:17 PM 995 20955 0 0 java ]]></content>
  </entry>
  
  <entry>
    <title>一文详解IPv4与IPv6协议</title>
    <url>/post/datacenter/a-detailed-explanation-of-ipv4-and-ipv6-protocols.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>IPv4</tag>
      <tag>IPv6</tag>
    </tags>
    <content type="html"><![CDATA[前段时间的工作大多与通信协议相关，随着协议相关工作的不断深入，相关数据包的分析占据了不少工作时间。
在数据报文分析中，发现大学期间IP协议内容已经重新还给了老师，相关知识完全没有了印象，这篇文章算是一篇复习文，对相关IP协议进行重学习。
 IPv4 协议详解 IPv6 协议详解 IPv4 IPv6 报文对比  IPv4  IPv4 协议简介 IPv4 地址数量 IPv4 协议特点 IPv4 报文结构 IPv4 报文最大长度  IPv4 简介 IPv4（Internet Protocol version 4）是网际协议的第四个修订版本，也是该协议第一个被广泛部署和使用的版本。其在1981年9月由IETF发布的RFC791中被描述，是一种面向无连接的协议，可以在使用分组交换的链路层（如以太网）上运行。在数据传输方面，IPv4协议会尽最大努力交付数据包，但不能保证所有数据包能够成功到达目的地，或者按照正确的顺序到达，这些方面由上层的传输协议（如TCP协议）处理。
IPV4协议报文结构
IPv4 地址数量 IPv4协议使用32位（4字节）地址，其地址空间为4,294,967,296（2^32）个。其中一些地址被保留用于特定用途，如专用网络（约1800万个地址）和多播地址（约2.7亿个地址），这减少了可供互联网路由的地址数量。 随着地址被分配给最终用户，IPv4地址枯竭问题也日益严重。虽然基于分类网络、无类别域间路由和网络地址转换的地址结构重构减缓了地址枯竭的速度，但在2019年11月26日，全球近43亿个IPv4地址已分配完毕。
IPv4地址数量的限制刺激了IPv6的部署，IPv6是唯一的长期解决方案。 IPv6使用128位地址空间，提供了更多的地址，以及更好的安全性和性能。IPv6的广泛部署需要时间和努力，但已经成为解决IPv4地址短缺问题的主要途径。
IPv4 协议特点  面向无连接：  IPv4是一种面向无连接的协议，每个数据包都是独立的，数据包的传输不需要建立和维护连接状态。这使得IPv4的数据包传输速度较快，但同时也增加了数据包传输的可靠性和安全性方面的挑战。
 分组交换：  IPv4协议采用分组交换的技术，将数据分割成一系列小的数据包进行传输，每个数据包都包含了目标地址和源地址等必要的控制信息，这使得数据传输更加高效和灵活。同时，IPv4协议还支持多种传输协议，如TCP、UDP等，可以适应不同的数据传输需求。
 简单、可靠、稳定：  IPv4协议的设计非常简单、可靠、稳定，已经被广泛应用于互联网和局域网等各种网络环境中，具有良好的兼容性和稳定性。
 地址格式：  IPv4地址是一个32位的二进制数，通常用点分十进制表示法来表示，被分为四段，每段可以取0-255之间的整数。IPv4地址的短缺成为了一个问题，因此引入了私有地址和网络地址转换等技术来缓解IPv4地址短缺的问题。
 安全性：  IPv4协议的安全性较低，容易受到各种网络攻击，如IP欺骗、数据包伪造等。因此，为了提高IPv4协议的安全性，通常需要通过路由器、防火墙等网络安全设备来进行加强和保护。
IPv4 报文结构 IPv4报文的最大长度是65,535字节，这个长度是由IP报文中的16位总长度字段决定的，下图为IPv4报文的结构：
IPv4协议首部报文结构
IPV4协议首部报文抓包
  版本(Version)： 占用4比特位，表示IP协议的版本号，IPv4的值为4。
  首部长度(Internet Header Length)： 占用4比特位，表示IP首部的长度，首部长度说明首部有多少32位字(4字节，也就是说单位为4字节)。这个字段的最小值是5(二进制0101)，相当于54=20字节；最大十进制值是15，相当于154=60字节
  服务类型(Type of Service，TOS)： 占用8比特位，表示IP报文的服务类型，用于指定QoS(Quality of Service)和流量控制等参数。
  总长度(Total Length)： 占用16比特位，表示整个IP数据报的长度，包括IP首部和数据部分，单位为字节。这个字段的最小值是20（20字节首部+0字节数据），最大值是2^16-1=65,535。
  标识(Identification)： 占用16比特位，这个字段主要被用来唯一地标识一个报文的所有分片，因为分片不一定按序到达，所以在重组时需要知道分片所属的报文。每产生一个数据报，计数器加1，并赋值给此字段。
  标志(Flags)：占用3比特位，用于标识IP分片的状态。
  ** 位0：保留，必须为0； ** 位1：禁止分片（Don’t Fragment，DF），当DF=0时才允许分片； ** 位2：更多分片（More Fragment，MF），MF=1代表后面还有分片，MF=0 代表已经是最后一个分片。
  分片偏移(Fragment Offset)： 占用13比特位，用于表示分片相对于原始数据报的偏移量。
  生存时间(Time to Live)： 占用8比特位，表示数据报在网络中最多可以被经过的路由器数量，用于防止数据报在网络中无限循环。
  协议(Protocol)： 占用8比特位，表示数据报中的数据部分使用的协议类型，例如TCP、UDP、ICMP等。
  校验和(Header Checksum)： 占用16比特位，用于检测IP头部在传输过程中是否出现了错误。
  源地址(Source Address)： 占用32比特位，表示数据报的发送者IP地址。
  目标地址(Destination Address)： 占用32比特位，表示数据报的接收者IP地址。
  选项(Options) 附加的首部字段可选的跟在目的地址之后，但这并不被经常使用，从1到40个字节不等。如果首部长度大于5，那么选项字段必然存在。
  IPv4 报文长度 IPv4报文的最大长度是65,535字节，这个长度是由IP报文中的16位总长度字段决定的。该字段的最大值是65535，因为它是一个16位无符号整数，所以IP报文的最大长度不能超过该值。
需要注意的是，在实际情况下，IP报文的长度通常会受到网络设备（如路由器、防火墙等）和网络链路的限制，另外，由于网络传输存在MTU(Maximum Transmission Unit)的限制，实际上能够传输的最大数据长度通常不会超过MTU值，一般为1500个字节左右，因此实际传输的IP报文长度可能会比最大长度小得多。
网络传输MTU(Maximum Transmission Unit)大小并不是固定的，它的大小取决于底层网络传输协议和网络设备的配置。不同的网络传输协议和设备可能会有不同的MTU大小限制。
以太网是最常见的网络传输协议之一，其MTU大小通常为1500字节。在以太网上传输的数据包如果超过1500字节，就会被分割成多个小块进行传输。其他网络传输协议的MTU大小可能会有所不同，例如PPP协议的MTU大小通常为1480字节，ATM网络的MTU大小通常为48字节等。
此外，MTU还受到网络设备的配置影响。例如，路由器和交换机等网络设备可以通过配置MTU大小来优化网络传输效率和减少延迟。在实际应用中，为了保证网络传输的稳定性和效率，需要根据具体的网络环境和需求来设置MTU大小，并进行必要的优化和调整。
IPv6  IPv6 协议简介 IPv6 地址数量 IPv6 协议特点 IPv6 报文结构 IPv6 载荷长度  IPv6 简介 IPv6（Internet Protocol version 6）是网际协议的最新版本，主要是为了解决IPv4地址枯竭问题，同时它也在其他方面对于IPv4有许多改进，协议由1998年12月公布的 RFC2960 定义。
IPv6的设计目的是取代IPv4，然而长期以来IPv4在互联网流量中仍占据主要地位，IPv6的使用增长缓慢。在2022年4月，通过IPv6使用Google服务的用户百分率首次超过40%。
虽然IPv6在1994年就已被IETF指定作为IPv4的下一代标准，由于早期的路由器、防火墙及相关应用程序皆须改写，所以在世界范围内使用IPv6部署的网络服务与IPv4相比相对较少，技术上仍以双架构并存居多。
IPv6 地址数量 IPv6地址总长度为128比特位(16字节)，分为8组(每组2个字节)，每组以4个十六进制数形式表示，组间用冒号分隔。例如：FC00:0000:130F:0000:0000:09C0:876A:130B
因为IPv6地址使用128位（16字节）表示，其可以支持约3.4×10²³（2^128）个唯一地址。这个数量比IPv4地址空间（43亿个地址）大得多，可以满足未来数十年互联网的发展需求。IPv6地址空间的巨大规模不仅可以支持更多的设备连接到互联网，而且还可以提供更好的网络安全性和性能。
IPv6 协议特点   更大的地址空间： IPv6地址使用128位长度表示，可以支持约3.4×10²³（2^128）个唯一地址，这个数量比IPv4地址空间（43亿个地址）大得多，可以满足未来数十年互联网的发展需求。
  改进的寻址和路由机制： IPv6协议引入了一些新的寻址和路由机制，包括多播寻址、任播寻址和移动IPv6等，使得网络路由更加高效和灵活。
  简化的头部结构： IPv6协议头部长度固定为40字节，相比于IPv4头部结构更加简化，可以提高网络数据传输效率。
  可选的扩展首部： IPv6定义了许多可选的的扩展首部，不仅可提供比IPv4更多的功能，而且还可以提高路由器的处理效率，因为路由器对逐跳扩展首部外的其他扩展首部都不进行处理。
  更好的安全性和隐私保护： IPv6协议提供了更好的安全性和隐私保护，包括IPsec协议的强制支持、地址隐私扩展等，可以有效地保护网络和用户的隐私。
  更好的流量控制和服务质量： IPv6协议引入了流量控制和服务质量（QoS）机制，可以更好地管理网络流量和提供不同的服务质量，提高用户体验。
  IPv6 报文结构 IPv6数据报首部长度为固定的40字节，在IPv6中所有的扩展首部并不属于IPv6数据报的首部，扩展首部与其后面的数据部分合起来构成有效载荷。
IPv6报文首部与有效载荷
IPv6协议首部报文结构
IPV6协议首部报文抓包
  版本号(Version)： 占用4比特位，用于指示报文使用的IPv6协议版本号，固定为6。
  流量类别(Traffic Class)： 占用8比特位，用于区分不同的IPv6数据报的类别或优先级。。
  流量标签(Flow Label)： 占用20比特位，IPv6提出了流的抽象概念，流就是因特网上从特定源点到特定终点（单播或多播）的一系列IPv6数据报（如实时音视频数据的传送）。所有属于同一个流的IPv6数据报都具有同样的流量标签（相同的流量标签可进行同样的数据优先级设定）。因此，流标号对于实时音视频数据的传送特别有用，对于传统的非实时数据，流标号用处不大。
  负载长度(Payload Length)： 占用16比特位，用于指示IPv6报文中载荷(Payload)的长度，不包括IPv6头部的长度。
  下一个报头(Next Header)： 占用8比特位，用于指示IPv6头部后面的下一个报头类型，如TCP报头、UDP报头、ICMPv6报头等。
  跳数限制(Hop Limit)： 占用8比特位，类似于IPv4中的生存时间(TTL)字段，用于限制报文在网络中经过的最大跳数。
  源地址(Source Address)： 占用128比特位，表示发送端的IPv6地址。
  目标地址(Destination Address)： 占用128比特位，表示接收端的IPv6地址。
  IPv6 载荷长度 关于 IPv6 有效载荷长度：
IPv6报文有效载荷长度主要由Payload Length字段决定，Payload Length字段占用16比特位，用于表示载荷Payload的长度，即除去IPv6报头(固定为40字节)之外的部分。鉴于此字段为16比特，其最大值为2^16 - 1，即65,535字节。
然而，IPv6还支持一种叫做Jumbo Payload的选项。当使用这个选项时，载荷长度可以通过一个名为 Jumbo Payload Option 的扩展报头表示，该扩展报头中有一个32比特(4字节)的字段表示载荷长度。因此，最大载荷长度可以达到2^32 - 1，即4,294,967,295字节。
尽管IPv6有效载荷最大长度可达到4294967295字节，但其承载的传输层协议数据(如TCP、UDP)仍然受到IPv6网络中MTU的限制，因此仍然需要遵循最大报文长度65,535字节的限制。
IPv4、IPv6 报文比较 IPv4报文首部结构
IPv6报文首部与有效载荷
IPv6数据报首部长度为固定的40字节，所有的扩展首部并不属于IPv6数据报的首部，扩展首部与其后面的数据部分合起来构成有效载荷。
由于IPv6地址的长度扩展到了128比特位，使得IPv6数据报基本首部的长度增大到了40字节，比IPv4数据报首部固定部分的长度（20字节）增大了20字节。
其相比于IPv4报文：
取消了首部长度字段：IPv6数据报的首部长度是固定的40字节。
取消了服务类型字段：IPv6数据报首部中的流量类别和流量标签字段实现了区分服务字段的功能。
取消了总长度字段：改用有效载荷长度字段。这是因为IPv6数据报的首部长度是固定的40字节，只有其后面的有效载荷长度是可变的。
取消了标识、标志和片偏移字段：这些功能已包含在IPv6数据报的分片扩展首部中。
把生存时间TTL字段改称为跳数限制字段：这样名称与作用更加一致。
取消了协议字段：改用下一个首部字段。
取消了首部检验和字段：可以加快路由器处理IPv6数据报的速度。
取消了选项字段：改用扩展首部来实现选项功能。
参考 百科IPv4: https://zh.wikipedia.org/wiki/IPv4  
RFC791 IPV4: https://datatracker.ietf.org/doc/html/rfc791  
百科IPv6: https://zh.wikipedia.org/wiki/IPv4  
RFC2460 IPV6: https://datatracker.ietf.org/doc/html/rfc2460  
]]></content>
  </entry>
  
  <entry>
    <title>Marauder-高效缓存和预取加速手机APP</title>
    <url>/post/linux/synergized-caching-and-prefetching-for-low-risk-mobile-app-acceleration.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Android</tag>
      <tag>Marauder</tag>
      <tag>Mobile App</tag>
    </tags>
    <content type="html"><![CDATA[本次解读的文章来自MobiSys‘21，研究如何高效加速移动应用程序响应，研究人员来自UCLA、密歇根大学以及普林斯顿大学。
背景&amp;问题 移动应用程序已成为移动用户访问互联网服务的主要媒介，占智能手机用户关注时间的80%以上，手机应用成功的关键是较低的响应时间和用户感知的延迟。Android官方给出的报告显示用户对仅仅100毫秒量级的延迟反应消极，如果响应时间超过2-3秒，他们会放弃交互甚至删除应用。 鉴于移动应用程序性能的重要性，许多工作都致力于提高它们的响应能力。很多研究认为网络传输延迟是导致高响应时间的主要原因，本文也基于此。目前，有两种主要的技术来减轻网络延迟对应用响应的负面影响：缓存和预取。原则上，两都非常有效，特别是考虑到用户在与应用程序交互时表现出的重复模式。然而，每一种都有基本的缺点，这些缺点限制了它们在实践中的使用和有效性。
动机 实验设置   实验负载：论文收集75个各种类别的Android App，关注其中的50个（使用流行的OkHttp缓存库）；
  用户交互模拟：论文使用了人形应用测试框架Humanoid app测试框架，它采用深度神经网络从实际用户轨迹中学习交互模式；
  运行设置：对每个应用程序考虑默认的APK，以及嵌入Marauder的变体两种情况，在Google Pixel 4和Samsung Galaxy Note 9上运行；随机选择每个应用程序的5个用户交互轨迹，并以𝛿分钟为间隔进行应用。根据之前对用户-应用程序交互的研究，论文考虑10分钟到1天的𝛿值；如果未说明，默认值为𝛿 = 60𝑚𝑖𝑛𝑠。实验过程网络环境包括家庭WiFi和LTE网络；
  性能基准：本文主要的性能指标是用户响应时间IRT（interaction response time），即用户执行屏幕点击以触发交互的时间与交互的最终屏幕完全呈现的时间之间的时间。（具体测量方法详见论文介绍）；
  相关结论  现在的手机APP交互太慢：  图 3显示应用程序和跟踪的交互响应时间 (IRT) 分布，IRT 值的中位数和第 90 个百分位数在 WiFi 网络上分别为 1.6 和 3.7 秒，在 LTE 网络上跃升至 2.9 和 6.7 秒。因此，交互响应时间经常超过用户愿意容忍的 2-3 秒。 网络延迟是关键因素：  移动应用程序的响应时间取决于内容获取过程中产生的网络延迟，以及在APK中解析/呈现该内容和其他代码的客户端计算延迟。对于每个交互，论文比较了有和没有网络延迟的IRT。如图4所示，网络延迟分别占WiFi和LTE中值交互IRT的38%和64%。
现有优化存在局限：  上述实验中，应用程序采用默认运行模式，应用程序的APK或源服务器指定的任何缓存或预取都会被采用。然而，响应时间仍然过长。
缓存的问题：现有的缓存要求开发人员为他们服务的每个资源显式设置一个TTL。然而，设置这样的TTL是困难的，因为每个资源的理想TTL随时间而变化。论文的实验中，50%资源的理想TTLs的标准偏差是2小时，在这种情况下，开发人员面临着权衡；
预取的问题：应用程序对预取的支持通常选择获取大量内容的通用策略，其中大部分内容没有被使用。因此，尽管有潜在的加速，实际上预取在资源使用方面是非常浪费的。这种浪费会导致较高的通信成本，并且还会影响IRT加速，尤其是在带宽受限的设置中，其中明确请求的资源必须与不必要预取的资源竞争；
设计 Marauder是一个移动应用程序加速框架，其目标是同时利用缓存和预取的能力，尽量回避它们的风险和局限性。Marauder的关键思想是使用明智的预取来最大化已经缓存的对象的效用(即缓存命中)，然后使用这些缓存的对象来指导即时预取(即在当前交互期间)。
观察 观察一：论文发现为给定交互提取的文本文件（即 JSON、HTML、JavaScript、CSS 和 XML）直接嵌入了该交互所需的大多数非文本资源的列表，94%提取的非文本资源的URL可以从以前提取的文本文件中获得。所以，当应用程序解析和执行文本文件时，可以直接从应用程序的缓存中发出对任何引用文件的预取请求。这样，文本文件的早期解析和执行延迟会与被引用的非文本文件的网络获取延迟重叠。而且，这种JIT(just-in-time)的预取的风险会很低因为它只考虑直接列在文件中的资源，该文件被显式地请求并且将要被解析；
观察二：论文观察到缓存资源的内容通常保持不变，尽管相应的 TTL 已过期（这发生在47% 的资源中），这表明现有缓存TTL 设置过于保守。所以可以通过预取即将到期的资源，在它们没有改变的情况下延长它们的TTL(从而提高它们的命中率)；
观察三：论文发现给定文本文件引用的文件集通常在很长一段时间内保持稳定，即使文本内容发生变化也是如此。例如，中间文本文件的引用文件集在4小时内保持不变，尽管50%的文本文件仅在20分钟内保持不变。将此条件放宽到只有90%的引用文件未更改，将中位持续时间增加到9小时。所以可以通过在验证/更新不可缓存的文本文件并随后解析该文本文件时，对该文本文件所引用的文件发出预取请求，减少条件检查所带来的延迟。。
后台操作   Cache Refresher：刷新缓存资源，以提高整体缓存命中率。对于缓存中具有非零TTL的每个资源(即，不包括标记为非缓存的资源)，Marauder添加一个计时器事件，该事件在到期时检查资源的内容是否没有改变，以及其TTL是否可以延长。按照上面的观察二，Marauder对文本和非文本文件执行不同的TTL扩展过程；
  JIT Prefetcher：促进文本文件引用的资源的在线预取。每当一个文本文件被添加到应用程序缓存时(即使它被标记为无缓存), Marauder 都会异步生成一个工作线程来解析相应文本文件的正文以搜索引用的URL。开发人员使用各种正则表达式和 LinkedIn URL 检测器库静态分析文本文件，以查找所有类似于 URL 的字符串；
  运行流程 在用户交互期间，应用程序发出的请求正常访问缓存存在三种潜在的情况，每种情况都需要使用 Marauder 的不同工作流程。
  对于第一次请求文本文件(即，它不在缓存中，甚至过期)，或者任何请求非文本文件但在应用程序缓存中未命中的请求，Marauder会立即通过网络发出请求。收到响应后，Marauder将内容发送到应用程序，将其添加到缓存中，如果它是文本文件，则开始后台任务以提取引用的资源；
  对于在应用程序缓存中命中的文本文件请求(根据缓存库的默认命中标准)，Marauder用相应的内容响应应用程序，并立即对该文本文件引用的所有文件发出异步预取请求(使用离线生成的列表)。预取请求首先通过应用程序缓存，确保已经缓存的资源不会被重新下载。为了发出预取请求，Marauder首先填充动态查询参数的值；
  对于在应用程序缓存中未命中但与标记为无缓存的条目相关的文本文件请求，Marauder会发出一个对文本文件的请求，然后发出对其所有被引用子节点的预取请求。如果文本文件在某些预取资源之前到达，Marauder会小心地将应用程序显式发出的后续请求排队，这些请求已经有一个未完成的预取请求。通过这种方式，Marauder避免了重复请求和带宽浪费；一旦相应的预取响应到达，队列中的请求就会得到服务；
  实验 应用程序加速 图14 说明了与当前应用程序所采用的默认缓存和预取策略相比，Marauder能够降低交互响应时间(IRT)。例如，在LTE网络上，Marauder 将一半的应用程序的IRT提高了 19.8-27.4%（或 0.58-0.81 秒）；一半的应用程序的第 90 个百分位 IRT 改进为 29.7-43.5%（或 0.87-1.27 秒）。由于服务器的网络延迟较低，Marauder 在 WiFi 上的收益中位数和 90% 分别下降到 16.9-23.1% 和 26.1-33.2%。图 14 还显示，当用户会话之间的时间增加时，Marauder 的改进更加明显（即是前文的参数𝛿）。例如，在 LTE 上，随着𝛿 从 20 分钟增加到 4 小时，一半的应用程序的IRT 改进从 19.8% 增长到 24.4%。
Marauder性能深入分析  每个设计的效率：分别分析了JIT预取和Cache Refreshing的效果，后台缓存刷新以提高已缓存资源的命中率是 Marauder 收益最大的地方，在所考虑的 𝛿 值的中位数处提供 13.3-16.1% 的加速。相比之下，即时预取可提供 3.4-8.6% 的中值加速； 互动案例的测试：Marauder 优化的有效性受目标特定交互中的加载模式的影响。为了更好地理解这些关系，实验分析了当前应用程序的交互，并确定了影响 Marauder 提供的加速幅度的三种主要模式；  和先进方案对比 论文将 Marauder 与三种加速方法进行了比较：Paloma预取系统，以及应用程序支持的最激进和保守的预取策略。
总结 论文提出了一种移动应用程序加速系统Marauder，它综合利用缓存和预取技术，优化了TTL设置不当、缓存过时内容以及预取浪费带宽问题。同时，文章通过分析大量实验数据，得到了一些关键观察。相关实验设计非常巧妙值得学习。对缓存和预取这两种操作系统经典设计在应用程序加速领域做了深入分析，但是设计本身并不复杂。此外，从实现上来看，Marauder只是用上述优化的版本替换了大多数应用程序所依赖的缓存库，可以立即部署。
源代码和实验数据访问 https://github.com/muralisr/marauder  
The End
致谢
感谢本次论文解读者，来自华东师范大学的博士生李文通，主要研究方向为跨设备资源共享、移动计算。
 论文下载地址  
]]></content>
  </entry>
  
  <entry>
    <title>Nvidia 3060显卡 CUDA环境搭建</title>
    <url>/post/linux/how-to-setup-CUDA-environment-for-nvidia-3060.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Nvidia</tag>
      <tag>Ubuntu 22.04</tag>
      <tag>Cuda11.6</tag>
      <tag>Cudnn8.8</tag>
      <tag>Nouveau</tag>
    </tags>
    <content type="html"><![CDATA[本文详细记录了如何在Ubuntu 22.04上部署基于NVIDIA 510的CUDA。
写在前面 工作中遇到，简单整理 理解不足小伙伴帮忙指正 对每个人而言，真正的职责只有一个：找到自我。然后在心中坚守其一生，全心全意，永不停息。所有其它的路都是不完整的，是人的逃避方式，是对大众理想的懦弱回归，是随波逐流，是对内心的恐惧 ——赫尔曼·黑塞《德米安》
当前系统环境 系统环境
┌──[root@test]-[~] └─$hostnamectl Static hostname: test Icon name: computer-desktop Chassis: desktop Machine ID: addc7ca21ef24518a9465c499eb3c8b7 Boot ID: 14aa59cc6960431c95d328684b521844 Operating System: Ubuntu 22.04.2 LTS Kernel: Linux 5.19.0-43-generic Architecture: x86-64 Hardware Vendor: Micro-Star International Co., Ltd. Hardware Model: MS-7C83 显卡版本
┌──[root@test]-[~] └─$lspci -vnn | grep VGA 01:00.0 VGA compatible controller [0300]: NVIDIA Corporation GA106 [GeForce RTX 3060 Lite Hash Rate] [10de:2504] (rev a1) (prog-if 00 [VGA controller]) ┌──[root@test]-[~] └─$ 安装 NVIDIA 驱动程序，在安装之前，需要禁用 Nouveau 驱动程序。
Nouveau 是一个开源的NVIDIA显卡驱动程序，它由社区开发和维护。它可以在Linux系统上替代NVIDIA官方驱动程序，但它的性能和功能可能不如官方驱动程序。
如果使用 Nouveau 驱动程序，您可能无法使用NVIDIA的高级功能，如CUDA和深度学习库。如果您需要使用这些功能，建议安装NVIDIA官方驱动程序。
禁用 Nouveau 驱动程序
┌──[root@test]-[~] └─$sudo vim /etc/modprobe.d/blacklist-nouveau.conf ┌──[root@test]-[~] └─$cat /etc/modprobe.d/blacklist-nouveau.conf blacklist nouveau options nouveau modeset=0 ┌──[root@test]-[~] └─$sudo update-initramfs -u update-initramfs: Generating /boot/initrd.img-5.19.0-43-generic 没有输出说明操作成功
┌──[root@test]-[~] └─$reboot ┌──[root@test]-[~] └─$lsmod | grep nouveau ┌──[root@test]-[~] └─$ 安装Nvidia驱动 这里的版本 nvidia-driver-510 要和后面安装 cuda 的版本一样
如果之前安装过卸载驱动
# 查看显卡型号 lspci -vnn | grep VGA # 卸载旧驱动 sudo apt-get remove --purge nvidia* 离线安装
如果离线环境需要手动安装,下载驱动：https://www.nvidia.com/Download/index.aspx?lang=en-us
# 给run文件可执行权限  sudo chmod a+x NVIDIA-Linux-x86_64-515.86.01.run # 安装  sudo ./NVIDIA-Linux-x86_64-440.64.run -no-x-check -no-nouveau-check -no-opengl-files # -no-x-check：安装驱动时关闭X服务 # -no-nouveau-check：安装驱动时禁用nouveau # -no-opengl-files：只安装驱动文件，不安装OpenGL文件 非离线安装
非离线环境使用包管理工具安装，下面的选择这一种,选择安装驱动版本
┌──[root@test]-[~] └─$ubuntu-drivers devices == /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0 == modalias : pci:v000010DEd00002504sv00001462sd0000397Dbc03sc00i00 vendor : NVIDIA Corporation model : GA106 [GeForce RTX 3060 Lite Hash Rate] driver : nvidia-driver-530-open - distro non-free driver : nvidia-driver-470 - distro non-free driver : nvidia-driver-525-open - third-party non-free driver : nvidia-driver-535 - third-party non-free driver : nvidia-driver-520 - third-party non-free driver : nvidia-driver-510 - distro non-free driver : nvidia-driver-525 - third-party non-free driver : nvidia-driver-515-server - distro non-free driver : nvidia-driver-535-open - third-party non-free recommended driver : nvidia-driver-530 - third-party non-free driver : nvidia-driver-470-server - distro non-free driver : nvidia-driver-515-open - distro non-free driver : nvidia-driver-525-server - distro non-free driver : nvidia-driver-515 - third-party non-free driver : xserver-xorg-video-nouveau - distro free builtin ┌──[root@test]-[~] └─$ 安装
┌──[root@test]-[~] └─$sudo apt install nvidia-driver-510 -y 重启机器
┌──[root@test]-[~] └─$reboot 查看安装是否成功，对应版本信息
┌──[root@test]-[~] └─$nvidia-smi Thu Jun 15 11:49:43 2023 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510.108.03 Driver Version: 510.108.03 CUDA Version: 11.6 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:01:00.0 On | N/A | | 0% 38C P8 16W / 170W | 172MiB / 12288MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1386 G /usr/lib/xorg/Xorg 60MiB | | 0 N/A N/A 1650 G /usr/bin/gnome-shell 109MiB | +-----------------------------------------------------------------------------+ ┌──[root@test]-[~] └─$cat /proc/driver/nvidia/version NVRM version: NVIDIA UNIX x86_64 Kernel Module 510.108.03 Thu Oct 20 05:10:45 UTC 2022 GCC version: gcc version 11.3.0 (Ubuntu 11.3.0-1ubuntu1~22.04.1) ┌──[root@test]-[~] └─$ 安装Cuda CUDA是NVIDIA提供的一种并行计算平台和编程模型，旨在利用GPU的并行计算能力加速计算密集型应用程序。
CUDA包括CUDA驱动程序和CUDA Toolkit。支持多种编程语言，包括C、C++、Fortran和Python等。
 CUDA驱动程序是GPU和操作系统之间的接口. CUDA Toolkit则包括编译器、库和工具，用于开发CUDA应用程序。  如果以前安装过，卸载
sudo /usr/local/cuda-11.6/bin/cuda-uninstaller sudo rm -rf /usr/local/cuda-11.6 sudo: /usr/local/cuda-11.8/bin/uninstall_cuda_8.0.pl: command not found ┌──[root@test]-[~] └─$sudo /usr/local/cuda-11.6/bin/ bin2c cuda-gdbserver ncu nsys-ui nv-nsight-cu-cli computeprof cuda-memcheck ncu-ui nvcc nvprof compute-sanitizer cuda-uninstaller nsight_ee_plugins_manage.sh __nvcc_device_query nvprune crt/ cu++filt nsight-sys nvdisasm nvvp cudafe++ cuobjdump nsys nvlink ptxas cuda-gdb fatbinary nsys-exporter nv-nsight-cu ┌──[root@test]-[~] └─$sudo /usr/local/cuda-11.6/bin/cuda-uninstaller 在输出的终端 UI页面，空格选择全部，选择完成，卸载完成之后重新安装
┌──[root@test]-[~] └─$sudo /usr/local/cuda-11.6/bin/cuda-uninstaller Successfully uninstalled ┌──[root@test]-[~] └─$sudo rm -rf /usr/local/cuda-11.6 官网安装包下载
 https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=22.04&target_type=runfile_local  
┌──[root@test]-[~] └─$chmod +x cuda_* 这里cuda 选择 cuda_11.6.0_510.39.01_linux.run, 510 对应的版本
┌──[root@test]-[~] └─$ll cuda* -rwxr-xr-x 1 root root 3488951771 1月 11 2022 cuda_11.6.0_510.39.01_linux.run* -rwxr-xr-x 1 root root 3490450898 5月 5 2022 cuda_11.7.0_515.43.04_linux.run* -rwxr-xr-x 1 root root 4317456991 4月 17 23:04 cuda_12.1.1_530.30.02_linux.run* -rwxr-xr-x 1 root root 853 5月 17 19:52 cuda_log.log* -rw-r--r-- 1 root root 2472241638 7月 29 2021 cuda-repo-ubuntu2004-11-4-local_11.4.1-470.57.02-1_amd64.deb -rw-r--r-- 1 root root 2699477842 5月 5 2022 cuda-repo-ubuntu2204-11-7-local_11.7.0-515.43.04-1_amd64.deb ┌──[root@test]-[~] └─$ ┌──[root@test]-[~] └─$sudo ./cuda_12.1.1_530.30.02_linux.run 上面我们已经安装了驱动，所以不需要选择，直接安装 cuda 相关的就可以，安装成功输出
┌──[root@test]-[~] └─$sudo ./cuda_11.6.0_510.39.01_linux.run =========== = Summary = =========== Driver: Not Selected Toolkit: Installed in /usr/local/cuda-11.6/ Please make sure that - PATH includes /usr/local/cuda-11.6/bin - LD_LIBRARY_PATH includes /usr/local/cuda-11.6/lib64, or, add /usr/local/cuda-11.6/lib64 to /etc/ld.so.conf and run ldconfig as root To uninstall the CUDA Toolkit, run cuda-uninstaller in /usr/local/cuda-11.6/bin ***WARNING: Incomplete installation! This installation did not install the CUDA Driver. A driver of version at least 510.00 is required for CUDA 11.6 functionality to work. To install the driver using this installer, run the following command, replacing &lt;CudaInstaller&gt; with the name of this run file: sudo &lt;CudaInstaller&gt;.run --silent --driver Logfile is /var/log/cuda-installer.log 添加对应环境变量
┌──[root@test]-[/b1205] └─$echo $LD_LIBRARY_PATH /usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64 ┌──[root@test]-[/b1205] └─$echo $PATH /usr/local/cuda-11.6/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin ┌──[root@test]-[/b1205] └─$ 安装 cuDNN cuDNN 是NVIDIA提供的一个用于深度神经网络的加速库，它可以优化卷积、池化、归一化等操作，使得在GPU上运行深度神经网络的速度得到了大幅度提升。cuDNN需要与CUDA配合使用，因此在安装cuDNN之前，需要先安装相应版本的CUDA。
 https://developer.nvidia.com/rdp/cudnn-download  
这里需要注册账户登录一下,然后在这里下载
 https://developer.nvidia.com/rdp/cudnn-archive  
选择cuda对应的版本
┌──[root@test]-[~] └─$ls cudnn* cudnn-local-repo-ubuntu2204-8.8.1.3_1.0-1_amd64.deb sudo dpkg -i cudnn-local-repo-ubuntu2204-8.8.1.3_1.0-1_amd64.deb sudo cp /var/cudnn-local-repo-*/cudnn-local-*-keyring.gpg /usr/share/keyrings/ sudo apt-get update sudoapt-get install libcudnn8=8.8.1.3-1+cuda1 sudo apt-get install libcudnn8-dev=8.8.1.3-1+cuda1 sudo apt-get install libcudnn8-samples=8.8.1.3-1+cuda1 确实安装是否成功 ┌──[root@test]-[~] └─$nvcc -V &amp;&amp; nvidia-smi nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2021 NVIDIA Corporation Built on Fri_Dec_17_18:16:03_PST_2021 Cuda compilation tools, release 11.6, V11.6.55 Build cuda_11.6.r11.6/compiler.30794723_0 Thu Jun 15 14:42:58 2023 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510.108.03 Driver Version: 510.108.03 CUDA Version: 11.6 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:01:00.0 On | N/A | | 0% 51C P8 21W / 170W | 105MiB / 12288MiB | 12% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1386 G /usr/lib/xorg/Xorg 81MiB | | 0 N/A N/A 1650 G /usr/bin/gnome-shell 22MiB | +-----------------------------------------------------------------------------+ ┌──[root@test]-[~] └─$ 编写测试脚本测试
(py39) test@test:~/code/Face$ cat cuda_vim.py import numpy as np import time from numba import cuda @cuda.jit def increment_kernel(array): idx = cuda.grid(1) if idx &lt; array.size: array[idx] += 1 def main(): n = 1000000000 a = np.zeros(n, dtype=np.int32) threads_per_block = 1024 blocks_per_grid = (n + threads_per_block - 1) // threads_per_block start = time.time() increment_kernel[blocks_per_grid, threads_per_block](a) end = time.time() print(&#34;Time taken: &#34;, end - start) if __name__ == &#34;__main__&#34;: while True: main() (py39) test@test:~/code/Face$ Every 2.0s: nvidia-smi test: Thu Jun 15 14:44:47 2023 Thu Jun 15 14:44:47 2023 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510.108.03 Driver Version: 510.108.03 CUDA Version: 11.6 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:01:00.0 On | N/A | | 0% 55C P2 51W / 170W | 4025MiB / 12288MiB | 22% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1386 G /usr/lib/xorg/Xorg 81MiB | | 0 N/A N/A 1650 G /usr/bin/gnome-shell 22MiB | | 0 N/A N/A 32031 C python 3917MiB | +-----------------------------------------------------------------------------+ 遇到的问题 安装530高版本报下面的错：
┌──[root@test]-[~] └─$sudo ./cuda_12.1.1_530.30.02_linux.run Error! Could not locate dkms.conf file. File: /var/lib/dkms/nvidia-fs/2.15.3/source/dkms.conf does not exist. cat: /var/log/nvidia/.uninstallManifests/kernelobjects-components/uninstallManifest-nvidia_fs: No such file or directory make: *** No rule to make target &#39;uninstall&#39;. Stop. Error! DKMS tree already contains: nvidia-fs-2.15.3 You cannot add the same module/version combo more than once. =========== = Summary = =========== Driver: Not Selected Toolkit: Installed in /usr/local/cuda-12.1/ Please make sure that - PATH includes /usr/local/cuda-12.1/bin - LD_LIBRARY_PATH includes /usr/local/cuda-12.1/lib64, or, add /usr/local/cuda-12.1/lib64 to /etc/ld.so.conf and run ldconfig as root To uninstall the CUDA Toolkit, run cuda-uninstaller in /usr/local/cuda-12.1/bin To uninstall the kernel objects, run ko-uninstaller in /usr/local/kernelobjects/bin ***WARNING: Incomplete installation! This installation did not install the CUDA Driver. A driver of version at least 530.00 is required for CUDA 12.1 functionality to work. To install the driver using this installer, run the following command, replacing &lt;CudaInstaller&gt; with the name of this run file: sudo &lt;CudaInstaller&gt;.run --silent --driver Logfile is /var/log/cuda-installer.log ┌──[root@test]-[~] └─$ 解决办法，换了低版本的510
运行 nvvp 报错
┌──[root@test]-[~] └─$nvvp Nvvp: Cannot open display: WARNING: An illegal reflective access operation has occurred WARNING: Illegal reflective access by org.eclipse.osgi.storage.FrameworkExtensionInstaller (file:/usr/local/cuda-11.6/libnvvp/plugins/org.eclipse.osgi_3.10.1.v20140909-1633.jar) to method java.net.URLClassLoader.addURL(java.net.URL) WARNING: Please consider reporting this to the maintainers of org.eclipse.osgi.storage.FrameworkExtensionInstaller WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations WARNING: All illegal access operations will be denied in a future release Nvvp: Cannot open display: Nvvp: An error has occurred. See the log file /usr/local/cuda-11.6/libnvvp/configuration/1686795694122.log. ┌──[root@test]-[~] └─$ ssh 环境不行，需要做桌面环境
在桌面环境执行，报错
Gtk-Message: 09:10:26.571: Failed to load module &#34;canberra-gtk-module&#34; 安装下面的安装包
┌──[root@test]-[~] └─$sudo apt-get install libcanberra-gtk-module nvidia-driver-XXX-open 版本安装报错
nvidia-driver-530-open 是一个在发行版的非自由存储库中提供的NVIDIA驱动程序，它是由发行版的维护者维护的。这意味着它是与发行版的其余部分紧密集成的，并且由发行版的维护者提供支持和更新。
nvidia-driver-530 是一个第三方非自由驱动程序，它不是由发行版的维护者维护的。相反，它是由NVIDIA公司提供的，并且可能需要手动安装和配置。由于它不是由发行版的维护者提供的，因此您可能无法获得与发行版集成和支持相同的级别。
nvidia-driver-530-open是更受支持和更集成的选择，而nvidia-driver-530则需要更多的手动配置和支持。
nvidia-driver-530-open : Depends: libnvidia-gl-530 (= 530.41.03-0ubuntu0.22.04.2) but it is not going to be installed Depends: nvidia-dkms-530-open (&lt;= 530.41.03-1) Depends: nvidia-dkms-530-open (&gt;= 530.41.03) Depends: nvidia-kernel-common-530 (&lt;= 530.41.03-1) but it is not going to be installed Depends: nvidia-kernel-common-530 (&gt;= 530.41.03) but it is not going to be installed Depends: nvidia-kernel-source-530-open (= 530.41.03-0ubuntu0.22.04.2) but it is not going to be installed Depends: libnvidia-compute-530 (= 530.41.03-0ubuntu0.22.04.2) but it is not going to be installed Depends: libnvidia-extra-530 (= 530.41.03-0ubuntu0.22.04.2) but it is not going to be installed Depends: nvidia-compute-utils-530 (= 530.41.03-0ubuntu0.22.04.2) but it is not going to be installed Depends: libnvidia-decode-530 (= 530.41.03-0ubuntu0.22.04.2) but it is not going to be installed Depends: libnvidia-encode-530 (= 530.41.03-0ubuntu0.22.04.2) but it is not going to be installed Depends: nvidia-utils-530 (= 530.41.03-0ubuntu0.22.04.2) but it is not going to be installed Depends: xserver-xorg-video-nvidia-530 (= 530.41.03-0ubuntu0.22.04.2) but it is not going to be installed Depends: libnvidia-cfg1-530 (= 530.41.03-0ubuntu0.22.04.2) but it is not going to be installed Depends: libnvidia-fbc1-530 (= 530.41.03-0ubuntu0.22.04.2) but it is not going to be installed Recommends: libnvidia-compute-530:i386 (= 530.41.03-0ubuntu0.22.04.2) Recommends: libnvidia-decode-530:i386 (= 530.41.03-0ubuntu0.22.04.2) Recommends: libnvidia-encode-530:i386 (= 530.41.03-0ubuntu0.22.04.2) Recommends: libnvidia-fbc1-530:i386 (= 530.41.03-0ubuntu0.22.04.2) Recommends: libnvidia-gl-530:i386 (= 530.41.03-0ubuntu0.22.04.2) E: Unable to correct problems, you have held broken packages. 解决办法，下面的方式进行了尝试，未解决。换了不带 open 的版本
更新你的软件包列表和已安装的软件包：
sudo apt update sudo apt upgrade 尝试使用以下命令来修复可能存在的损坏软件包：
sudo apt --fix-broken install 使用以下命令来清理系统中已经安装的软件包的缓存：
sudo apt clean 尝试使用以下命令来删除已经损坏的软件包并重新安装
sudo apt remove nvidia-driver-530-open sudo apt autoremove sudo apt install nvidia-driver-530-open 博文部分内容参考 © 文中涉及参考链接内容版权归原作者所有，如有侵权请告知，这是一个开源项目，如果你认可它，不要吝啬星星哦 :)
 https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html  
 https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html#id8  
 https://blog.51cto.com/u_4029519/5909904  
© 2018-2023 liruilonger@gmail.com , All rights reserved. 保持署名-非商用-相同方式共享(CC BY-NC-SA 4.0)
]]></content>
  </entry>
  
  <entry>
    <title>图文详解PCB术语</title>
    <url>/post/hardware/detailed-explanation-of-PCB-terminology-with-pictures-and-texts.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>PCB, Gold Plating, Silver Plating</tag>
    </tags>
    <content type="html"><![CDATA[文章将会详细解释PCB的构成，以及在PCB的领域里面常用的一些术语，简要的组装方法，以及简介PCB的设计过程。
What&rsquo;s a PCB? PCB(Printed circuit board)是一个最普遍的叫法，也可以叫做“printed wiring boards” 或者 “printed wiring cards”。在PCB出现之前，电路是通过点到点的接线组成的。这种方法的可靠性很低，因为随着电路的老化，线路的破裂会导致线路节点的断路或者短路。
绕线技术是电路技术的一个重大进步，这种方法通过将小口径线材绕在连接点的柱子上，提升了线路的耐久性以及可更换性。
(1977年Z80计算机的绕线背板)
当电子行业从真空管、继电器发展到硅半导体以及集成电路的时候，电子元器件的尺寸和价格也在下降。电子产品越来越频繁的出现在了消费领域，促使厂商去寻找更小以及性价比更高的方案。于是，PCB诞生了。
Composition(组成) PCB看上去像多层蛋糕或者千层面&ndash;制作中将不同的材料的层，通过热量和粘合剂压制到一起。
从中间层开始吧。
FR4 PCB的基材一般都是玻璃纤维。大多数情况下，PCB的玻璃纤维基材一般就指&quot;FR4&quot;这种材料。&ldquo;FR4&quot;这种固体材料给予了PCB硬度和厚度。除了FR4这种基材外，还有柔性高温塑料(聚酰亚胺或类似)上生产的柔性电路板等等。
你可能会发现有不同厚度的PCB;然而 SparkFun的产品的厚度大部分都是1.6mm(0.063'')。有一些产品也采用了其它厚度，比如 LilyPad、Arudino Pro Micro boards采用了0.8mm的板厚。
廉价的PCB和洞洞板(见上图)是由环氧树脂或酚这样的材料制成，缺乏 FR4那种耐用性，但是却便宜很多。当在这种板子上焊接东西时，将会闻到很大的异味。这种类型的基材，常常被用在很低端的消费品里面。酚类物质具有较低的热分解温度，焊接时间过长会导致其分解碳化，并且散发出难闻的味道。
Copper (露铜的PCB，无阻焊&amp;丝印)
接下来介绍是很薄的铜箔层，生产中通过热量以及黏合剂将其压制倒基材上面。在双面板上，铜箔会压制到基材的正反两面。在一些低成本的场合，可能只会在基材的一面压制铜箔。当我们提及到”双面板“或者”两层板“的时候，指的是我们的千层面上有两层铜箔。当然，不同的PCB设计中，铜箔层的数量可能是1层这么少，或者比16层还多。
铜层的厚度种类比较多，而且是用重量做单位的，一般采用铜均匀的覆盖一平方英尺的重量(盎司oz)来表示。大部分PCB的铜厚是1oz，但是有一些大功率的PCB可能会用到2oz或者3oz的铜厚。将盎司(oz)每平方英尺换算一下，大概是 35um或者1.4mil的铜厚。
Soldermask(阻焊) 在铜层上面的是阻焊层。这一层让PCB看起来是绿色的(或者是SparkFun的红色)。阻焊层覆盖住铜层上面的走线，防止PCB上的走线和其他的金属、焊锡或者其它的导电物体接触导致短路。阻焊层的存在，使大家可以在正确的地方进行焊接 ，并且防止了焊锡搭桥。
在上图这个例子里，我们可以看到阻焊覆盖了PCB的大部分(包括走线)，但是露出了银色的孔环以及SMD焊盘，以方便焊接。
一般来说，阻焊都是绿色的，但几乎所有的颜色可以用来做阻焊。SparkFun的板卡大部分是红色的，但是IOIO板卡用了白色，LilyPad板卡是紫色的。
Silkscreen(丝印) 在阻焊层上面，是白色的丝印层。在PCB的丝印层上印有字母、数字以及符号，这样可以方便组装以及指导大家更好地理解板卡的设计。我们经常会用丝印层的符号标示某些管脚或者LED的功能等。
丝印层是最最常见的颜色是白色，同样，丝印层几乎可以做成任何颜色。黑色，灰色，红色甚至是黄色的丝印层并不少见。然而，很少见到单个板卡上有多种丝印层颜色。
Terminology(术语) 现在你知道了PCB的结构组成，下面我们来看一下PCB相关的术语吧。 孔环 &ndash; PCB上的金属化孔上的铜环。
Examples of annular rings. 孔环的例子
DRC &ndash; 设计规则检查。一个检查设计是否包含错误的程序，比如，走线短路，走线太细，或者钻孔太小。
钻孔命中 &ndash; 用来表示设计中要求的钻孔位置和实际的钻孔位置的偏差。钝钻头导致的不正确的钻孔中心是PCB制造里的普遍问题。
不是太准确的drill hit示意图
(金)手指 &ndash; 在板卡边上裸露的金属焊盘，一般用做连接两个电路板。比如计算机的扩展模块的边缘、内存条以及老的游戏卡。
邮票孔 &ndash; 除了V-Cut外，另一种可选择的分板设计方法。用一些连续的孔形成一个薄弱的连接点，就可以容易将板卡从拼版上分割出来。SparkFun的Protosnap板卡是一个比较好的例子。
ProtoSnap上的邮票孔使PCB能简单的弯折下来。
焊盘 &ndash; 在PCB表面裸露的一部分金属，用来焊接器件。
上边是 插件焊盘，下边是贴片焊盘
拼板 &ndash; 一个由很多可分割的小电路板组成的大电路板。自动化的电路板生产设备在生产小板卡的时候经常会出问题，将几个小板卡组合到一起，可以加快生产速度。
钢网 &ndash; 一个薄金属模板(也可以是塑料)，在组装的时候，将其放在PCB上让焊锡透过某些特定部位。
钢网(原图挂了，自己找的配图)
Pick-and-place - 将元器件放到线路板上的机器或者流程。
平面 &ndash; 线路板上一段连续的铜皮。一般是由边界来定义，而不是 路径。也称作”覆铜“
图示PCB上大部分地方没有走线，但是有地的覆铜
金属化过孔 &ndash; PCB上的一个孔，包含孔环以及电镀的孔壁。金属化过孔可能是一个插件的连接点，信号的换层处，或者是一个安装孔。
FABFM PCB上的一个插件电阻。电阻的两个腿已经穿过了PCB的过孔。电镀的孔壁可以使PCB正反两面的走线连接到一起。
Pogo pin &ndash; 一个弹簧支撑的临时接触点，一般用作测试或烧录程序。
有尖头的pogo pin, 在测试针床中用的很多。
回流焊 &ndash; 将焊锡融化，使焊盘(SMD)和器件管脚连接到一起。
丝印 &ndash; 在PCB板上的字母、数字、符号或者图形等。基本上每个板卡上只有一种颜色，并且分辨率相对比较低。
丝印指出了这个LED是电源指示灯。
开槽 &ndash; 指的是PCB上任何不是圆形的洞。开槽可以电镀也可以不电镀。由于开槽需要额外的切割时间，有时会增加板卡的成本。
在ProtoSnap - Pro Mini板卡上的复杂开槽。同样有很多邮票孔。注意: 由于开槽的刀具是圆形的，开槽的边缘不能完全做成直角。
锡膏层 &ndash; 在往PCB上放置元器件之前，会通过钢网在表贴器件的焊盘上形成的一定厚度的锡膏层。在回流焊过程中，锡膏融化，在焊盘和器件管脚间建立可靠的电气和机械连接。
在放置元器件之前，PCB上短暂的锡膏层，记得去了解一下钢网的定义。
焊锡炉 &ndash; 焊接插件的炉子。一般里面有少量的熔融的焊锡，板卡在上面迅速的通过，就可以将暴露的管脚上锡焊接好。
阻焊 &ndash; 为了防止短路、腐蚀以及其它问题，铜上面会覆盖一层保护膜。保护膜一般是绿色，也可能是其它颜色(SparkFun红色，Arduino蓝色，或者Apple黑色)。一般称作“阻焊”。
Solder mask covers up the signal traces but leaves the pads to solder to.阻焊覆盖了信号线，但是露出了焊盘以便于焊接。
连锡 &ndash; 器件上的两个相连的管脚，被一小滴焊锡错误的连接到了一起。
表面贴装 &ndash; 一种组装的方法，器件只需要简单的放在板卡上，不需要将器件管脚穿过板卡上的过孔。
热焊盘 &ndash; 指的是连接焊盘到平面间的一段短走线。如果焊盘没有做恰当的散热设计，焊接时很难将焊盘加热到足够的焊接温度。不恰当的散热焊盘设计，会感觉焊盘比较黏，并且回流焊的时间相对比较长。(译者注，一般热焊盘做在插件与波峰焊接触的一面。不知道这个文章里面为什么会提到reflow，reflow主要要考虑的是热平衡，防止立碑。)
在左边，焊盘通过两个短走线(热焊盘)连接到地平面。在右边，过孔直接连接到地平面，没有采用热焊盘。
走线 &ndash; 在电路板上，一般连续的铜的路径。
一段连接复位点和板卡上其它地方的细走线。一个相对粗一点的走线连接了5V电源点。
V-score &ndash; 将板卡进行一条不完全的切割，可以将板卡通过这条直线折断。(译者注：国内常叫做“V-CUT”)
过孔 &ndash; 在板卡上的一个洞，一般用来将信号从一层切换到另外一层。塞孔指的是在过孔上覆盖阻焊，以防被焊接。连接器或者器件管脚过孔，因为需要焊接，一般不会进行塞孔。
同一个PCB上塞孔的正反两面。这个过孔将正面的信号，通过在板卡上的钻孔，传输到了背面。
波峰焊 &ndash; 一个焊接插件器件的方法。将板卡匀速的通过一个产生稳定波峰的熔融焊锡炉，焊锡的波峰会将器件管脚和暴露的焊盘焊接到一起。
简要的介绍一下如何设计自己的PCB板卡。
Designing your own! 设计自己的! 你希望开始设计自己的PCB吗。在PCB设计中的曲曲弯弯在这边说太复杂了。不过，如果你真的想开始，下面有几个要点。
 找到一个CAD的工具：在PCB设计的市场里，有很多低价或者免费的选择。当找一个工具的时候，可以考虑以下几点。 论坛支持：有没有很多人使用这个工具?越多的人使用，你越容易找到你需要的器件的已经设计好的封装库。 很容易用。如果不好用的话，你也不会用。 性能：很多程序对设计有限制，比如层数，器件数，以及板卡尺寸等。大部分需要你去购买授权去升级性能。 可抑制性：一些免费的程序不允许导出或者迁移到其它软件，将你限制在唯一的供应商上。可能软件的低价以及便捷性值得这样的付出，但有时候不太值得。 去看看其他人的布板设计。开源硬件让这个事情越来越容易。 练习，练习，还是练习。 保持低的期望值。你设计的第一个板卡可能有很多问题，但是第20个可能就少很多，但是还会有一些问题。但是你很难将所有问题清除。 原理图相当重要。尝试去设计一个没有好的原理图支持的PCB板卡是徒劳的。 ]]></content>
  </entry>
  
  <entry>
    <title>芯片行业最大的收购，英国批准了</title>
    <url>/post/news/UK-has-approved-broadcom-69-billion-takeover-of-VMware.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Broadcom</tag>
      <tag>VMware</tag>
    </tags>
    <content type="html"><![CDATA[据路透社报道，英国竞争监管机构周三暂时批准了美国科技公司博通以690亿美元收购VMware 的交易，称该交易不会削弱关键计算机服务器产品供应的竞争。
竞争与市场管理局 (CMA) 临时批准了博通有史以来规模最大的收购，此前博通向一些竞争对手提供了互操作性补救措施以解决问题，随后欧盟监管机构上周批准了此次收购。
CMA 表示：“在审查了从 Broadcom、VMware 和其他相关方收集的证据后，独立的 CMA 小组暂时发现该交易不会大幅减少英国服务器硬件组件供应的竞争。”
拟议中的交易凸显了芯片制造商博通向企业软件领域多元化发展的目标，但与此同时，全球监管机构加大了对大型科技公司交易的审查力度。
英国监管机构曾在 3 月份表示担心该交易可能会使服务器变得更加昂贵，并表示现在将就其临时批准进行咨询，然后在 9 月 12 日之前发布最终报告。
博通对无条件批准表示欢迎，并表示预计将在本财年完成交易。
CMA 表示，它探讨了这样的担忧：如果合并后的博通公司的产品与 VMware 的服务器虚拟化软件的配合效果较差，该交易可能会损害博通竞争对手的竞争能力。
“(CMA) 小组认为，该交易不太可能损害创新，特别是因为有关新产品调整的信息只需要在为时已晚而无法为博通带来商业利益的阶段与 VMware 共享。”
这笔 690 亿美元的交易包括 610 亿美元的股本和其余的债务，也正在接受美国联邦贸易委员会的审查。
CMA 今年成为第一个阻止微软收购《使命召唤》制造商动视暴雪交易的主要监管机构，但此后表示可能会再次考虑修改后的提议。目前，有关各方正在努力解决争端。
全球最大的半导体收购案，取得重要进展 据彭博社报道，博通公司以 610 亿美元收购云计算公司VMware Inc.的交易即将获得欧盟合并官员的批准，这为全球有史以来最大的半导体制造商收购案铺平了道路。
据不愿透露姓名的知情人士透露，在与这些公司进行数月谈判后，欧盟委员会最快将于周三表示同意。在会谈期间，博通签署了行为补救措施，包括承诺将互操作性标准引入其技术，以允许竞争对手更公平地竞争。
金融时报指出，四位知情人士表示，欧盟执行机构欧盟委员会将于周三表示，它已接受博通的让步，即 VMware 的软件将继续与竞争对手的硬件兼容。这些人士表示，事实证明，这一措施足以解决欧盟竞争主管机构的担忧，而博通无需出售部分 VMware 业务。
在此前，欧盟调查芯片及设备制造商博通（Broadcom）和VMware的610亿美元收购案，按照欧盟当初发布的初步结果显示，收购案可能造成博通网络及存储方面竞争对手无法取得VMware软件，或是硬件与VMware无法兼容。
在博通宣布以610亿买下VMware，并预计自身公司里包含基础设施及安全软件的部门也会被更名为VMware。但消息公布后引起欧盟关注，一些VMware用户也根据博通过去收购CA及赛门铁克的黑历史，担心未来可能会被迫购买博通硬件。
欧盟在去年接到收购案的通报，欧盟委员会展开调查，初步调查结果显示，这桩交易可能允许博通在网卡和存储方面市场限制竞争。委员会认为，这可能降低VMware的服务器虚拟化软件和博通竞争者硬件的兼容性，或转向有利于博通硬件，或不允许竞争对手硬件使用VMware软件，或减少其获取渠道，以排斥竞争者。而上述情形将造成解决方案的价格上升、品质下降、影响企业客户的创新，最终伤害消费者。
此外，欧盟委员会也将调查博通收购VMware是否会影响其他厂商开发SmartNIC（智能网卡）。2020年，VMware和三家SmartNIC厂商（英伟达、英特尔及AMD）发起Project Monterey。
欧盟当初表示，一旦博通买下VMware，可能限制VMware参与Project Monterey来保护自身网卡营收，阻止其进行技术创新。此外欧盟也会考量，博通是否开始将VMware虚拟化软件和自家软件（主要是大型主机及安全软件）的綑绑销售，而不再单独销售VMware，造成市场选择减少，且排挤竞争软件供应商。
报道表示，尽管布鲁塞尔监管机构将与加拿大、巴西和南非监管机构一起批准该交易，但此次收购仍面临美国、英国和中国的竞争调查。
该交易早些时候曾面临严格审查，委员会在 4 月份强调了阻止该交易的潜在原因，除非有足够的补救措施。该公司警告称，此次交易可能会导致企业客户“价格上涨、质量下降、创新减少”。
英国竞争与市场管理局将于本月晚些时候公布博通创纪录交易的临时调查结果，法定截止日期为 9 月 12 日 。英国监管机构发现自己在科技交易审查方面越来越孤立，对企业未来的行为承诺采取了更强硬的立场，例如在对微软公司以 690 亿美元收购动视暴雪公司的调查中。
博通不再是半导体公司！ Broadcom 的存在是从一个衍生产品衍生出来的。大约二十年前，惠普开始了它的小型化进程。首先分拆安捷伦，其中包含与 PC 或打印机无关的大杂烩业务。安捷伦又将自己拆分成几个部分，其中之一是惠普曾经的内部芯片业务，重新命名为 Avago。作为卖方分析师，我们多年来一直密切关注 Avago。深埋在惠普内部，我们知道它销售的过滤器可以进入手机，偶尔会提供一些非常有趣但晦涩的信息。然后分拆发生了。
Avago 通过私募股权获得了生命，就起源故事而言，这是接下来会发生什么的关键线索。Avago 首席执行官 Hock Tan 很早就意识到了大多数其他半导体首席执行官没有意识到的事情——半导体已经不再是一个增长型行业。
80 年代和 90 年代的繁荣时期已经结束，现在半决赛竞争激烈，周期性严重，利润率低。因此，Avago 开始疯狂收购，收购的公司数不胜数，并在 15 年里推动股价上涨 3,000%。
这一成功的秘诀是一本相当简单的剧本。收购在市场上处于领先地位而竞争对手很少的公司。然后剥离出售给竞争行业的细分市场，削减管理和公司管理费用，并推动现金流——这为下一次收购提供了更多的杠杆和火力。并重复。
该公司在这方面取得了巨大的成功，并有效地促进了美国半导体行业的彻底整合，该行业从 20 年前的 2000 家公司增加到今天的 200 家左右。
对于被收购的公司来说，整合过程是支撑。新的管理团队将消除他们能找到的所有费用——不再有公司的纪念品，不再有免费的咖啡，众所周知的是没有企业 IT 部门。对于从裁员中幸存下来的初级经理来说，这太棒了。他们被赋予了自主权，消除了官僚主义，丰富的选择包和繁重的工作量。
随着时间的推移，另一个趋势也变得明显——公司将大幅降低研发，我们将在下面回到这个话题。Avago 的另一项重要技能是其 CEO 和交易团队成为专家，并找到能够说服目标董事会出售非财务工具。有时这意味着离任高管的退休协议，创始人的职位和头衔，或公司名称的保留。因此，当他们在 2015 年收购博通时，Avago 更改了名称，因为这是完成交易的必要条件。
故事是这样的，当亚历山大大帝到达印度河时，他抽泣着，感叹没有更多的土地可以征服。由于美国政府CFIUS 最后一刻的模糊命令，Broadcom 于 2019 年未能收购高通，从而进入了印度河。像所有其他成功的汇总故事一样，博通需要不断收购更大的业务以保持机器运转。到 2019 年，实际上只有两家大到足以推动博通前进的公司——高通和英特尔。高通已经失败了，而英特尔（当时）太大了，无法考虑。
于是博通转向了软件公司。这个领域有很多大目标。这些公司不一定像博通的半导体目标那样在市场上占据主导地位，但它们确实与那些被锁定在长期合同和多重 IT 依赖关系的客户之间有着非常密切的关系。这意味着稳定的现金流。
事实上，博通不是一家半导体公司。它也不是一家软件公司。它是一只私募股权基金，通过无休止的一系列收购将现金流最大化。这让半导体行业的许多人感到沮丧，并且可能让软件行业的人感到困惑。对于现在必须掌握 SaaS 指标的卖方半成品分析师来说，这无疑是一个挑战。但对于股东来说，它仍然是一个引人注目的模式。
多年来，我们了解到博通的一件事是，他们一直在寻找新的交易。所以我们可以根据经验说，在 VMWare 收购结束的那一天，银行家会接到电话询问“下一步是什么？”
这种情况能持续多久？汇总有几个大问题。一是我们上面提到的对新交易的不断追求。二是融合。在某些时候，这些组织变得如此庞大，以至于他们开始绊倒自己。博通通过将如此多的自主权下放到各个部门，在很大程度上避免了这个问题，但在某些时候，这不得不陷入困境。尤其是当一切都在一家上市公司的保护伞下时。
另一个问题是整个模型依赖于来自基础业务的稳定现金流。这就是私募股权公司长期避开技术的原因，这些公司存在技术风险，而私募股权基金通常青睐的更传统的公司不一定存在技术风险。这就是我们关注博通半导体业务研发的原因。今天没有明显的缺陷——它们在网络领域仍然非常强大，并且是围绕手机 BAW 射频滤波器双头垄断的主导方。
话虽如此，我们越来越多地从网络业务人员那里听到，他们对博通的发展速度感到失望。新的芯片和功能需要越来越长的时间才能到达，这为超大规模企业中的初创企业和内部解决方案打开了大门。在射频领域，至少部分 BAW 滤波器市场存在技术中断的真正可能性，高通和村田似乎对此有强烈的感觉。
诚然，博通的团队非常精明。但如果他们的任何业务开始出现见顶迹象，他们会毫不犹豫地出售或分拆。当然，大多数买家可能都知道这一点，到目前为止，除了偶尔在合并后出售不受欢迎的业务线之外，博通还没有从其投资组合中退出一次。
就目前的情况来看，博通可能还能让这种模式持续很长时间。那里有大量的软件目标，而且本月它们都变得更便宜了。我们最好的猜测是，唯一能让博通放慢脚步的是高管团队的能量水平。Hock Tan 现在已经 70 岁左右，我们猜测他近期没有兴趣退休在纳帕谷种植葡萄酒。
]]></content>
  </entry>
  
  <entry>
    <title>最古老的Linux之一：活了30年仍在运行</title>
    <url>/post/linux/Slackware-Linux-30th-anniversary.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Slackware</tag>
    </tags>
    <content type="html"><![CDATA[Slackware不是第一个 Linux  发行版，但它仍然是迄今为止最古老的且仍在运行的Linux发行版之一。本周，Slackware Linux将迎来其30周年生日。
Slackware Linux以简单、稳定和安全著称，Slackware Linux主要由Slackware团队维护和开发，该团队由Slackware的创建者Patrick Volkerding领导。Slackware Linux的安装和使用相对较为繁琐，需要手动配置和安装软件包。但它也因此提供了更高的定制性和灵活性，用户可以根据自己的需求自由选择软件版本和配置。Slackware Linux的软件包管理方式较为独特，采用tar包和脚本的方式进行管理和安装。用户需要手动下载和安装所需的软件包，然后进行配置和编译。尽管相对于其他现代Linux发行版，Slackware Linux的界面和功能较为简单，但它的稳定性和安全性仍然吸引着一些用户。此外，Slackware Linux还支持多种硬件平台，包括x86、x64、ARM等。
Slackware 1.0版本发布于1993年7月16日，当前最新的版本Slackware 15则在2021年进入测试阶段，并于去年初正式发布。与其他发行版相似，Slackware最初源自开发者对其他Linux发行版的不满，Debian比Slackware还要年轻一点。其实更早之前，MCC Interim Linux可以说是针对普通用户的第一个发行版，它的第一个候选版本0.97版本在1991年内核诞生几个月后就出现了，但是Interim缺乏许多今天已经具备的特性，例如包管理器。接着其他几个发行版紧随其后，特别是SLS（Softlanding Linux 系统）很快就激发了它的两个后代。正如已故的Debian Linux创始人Ian Murdock在1993年最初的声明中提到，他对SLS的不满激发了对Debian创造。
Slackware Linux 最初是一个修复和改进SLS的项目，Slackware团队迄今为止仍在对其进行维护，不得不说团队成功地完成了它们最初的使命。目前Slackware Linux存在三种变体，同名形式仍然是x86-32系统，而Slackware64是面向64位x86设备的发行版，此外还有Arm64版本。
令人惊讶的是，今天Slackware 15的安装界面与上世纪90年代并无不同，没有诸如图形引导之类的东西，看上去俨然似软件“古董”。它启动到登录提示符，然后您需要手动运行setup程序，并使用90年代DOS风格的文本模式菜单来勾选您想要安装的组件。默认情况下，它没有配置图形登录界面，甚至没有配置普通用户帐户，您需要键入startx并启动桌面，并预配置AMD Radeon显卡驱动程序，准备好连接到无线网络等等。
Slackware 15并不是您想象中的那种轻量级的Linux发行版，运行完整更新将会填满您的16GB根分区。它具有在线存储库、自动依赖性解析以及您期望的那些21世纪发行版中拥有的花里胡哨的功能。
尽管如此，今天的Slackware Linux实际上是一个名符其实的现代发行版，使用它的时候，不要因其过于简单的文本安装模式和、缺乏图形桌面等细节修饰而觉得自己仍活在上个世纪。我们无法确定Slackware的风格是否源于传统，亦或是有意为之（也许是为了吓跑烦人的新手），当然，也可能两者兼有之。如今，它比Alpine Linux或是Arch Linux等更年轻的发行版更容易安装，想起来就像BSD一样，哦，它也是无systemd的（即system daemon，是linux下的一种init软件）。
]]></content>
  </entry>
  
  <entry>
    <title>嵌入式网络接口该怎么设计</title>
    <url>/post/hardware/how-to-design-embedded-network-interface.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>Network</tag>
      <tag>Linux</tag>
      <tag>SOC</tag>
      <tag>MAC</tag>
    </tags>
    <content type="html"><![CDATA[本篇文章是关于嵌入式网络接口的一些知识介绍。
嵌入式网络简介 嵌入式下的网络硬件接口 提起网络，我们一般想到的硬件就是“网卡”，现在网卡已经是通过一个芯片来完成了，嵌入式网络硬件分为两部分：MAC和PHY，大家都是通过看数据手册来判断一款SOC是否支持网络，如果一款芯片数据手册说自己支持网络，一般都是说的这款SOC内置MAC，MAC类似I2C控制器、SPI控制器一样的外设。
但是光有MAC还不能直接驱动网络，还需要另外一个芯片：PHY，因此对于内置MAC的SOC，其外部必须搭配一个PHY芯片。内部没有MAC的SOC芯片，就需要使用外置的MAC+PHY一体芯片了，这里就要牵扯出常见的两个嵌入式网络硬件方案了。
SOC内部没有网络MAC外设 对于内部没有MAC的SOC芯片，可以使用外置 MAC+PHY一体的网络芯片来实现网络功能。比如三星linux开发板里面用的最多的DM9000，DM9000对SOC提供了一个SRAM接口，SOC会以SRAM的方式操作DM9000。
有些外置的网络芯片更强大，内部甚至集成了硬件TCP/IP协议栈，对外提供一个SPI接口，比如W5500。这个一般用于单片机领域，单片机通过SPI接口与W5500进行通信，由于W5500内置了硬件TCP/IP协议栈，因此单片机就不需要移植软件协议栈，直接通过SPI来操作W5500，简化了单片机联网方案。
这种方案的优点就是让不支持网络的SOC能够另辟蹊径，实现网络功能，但是缺点就是网络效率不高，因为一般芯片内置的MAC会有网络加速引擎，比如网络专用DMA，网络处理效率会很高。而且此类芯片网速都不快，基本就是10/100M。另外，相比PHY芯片而言，此类芯片的成本也比较高，可选择比较少。
SOC与外部MAC+PHY芯片的连接如图1-1所示：
图1-1 主控SOC与外置MAC+PHY芯片连接
SOC内部集成网络MAC外设 我们一般说某个SOC支持网络，说的就是他内部集成网络MAC外设，此时我们还需要外接一个网络PHY芯片。。
目前几乎所有支持网络的SOC都是内置MAC外设，比如STM32F4/F7/H7系列、NXP的I.MX系列，内部集成网络MAC的优点如下：
1）内部MAC外设会有专用的加速模块，比如专用的DMA，加速网速数据的处理。
2）网速快，可以支持10/100/1000M网速。
3）外接PHY可选择性多，成本低。
内部的MAC外设会通过MII或者RMII接口来连接外部的PHY芯片，MII/RMII接口用来传输网络数据。另外主控需要配置或读取PHY芯片，也就是读写PHY的内部寄存器，所以还需要一个控制接口，叫做MIDO，MDIO很类似IIC，也是两根线，一根数据线叫做MDIO，一根时钟线叫做MDC。
SOC内部MAC外设与外部PHY芯片的连接如图1-2所示：
图1-2 内部MAC与外部PHY之间的连接
大家在做项目的时候，如果要用到网络功能，强烈建议大家选择内部带有网络MAC外设的主控SOC！I.MX6ULL就有两个10M/100M的网络MAC外设，正点原子ALPHA开发板板载了两颗PHY芯片，型号为LAN8720。因此，本章节只讲解SOC内部MAC+外置PHY芯片这种方案。
MII/RMII接口 前面我们说了，内部MAC通过MII/RMII接口来与外部的PHY芯片连接，完成网络数据传输，本节我们就来学习一下什么是MII和RMII接口。
MII接口 MII全称是Media Independent Interface，直译过来就是介质独立接口，它是IEEE-802.3定义的以太网标准接口，MII接口用于以太网MAC连接PHY芯片，连接示意图如图1-3所示：
图1-3 MII接口
MII接口一共有16根信号线，含义如下：
 TX_CLK：发送时钟，如果网速为100M的话时钟频率为25MHz，10M网速的话时钟频率为2.5MHz，此时钟由PHY产生并发送给MAC。 TX_EN：发送使能信号。 TX_ER：发送错误信号，高电平有效，表示TX_ER有效期内传输的数据无效。10Mpbs网速下TX_ER不起作用。 TXD[3:0]：发送数据信号线，一共4根。 RXD[3:0]：接收数据信号线，一共4根。 RX_CLK：接收时钟信号，如果网速为100M的话时钟频率为25MHz，10M网速的话时钟频率为2.5MHz，RX_CLK也是由PHY产生的。 RX_ER：接收错误信号，高电平有效，表示RX_ER有效期内传输的数据无效。10Mpbs网速下RX_ER不起作用。 RX_DV：接收数据有效，作用类似TX_EN。 CRS：载波侦听信号。 COL：冲突检测信号。  MII接口的缺点就是所需信号线太多，这还没有算MDIO和MDC这两根管理接口的数据线，因此MII接口使用已经越来越少了。
RMII接口 RMII全称是Reduced Media Independent Interface，翻译过来就是精简的介质独立接口，也就是MII接口的精简版本。RMII接口只需要7根数据线，相比MII直接减少了9根，极大的方便了板子布线，RMII接口连接PHY芯片的示意图如图1-4所示：
图1-4 RMII接口
 TX_EN：发送使能信号。 TXD[1:0]：发送数据信号线，一共2根。 RXD[1:0]：接收数据信号线，一共2根。 CRS_DV：相当于MII接口中的RX_DV和CRS这两个信号的混合。 REF_CLK：参考时钟，由外部时钟源提供， 频率为50MHz。这里与MII不同，MII的接收和发送时钟是独立分开的，而且都是由PHY芯片提供的。 除了MII和RMII以外，还有其他接口，比如GMII、RGMII、SMII、SMII等，关于其他接口基本都是大同小异的，这里就不做讲解了。正点原子ALPAH开发板上的两个网口都是采用RMII接口来连接MAC与外部PHY芯片。  MDIO接口 MDIO全称是Management Data Input/Output，直译过来就是管理数据输入输出接口，是一个简单的两线串行接口，一根MDIO数据线，一根MDC时钟线。驱动程序可以通过MDIO和MDC这两根线访问PHY芯片的任意一个寄存器。MDIO接口支持多达32个PHY。同一时刻内只能对一个PHY进行操作，那么如何区分这32个PHY芯片呢？和IIC一样，使用
器件地址即可。同一MDIO接口下的所有PHY芯片，其器件地址不能冲突，必须保证唯一，具体器件地址值要查阅相应的PHY数据手册。
因此，MAC和外部PHY芯片进行连接的时候主要是MII/RMII和MDIO接口，另外可能还需要复位、中断等其他引脚。
RJ45接口 网络设备是通过网线连接起来的，插入网线的叫做RJ45座，如图1-5所示：
图1-5 RJ45座子
RJ45座要与PHY芯片连接在一起，但是中间需要一个网络变压器，网络变压器用于隔离以及滤波等，网络变压器也是一个芯片，外形一般如图1-6所示：
图1-6 网络变压器
但是现在很多RJ45座子内部已经集成了网络变压器，比如最常用的HR911105A就是内置网络变压器的RJ45座。内置网络变压器的RJ45座和不内置的引脚一样，但是一般不内置的RJ45座会短一点。
因此，大家在画板的时候一定要考虑你所使用的RJ45座是否内置网络变压器，如果不内置的话就要自行添加网络变压器部分电路！同理，如果你所设计的硬件是需要内置网络变压器的RJ45座，肯定不能随便焊接一个不内置变压器的RJ45座，否则网络工作不正常！
RJ45座子上一般有两个灯，一个黄色(橙色)，一个绿色，绿色亮的话表示网络连接正常，黄色闪烁的话说明当前正在进行网络通信。这两个灯由PHY芯片控制，PHY芯片会有两个引脚来连接RJ45座上的这两个灯。内部MAC+外部PHY+RJ45座(内置网络变压器)就组成了一个完整的嵌入式网络接口硬件，如图1-7所示：
图1-7 嵌入式网络硬件接口示意图
PHY芯片基础知识 PHY是IEEE 802.3规定的一个标准模块，前面说了，SOC可以对PHY进行配置或者读取PHY相关状态，这个就需要PHY内部寄存器去实现。PHY芯片寄存器地址空间为5位，地址 0~31共32个寄存器，IEEE定义了0~15这16个寄存器的功能，16~31这16个寄存器由厂商自行实现。
也就是说不管你用的哪个厂家的PHY芯片，其中0~15这16个寄存器是一模一样的。仅靠这16个寄存器是完全可以驱动起PHY芯片的，至少能保证基本的网络数据通信，因此 Linux  内核有通用PHY驱动，按道理来讲，不管你使用的哪个厂家的PHY芯片，都可以使用Linux的这个通用PHY驱动来验证网络工作是否正常。
事实上在实际开发中可能会遇到一些其他的问题导致Linux内核的通用PHY驱动工作不正常，这个时候就需要驱动开发人员去调试了。但是，随着现在的PHY芯片性能越来越强大，32个寄存器可能满足不了厂商的需求，因此很多厂商采用分页技术来扩展寄存器地址空间，以求定义更多的寄存器。
这些多出来的寄存器可以用于实现厂商特有的一些技术，因此Linux内核的通用PHY驱动就无法驱动这些特色功能了，这个时候就需要PHY厂商提供相应的驱动源码了，所以大家也会在Linux内核里面看到很多具体的PHY芯片驱动源码。
不管你的PHY芯片有多少特色功能，按道理来讲，Linux内核的通用PHY驱动是绝对可以让你这PHY芯片实现基本的网络通信，因此大家也不用担心更换PHY芯片以后网络驱动编写是不是会很复杂。
IEEE802.3协议英文原版中的 “22.2.4 Management functions”章节，此章节对PHY的前16个寄存器功能进行了规定，如图1-8所示：
图1-8 IEEE规定的前16个寄存器
关于这16个寄存器的内容协议里面也进行了详细的讲解，这里就不分析了。大家可以找个具体的PHY芯片数据手册对比看一下，比如百M网络最常用的LAN8720A这个PHY，大家可以看一下LAN8720前面几个寄存器结构是否和图1-8中的一样。
关于嵌入式Linux的网络接口设计就讲到这里。
]]></content>
  </entry>
  
  <entry>
    <title>Linux 超级漂亮的 Shell</title>
    <url>/post/linux/beautiful-linux-shell.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Shell</tag>
    </tags>
    <content type="html"><![CDATA[Linux/Unix 提供了很多种 Shell，为什么要这么多 Shell？
zsh 介绍 Linux shell 那我问你，你同类型的衣服怎么有那么多件？花色，质地还不一样。写程序比买衣服复杂多了，而且程序员往往负责把复杂的事情搞简单，简单的事情搞复杂。牛程序员看到不爽的 Shell，就会自己重新写一套，慢慢形成了一些标准，常用的 Shell 有这么几种，sh、bash、csh 等，想知道你的系统有几种 shell，可以通过以下命令查看：
cat/etc/shells 显示如下：
zsh 简介 Zsh 是一个 Linux 下强大的 shell, 由于大多数 Linux 产品安装，以及默认使用bash shell, 但是丝毫不影响极客们对 zsh 的热衷, 几乎每一款 Linux 产品都包含有 zsh，通常可以用 apt-get、urpmi 或 yum 等包管理器进行安装
Zsh 具有以下主要功能
 开箱即用、可编程的命令行补全功能可以帮助用户输入各种参数以及选项 在用户启动的所有 shell 中共享命令历史 通过扩展的文件通配符，可以不利用外部命令达到 find 命令一般展开文件名 改进的变量与数组处理 在缓冲区中编辑多行命令 多种兼容模式，例如使用 /bin/sh 运行时可以伪装成 Bourne shell 可以定制呈现形式的提示符；包括在屏幕右端显示信息，并在键入长命令时自动隐藏 可加载的模块，提供其他各种支持：完整的 TCP 与 Unix 域套接字控制，FTP 客户端与扩充过的数学函数 完全可定制化  zsh 与 oh-my-zsh 终极配置 之前是因为看到这篇文章：终极 Shell——Zsh 才选择使用 zsh，被它的自动完成、补全功能吸引了。官网：http://www.zsh.org
选择 oh-my-zsh, oh-my-zsh 是基于 zsh 的功能做了一个扩展，方便的插件管理、主题自定义，以及漂亮的自动完成效果。
在 Github 上找关于 zsh 的项目时发现的，试用了一下觉得很方便，不用像上面文章里面提到的那么复杂，配置一些插件的名称即可使用相应的功能。
官网：https://github.com/robbyrussell/oh-my-zsh
安装 zsh 安装 zsh 对于一般的 Ubuntu 系统，配置好正确的源之后，就能直接键入以下命令安装：
sudo apt-get install zsh 配置 zsh zsh的配置是一门大学问，这里不赘述，直接给出一个配置文件，大家可以下载后放入 zsh 配置文档直接使用。（我的一个法国朋友手配的，相当顺手）
把.zshrc拷贝到相应用户的home目录即可(也可以把你的bash的配置文件(~/.bash_prorile 或者 ~/.profile 等) 给拷贝到zsh的配置文件~/.zshrc里，因为zsh兼容bash)
取代bash，设为默认shell sudo usermod -s /bin/zsh username 或者
chsh -s /bin/zsh chsh -s `whichzsh` 如果要切换回去 bash：
chsh -s /bin/bash 当然你实在不愿意把 zsh 当成默认的 shell, 而又想使用它, 那么你可以每次进入是都使用zsh进入, 而输入exit退出
安装oh-my-zsh 直接用 zsh 会很麻烦，因为 zsh 功能很强大但是太复杂，所以需要 oh-my-zsh 来将它简单化
直接用 git 从 github 上面下载包 git clone git://github.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zsh 备份已有的 zshrc, 替换 zshrc cp ~/.zshrc ~/.zshrc.orig cp~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc 直接使用脚本安装 cd oh-my-zsh/tools ./install.sh 你可以直接直接使用如下命令安装
sh -c &#34;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&#34; sh -c &#34;$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&#34; 其本质就是下载并执行了 github 上的 install.sh 脚本, 该脚本位于oh-my-zsh/tools/install.sh
配置主题 oh-my-zsh 集成了大量的主题, 位于oh-my-zsh/theme
配置主题, 可以通过修改~/.zshrc中的环境变量ZSH_THEME来完成
ZSH_THEME=&#34;agnoster&#34; # (this is one of the fanc yones) 如果你觉得主题太多你可以选择使用随机模式, 来由系统随机选择
ZSH_THEME=&#34;random&#34;#(...please let it be pie... please be some pie..) 详细的主题信息, 可以参见 zsh 主题介绍
配置插件 修改～/.zshrc中plugins
plugins=(gitbundlerosxrakeruby)
详细的插件信息, 可以参见 zsh 插件 Plugins 介绍
更新 oh-my-zsh 默认情况下, 您将被提示检查每几周的升级. 如果你想我 ZSH 自动升级本身没有提示你, 修改 ~/.zshrc
disable_update_prompt=true 禁用自动升级, 修改~/.zshrc disable_auto_update=true 当然你也可以选择手动更新 如果你想在任何时间点升级（也许有人刚刚发布了一个新的插件，你不想等待一个星期？) 你只需要运行：
upgrade_oh_my_zsh
卸载 oh-my-zsh 如果你想卸载oh-my-zsh, 只需要执行uninstall_oh_my_zsh zsh， 从命令行运行，这将删除本身和恢复你以前的 bash 或者 zsh 配置。
]]></content>
  </entry>
  
  <entry>
    <title>GPU和CPU芯片谁更复杂</title>
    <url>/post/datacenter/which-is-more-complex-GPU-or-CPU-chip.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>CPU</tag>
      <tag>GPU</tag>
    </tags>
    <content type="html"><![CDATA[高端的GPU，如NVIDIA的A100或AMD的Radeon Instinct MI100，包含了大量的CUDA核心或流处理器，以支持大规模并行计算。
高端的CPU，如Intel的Xeon系列或AMD的EPYC系列，通常具有更多的核心、更高的频率和更复杂的超线程技术，以提高性能。
那么GPU和CPU到底哪个更复杂呢？我们用应用场景、晶体管数量、架构设计几个维度来看看。
应用场景 GPU具有大量的计算核心、专用的存储器和高速数据传输通道。GPU的设计注重于大规模并行计算、内存访问和图形数据流处理等方面，以满足图形渲染和计算密集型应用的要求。
GPU 的核心设计理念是并行处理。相比于 CPU，GPU 拥有更多的处理单元，因此，它可以同时执行大量的并行任务。这使得 GPU 特别适合处理可以并行化的工作负载，如图形渲染、科学计算和深度学习等。
CPU则专注于通用计算和广泛的应用领域。这些CPU通常具有多个处理核心、高速缓存层次和复杂的指令集体系结构。
晶体管数量 从晶体管的数量来看，顶级的GPU通常包含更多的晶体管，这主要是因为它们需要大量的并行处理单元。例如，
CPU: NVIDIA A100 GPU  包含了540亿个晶体管。
CPU: AMD的EPYC 7742，包含约390亿个晶体管。
架构设计 从架构和设计的角度来看，CPU可能会被认为更复杂。CPU需要处理各种各样的任务，并且需要优化以尽可能快地执行这些任务。为了达到这个目标，CPU使用了许多复杂的技术，如流水线、乱序执行、分支预测、超线程等。
顶级的GPU可能在硬件规模（例如，晶体管数量）上更大，而顶级的CPU在架构和设计上可能更复杂。
GPU架构
GPU 的一些关键架构特性：
 大量的并行处理单元（核心）：GPU 中的每一个处理单元可以被看作是一个微型的 CPU，它们可以同时执行指令。例如，NVIDIA 的一种 GPU 架构，叫做 Turing，有数千个并行处理单元（被称为 CUDA 核心）。 分层的内存架构：GPU 有一个复杂的内存架构，包括全局内存、共享内存、本地内存和常量内存等。全局内存可以被所有核心访问，而其他类型的内存则用于缓存数据，以减少对全局内存的访问延迟。 线程调度和执行：GPU 使用硬件进行线程调度，这使得它可以在执行大量线程时保持高效率。在 NVIDIA 的 GPU 中，线程是以 warp （32个线程）的形式进行调度和执行的。 特殊功能单元：除了标准的计算核心外，GPU 还有一些特殊的功能单元，如纹理单元和光栅化单元，这些都是为图形渲染特化的。在最新的 GPU 中，还有一些专门为深度学习和人工智能设计的单元，如张量核心和RT核心。  流多处理器和 SIMD 架构：GPU 使用了 SIMD（单指令多数据流）架构，这意味着在一个时钟周期内，一条指令可以在多个数据上并行执行。在 NVIDIA 的 GPU 中，每个流多处理器（SM）包含了数百个 CUDA 核心，以及其他资源如寄存器、缓存和功能单元。
具体的 GPU 架构设计会根据制造商和产品线的不同而有所不同。例如，NVIDIA 的架构（如 Turing 和 Ampere）和 AMD 的架构（如 RDNA）有一些关键的差异。然而，所有的 GPU 架构都遵循并行处理的基本理念。
CPU架构 CPU（中央处理单元）的架构设计涉及众多领域，包括硬件设计、微体系结构、指令集设计等等。
 指令集架构（ISA）：这是 CPU 的基础，定义了 CPU 可以执行哪些操作（例如，加法、乘法、逻辑操作等），以及如何编码这些操作。常见的 ISA 包括 x86（Intel 和 AMD）、ARM、RISC-V 等。 流水线：在现代的 CPU 中，指令被分解为多个阶段，例如，取指、译码、执行、访存和写回。这些阶段被组织成一个流水线，这样每个时钟周期内，可以有多个指令在不同阶段同时进行，从而提高了指令的吞吐量。 缓存和内存层次结构：为了减少访问内存的延迟，CPU 包含了一套复杂的缓存系统。这通常包括 L1、L2、L3 缓存等多个级别。除此之外，还有 TLB（转译后援缓冲器）等机制来加速虚拟地址到物理地址的转换。 乱序执行和寄存器重命名：这些是现代 CPU 的关键优化手段。乱序执行允许 CPU 在等待某些慢指令（如内存访问）完成时，先执行其他无关的指令。寄存器重命名则是解决数据冒险的一种方法，它允许 CPU 重新排列指令的执行顺序，而不会影响最后的结果。 分支预测：分支预测是一种优化方法，用于预测条件跳转指令的结果。如果预测正确，CPU 可以提前取指和执行后续的指令，从而避免了因为等待跳转结果而产生的停顿。 多核和多线程：现代的 CPU 通常包含多个处理核心，每个核心都可以独立执行指令。此外，一些 CPU 还支持多线程技术（如 Intel 的超线程），可以让一个核心同时执行多个线程，从而提高了核心的利用率。  以上只是 CPU 架构设计的一部分。实际上，CPU 的设计是一个极其复杂的过程，需要考虑的因素非常多，包括性能、能耗、面积、成本、可靠性等等。
]]></content>
  </entry>
  
  <entry>
    <title>如何在Linux中搭建DNS服务</title>
    <url>/post/linux/dns-server-configuration-in-linux.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>DNS</tag>
    </tags>
    <content type="html"><![CDATA[要在Linux系统上搭建DNS服务，你可以按照以下步骤进行操作：
安装BIND软件包 sudo yum install bind bind-utils 配置主DNS服务器 打开/etc/named.conf文件，编辑DNS服务器的配置。根据你的域名和网络环境，修改以下示例配置为适当的值：
options { listen-on port 53 { any; }; allow-query { any; }; recursion yes; }; zone &#34;example.com&#34; IN { type master; file &#34;/var/named/example.com.zone&#34;; allow-update { none; }; }; 创建主DNS区域文件 创建一个区域文件以存储DNS记录。在/var/named/目录下创建一个名为example.com.zone的文件，并添加相应的DNS记录。示例：
$TTL 86400 @ IN SOA ns1.example.com. root.example.com. ( 2018010101 ; Serial 3600 ; Refresh 1800 ; Retry 604800 ; Expire 86400 ; Minimum TTL ) @ IN NS ns1.example.com. @ IN A 192.168.1.10 www IN A 192.168.1.20 配置反向解析 打开/etc/named.conf文件，并添加反向解析配置。示例：
zone &#34;1.168.192.in-addr.arpa&#34; IN { type master; file &#34;/var/named/1.168.192.zone&#34;; allow-update { none; }; }; 创建反向解析区域文件 在/var/named/目录下创建一个名为1.168.192.zone的文件，用于反向解析。添加以下内容：
$TTL 86400 @ IN SOA ns1.example.com. root.example.com. ( 2018010101 ; Serial 3600 ; Refresh 1800 ; Retry 604800 ; Expire 86400 ; Minimum TTL ) @ IN NS ns1.example.com. 10 IN PTR example.com. 20 IN PTR www.example.com. 设置防火墙规则 如果您的防火墙处于启用状态，请确保允许DNS流量通过
sudo firewall-cmd --add-service=dns --permanent sudo firewall-cmd --reload 启动并启用DNS服务 sudo systemctl start named sudo systemctl enable named 现在，你的Linux系统上的DNS服务器就已经搭建完成了。您可以在其他设备上将DNS服务器设置为您的CentOS主机的IP地址，以使用该DNS服务器进行域名解析。
请注意，在实际生产环境中，你可能需要更复杂的配置来满足网络需求，例如添加其他区域或配置转发等。
DNS原理及解析流程 DNS是互联网中用于将域名解析为IP地址的系统。它充当了一个分布式数据库，将人类可读的域名映射到计算机可理解的IP地址。
DNS的解析流程如下：
 用户在浏览器中输入一个域名，比如www.example.com 操作系统首先会检查本地缓存（称为本地DNS缓存），看是否已经有该域名的解析结果。如果有，则直接返回并跳至第8步。如果没有，继续进行后续步骤。 操作系统向预配置的本地DNS服务器发送一个DNS查询请求。这个本地DNS服务器通常由用户的ISP（互联网服务提供商）或者自定义的DNS服务器提供。 本地DNS服务器收到查询请求后，首先检查自己的缓存，如果存在对应的域名解析结果，直接返回给操作系统。如果没有，则继续进行后续步骤。 本地DNS服务器根据域名的顶级域（TLD）来选择合适的根域名服务器（Root DNS Server）发送查询请求。根域名服务器负责管理顶级域名服务器的地址信息。 根域名服务器返回给本地DNS服务器一个顶级域名服务器的地址。 本地DNS服务器再次向顶级域名服务器发送查询请求。顶级域名服务器负责管理对应顶级域下的权威域名服务器（Authoritative DNS Server）的地址信息。 本地DNS服务器收到权威域名服务器的地址后，向权威域名服务器发送最终的查询请求。 权威域名服务器收到查询请求后，在自己的数据中查找该域名的解析结果。 如果权威域名服务器找到了该域名的解析结果，它将返回给本地DNS服务器。 本地DNS服务器收到解析结果后，会将其缓存下来，并将解析结果返回给操作系统。 操作系统将解析结果传递给应用程序，例如浏览器。 应用程序利用解析结果中的IP地址与服务器建立连接，并完成后续的通信过程。 整个DNS解析流程可能涉及多次查询和响应，但由于DNS系统的分布式结构和缓存机制，大部分解析结果可以从本地DNS缓存或者本地DNS服务器的缓存中获取，从而提高解析速度和减轻DNS服务器的负载压力。  需要注意的是，DNS解析并非一次性完成的，DNS记录可能会发生变化，因此在某些情况下，需要等待DNS记录的刷新时间（TTL）过期后才能获取到最新的解析结果。
原文连接: 如何在Linux中搭建DNS服务  
]]></content>
  </entry>
  
  <entry>
    <title>AMD发布了最新款AI芯片Instinct MI300</title>
    <url>/post/datacenter/instinct-MI300-announced-by-amd.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>AMD</tag>
      <tag>MI300</tag>
      <tag>GPU</tag>
      <tag>CPU</tag>
    </tags>
    <content type="html"><![CDATA[今年6月，AMD发布了一款专门针对AI需求的最新款芯片：Instinct MI300。
MI300将CPU、GPU和内存封装在了一起，晶体管数量高达1460亿个，接近英伟达H100的两倍。其搭载的HBM（高带宽内存）密度也达到了H100的2.4倍。也就是说，MI300在理论上可以运行比H100更大的AI模型。
受益于AI训练的增长，GPU需求肉眼可见的从游戏市场向高性能计算领域倾斜，就连刚开启GPU产品线的英特尔，也迫不及待的PPT首发了面向高性能计算场景的Falcon Shores架构芯片。
伴随英伟达一路冲向万亿美元市值，资本市场对GPU行业老二的期待值也达到了顶峰。今年以来，AMD股价累计上涨已经超过90%。
英特尔的Falcon Shores，预计2024年推出
然而MI300发布会结束，AMD股价下跌3.6%，反倒是英伟达上涨3.9%。资本市场表达好恶，向来是这么冷酷无情。
原因可能在于，AMD没有在发布会中透露这款芯片的客户，这也是市场对英伟达以外的AI芯片最大的担心。
长期以来，AMD在GPU市场一直被英伟达按在地上反复摩擦，Instinct产品线其实已经迭代了好几年，但相比英伟达的连战连捷，AMD在高性能计算领域的存在感一直比较稀薄。
AI训练打开的市场空间，一度被市场视为AMD与英伟达拉进距离的机会，但事情似乎没那么简单。
离不开CPU，但离得开英特尔 虽说在AI训练上，更擅长大规模并行计算的GPU承担了大部分计算工作，而整个系统仍需要CPU进行调度和统筹。也就是说，尽管GPU的需求量大幅度增加，但CPU仍是必需品。
作为一家同时拥有CPU和GPU设计能力的芯片公司，AMD被看好也不意外。更何况过去几年，AMD在CPU市场连战连捷。
AMD现任CEO苏姿丰在2014年接手，时值推土机架构性能孱弱，让英特尔心安理得的挤牙膏。而在卖掉Imageon后，AMD和拒绝为苹果设计iPhone芯片的英特尔一起，完美错过了智能手机的浪潮，公司一片风雨飘摇。
面临多条战线的失血，苏姿丰只能将有限的资源集中在核心的CPU业务上，从苹果请回了架构大师吉姆·凯勒，开始Zen架构处理器的研发。
2017年，Zen架构处理器横空出世，把挤牙膏上瘾的英特尔打了个措手不及。2019年，Zen处理器更换为台积电7nm工艺，此时英特尔10nm工艺姗姗来迟。
虽然英特尔还占据着大部分市场份额，但AMD的反攻速度实在太快，尤其是在服务器市场，几乎是从0杀到了接近20%的市占率。
2023年Q1，AMD的x86处理器市场份额达到了34.6%这一历史峰值[2]，这也是AMD市值超过英特尔的重要背景。
今年5月，全球超级计算机Top500强公布：前500强中，使用AMD CPU进行驱动的超算达到121台，使用英特尔CPU的超算则从2016年的454台下降至360台，虽然看着不少，但其中很多是英特尔10年前的家底——至强（Xeon）处理器[3]。
但同一时期，AMD与英伟达差距也越来越大。
难以逾越的CUDA 英伟达不仅是一流的硬件公司，更是一流的软件公司。
虽然在理论性能上，MI300的一些参数甚至领先于英伟达，但市场对英伟达对手们最大的担心往往在于，就算硬件性能可以跟英伟达比肩，但是软件解决方案仍难以与英伟达的CUDA对抗。
2006年，英伟达推出了CUDA平台，让开发者能够给予GPU进行编程和开发，最终形成了一个庞大稳固的生态。在推出CUDA之前，全球能用GPU进行编程的不足100人，目前CUDA的使用者超过400万。
每一个成功的硬件公司背后，往往都有一个更强大的软件团队，苹果和英伟达都是如此。即便是光刻机制造商ASML也不例外，他们的官方网站上有这样一段话：
您可能将ASML视为一家硬件公司，但实际上我们拥有世界上最大、最具开创性的软件社区之一。如果没有我们开发的软件，我们的客户就不可能制造出10纳米或更小的尺寸的芯片。
想要芯片真正在具体场景的满足各种需求，就需要开发者对硬件进行编程以实现各种功能。如果说硬件编程的过程相当于进行各种复杂计算，那么CUDA就是提供给使用者的一部计算器。
无论对英伟达的刀法多么怀恨在心，都不能否认黄仁勋对通用计算和人工智能的超前押注。
AMD显然深知软件和生态的重要性，但对标CUDA的ROCm在2016年推出时，就已经比英伟达晚了十年。
直到2023年4月，ROCm都仅支持Linux平台；而CUDA自问世以来，就提供Windows和Linux两个版本，后期还为苹果用户增设Mac OS版本。
相比英伟达不遗余力的推广和洗脑，AMD在生态建设上也显得投入不足，据说早年英伟达对项目的GPU试用申请几乎是有求必应，动不动就去高校实验室发显卡。深度学习大神杰夫·辛顿带着学生训练AlexNet模型，就用了三块GTX 580。
另外，AMD的软件能力也令人不安——AMD在今年6月发布了一份EPYC 7002 “Rome”服务器芯片指南，承认由于时钟倒计时器存在 BUG，导致第二代EPYC芯片运行1044天后，会出现内核卡死。如果有服务器使用这款芯片，需要每隔2.93年重新启动一次。
原因也不难理解，直到推出ROCm的2016年，AMD甚至还没摆脱亏损。在这期间，AMD只能把有限的资源都聚焦在CPU的研发上，无法为GPU部门投入太多资源，更不要说ROCm的软件团队了。
而当AMD在CPU市场收复失地，希望依靠AI卷土重来时，英伟达已经慢慢补齐了短板。
英伟达的反攻 2020年9月，英伟达宣布准备以400亿美元的价格准备收购移动CPU架构商Arm，其背后意图人尽皆知：一方面是整合移动端资源，另一方面则是入局CPU。
正如前文所说，尽管AI时代需要更多的GPU，但CPU仍不可或缺。当CPU与GPU共同在服务器中的工作时，实际场景更像是一个大学生（CPU）带领一群小学生（GPU）组队完成各种任务。这个时候，配合就显得尤为重要。
因此，英伟达之所以自己做CPU，并非完全因为英特尔或AMD，而是从自身产品需求出发，使CPU和GPU紧密耦合，以发挥最大性能。比如CPU和GPU中，需要用到尽可能相似技术的一致内存，以保证数据之间的无缝共享[8]。
虽然收购基本没有成功的可能性，但英伟达依然按部就班的招兵买马。2021年4月，黄仁勋在自家厨房里宣布，英伟达即将推出首款5nm制程工艺CPU Grace，基于Arm架构，面向超大型 AI 模型的和高性能计算。
紧接着就是具体工作的有序展开：英伟达首先选定了根据地以色列，那里有全球第三多的纳斯达克上市公司（仅次于美国和中国）；然后对外招聘600名硬件工程师、软件工程师和芯片设计师，搭建CPU研发团队[7]。
最后，英伟达挖来了英特尔在以色列的CPU架构专家Rafi Marom，后者曾参与10nm制程的Tiger Lake和Alder lake芯片开发工作。
在2022年3月的GTC大会上，英伟达对外宣布Grace CPU性能：拥有144个Arm内核和1TB/s的内存带宽，性能较当前最先进的DGX A100搭载的双CPU相比高1.5倍以上。
不过，原本预计在今年上半年可以开始供货的Grace芯片，目前已推迟至下半年。
APU Instinct MI300本质上是一颗“APU”，这是AMD早在2009年提出的一个概念——将CPU和GPU集成在一起，使得二者高速互联，实现1+1&gt;2的效果。
在2006年收购了GPU公司ATI后，AMD成为了当时唯一同时拥有CPU和GPU设计能力的芯片公司，而且在两个市场都是行业老二——但坏消息是，市场主流玩家也就两个。
在这种局面下，AMD希望借助APU打开市场局面。2011年，第一代APU推出后，AMD持续宣传APU是“x86架构三十年来的最大革命”，并向投资者强调，这款产品存在着“强劲且被压抑”的需求。
市场最初也对APU概念充满期待，结果2012年Q3财报出炉，AMD收入下滑25%，顺便减记了1亿美元的库存——APU需求量并不高，芯片根本卖不出去[1]。紧接着，公司股价跌到1.86美元的历史性低点，苏姿丰临危受命，开始掌舵风雨飘摇中的AMD。
APU的优势在于，由于CPU和GPU集成在了一起，数据传输效率得到了大幅度提高。苹果的M1 Ultra也采用了类似的“把几个小芯片拼成一块大芯片”的思路，换来了更强的数据吞吐能力。
但在2009年，APU的理念显得过于超前。
一方面，APU涉及芯片的先进封装技术，在当时既不成熟，成本也难以控制。另一方面，APU在需求高度多元化的消费市场很难行得通。
比如10种型号的CPU和GPU，理论上有100种组合方案，这就导致做10种方案无法满足市场需求，做100种方案难以收回生产成本。
因此在很长一段时间里，APU只能在PS4游戏机这类高度标准化的产品上才能找到市场。但深度学习的大爆发改变了这一点。
相比游戏和渲染，AI训练对算力和数据吞吐效率的需求成百上千倍的增加，目前针对AI市场推出的芯片产品，除了算力的堆砌，往往都采用3D堆叠和先进封装等方式，增加数据传输的效率，这与APU的优势不谋而合。
英特尔尚未正式发布的Falcon Shores，同样采用了将CPU、GPU、内存封装在一起的思路，只不过英特尔将其称为“XPU”。
但目前来看，最接近这个目标的反而是英伟达的Grace Hopper芯片。
英伟达的Grace Hopper将CPU和GPU集成在了一起
尾声 在2009年APU的概念被提出时，AMD正经历公司历史上的最低谷，APU多少有些毕功一役的憋大招成分。
但也正是因为处于低谷，导致AMD无法拿出足够的资金与技术支持，让APU的革命性理念真正落地，最终只变成了简单的CPU+GPU的组合。
从商业角度看，最适合在2009年搞点革命性产品的反而是富可敌国的英特尔，但英特尔当时在干什么呢——心安理得的挤牙膏，同时拒绝为iPhone设计芯片。
这似乎是高科技公司常常会出现的状况——在鼎盛年代忽视新的技术浪潮，在低谷期如梦方醒仓促憋大招。
事实上，英特尔还尝试过“联A抗N”——2017年，英特尔宣布将在自家CPU上集成AMD的GPU，合作推出新的芯片。
结果没过多久，英特尔就挖走了AMD的核心技术负责人之一：图形主管Raja Koduri，为英特尔开发高端独立GPU。
]]></content>
  </entry>
  
  <entry>
    <title>取代C++？谷歌开源编程语言Carbon</title>
    <url>/post/programming/google-open-source-programming-language-carbon.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>Google</tag>
      <tag>C++</tag>
      <tag>Carbon</tag>
    </tags>
    <content type="html"><![CDATA[号称替代C++，谷歌发布新的编程语言，已经过去了一年。去年7月，在多伦多举办的CppNorth大会上，谷歌宣布正式开源内部打造的编程语言Carbon，并称他是C++的继任者。
谷歌的工程师认为呢，尽管C++仍然是性能关键性软件的主流编程语言，并且拥有庞大且不断增长的代码库。但同时呢，也存在着应用性较差，掌握难度大以及由于语言功能过于丰富而导致的混乱的问题。
所以呢，他们决定自己开发一门语言来代替C++，于是呢，这个速度更快，并且可以和C++代码互相操作的新语言就应运而生了，Carbon编程语言已经在GitHub上开源了。
其实呢，Google在编程语言方面确实还挺强的，之前开源的go long呢，目前已经是使用非常广泛的一个编程语言了，但是这次Carbon能否延续Go语言的传奇？目前下结论呢，还为时尚早，毕竟上一个号称代替C++的 Rust  语言目前混的并不咋滴。
在谈到为何要替代 C++ 时，谷歌工程师Carruth表示，C++作为长期以来构建性能关键应用的首选语言，它自身的很多问题困扰着现代开发人员。C++积累了数十年的技术负债，带有的很多过时实践都是其前身C语言的一部分。C++ 的维护者优先考虑向后兼容，以便继续支持广泛使用的项目，例如Linux及其包管理生态系统等。
此外，C++语言的发展也受到了官僚委员会程序的阻碍，该程序以标准化而非设计为导向。这种做法导致很难添加新功能。C++在很大程度上处于一个隔绝的开发过程，其中可能经过数年才会做出一些重要决定。
因此，Carruth希望通过更开放的社区主导环境来构建Carbon语言，并已开源。到2023年7月中旬，该项目已在GitHub上获得30.8k的Stars。
Carbon的设计理念和特性 谷歌希望在2022年年底推出Carbon的核心工作版本，即v0.1。Carbon将建立在现代编程原则的基础上，包含一个泛型系统，使开发人员不再需要为每个实例检查和再核对代码。
C++ 语言中亟需的一个特性是内存安全。内存访问 bug是安全漏洞的罪魁祸首之一，Carbon 设计人员将探索追踪未初始化状态的更好方法、设计支持动态边界检查的 API和惯用语，并构建全面的默认debug构建模式。随着时间的推移，设计人员还计划构建一个安全的Carbon子集。
Carbon语言将支持以下功能：  性能关键型软件； 软件和语言演变； 易于阅读、理解和编写的代码； 实用的安全和测试机制； 快速且可扩展的开发； 现代操作系统平台、硬件架构和环境； 与现有C++代码的互操作性和迁移  同时，Carbon语言的亮点包括如下：  Introducer关键字和简单语法； 函数输入参数为只读值； 指针提供间接访问和变体； 使用表达式命名类型； 软件包为root命名空间； 通过包名导入APIs； 用显式对象参数来声明方法； 单继承、默认使用最终类； 强大且经过定义检查的泛型； 类型显式地实现接口。  Carbon设计团队将着手创建一个内置包管理器，这在C++中非常欠缺。此外，团队还计划编写一些将C++代码迁移到Carbon代码的工具。下图左为C++代码，右为Carbon编写的相同函数：
为何不大力发展Rust语言呢？ 有人或许会问了：最近有专门为解决内存安全性能应用的需求而构建的Rust语言，为何不直接使用它呢？Carruth对此表示，如果Rust适合你，就继续使用。但是，将C++的生态系统转移到Rust非常困难。
相比之下，Carbon是建立在已有C++生态系统之上，适合那些已经拥有大量C++代码库的开发人员，这些库很难转换到Rust。
目前 Carbon 语言的代码已完全开源。Chandler 表示，虽然 Carbon 诞生自谷歌内部，且目前的项目负责人主要（不完全）由谷歌员工组成，但它的目标是要成为一个 “独立且由社区驱动的开源项目”。
如果你对 Carbon 感兴趣，可以下载源代码并在自己的设备上进行试验，或者通过 Compiler Explorer 直接在浏览器中体验 Carbon 编程语言。
对于 Carbon 项目，有开发者透露了一些背景信息：2020 年 2 月，C++ 标准委员会就 “破坏 ABI 兼容性以保证性能” 提案进行了投票，这项工作主要由谷歌员工推动，但最终投票没有通过。因此，许多谷歌员工已经停止参与 C++ 的标准化工作，并辞去他们在委员会中的正式职务，clang 的开发工作也大大放缓。基于这些背景，再结合谷歌对 Carbon 设定的目标，这名开发者认为，谷歌确实希望把 Carbon 打造成替代 C++ 的语言。
]]></content>
  </entry>
  
  <entry>
    <title>2029年前，NAND是否能够替代HDD</title>
    <url>/post/storage/could-nand-capacity-replace-hdds-by-2029.html</url>
    <categories><category>Storage</category>
    </categories>
    <tags>
      <tag>SSD</tag>
      <tag>NAND</tag>
      <tag>HDD</tag>
    </tags>
    <content type="html"><![CDATA[分析：Pure Storage表示，2028年后将不再出售新的硬盘驱动器（HDD）。Pure的这一大胆声明意味着，2029年本应销售的所有新HDD容量（不包括更换故障驱动器的容量）将被SSD容量取而代之。这将是大量额外的SSD出货量，并提出了一个问题：全球NAND制造能力是否能够生产足够的闪存？
我们试图从NAND制造的角度来判断Pure Storage的预测是否可能，暂时忽略了HDD和SSD的总拥有成本（TCO）的影响。这需要有所解释。
HDD和SSD价格趋势 硬盘驱动器的出货量在单位数量方面已经连续下降了五年以上，因为SSD在越来越多的驱动器类别中提供了一个引人注目的替代方案。SSD比HDD更快地响应I/O请求，因为它们不必等待读写头在磁盘表面上移动到正确的磁道，然后等待磁盘的旋转将正确的数据块带到头部。
Statista图表显示，从2010年开始，每年的磁盘驱动器出货量都在下降 SSD成本一直在稳步下降，但随着晶圆中闪存容量的增加，下降速度在减小。这是由以下三件事引起的：NAND晶圆组件缩小，3DNAND技术中添加了更多层单元，以及使用TLC（3位/单元）现在已成为主流的多位单元。数据还原技术（压缩和重复数据删除）意味着SSD可以比其原始物理容量存储更多的数据，将其每TB的成本与不使用数据还原的HDD的成本更接近。
Wells Fargo图表比较了2012年至2020年的PC HDD和SSD价格。这显示了SSD价格（每TB）比HDD价格下降得更快 整体而言，由于这个原因，硬盘驱动器（HDD）市场已经出现下滑。笔记本电脑几乎完全转向了SSD，台式电脑也在紧随其后，而快速（10000转每分钟的2.5英寸）的关键任务HDD也正在被SSD所取代。HDD业务市场正逐渐围绕着近线（7400转每分钟的3.5英寸）驱动器聚集。然而，QLC（每个单元4位）SSD技术进一步降低了每TB的成本，而持续的层次增加将维持这一下降趋势。
HDD制造商表示，他们的技术进步，如HAMR，将继续将他们的价格降低，并因此保持他们对SSD的成本优势。闪存驱动器的价格不会低于HDD价格，因为HDD的成本下降将保持磁介质的经济性。
Wells Fargo高级分析师Aaron Rakers在2019年预测，企业存储买家将在价格降至硬盘驱动器的5倍时开始更喜欢SSD。他注意到2017年企业SSD比大容量近线磁盘驱动器高出18倍。这在2019年降至9倍。2020年，Rakers表示，企业SSD的一般成本为185美元/TB，而近线HDD的成本约为19美元/TB，这意味着企业SSD的价格溢价为9.7倍。这在几个季度内保持不变，如下图所示：
HDD的客户通常希望更快的数据访问速度，如果SSD的成本与HDD相等，或者至少不比HDD高太多，并且SSD的使用寿命与HDD一样长，他们会使用SSD。正如分析师指出的，发生这种情况的障碍是，世界上没有足够的NAND制造能力来取代所有客户购买的HDD容量。
Pure已成为第一个明确表示闪存将取代硬盘驱动器的闪存存储供应商。它声称其QLC闪存系统可以取代近线存储驱动器阵列，因为其5年TCO低于近线驱动器阵列。闪存每TB成本的持续下降，来自更高密度驱动器，将加强这一优势。
总拥有成本（TCO）的计算从采购价格开始，然后考虑驱动器的使用寿命、功耗和冷却成本，以及寿命周期结束时的处置成本。由于这些成本是未来的估计值，而且磁盘驱动器制造商可以根据不同的假设提供不同的数据。
即使不考虑成本和价格问题，问题仍然是是否有足够的NAND制造能力来替代每年销售的数百万个硬盘驱动器。
NAND制造能力 Micron高级总监Colm Lysaght在2019年表示：“很明显，SSD的每GB价格会随着时间的推移接近HDD的每GB价格。但是，从近线HDD到SSD进行“批量切换”所需的EB数量对于NAND闪存行业来说太大了。生成所需EB所需的资本投资……是过于昂贵的。”
“SSD可能会蚕食（甚至可能吞噬）近线HDD市场，但两者将在未来许多年内共存。”
像Wikibon的David Floyer等分析师表示，NAND制造能力不会成为限制因素。他预测，2026年，NAND生产效率将导致SSD在每TB美元的基础上比HDD更便宜。这是一个大胆的说法。
现在是2023年，Pure基本上表示它认为闪存制造能力不会是一个限制因素——“生成所需EB所需的资本投资”不再是“过于昂贵”。这是真的吗？NAND制造能力能否应对硬盘驱动器容量替代的需求？
NAND制造建模 要计算NAND代工厂行业是否能够生产满足SSD数据存储需求增长和2029年取代新HDD所需的容量，我们必须估计存储数据增长率、当前出货的HDD和SSD容量以及NAND代工厂容量。代工厂的产能需要增加多少才能满足需求，增加的NAND层数和每单元位数会如何影响制造产量？
我们制定了一个初步的简化电子表格模型，并与一些分析师和供应商分享了该模型，他们的意见改进并完善了该模型。简而言之，它表明2029年将有5.7%的NAND代工厂产能缺口，即405EB。
模型假设 我们从2022年出货的约1320EB的HDD容量和277EB的SSD容量开始，这是通过检查供应商和研究机构的数字得出的。如果2022年出货的1320EB的HDD容量在2029年降至零，而数据增长继续，那么SSD行业将不得不增长以满足其自身的市场需求，并提供缺失的HDD贡献。
它将受到存储数据增长的间接影响。这将涉及相当简单的假设，但这在这里是可以接受的，因为我们只试图看看制造能力的基础是否已经到位。Objective Analysis的Jim Handy表示：“IDC的数据领域经常被引用为衡量数据生产增长速度的指标。他们说，2026年将生成221178EB的数据，并可以趋势到2029年的394127EB。”这提供了从2026年到2029年的21.2%的数据增长复合年增长率。
我们将其应用于2022年出货的HDD容量，得出2029年需要5071EB。接下来，我们问2022年有多少NAND制造能力存在？Handy告诉我们：“2021年的EB出货量为538EB，因此他们至少有能力在2022年出货这么多。”这就是我们的起点。
我们使用21.2%的复合年增长率将其趋势到2029年，达到2029年的2067EB。然后，我们加上HDD替换数量，5071EB，得到2029年需要的7137.8EB的NAND制造能力。
额外的3D层叠和QLC将带来多少产能？Handy告诉我们：“需要新的产能，它将以每年约30%的速度上线，以支持每年EB增长。闪存层大约每两年翻一番。这给了该行业约40%的增长率，因此他们不必以那么高的速度增加层数。我不知道层数翻倍是否会需要双倍的晶圆厂工具和地板空间来处理固定数量的晶圆。”
这是一个很大的变数
我们将每年增加的40%的层叠率总结为2029年额外的3375.9EB的制造能力。如果从TLC（3位/单元）切换到QLC（4位/单元）NAND，则会增加33.3%。这将我们带到5671.2EBx1.33=7542.8EB，比需要的7137.8EB少405EB。这意味着需要额外的NAND制造能力，这意味着必须建造新的NAND代工厂。
层叠率增长因子是一个关键数字。如果将其降低到每年30%，那么我们最终将在2029年面临2647.9EB的NAND制造缺口。
供应商观点 Dell发言人表示：“我们的看法是，我们的产品组合使Dell能够灵活地为客户提供非常广泛的驱动器选择。随着对高密度和低成本HDD的持续创新，我们预计现在还不能说客户需求不会在2028年之后存在。”
Seagate首席技术官JohnMorris告诉我们：“简单地说，我们认为用NAND取代HDD在任何情况下都是不可能的。这不仅因为需要大量资本支出来指数级增加NAND供应，而且因为NAND无法达到驱动摆脱HDD所需的每位成本。”
“更重要的是，SSD和HDD一直在相互协作，因为它们在企业世界中各自满足不同的工作负载需求。在大多数存储市场中，它们被部署在一起，以创建最具成本效益的解决方案。这种架构将在可预见的未来持续存在。我们对HDD价值的信念体现在我们对未来HDD创新的持续投资中。”
他还建议：“与其看总的HDD和SSDEB，不如只关注近线HDD和企业SSD EB（以及相关的复合年增长率）。根据2022年的实际数据，近线HDD EB约为企业SSD的7倍。”
Pure Storage的回应 Pure Storage发言人表示：“在看NAND市场供应时，我们鼓励您超越当前的SSD消费，而是看整个市场……TrendFocus的研究……将整个NAND市场认定为约2.5倍于今天使用的SSD。
“虽然SSD NAND不同于用于手机/平板电脑/USB闪存驱动器/汽车和家电的NAND，但这表明NAND制造和晶圆厂的可用池要大得多，可以从中驱动增长和平衡需求。最终，制造商将扩大生产并改装他们的晶圆厂，以满足最有利可图的市场，我们确实预计从HDD到SSD的转换将是该需求的主要因素。
“您的模型假设传统HDD将1:1转换为SSD。虽然这对于这种规模的模型是合适的，但我们确实相信，随着企业从HDD转向SSD，会出现额外的容量整合机会，原因是增加了数据减少、提高了性能（减少了复制文件的需求）、降低了故障率和改进了纠删码效率。”
总而言之，我们（非常简化的）建模演练表明，只要层数以每年40%的速度增加，SSD NAND制造产出缺口将低于6%。这还没有考虑到Pure的观点，即总的NAND制造能力“约为目前用于SSD的约2.5倍”。
5.7%的差异也在误差范围之内。基本上，我们认为在建造闪存以取代2029年的新HDD方面，似乎不存在NAND代工容量问题。
我们将会密切关注，看看未来会带来什么。
原文地址: 2029年前，NAND是否能够替代HDD  
]]></content>
  </entry>
  
  <entry>
    <title>RS422/485接口电路设计要点</title>
    <url>/post/hardware/design-points-of-rs422-485-circuit.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>UART</tag>
      <tag>RS422</tag>
      <tag>RS485</tag>
    </tags>
    <content type="html"><![CDATA[RS-422标准全称是“平衡电压数字接口电路的电气特性”，它定义了接口电路的特性。
实际上还有一根信号地线，共5根线。由于接收器采用高输入阻抗和发送驱动器比RS232更强的驱动能力，故允许在相同传输线上连接多个接收节点，最多可接10个节点。一个主设备（Master），其余为从设备（Slave），从设备之间不能通信，所以RS-422支持点对多的双向通信。接收器输入阻抗为4k，故发端最大负载能力是10&amp;TImes;4k+100Ω（终接电阻）。
 RS-422和RS-485电路  原理基本相同，都是以差动方式发送和接收，不需要数字地线。差动工作是同速率条件下传输距离远的根本原因，这正是二者与RS232的根本区别，因为RS232是单端输入输出，双工工作时至少需要数字地线。发送线和接收线三条线（异步传输），还可以加其它控制线完成同步等功能。
RS-422通过两对双绞线可以全双工工作收发互不影响，而RS485只能半双工工作，发收不能同时进行，但它只需要一对双绞线。RS422和RS485在19kpbs下能传输1200米。用新型收发器线路上可连接台设备。
典型的RS422接口电路 图 1 典型的RS422接口电路
典型的RS485接口电路 图 2 典型的RS485接口电路
图 3 全双工RS485接口电路拓扑
设计要点   接口保护用途的TVS管D1-8，通常选择最大反向工作电压VRWM为5.0V的双向TVS管，如Diodes SMBJ5.0CA。注：这里可以选择耐压更高的TVS元件。
  DI和RO引脚都使用10k电阻上拉，是为防止误触发，产生误动作，因为“UART以一个前导“0”触发一次接收动作”。
  图 1所示，差分接收器的端接电阻一般取值120 Ω，来源于通常RS422/485传输线所用的特征阻抗约为120 Ω。图 3所示的RS485多点应用中，若在SCH&amp;PCB设计时不清楚后期现场布线中哪两个设备距离最远，可在所有差分接收端都预留120 Ω端接电阻，以便后期现场应用时通过拨码开关选择性接入。
  由于RS422/485差分接收器的特性是，VIA - VIB的绝对值必须大于200 mV，否则无法正确识别高低电平。所以，图 1所示，当使用3.3V电源时，故障安全偏置电阻R5和R6最大取值为930 Ω；当使用5.0V电源时，R5和R6最大取值为1440 Ω。
  说明：故障安全偏置电阻，是为了解决“总线空闲、开路或短路”情况下，接收端状态不确定的问题。由于RS422只支持点对点应用，且故障安全偏置电阻只需要在接收端使用，所以图 1和图 2电路，R3-4不是必要的，R5-6和R12-15是必要的。注：如果R12/R13在发送端已经有，那么在接收端就不是必要的。
 图 1所示，在RS422点对点应用中，两端的差分接收器都需要120 Ω并联端接电阻。图 3所示，在RS485多点应用中，只需在最远的两点接收端使用120 Ω并联端接电阻，中间各支路不需要。
  图 2和图 3所示，各支路的A&amp;B引脚和Z&amp;Y引脚都串联0R电阻，当某路故障时将RS485总线拉低时，逐一断开电阻，方便排查故障。
  SCH&amp;PCB设计时，两个设备间的RS422/485通信线，除了两对差分线外，至少需要一根地线，防止共模电压超出规定的范围而导致通信故障。
  有选择的情况下，RS422/485通信电缆中，信号线不应与电源线并行或尽量远离电源线，若无法避免，信号线最好使用带屏蔽的双绞线。且现场布线，采用菊花链拓扑，不采用星形或环形拓扑，以免因反射等因素导致通信错误。
  原文地址： RS422/485接口电路设计要点  
]]></content>
  </entry>
  
  <entry>
    <title>5款超强大的FPGA开发板</title>
    <url>/post/fpga/5-ultra-powerful-fpga-development-boards.html</url>
    <categories><category>FPGA</category>
    </categories>
    <tags>
      <tag>FPGA</tag>
      <tag>Xilinx</tag>
      <tag>Microsemi</tag>
      <tag>Stratix 10</tag>
    </tags>
    <content type="html"><![CDATA[随着人工智能、深度学习在市场越来越受欢迎，除了GPU、众多独角兽公司的AI专用芯片，FPGA同样是深度学习的热门平台之一。本文将给大家介绍5款强大到不可思议的FPGA开发板，当然价格也是高的离谱，肯能对于大多数工程师来说，这些属于求而不得的高端“玩具”。
RTG4开发套件 RTG4-DEV-KIT是Microsemi公司的产品，当然目前的话已经被Microchip收购，这是一套为高端的客户提供的评估和开发平台，主要用于数据传输，串行连接，总线接口等高密度高性能FPGA的高速设计等应用 。
该开发板采用RT4G150器件，采用陶瓷封装，提供150,000个逻辑元件，具有1,657个引脚，下图是RTG4-DEV-KIT开发板的外设接口功能图。
RTG4开发套件主要包括的硬件功能如下：
 两个1GB DDR3同步动态随机存取存储器(SDRAM) 2GB SPI Flash PCI Express Gen1 接口1个 PCIe x4 接口 一对SMA连接器，用于测试全双工SERDES通道 两个带有HPC/LPC引脚排列的FMC连接器，用于扩展 用于10/100/1000以太网的RJ45接口 USB micro-AB连接器 SPI，GPIO的接口 FTDI编程器接口用于编程外部SPI Flash JTAG编程接口 用于应用程序编程和调试的RVI接口 Flashpro编程接口 用于调试的嵌入式跟踪宏(ETM)单元接口 用于用户应用的双列直插式封装(DIP)开关 按钮开关和LED 电流测量测试点  RTG4-DEV-KIT开发板的硬件框图
从硬件框图上也能看到RTG4-DEV-KIT复杂的电源管理系统，12V的DC直流供电，通过DCDC/以及LDO分配到各个功能部分的供电。
英特尔Stratix 10开发套件 Intel Stratix 10开发套件是包含各类软硬件的完整设计环境，用于评估Stratix 10 FPGA的功能。该套件可用于通过符合PCI-SIG的开发板来开发和测试PCI Express 3.0设计。使用这些开发板可开发和测试由DDR4、DDR3、QDR IV和RLDRAM III存储器组成的存储器子系统。通过使用FPGA夹层卡 (FMC) 连接器与FMC夹层卡连接，还可以开发模块化和可扩展设计。该套件支持JESD204B、Serial RapidIO、10Gbps以太网 (10GbE)、SONET、通用公共无线电接口 (CPRI)、OBSAI等诸多协议。
英特尔Stratix 10开发套件硬件框图
开发板板载的主要FPGA是Intel公司的Stratix 10系列产品，相比前一代产品成本提供2X性能和超低功耗，具有几个开创性的创新如新型的HyperFlex™和架构，能满足日益增长的带宽和处理性能，从而满足功率预算。嵌入硬件系统基于四核64位ARM Cortex-A53，采用Intel 14-nm Tri-Gate (FinFET)技术和混合性3D片上系统(SiP)技术，单片核多达550万和逻辑单元，多达96个全双工收发器，数据速率高达28.3Gbps，主要用在计算和存储，网络设备，光传输网络，广播，军用雷达，医疗设备，测试和测量以及5G无线设备，ASIC原型。
ADS8-V1 评估板 确切的说，ADS8-V1 评估板并不是一块专为FPGA评估的板卡，而是为了支持ADI公司高速数据转换，当连接到指定的 ADI 高速 ADC 评估板时，ADS8-V1 可用作数据采集板。ADS8-V1 上的 FPGA 设计用于支持最高速 JESD204B 模数转换器，可充当数据接收器，同时 ADC 为数据发射器。
ADS8-V1EBZ接口外设如下：
 Xilinx Kintex Ultrascale XCKU040-3FFVA1156E FPGA 一(1)个FMC +连接器 一(1)个FMC +连接器支持二十(20)个16Gbps收发器 DDR4 SDRAM 简单的USB 3.0端口接口  ADI强大的数据采集评估板可以应用于航空航天和防务、电子监控和对抗、仪器仪表和测量、通信测试设备、信号发生器(通过射频传输音频)、5G领域等。
REFLEX CES XpressVUP-LP9P REFLEX CES XpressVUP-LP9P是基于Virtex Ultrascale + VU9P FPGA的低配置PCIe网络处理FPGA板，专为HPC等网络应用而设计。该板提供2组DDR4，2组QDR2 +存储器和2个QSFP28网箱，用于多个10GbE / 40GbE / 100GbE网络解决方案。其主要的功能包括了：PCIe Gen3 x16、Xilinx Virtex UltraScale + VU9P FPGA、板载两个DDR4和两个QDR2 +独立组、两个QSFP28光纤笼用于多网络解决方案、具有16个通道，8 Gb/s链路速率的PCIe接口(Gen3)等。
XpressVUP-LP9P技术规格 FPGA和配置模块  Xilinx Virtex UltraScale + 16nm FPGA：XCVU9P-L2FLGB2104E(生产) XCVU9P-L2FLGB2104E(生产) 2,6 M系统逻辑单元 270 Mb UltraRAM(UltraScale +提供高密度，双端口，同步存储器模块) 用于外部Xilinx USB电缆的JTAG连接器 双四SPI(x8)配置模式的2x Nor Flash  通讯接口  PCI Express x16(第1,2或3代) 2 x QSFP28四光纤笼(2 x 4 XCVR：每条链路28 Gb/ s)，支持10GbE / 40GbE / 100GbE QSFP28模块支持的其他协议  存储  板载DDR4,2x组64位+ 4位ECC，总共8GB 板载QDR-II +，2x存储区，18位，总共144Mbits  功率  最大100W 提供定制散热器  其他资源  板载可编程PLL振荡器(Si5345)，高度灵活和可配置的时钟发生器。 板载高精度振荡器为精确时间协议(PTP)以太网提供时钟精确20MHz-0.05ppm，同步协议标准化IEEE 1588 一个用于PPS(每秒脉冲)的同轴连接器，允许多个电子部件同步  REFLEX CES XpressVUP-LP9P硬件框图
值得一提的是，XpressVUP在POWER9 CPU主机处理器(IBM)上支持CAPI 2.0，并且还支持IBM SNAP框架，只需很少的FPGA专业知识，SNAP框架允许应用工程师在服务器环境中快速创建基于FPGA的加速程序。它使用IBM CAPI 2.0接口，该接口可在标准PCIe物理通道上运行，但具有CPU和FPGA之间较低延迟和一致内存共享的优势。
Digilent NetFPGA-SUME NetFPGA-SUME是Digilent，剑桥大学和斯坦福大学之间的合作项目，是高性能和高密度网络设计的理想平台。
NetFPGA-SUME采用赛灵思Virtex-7 690T FPGA，支持30个13.1 GHz GTH收发器，四个SFP + 10Gb/s端口，五个独立的高速存储器组，由500MHz QDRII +和1866MT / s DDR3 SoDIMM器件构建，以及一个八通道第三代PCIe，可提供超大的吞吐量，并可支持大量高速数据流FPGA架构和存储器件，其它功能包括在FMC和QTH扩展连接器以及SATA端口上共展示20个收发器。
NetFPGA-SUME的主要任务是为学生，研究人员和开发人员提供最先进的网络平台，无论是学习基础知识还是创建新的硬件和软件应用程序，该板可轻松支持四个10Gb/s以太网端口上的同时线速处理，并可在板上操作和处理数据。
Digilent NetFPGA-SUME特征：  Xilinx Virtex-7 XC7V690T FFG1761-3 Xilinx CPLD XC2C512用于FPGA配置 PCIe Gen3 x8(8Gbps /通道) 两个512Mbits Micron StrataFlash(PC28F512G18A) 编程：赛灵思Vivado 设计套件 三个x36 72Mbits QDR II SRAM(CY7C25652KV18-500BZC) 两个4GB DDR3 SODIMM(MT8KTF51264Hz-1G9E1) 用于JTAG编程和调试的Micro USB连接器(与UART接口共享) 一根Micro USB线用于编程/ UART QTH连接器(8个RocketIO GTH收发器) 四个SFP +接口(4个RocketIO GTH收发器)，支持10Gbps 两个SATA-III端口 用户LED和按钮 一个HPC FMC连接器(10个RocketIO GTH收发器) 一个Pmod端口  总结 FPGA的强大还是在于其超灵活的可编程能力，随着人工智能越来越受市场的喜爱，无论是GPU还是专用的AI芯片都不可能像FPGA这样便于新进入这个领域的企业折腾、创新，带着这种与生俱来的优势，相信FPGA的春天还很漫长。
原文连接: 5款超强大的FPGA开发板  
]]></content>
  </entry>
  
  <entry>
    <title>三大网口类型：千兆网口、2.5G网口和5G网口</title>
    <url>/post/datacenter/three-typical-bandwidth-network-adapters.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>1G</tag>
      <tag>2.5G</tag>
      <tag>5G</tag>
    </tags>
    <content type="html"><![CDATA[当涉及到网络连接速度时，选择正确的网口类型非常重要。在现代网络中， 千兆网口  、2.5G网口和5G网口是常见的选项。本文将详细介绍这些网口类型以及如何选择适合你的需求的网口。
千兆网口 千兆网口，也称为Gigabit Ethernet，是最常见和广泛使用的网口类型之一。它提供的传输速度为1千兆位每秒（1 Gbps），也就是每秒可以传输1亿个位。千兆网口是许多家庭和办公室网络的标准连接方式。
千兆网口的主要优点是速度快、稳定性高和成本相对较低。它可以满足大多数家庭和小型办公室的网络需求，可以处理高清视频流、在线游戏和常见的网络任务。此外，千兆网口的设备和网络线材广泛可用，成本相对较低。
然而，千兆网口的主要限制是其传输速度有上限。对于需要更高速度的场景，千兆网口可能无法满足需求。例如，如果你需要同时传输大量数据或处理高带宽应用程序，千兆网口可能会成为瓶颈，限制了网络性能。
2.5G网口 2.5G网口是一种介于千兆网口和5G网口之间的新型网口类型。它提供的传输速度为2.5千兆位每秒（2.5 Gbps），比千兆网口快2.5倍，但比5G网口慢一半。2.5G网口的出现是为了填补千兆网口和5G网口之间的速度差距，提供更好的性能选择。
2.5G网口的优点是在提供更高速度的同时保持成本相对较低。它可以适应一些需要更高带宽的应用场景，如视频编辑、大规模数据传输和多设备同时使用网络等。2.5G网口还兼容千兆网口设备，因此可以逐步升级网络而无需更换所有设备。
然而，2.5G网口的主要限制是设备和线缆的可用性相对较少。相比之下，千兆网口设备和线缆更加普遍和便宜。此外，2.5G网口的速度虽然比千兆网口快，但对于某些高性能场景来说仍然不够。
5G网口 5G网口是一种高速网口类型，提供的传输速度为5千兆位每秒（5 Gbps），是目前可用的最高速度之一。它适用于需要处理大规模数据传输、实时视频流、虚拟现实和其他高带宽应用程序的场景。
5G网口的主要优点是其出色的传输速度和性能。它可以满足对高速网络连接有严格要求的专业用户和企业需求。对于需要同时处理多个高带宽任务的场景，5G网口是一个理想选择。
然而，5G网口的主要限制是其设备和线缆的成本相对较高，并且在市场上的可用性较为有限。除此之外，大多数家庭和小型办公室的常规网络需求可能不需要如此高的速度，因此选择5G网口可能会超出实际需求。
三者区别    网口类型 传输速度 主要优点 主要限制     千兆网口 1 Gbps 速度快、稳定性高、成本相对较低 速度有上限，可能成为网络性能瓶颈   2.5G网口 2.5 Gbps 提供更高速度、成本相对较低、兼容千兆网口 设备和线缆可用性相对较少，速度不足以满足某些高性能场景   5G网口 5 Gbps 出色的传输速度和性能 设备和线缆成本较高，市场上的可用性有限    这个表格清晰地列出了千兆网口、2.5G网口和5G网口之间的主要区别。千兆网口在速度、成本和稳定性方面具有优势，但速度有上限。2.5G网口提供更高的速度，兼容千兆网口设备，但设备和线缆可用性较少。5G网口具有出色的速度和性能，但设备和线缆成本较高且市场上的可用性有限。
如何选择 在选择千兆网口、2.5G网口和5G网口时，有几个关键因素需要考虑：
  需求：首先确定你的网络需求。如果你只是进行一般的上网浏览、电子邮件和常见的网络任务，千兆网口通常已经足够了。如果你需要处理大规模数据传输或需要更高速度的特定应用程序，可以考虑2.5G网口或5G网口。
  设备和线缆的可用性：检查市场上设备和线缆的可用性。千兆网口设备和线缆相对更普遍和便宜，而2.5G网口和5G网口的设备和线缆相对较少。确保你能够轻松获得所需的设备和线缆。
  成本：考虑你的预算。千兆网口通常是最经济实惠的选择，而2.5G网口和5G网口的设备和线缆成本较高。权衡你的需求和预算，选择最适合的选项。
  未来扩展性：考虑你的网络未来是否需要升级。如果你计划在未来增加更多设备或需要更高速度，可以选择具有更高性能的网口类型，如2.5G网口或5G网口，以便于网络的扩展和升级。
  笔者特地整理成表格，方便大家记忆：
   考虑因素 千兆网口 2.5G网口 5G网口     传输速度 1 Gbps 2.5 Gbps 5 Gbps   主要优点 速度快、稳定性高、成本相对较低 提供更高速度、成本相对较低、兼容千兆网口 出色的传输速度和性能   主要限制 速度有上限，可能成为网络性能瓶颈 设备和线缆可用性相对较少，速度不足以满足某些高性能场景 设备和线缆成本较高，市场上的可用性有限   适用场景 一般上网浏览、常见网络任务 需要更高带宽的应用场景、逐步升级网络 高速网络连接需求、高带宽应用程序   设备可用性 广泛可用、成本相对较低 相对较少、设备成本较高 有限可用性、设备成本较高   成本 相对较低 相对较低 较高   未来扩展性 有限 适中 适中    总结 综上所述，选择适合你的需求的网口类型是关键。千兆网口是最常见和经济实惠的选择，适用于大多数家庭和小型办公室的常规网络需求。如果你需要更高速度和性能，可以考虑2.5G网口或5G网口，但要注意设备和线缆的可用性以及成本因素。
]]></content>
  </entry>
  
  <entry>
    <title>10道经典的嵌入式C语言题目</title>
    <url>/post/programming/ten-classical-embedded-c-language-examination-questions.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>C</tag>
      <tag>Embedded</tag>
      <tag>Struct</tag>
    </tags>
    <content type="html"><![CDATA[10个C语言面试题，涉及指针、进程、运算、结构体、函数、内存，看看你能做出几个
gets()函数 问：请找出下面代码里的问题：
#include&lt;stdio.h&gt; int main(void) { char buff[10]; memset(buff,0,sizeof(buff)); gets(buff); printf(&#34;\nThe buffer entered is [%s]\n&#34;,buff); return 0; } 答：上面代码里的问题在于函数gets()的使用，这个函数从stdin接收一个字符串而不检查它所复制的缓存的容积，这可能会导致缓存溢出。这里推荐使用标准函数fgets()代替。
strcpy()函数 问：下面是一个简单的密码保护功能，你能在不知道密码的情况下将其破解吗？
#include&lt;stdio.h&gt;  int main(int argc, char *argv[]) { int flag = 0; char passwd[10]; memset(passwd,0,sizeof(passwd)); strcpy(passwd, argv[1]); if(0 == strcmp(&#34;LinuxGeek&#34;, passwd)) { flag = 1; } if(flag) { printf(&#34;\nPassword cracked \n&#34;); } else { printf(&#34;\nIncorrect passwd \n&#34;); } return 0; } 答：破解上述加密的关键在于利用攻破strcpy()函数的漏洞。所以用户在向“passwd”缓存输入随机密码的时候并没有提前检查“passwd”的容量是否足够。
所以，如果用户输入一个足够造成缓存溢出并且重写“flag”变量默认值所存在位置的内存的长“密码”，即使这个密码无法通过验证，flag验证位也变成了非零，也就可以获得被保护的数据了。例如：
$ ./psswd aaaaaaaaaaaaa Password cracked 虽然上面的密码并不正确，但我们仍然可以通过缓存溢出绕开密码安全保护。
要避免这样的问题，建议使用 strncpy()函数。
作者注：最近的编译器会在内部检测栈溢出的可能，所以这样往栈里存储变量很难出现栈溢出。在我的gcc里默认就是这样，所以我不得不使用编译命令‘-fno-stack-protector’来实现上述方案。
main()的返回类型 问：下面的代码能 编译通过吗？如果能，它有什么潜在的问题吗？
#include&lt;stdio.h&gt;  void main(void) { char *ptr = (char*)malloc(10); if(NULL == ptr) { printf(&#34;\nMalloc failed \n&#34;); return; } else { // Do some processing  free(ptr); } return; } 答：因为main()方法的返回类型，这段代码的错误在大多数编译器里会被当作警告。
main()的返回类型应该是“int”而不是“void”。因为“int”返回类型会让程序返回状态值。这点非常重要，特别当程序是作为依赖于程序成功运行的脚本的一部分运行时。
内存泄露 问：下面的代码会导致内存泄漏吗？
#include&lt;stdio.h&gt;  void main(void) { char *ptr = (char*)malloc(10); if(NULL == ptr) { printf(&#34;\nMalloc failed \n&#34;); return; } else { // Do some processing  } return; } 答：尽管上面的代码并没有释放分配给“ptr”的内存，但并不会在程序退出后导致内存泄漏。在程序结束后，所有这个程序分配的内存都会自动被处理掉。
但如果上面的代码处于一个“while循环”中，那将会导致严重的内存泄漏问题！
提示：如果你想知道更多关于内存泄漏的知识和内存泄漏检测工具，可以来看看我们在Valgrind上的文章。
free()函数 问：下面的程序会在用户输入’freeze’的时候出问题，而’zebra’则不会，为什么？
#include&lt;stdio.h&gt;  int main(int argc, char *argv[]) { char *ptr = (char*)malloc(10); if(NULL == ptr) { printf(&#34;\nMalloc failed \n&#34;); return -1; } else if(argc == 1) { printf(&#34;\nUsage \n&#34;); } else { memset(ptr, 0, 10); strncpy(ptr, argv[1], 9); while(*ptr != &#39;z&#39;) { if(*ptr == &#39;&#39;) break; else ptr++; } if(*ptr == &#39;z&#39;) { printf(&#34;\nString contains &#39;z&#39;\n&#34;); // Do some more processing  } free(ptr); } return 0; } 答：这里的问题在于，代码会（通过增加“ptr”）修改while循环里“ptr”存储的地址。当输入“zebra”时，while循环会在执行前被终止，因此传给free()的变量就是传给malloc()的地址。
但在“freeze”时，“ptr”存储的地址会在while循环里被修改，因此导致传给free()的地址出错，也就导致了seg-fault或者崩溃。
使用_exit退出 问：在下面的代码中，atexit()并没有被调用，为什么？
#include&lt;stdio.h&gt;  void func(void) { printf(&#34;\nCleanup function called \n&#34;); return; } int main(void) { int i = 0; atexit(func); for(;i&lt;0xffffff;i++); _exit(0); } 这是因为_exit()函数的使用，该函数并没有调用atexit()等函数清理。如果使用atexit()就应当使用exit()或者“return”与之相配合。
void*和C结构体 问：你能设计一个能接受任何类型的参数并返回interger（整数）结果的函数吗？
答：如下：
int func(void *ptr) 如果这个函数的参数超过一个，那么这个函数应该由一个结构体来调用，这个结构体可以由需要传递参数来填充。
* 和 ++ 操作 问：下面的操作会输出什么？为什么？
#include&lt;stdio.h&gt;  int main(void) { char *ptr = &#34;Linux&#34;; printf(&#34;\n[%c] \n&#34;,*ptr++); printf(&#34;\n[%c] \n&#34;,*ptr); return 0; } 答：输出结果应该是这样：
[L]
[i]
因为“++”和“ * ” 的优先权一样，所以“ * ptr++ ”相当于 “ * (ptr++) ”。即应该先执行 ptr++，然后才是 * ptr，所以操作结果是“L”。第二个结果是“i”。
问：修改代码片段 问：下面的代码段有错，你能指出来吗？
#include&lt;stdio.h&gt;  int main(void) { char *ptr = &#34;Linux&#34;; *ptr = &#39;T&#39;; printf(&#34;\n[%s] \n&#34;, ptr); return 0; } 答：这是因为，通过 * ptr = ‘T’，会改变内存中代码段（只读代码）“Linux”的第一个字母。这个操作是无效的，因此会造成segment-fault或者崩溃。
返回本地变量的地址 问：下面代码有问题吗？如果有，该怎么修改？
#include&lt;stdio.h&gt;  int* inc(int val) { int a = val; a++; return &amp;a; } int main(void) { int a = 10; int *val = inc(a); printf(&#34;\nIncremented value is equal to [%d] \n&#34;, *val); return 0; } 答：尽管上面的程序有时候能够正常运行，但是在“inc()”中存在严重的漏洞。这个函数返回本地变量的地址。因为本地变量的生命周期就是“inc()”的生命周期，所以在inc结束后，使用本地变量会发生不好的结果。这可以通过将main()中变量“a”的地址来避免，这样以后还可以修改这个地址存储的值。
]]></content>
  </entry>
  
  <entry>
    <title>Linux操作系统学习——启动</title>
    <url>/post/linux/linux-operating-system-study-boot.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Boot</tag>
    </tags>
    <content type="html"><![CDATA[Linux操作系统内核是服务端学习的根基，也是提高编程能力、源码阅读能力和进阶知识学习能力的重要部分，本文开始将记录Linux操作系统中的各个部分源码学习历程。
前言 关于如何学习源码，个人觉得可以从以下角度入手，有效地提高阅读和学习的效率。（学习语言就不说了，这是基本功。学习IDE推荐Source Insight或者Visual Studio，网站源码阅读推荐woboq）
理解代码的组织结构。 以Linux源码举例，首先你得知道操作系统分为哪几个部分，他们单独做了什么功能，如何进行配合完成更为具体的功能。建立整体的印象有助于后续深入学习的时候方便理解，毕竟代码是用的不是看的，理解他的作用有利于理解为什么要这么做。
深入各个模块学习 模块接口： 这里推荐微软的画图工具visio或者思维导图xmind，用其画图可以将各个模块的接口列出，并绘制各个模块之间的关系，通过了解接口可以清楚各个模块之间的关系，即绘制模块组织图
工作流程： 通过上面一步得到各模块间的关系，然后实际用断点或log等方式看一看整体的工作流程，在模块组织图的基础上绘制程序流程图
模块粘合层： 我们的代码有很多都是用来粘合代码的，比如中间件（middleware）、Promises 模式、回调（Callback）、代理委托、依赖注入等。这些代码模块间的粘合技术是非常重要的，因为它们会把本来平铺直述的代码给分裂开来，让你不容易看明白它们的关系。这些可以作为程序流程图的补充，让其中本来无法顺畅衔接的地方变得通畅无阻。
模块具体实现： 这是最难得地方，涉及到大量具体源码的学习。深入细节容易迷失在细节的海洋里，因此需要有一些重点去关注，将非重点的内容省略。通过学习绘制模块具体架构图和模块的算法时序图，可以帮助你更好的掌握源码的精髓。
需要关注的包括 代码逻辑。 代码有两种逻辑，一种是业务逻辑，这种逻辑是真正的业务处理逻辑；另一种是控制逻辑，这种逻辑只是用控制程序流转的，不是业务逻辑。比如：flag 之类的控制变量，多线程处理的代码，异步控制的代码，远程通讯的代码，对象序列化反序列化的代码等。这两种逻辑你要分开，很多代码之所以混乱就是把这两种逻辑混在一起了。
重要的算法。 一般来说，我们的代码里会有很多重要的算法，我说的并不一定是什么排序或是搜索算法，可能会是一些其它的核心算法，比如一些索引表的算法，全局唯一 ID 的算法、信息推荐的算法、统计算法、通读算法（如 Gossip）等。这些比较核心的算法可能会非常难读，但它们往往是最有技术含量的部分。
底层交互。 有一些代码是和底层系统的交互，一般来说是和操作系统或是 JVM 的交互。因此，读这些代码通常需要一定的底层技术知识，不然，很难读懂。
可以忽略的包括 出错处理。 根据二八原则，20% 的代码是正常的逻辑，80% 的代码是在处理各种错误，所以，你在读代码的时候，完全可以把处理错误的代码全部删除掉，这样就会留下比较干净和简单的正常逻辑的代码。排除干扰因素，可以更高效地读代码。
数据处理。 只要你认真观察，就会发现，我们好多代码就是在那里倒腾数据。比如 DAO、DTO，比如 JSON、XML，这些代码冗长无聊，不是主要逻辑，可以不理。
忽略过多的实现细节。 在第一遍阅读源码时，已弄懂整体流程为主，至于具体的实现细节先简单的理清处过一遍，不用过于纠结。当梳理清楚全部的框架逻辑后，第二遍再深入的学习研究各个模块的实现，此时应该解决第一遍中的疑惑。第三遍可以跳出代码的实现，来看Linux的设计思路、编程艺术和演进之路。
重在实践。 Linux的代码都是可以调试的，看很多遍也许不如跟着调试走一遍，然后再自己修改修改做一些小测试。
传授知识。 当你能将知识讲述给别人听，并让别人听懂时，你已经可以自豪的说洞悉了这些知识。所以不妨从一个小的例子开始自说自话，看能不能自圆其说，甚至写成博客、做成PPT给大家讲解。
说了一大堆的废话，下面就正式开始操作系统的深入学习记录之旅了。
混沌初开 本文分析从按下电源键到加载BIOS以及后续bootloader的整个过程。犹如盘古开天辟地一般，该过程将混沌的操作系统世界分为清晰的内核态和用户态，并经历从实模式到保护模式的变化。这里先简单介绍一下名词，便于后续理解。
实模式（Real Mode)：又名 Real Address Mode，在此模式下地址访问的是真实地内存地址所在位置。在此模式下，可以使用20位（1MB）的地址空间，软件可以不受限制的操作所有地址的空间和IO设备。
保护模式（Protected Mode)：又名 Protected Virtual Address Mode，采用虚拟内存、页等机制对内存进行了保护，比起实模式更为安全可靠，同时也增加了灵活性和扩展性。
从启动电源到BIOS 当我们按下电源键，主板会发向电源组发出信号，接收到信号后，电源会提供合适的电压给计算机。当主板收到电源正常启动的信号后，主板会启动CPU。CPU重置所有寄存器数据，并设置初始化数据，这个初始化数据在X86架构里如下所示：
1IP 0xfff0 2CS selector 0xf000 3CS base 0xffff0000 4IP/EIP (Instruction Pointer) : 指令指针寄存器，记录将要执行的指令在代码段内的偏移地址 5CS（Code Segment Register）：代码段寄存器，指向CPU当前执行代码在内存中的区域（定义了存放代码的存储器的起始地址） 实模式采取内存段来管理 0 - 0xFFFFF的这1M内存空间，但是由于只有16位寄存器，所以最大地址只能表示为0xFFFFF（64KB)，因此不得不采取将内存按段划分为64KB的方式来充分利用1M空间。也就是上所示的，采取段选择子 + 偏移量的表示法。这种方法在保护模式中对于页的设计上也沿用了下来，可谓祖传的智慧了。具体的计算公式如下所示：
1PhysicalAddress = Segment Selector * 16 + Offset 该部分由硬件完成，通过计算访问0XFFFF0，如果该位置没有可执行代码则计算机无法启动。如果有，则执行该部分代码，这里也就是我们故事的开始，BIOS程序了。
BIOS到BootLoader BIOS执行程序存储在ROM中，起始位置为0XFFFF0，当CS:IP指向该位置时，BIOS开始执行。BIOS主要包括以下内存映射：
10x00000000 - 0x000003FF - Real Mode Interrupt Vector Table 20x00000400 - 0x000004FF - BIOS Data Area 30x00000500 - 0x00007BFF - Unused 40x00007C00 - 0x00007DFF - Our Bootloader 50x00007E00 - 0x0009FFFF - Unused 60x000A0000 - 0x000BFFFF - Video RAM (VRAM) Memory 70x000B0000 - 0x000B7777 - Monochrome Video Memory 80x000B8000 - 0x000BFFFF - Color Video Memory 90x000C0000 - 0x000C7FFF - Video ROM BIOS 100x000C8000 - 0x000EFFFF - BIOS Shadow Area 110x000F0000 - 0x000FFFFF - System BIOS 12 其中最重要的莫过于中断向量表和中断服务程序。BIOS程序在内存最开始的位置（0x00000）用1 KB的内存空间（0x00000～0x003FF）构建中断向量表，在紧挨着它的位置用256字节的内存空间构建BIOS数据区（0x00400～0x004FF），并在大约57 KB以后的位置（0x0E05B）加载了8 KB左右的与中断向量表相应的若干中断服务程序。中断向量表中有256个中断向量，每个中断向量占4字节，其中两个字节是CS的值，两个字节是IP的值。每个中断向量都指向一个具体的中断服务程序。
BIOS程序会选择一个启动设备，并将控制权转交给启动扇区中的代码。主要工作即使用中断向量和中断服务程序完成BootLoader的加载，最终将boot.img加载至0X7C00的位置启动。Linux内核通过Boot Protocol定义如何实现该引导程序，有如GRUB 2和syslinux等具体实现方式，这里仅介绍GRUB2。
BootLoader的工作 boot.img由boot.S编译而成，512字节，安装在启动盘的第一个扇区，即MBR。由于空间有限，其代码十分简单，仅仅是起到一个引导的作用，指向后续的核心镜像文件，即core.img。core.img包括很多重要的部分，如lzma_decompress.img、diskboot.img、kernel.img等，结构如下图。
整个加载流程如下：
1、boot.img加载core.img的第一个扇区，即diskboot.img，对应代码为diskboot.S
2、diskboot.img加载core.img的其他部分模块，先是解压缩程序 lzma_decompress.img，再往下是 kernel.img，最后是各个模块 module 对应的映像。这里需要注意，它不是 Linux 的内核，而是 grub 的内核。注意，lzma_decompress.img 对应的代码是 startup_raw.S，本来 kernel.img 是压缩过的，现在执行的时候，需要解压缩。
3、加载完core之后，启动grub_main函数。
4、grub_main函数初始化控制台，计算模块基地址，设置 root 设备，读取 grub 配置文件，加载模块。最后，将 GRUB 置于 normal 模式，在这个模式中，grub_normal_execute (from grub-core/normal/main.c) 将被调用以完成最后的准备工作，然后显示一个菜单列出所用可用的操作系统。当某个操作系统被选择之后，grub_menu_execute_entry 开始执行，它将调用 GRUB 的 boot 命令，来引导被选中的操作系统。
 在这之前，我们所有遇到过的程序都非常非常小，完全可以在实模式下运行，但是随着我们加载的东西越来越大，实模式这 1M 的地址空间实在放不下了，所以在真正的解压缩之前，lzma_decompress.img 做了一个重要的决定，就是调用 real_to_prot，切换到保护模式，这样就能在更大的寻址空间里面，加载更多的东西。
开机时的16位实模式与内核启动的main函数执行需要的32位保护模式之间有很大的差距，这个差距谁来填补？head.S做的就是这项工作。就像 kernel boot protocol 所描述的，引导程序必须填充 kernel setup header （位于 kernel setup code 偏移 0x01f1 处） 的必要字段，这些均在head.S中定义。在这期间，head程序打开A20，打开pe、pg，废弃旧的、16位的中断响应机制，建立新的32位的IDT……这些工作都做完了，计算机已经处在32位的保护模式状态了，调用32位内核的一切条件已经准备完毕，这时顺理成章地调用main函数。后面的操作就可以用32位编译的main函数完成，从而正式启动内核，进入波澜壮阔的Linux内核操作系统之中。
总结 本文介绍了从按下电源开关至加载完毕BootLoader的整个过程，后续将继续分析从实模式进入保护模式，从而启动内核创建0号、1号、2号进程的整个过程。
]]></content>
  </entry>
  
  <entry>
    <title>Linux三剑客（grep,sed,awk）</title>
    <url>/post/linux/linux-three-musketeers.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Grep</tag>
      <tag>Sed</tag>
      <tag>Awk</tag>
    </tags>
    <content type="html"><![CDATA[在Linux系统中，awk、grep、sed等命令被广泛用于文本处理。它们是非常强大的命令行工具，可以用于搜索、替换、过滤、排序等多种操作。
本文将介绍这些命令的基本用法和示例，帮助读者更好地了解它们的功能和用法。
grep命令 grep是一种非常常见的文本搜索工具，它可以搜索指定字符串在一个或多个文件中出现的行，并将结果输出到标准输出。它的语法格式如下：
grep [OPTIONS] PATTERN [FILE...] 其中，OPTIONS表示选项，PATTERN表示要搜索的模式，FILE表示要搜索的文件名。
下面是一些grep命令的常用选项：
 -i：忽略大小写 -v：显示不匹配的行 -n：显示行号 -c：显示匹配行的数量 -r：递归搜索子目录 -e：搜索多个模式  下面是一些grep命令的实例：
在文件中搜索指定字符串 grep &#34;hello&#34; file.txt 在文件中搜索多个字符串 grep -e &#34;hello&#34; -e &#34;world&#34; file.txt 在文件中搜索并显示匹配行号 grep -n &#34;hello&#34; file.txt 在文件中搜索并显示不匹配的行 grep -v &#34;hello&#34; file.txt 在目录中递归搜索指定字符串 grep -r &#34;hello&#34; directory/ sed命令 sed是一种流编辑器，它可以执行各种文本操作，如替换、删除、插入等。它的语法格式如下：
sed [OPTIONS] COMMAND [FILE...] 其中，OPTIONS表示选项，COMMAND表示要执行的sed命令，FILE表示要处理的文件名。
下面是一些常用的sed命令：
 s：替换指定模式 d：删除指定行 i：插入指定字符串 c：替换指定行 y：字符转换 p：打印匹配的行  下面是一些sed命令的实例：
替换文件中的指定字符串 sed &#39;s/hello/world/&#39; file.txt 删除文件中的指定行 sed &#39;3d&#39; file.txt 在文件中指定行后插入指定字符串 sed &#39;2i\hello world&#39; file.txt 替换文件中指定行的内容 sed &#39;3c\hello world&#39; file.txt awk命令 awk是一种文本处理工具，它可以用于格式化、过滤、计算等操作。它的语法格式如下：
awk [OPTIONS] &#39;PATTERN { ACTION }&#39; [FILE...] 其中，OPTIONS表示选项，PATTERN表示要匹配的模式，ACTION表示要执行的操作，FILE表示要处理的文件名。
下面是一些常用的awk命令：
 print：打印指定内容 if：条件判断 for：循环结构 sum：计算指定  下面是一些awk命令的实例：
打印文件中的所有行 awk &#39;{print}&#39; file.txt 打印文件中第二列的内容 awk &#39;{print $2}&#39; file.txt 计算文件中所有数字的总和 awk &#39;{sum += $1} END {print sum}&#39; file.txt 打印文件中包含指定字符串的行 awk &#39;/hello/ {print}&#39; file.txt 在文件中指定列后面添加指定字符串 awk &#39;{$3 = $3 &#34;hello&#34;} {print}&#39; file.txt 以上是grep、sed、awk命令的基本语法和示例。这些命令可以通过选项和参数进行进一步定制和扩展。例如，grep命令可以通过-i选项忽略大小写，-r选项递归搜索子目录，-n选项显示行号等。
在实际应用中，这些命令通常会被结合使用。例如，可以使用grep命令搜索指定字符串，然后使用sed命令进行替换，最后使用awk命令进行计算和格式化。
此外，这些命令也可以通过管道符号（|）进行连接。例如，可以使用grep命令搜索指定字符串，然后将结果通过管道符号传递给sed命令进行替换，最后将结果再次通过管道符号传递给awk命令进行计算和格式化。
总之，grep、sed、awk等命令是Linux系统中非常重要和常用的文本处理工具。熟练掌握它们的基本语法和用法，可以大大提高文本处理的效率和质量。
]]></content>
  </entry>
  
  <entry>
    <title>如何在 Linux 中配置 IPv4 和 IPv6 地址</title>
    <url>/post/linux/how-to-configure-ipv4-and-ipv6.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>IPv4, IPv6</tag>
    </tags>
    <content type="html"><![CDATA[IPv4和IPv6是Internet上常用的两种IP地址协议。在Linux系统中，您可以通过配置网络接口来设置IPv4和IPv6地址。本文将详细介绍如何在Linux中配置IPv4和IPv6地址。
步骤 1：确定网络接口 在开始配置IP地址之前，您需要确定要配置的网络接口。执行以下命令来列出当前系统上的网络接口：
ifconfig -a 该命令将显示所有可用的网络接口及其相关信息。确定要配置的网络接口的名称，例如eth0或enp0s3。
步骤 2：配置 IPv4 地址 ###临时配置
要临时配置IPv4地址，可以使用ifconfig命令。执行以下命令来设置IPv4地址：
sudo ifconfig &lt;interface&gt; &lt;ipv4_address&gt; netmask &lt;netmask&gt; 将替换为要配置的网络接口的名称，&lt;ipv4_address&gt;替换为您要分配的IPv4地址，替换为子网掩码。
例如，要将IP地址为192.168.1.10，子网掩码为255.255.255.0的IPv4地址分配给eth0接口，执行以下命令：
sudo ifconfig eth0 192.168.1.10 netmask 255.255.255.0 永久配置 要永久配置IPv4地址，您需要编辑网络接口的配置文件。执行以下命令来打开配置文件：
sudo nano /etc/network/interfaces 在文件中找到要配置的接口部分，添加以下行：
auto &lt;interface&gt; iface &lt;interface&gt; inet static address &lt;ipv4_address&gt; netmask &lt;netmask&gt; gateway &lt;gateway_address&gt; 将替换为要配置的网络接口的名称，&lt;ipv4_address&gt;替换为您要分配的IPv4地址，替换为子网掩码，&lt;gateway_address&gt;替换为网关地址。
保存文件并关闭文本编辑器。然后，执行以下命令以使更改生效：
sudo systemctl restart networking 现在，您的Linux系统将使用配置的IPv4地址。
步骤 3：配置 IPv6 地址 临时配置 要临时配置IPv6地址，可以使用ifconfig命令。执行以下命令来设置IPv6地址：
sudo ifconfig &lt;interface&gt; inet6 add &lt;ipv6_address&gt;/&lt;prefix_length&gt; 将替换为要配置的网络接口的名称，&lt;ipv6_address&gt;替换为您要分配的IPv6地址，&lt;prefix_length&gt;替换为前缀长度。
例如，要将IPv6地址为2001:0db8:85a3:0000:0000:8a2e:0370:7334，前缀长度为64的IPv6地址分配给eth0接口，执行以下命令：
sudo ifconfig eth0 inet6 add 2001:0db8:85a3:0000:0000:8a2e:0370:7334/64 永久配置 要永久配置IPv6地址，您需要编辑网络接口的配置文件。执行以下命令来打开配置文件：
sudo nano /etc/network/interfaces 在文件中找到要配置的接口部分，添加以下行：
iface &lt;interface&gt; inet6 static address &lt;ipv6_address&gt;/&lt;prefix_length&gt; 将替换为要配置的网络接口的名称，&lt;ipv6_address&gt;替换为您要分配的IPv6地址，&lt;prefix_length&gt;替换为前缀长度。
保存文件并关闭文本编辑器。然后，执行以下命令以使更改生效：
sudo systemctl restart networking 现在，您的Linux系统将使用配置的IPv6地址。
步骤 4：验证配置 要验证IPv4和IPv6地址的配置是否成功，可以执行以下命令来查看网络接口的IP地址信息：
ifconfig &lt;interface&gt; 将&lt;interface&gt;替换为您配置的网络接口的名称。该命令将显示指定接口的IP地址信息，包括IPv4和IPv6地址。 结论 通过本文的指导，您已经学会了在Linux中配置IPv4和IPv6地址的详细步骤。根据您的网络需求，您可以临时或永久地配置这些地址。
具体的配置方式可能因Linux发行版和版本而有所不同。本文提供了一般的配置方法，但如果您的系统有特定的要求或网络环境，请参考相关文档或咨询系统管理员。
]]></content>
  </entry>
  
  <entry>
    <title>风河携手三星推进软件定义汽车演进</title>
    <url>/post/news/wind-river-and-samsung-drive-the-evolution-of-software-defined-vehicles.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Wind River</tag>
      <tag>Samsung</tag>
      <tag>SNV</tag>
    </tags>
    <content type="html"><![CDATA[领先的关键任务智能系统软件提供商风河公司宣布与三星电子系统LSI业务部建立新的合作关系。双方将致力于加速软件定义汽车的发展，开发高质量车载信息娱乐解决方案（IVI）、座舱监控和高级驾驶辅助系统。
汽车工业正在向软件定义汽车演进，越来越多的功能和特性都将由软件来驱动，并能快速简便地进行更新。确保开发人员拥有卓越的工具、流程和结构，有效实现软件的创建、测试与更新，这对整个行业来说皆属最高优先事项。
为了加速迭代演进，风河将基于三星Exynos Auto V920芯片组向客户提供自己的软件技术，创建一整套完全集成化的汽车软件与硬件解决方案，其中的核心是Wind River Helix™ Virtualization Platform（虚拟化平台）。这是一套获得安全认证的多核、多操作系统平台，支持最终用户融合不同安全性要求的运行时环境，包括VxWorks®实时操作系统(RTOS)、Linux和Android。
要设计和开发互联自动驾驶电动汽车，必须采用新的软件定义方法，以及高性能计算系统。我们与三星携手合作，支持OEM厂商和Tier 1供应商继承和发展过去三十多年来在航空航天与国防、工业、医疗和电信等诸多领域所积累的技术和经验，构建对未来软件定义车辆至关重要的软件系统，满足多种不同层次的关键性认证要求。
Exynos Auto V920是我们最新的5nm汽用处理器，提供强大的智能化性能和更高水平的车内体验，让汽车拥有更高的安全性。这套系统能够以高效节能的方式在多个虚拟机上同时运行多个应用，从而满足了行业的低功耗需求。通过与风河公司的密切合作，我们能够加强业界领先的IVI开发工作，并通过风河领先的运行时环境进一步扩展到多个安全关键领域。
]]></content>
  </entry>
  
  <entry>
    <title>英特尔宣布，出售设备公司股份</title>
    <url>/post/news/intel-announces-sale-of-stake-IMS.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Intel</tag>
      <tag>IMS</tag>
      <tag>Bain Capital</tag>
    </tags>
    <content type="html"><![CDATA[英特尔公司今天宣布，已同意将其 IMS Nanofabrication GmbH (“IMS”) 业务约 20% 的股份出售给 Bain Capital Special Situations（“贝恩资本”）处理的一项交易中，IMS 的估值约为 43 亿美元。
该交易预计将于 2023 年第三季度完成。IMS 将作为独立子公司运营，并继续由首席执行官 Elmar Platzgummer 博士领导。
英特尔方面透露：“考虑到估值水平和投资水平，这将成为我们有史以来最好的收购之一。”
自 2015 年发明多电子束技术并推出首款商用多光束掩模写入器以来，总部位于奥地利维也纳的 IMS 一直是先进技术节点多光束掩模写入领域的行业领导者。英特尔最初于 2009 年投资 IMS，并最终于 2015 年收购了该业务。自收购以来，IMS 为英特尔带来了显著的投资回报，同时将其员工和生产能力增长了四倍，并交付了另外三代产品。
如今，随着 EUV 技术在前沿技术中得到广泛采用，创建高级 EUV（极紫外光刻）掩模所需的多光束掩模写入工具已成为半导体制造生态系统中越来越重要的组成部分。这项投资将使 IMS 能够通过加速创新和实现更深入的跨行业合作来抓住多光束掩模写入工具的重要市场机会。
企业发展高级副总裁 Matt Poirier 表示：“光刻技术的进步对于推动半导体行业的持续进步至关重要，而掩模写入在行业向新图案技术（例如高数值孔径 EUV）过渡中发挥着核心作用。”在英特尔。“贝恩资本的投资和合作将为 IMS 提供更大的独立性，并带来战略视角，帮助加速下一阶段的光刻技术创新，最终使整个生态系统受益。”
Platzgummer 表示：“我们很高兴能获得贝恩资本这一宝贵的合作伙伴，贝恩资本在与企业合作推动增长和价值创造方面拥有悠久的历史。随着 EUV 变得更加普遍和高效，他们与我们一样坚信 IMS 面临着有意义的机遇。High-NA EUV 在本世纪下半叶从开发转向大批量制造。我们期待扩大我们的能力，为世界上最大的芯片生产商提供支持，他们依靠我们的技术来生产当前和下一代半导体产品。
贝恩资本 (Bain Capital) 合伙人 Marvin Larbi-Yeboa 表示：“作为半导体制造和纳米技术行业新兴技术的全球领导者和创新者，我们相信 IMS 处于有利地位，能够利用随着芯片产能增加而带来的有吸引力的长期有利因素在线并建立其领先的竞争地位、技术差异化和尖端的产品能力。”
贝恩资本董事总经理 Will Tetler 补充道：“我们期待与 IMS 卓越的管理团队和英特尔合作，利用我们深厚的行业经验和价值创造能力，通过进一步投资支持业务的长期增长战略其领先的技术和产品组合使 IMS 能够扩大其竞争市场地位。”
]]></content>
  </entry>
  
  <entry>
    <title>STM32的完整启动流程分析</title>
    <url>/post/mcu/STM32-whole-boot-up-process-analysis.html</url>
    <categories><category>MCU</category>
    </categories>
    <tags>
      <tag>STM32</tag>
      <tag>boot up</tag>
      <tag>process</tag>
    </tags>
    <content type="html"><![CDATA[关于STM32的启动流程，网上有的资料在讨论几种boot模式，有的在回答启动文件的内容，在查阅了很多资料后，本文给出一个比较全面的总结和回答。
根据boot引脚决定三种启动模式 复位后，在 SYSCLK 的第四个上升沿锁存 BOOT 引脚的值。BOOT0 为专用引脚，而 BOOT1 则与 GPIO 引脚共用。一旦完成对 BOOT1 的采样，相应 GPIO 引脚即进入空闲状态，可用于其它用途。BOOT0与BOOT1引脚的不同值指向了三种启动方式：
  从主Flash启动。主Flash指的是STM32的内置Flash。选择该启动模式后，内置Flash的起始地址将被重映射到0x00000000地址，代码将在该处开始执行。一般我们使用JTAG或者SWD模式下载调试程序时，就是下载到这里面，重启后也直接从这启动。
  从系统存储器启动。系统储存器指的是STM32的内置ROM，选择该启动模式后，内置ROM的起始地址将被重映射到0x00000000地址，代码在此处开始运行。ROM中有一段出厂预置的代码，这段代码起到一个桥的作用，允许外部通过UART/CAN或USB等将代码写入STM32的内置Flash中。这段代码也被称为ISP(In System Programing)代码，这种烧录代码的方式也被称为ISP烧录。关于ISP、ICP和IAP之间的区别将在后续章节中介绍。
  从嵌入式SRAM中启动。显然，该方法是在STM32的内置SRAM中启动，选择该启动模式后，内置SRAM的起始地址将被重映射到0x00000000地址，代码在此处开始运行。这种模式由于烧录程序过程中不需要擦写Flash，因此速度较快，适合调试，但是掉电丢失。
  总结：上面的每一种启动方式我都描述了“xxx的起始地址被重映射到了0x00000000地址，从而代码从xxx开始启动”，如下图是STM32F4xx中文参考手册中的图，可以看到类似的表述。同时，在下图中也展示了STM32F4xx中统一编址下，各内存的地址分配，注意一点，即使相应的内存被映射到了0x00000000起始的地址，通过其原来地址依然是可以访问的。
启动后bootloader做了什么？ 根据BOOT引脚确定了启动方式后，处理器进行的第二大步就是开始从0x00000000地址处开始执行代码，而该处存放的代码正是bootloader。
bootloader，也可以叫启动文件，无论性能高下，结构简繁，价格贵贱，每一种微控制器(处理器)都必须有启动文件，启动文件的作用便是负责执行微控制器从“复位”到“开始执行main函数”中间这段时间(称为启动过程)所必须进行的工作。最为常见的51，AVR或MSP430等微控制器当然也有对应启动文件，但开发环境往往自动完整地提供了这个启动文件，不需要开发人员再行干预启动过程，只需要从main函数开始进行应用程序的设计即可。同样，STM32微控制器，无论是keiluvision4还是IAR EWARM开发环境，ST公司都提供了现成的直接可用的启动文件。
网上有很多资料分析了STM32的启动文件的内容，在此我只进行简单的表述。启动文件中首先会定义堆栈，定义中断/异常向量表，而其中只实现了复位的异常处理函数Reset_Handler，该函数内容如下(STM32F4XX，IAR编译器)，可以看到其主要执行了SystemInit和__iar_program_start两个函数，其主要功能除了初始化时钟，FPU等，还会执行一个重要功能，那就是内存的搬移、初始化操作。 这是我想重点介绍的内容，同时也会回答一个疑问，就是如果从Flash启动的话，代码究竟是运行在哪儿的？在我之前接触ARM9、CortexA系列的时候，一般都是把代码搬到内部的SRAM或者外部DDR中执行的，STM32是如何呢？答案下一小节揭晓。
bootloader中对内存的搬移和初始化 本节针对程序在内置Flash中启动的情况进行分析。
我们知道烧录的镜像文件中包含只读代码段.text，已初始化数据段.data和未初始化的或者初始化为0的数据段.bss。代码段由于是只读的，所以是可以一直放在Flash中，CPU通过总线去读取代码执行就OK，但是.data段和.bss段由于会涉及读写为了，为了更高的读写效率是要一定搬到RAM中执行的，因此bootloader会执行很重要的一步，就是会在RAM中初始化.data和.bss段，搬移或清空相应内存区域。
因此我们知道，当启动方式选择的是从内置Flash启动的时候，代码依旧是在Flash中执行，而数据则会被拷贝到内部SRAM中，该过程是由bootloader完成的。bootloader在完成这些流程之后，就会将代码交给main函数开始执行用户代码。
  现在让我们思考一个问题，PC机在运行程序的时候将程序从外存（硬盘）中，调入到RAM中运行，CPU从RAM中读取程序和数据；而单片机的程序则是固化在Flash中，CPU运行时直接从Flash中读取程序，从RAM中读取数据，那么PC机能从Flash之类的存储介质中直接读代码执行吗？
  答案是不行。因为x86构架的CPU是基于冯.诺依曼体系的，即数据和程序存储在一起，而且PC机的RAM资源相当丰富，从几十M到几百M甚至是几个G，客观上能够承受大量的程序数据。但是单片机的构架大多是哈弗体系的，即程序和数据分开存储，而且单片的片内RAM资源是相当有限的，内部的RAM过大会带来成本的大幅度提高。
  ISP、IAP、ICP三种烧录方式 虽然这个小节稍稍偏题，但是由于上面在3中启动方式中介绍过了ISP烧录，因此一并在此介绍剩下的两种烧录方式。
  ICP(In Circuit Programing)。在电路编程，可通过CPU的Debug Access Port 烧录代码，比如ARM Cortex的Debug Interface主要是SWD(Serial Wire Debug)或JTAG(Joint Test Action Group)；
  ISP(In System Programing)。在系统编程，可借助MCU厂商预置的Bootloader 实现通过板载UART或USB接口烧录代码。
  IAP(In Applicating Programing)。在应用编程，由开发者实现Bootloader功能，比如STM32存储映射Code分区中的Flash本是存储用户应用程序的区间（上电从此处执行用户代码），开发者可以将自己实现的Bootloader存放到Flash区间，MCU上电启动先执行用户的Bootloader代码，该代码可为用户应用程序的下载、校验、增量/补丁更新、升级、恢复等提供支持，如果用户代码提供了网络访问功能，IAP 还能通过无线网络下载更新代码，实现OTA空中升级功能。
  IAP和ISP 的区别。
   ISP程序一般是芯片厂家提供的。IAP一般是用户自己编写的 ISP一般支持的烧录方式有限，只有串口等。IAP就比较灵活，可以灵活的使用各种通信协议烧录 isp一般需要芯片进行一些硬件上的操作才行，IAP全部工作由程序完成，不需要去现场 isp一般只需要按格式将升级文件通过串口发送就可以。IAP的话控制相对麻烦，如果是OTA的话还需要编写后台的。 注意，这里介绍的bootloader功能显然跟之前介绍的启动文件bootloader有所区别，其目的是为了能接受外部镜像进行烧录，而不是为了运行普通用户程序。 ]]></content>
  </entry>
  
  <entry>
    <title>什么是大数据时代</title>
    <url>/post/news/what-is-the-era-of-big-data.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Big Data</tag>
    </tags>
    <content type="html"><![CDATA[最早提出&quot;大数据&quot;时代到来的是全球知名咨询公司麦肯锡，麦肯锡称:&ldquo;数据，已经渗透到当今每一个行业和业务职能领域，成为重要的生产因素。
人们对于海量数据的挖掘和运用，预示着新一波生产率增长和消费者盈余浪潮的到来。&rdquo; &ldquo;大数据&quot;在物理学、生物学、环境生态学等领域以及军事、金融、通讯等行业存在已有时日，却因为近年来互联网和信息行业的发展而引起人们关注。简单来说：大量数据 + 云计算 = 大数据时代
大数据特征 ###数据量大(Volume)
第一个特征是数据量大。大数据的起始计量单位至少是P(1000个T)、E(100万个T)或Z(10亿个T)。
类型繁多(Variety) 第二个特征是数据类型繁多。包括网络日志、音频、视频、图片、地理位置信息等等，多类型的数据对数据的处理能力提出了更高的要求。
价值密度低(Value) 第三个特征是数据价值密度相对较低。如随着物联网的广泛应用，信息感知无处不在，信息海量，但价值密度较低，如何通过强大的机器算法更迅速地完成数据的价值&quot;提纯&rdquo;，是大数据时代亟待解决的难题。
速度快、时效高(Velocity) 第四个特征是处理速度快，时效性要求高。这是大数据区分于传统数据挖掘最显著的特征。
既有的技术架构和路线，已经无法高效处理如此海量的数据，而对于相关组织来说，如果投入巨大采集的信息无法通过及时处理反馈有效信息，那将是得不偿失的。可以说，大数据时代对人类的数据驾驭能力提出了新的挑战，也为人们获得更为深刻、全面的洞察能力提供了前所未有的空间与潜力。
那么数据生活距离我们遥远吗？ 正相反，数据与我们日常生活的联系从未如此紧密过，从没有像今天如此活跃，具体的记录着人类与世界。从最初的计算机，摄像头到家用计算机，智能手机，再到大数据和人工智能，我们不断升级采集和利用数据的方式。而现在，从一辆车的每日碳排放量统计到全球气温的检测，从预测个人在网上喜好分析到总统选举时投票趋势的预测，我们都可以做到。
数据将人与人，人与世界连接起来，构成一张繁密的网络，每个人都在影响世界，又在被他人影响着。传统的统计方法已经无法处理这种相互影响的数据，这么办？答案是让机器自己来处理数据，从数据中习得知识。
这便是当代人工智能的本质。与传统的数据记录定义不同，这种数据是有“生命”的。它更像是我们身体的一种自然延伸：聆听我们的声音，拓宽我们的视野，加深我们的记忆，甚至组成一个以数据形式存在的“我”。
生活中的大数据很多，以下是几个例子：  互联网搜索：每天有数百万的搜索请求，包含了大量的关键字和查询信息。通过分析这些数据，搜索引擎可以优化搜索算法，提高搜索结果的准确性。 电子商务：在线购物网站产生了大量的数据，包括用户浏览、购买、评论、评分等信息。通过分析这些数据，商家可以更好地了解消费者需求，优化产品和服务。 社交媒体：社交媒体如Facebook、Twitter、微信等产生了海量的数据，包括用户关系、兴趣爱好、社交网络等。通过分析这些数据，企业可以了解消费者的行为和需求，优化宣传和营销策略。 医疗健康：医疗领域的大数据可以帮助医生更好地诊断疾病、预测疾病风险、优化治疗方案等。例如，通过分析患者的基因组数据，可以提前预测某些疾病的发作可能性，及时采取干预措施。 交通运输：现代交通系统中产生了大量的数据，包括车辆位置、速度、路况、交通流量等。通过分析这些数据，交通运输管理者可以更好地规划交通、提高交通效率，优化城市交通管理。  大数据的利与弊可以概括为以下几点： 利：   提供更准确的信息：大数据可以提供海量、多样化、实时的数据，帮助企业和政府更好地了解市场、用户需求、社会趋势等，从而做出更准确的决策。
  优化产品和服务：通过分析大数据，企业可以了解消费者的需求和习惯，优化产品和服务，提高用户满意度和忠诚度。
  提高效率和生产力：大数据可以优化生产流程、提高生产效率，让企业更快地响应市场变化，提高生产力。
  弊：   隐私问题：大数据涵盖了大量的个人信息和数据，如果这些数据被不法分子获取，就会造成极大的隐私泄露风险。
  误导性：大数据中有时会出现伪相关关系，需要进行深入的数据分析和挖掘，否则可能会引起误导。
  质量问题：大数据中可能包含有误的、不准确的数据，这会影响到对数据的分析和应用。
  技术门槛高：大数据的处理需要高级的技术和工具，这会导致技术门槛较高，对于一些小型企业和普通用户来说比较困难。
  总之，大数据虽然带来了很多的机遇和优势，但也面临着一些挑战和风险，需要我们在使用大数据时保持警惕和谨慎。
]]></content>
  </entry>
  
  <entry>
    <title>VXLAN：数据中心网络的未来</title>
    <url>/post/datacenter/vxlan-the-future-of-data-center.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>VXLAN</tag>
    </tags>
    <content type="html"><![CDATA[随着云计算和虚拟化技术的快速发展，数据中心网络正面临着越来越大的挑战。
传统的网络架构在适应大规模数据中心的需求方面存在一些限制，如扩展性、隔离性和灵活性等方面。为了克服这些限制，并为数据中心网络提供更好的性能和可扩展性， VXLAN  （Virtual Extensible LAN）作为一种新兴的网络虚拟化技术应运而生。本文将详细介绍VXLAN的工作原理、优势以及在数据中心网络中的应用，探讨VXLAN作为数据中心网络的未来发展趋势。
VXLAN概述 VXLAN是一种网络虚拟化技术，旨在解决传统以太网的限制，并提供更好的可扩展性和隔离性。VXLAN通过在现有的IP网络上创建一个虚拟的二层网络，将传统的以太网帧封装在UDP报文中进行传输。这种封装使得VXLAN可以在现有的网络基础设施上运行，而无需对网络进行大规模改造。
VXLAN使用一个24位的VXLAN标识符（VNI）来标识虚拟网络，允许同时存在多个独立的虚拟网络。VXLAN报文的目的地MAC地址被替换为VXLAN网络中的虚拟机或物理主机的MAC地址，从而实现虚拟机之间的通信。VXLAN还支持多路径传输（MP-BGP EVPN）以及网络中的多租户隔离。
VXLAN工作原理 VXLAN的工作原理可以简单地分为封装和解封装两个过程。
  封装：当虚拟机（VM）发送一个以太网帧时，VXLAN模块将这个以太网帧封装在一个UDP报文中。报文的源IP地址是VM所在主机的IP地址，目的IP地址是VXLAN隧道的远程端点的IP地址。VXLAN头中的VNI字段标识了目标虚拟网络。随后，UDP报文被发送到底层网络中，到达目标主机。
  解封装：当接收到一个VXLAN报文时，VXLAN模块会解析UDP报文头，提取出封装的以太网帧。通过查找VNI字段，VXLAN模块可以确定目标虚拟网络，并将以太网帧发送到相应的虚拟机或物理主机。
  这种封装和解封装的过程使得VXLAN可以在底层网络上透明地传输以太网帧，同时提供了逻辑上隔离的虚拟网络。
VXLAN的优势 VXLAN作为一种新兴的网络虚拟化技术，在数据中心网络中具有以下优势：
  可扩展性：VXLAN使用24位的VNI标识符，可以支持高达16,777,216个虚拟网络，每个虚拟网络都可以拥有独立的二层命名空间。这种可扩展性使得VXLAN能够满足大规模数据中心的需求，并支持多租户隔离。
  跨子网通信：传统以太网在跨越不同子网时需要依赖于三层路由器进行转发。而VXLAN通过使用底层IP网络作为传输介质，可以实现虚拟网络的跨子网通信，使得虚拟机可以自由迁移而无需改变IP地址。
  灵活性：VXLAN可以在现有的网络基础设施上运行，无需进行大规模的网络改造。它可以与现有的网络设备和协议兼容，如交换机、路由器和BGP等。这种灵活性使得组建和管理虚拟网络变得更加简单和高效。
  多路径传输：VXLAN结合了多路径传输（MP-BGP EVPN）的特性，可以在数据中心网络中实现负载均衡和冗余。它可以根据网络负载和路径可用性来选择最佳的路径进行数据传输，提供更好的性能和可靠性。
  安全性：VXLAN支持隧道加密，可以在底层IP网络上保护数据的机密性和完整性。通过使用安全协议（如IPsec）或虚拟专用网络（VPN），VXLAN可以提供更高级别的数据传输安全。
  VXLAN在数据中心网络中的应用 VXLAN在数据中心网络中有广泛的应用场景，以下是其中一些典型的应用：
  虚拟机迁移：VXLAN使得虚拟机可以自由地在不同的物理主机之间迁移，而无需改变IP地址。这种灵活性和可扩展性对于实现数据中心的负载均衡、资源调度和容错性非常重要。
  多租户隔离：通过使用不同的VNI，VXLAN可以将数据中心划分为多个独立的虚拟网络，实现不同租户之间的隔离。这种隔离性保证了租户之间的数据安全和隐私，并且可以为每个租户提供独立的网络策略和服务质量保证。
  跨数据中心连接：VXLAN可以扩展到跨多个数据中心的网络环境中，使得不同数据中心之间可以建立虚拟网络连接。这种功能可以支持数据中心间的资源共享、业务扩展和灾备备份等需求。
  云服务提供商：VXLAN可以帮助云服务提供商构建高度可扩展的虚拟化网络基础设施。通过使用VXLAN，云服务提供商可以提供灵活的虚拟网络服务，并支持多租户环境下的资源隔离和安全性。
  虚拟网络功能（VNF）：VXLAN与网络功能虚拟化（NFV）相结合，可以实现虚拟网络功能的部署和管理。VXLAN可以作为底层网络虚拟化技术，为VNF提供灵活的网络连接和隔离，从而实现网络功能的快速部署和弹性扩展。
  结论 VXLAN作为一种新兴的数据中心网络虚拟化技术，具有强大的可扩展性、灵活性和隔离性，为数据中心网络的未来发展提供了新的方向和解决方案。通过使用VXLAN，数据中心可以实现虚拟机迁移、多租户隔离、跨数据中心连接以及云服务提供商的支持。VXLAN的工作原理和优势使其成为构建高性能、可靠和安全的数据中心网络的关键技术之一。
随着云计算和大数据应用的不断发展，数据中心网络将继续面临更多的挑战和需求。VXLAN作为数据中心网络的未来发展趋势之一，将继续演进和完善，以满足不断变化的业务需求，并推动数据中心网络的创新和发展。
]]></content>
  </entry>
  
  <entry>
    <title>Linux内核中使用的C语言技巧</title>
    <url>/post/programming/c-language-tricks-used-in-the-linux-kernel.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>C</tag>
      <tag>Embedded</tag>
      <tag>Struct</tag>
    </tags>
    <content type="html"><![CDATA[下面是Linux内核中常常使用到的C语言技巧，比较实用，小伙伴们学起来！注意需要GCC编译器才支持这些特性。
typeof的使用 下面是我们常用的返回最大值宏定义，这个写法存在一些问题。
#define max(a,b) ((a) &gt; (b) ? (a) : (b)) 如果a传入i++，b传入j++，那么这个比较大小就会出错。例如：
#define max(a,b) ((a)&gt;(b)?(a):(b))  int x = 1, y = 2; printf(&#34;max=%d\n&#34;, max(x++, y++)); printf(&#34;x = %d, y = %d\n&#34;, x, y); 上面代码输出：max=2，x=2，y=4，结果是错误。为修改此宏，可以定义一个变量将a和b的值分别赋给该变量，并将该变量作为参数传递给max宏进行比较。在GNU C语言中，可以使用以下代码实现：
#define max(a,b) ({ \ int _a = (a); \  int _b = (b); \ _a &gt; _b ? _a : _b; }) 如果不知道具体的数据类型，可以使用typeof类转换宏，Linux内核中的例子：
#define max(a, b) ({ \ typeof(a) _a = (a); \ typeof(b) _b = (b); \ (void) (&amp;_a == &amp;_b); \ _a &gt; _b ? _a : _b; })  typeof(a) _a = (a):定义一个a类型的变量_a，将a赋值给_a typeof(b) _b = (b):定义一个b类型的变量_b，将b赋值给_b (void) (&amp;_a == &amp;_b):判断两个数的类型是否相同，如果不相同，会抛出一个警告。因为a和b的类型不一样，其指针类型也会不一样，两个不一样的指针类型进行比较操作，会抛出警告。  typeof用法举例：
//typeof的参数可以是表达式或类型  //参数是类型 typeof(int *) a,b;//等价于：int *a,*b;  //参数是表达式 int foo(); typeof(foo()) var;//声明了int类型的var变量，因为表达式foo()是int类型的。由于表达式不会被执行，所以不会调用foo函数。 柔性数组 柔性数组，也称为零长数组，主要用于变长结构体。因此，它有时被称为变长数组。使用方法是在结构体的末尾声明一个长度为0的数组，从而使该结构体具有可变长度。对于编译器来说，长度为0的数组不占用空间，因为数组名本身只是一个偏移量，代表了一个不可修改的地址常量符号。
结构体中定义零长数组：
&lt;mm/percpu.c&gt; struct pcpu_chunk { struct list_head list; unsigned long populated[]; /* 变长数组 */ }; 数据结构最后一个元素被定义为零长度数组，不占结构体空间。这样，我们可以根据对象大小动态地分配结构的大小。
struct line { int length; char contents[0]; }; struct line *thisline = malloc(sizeof(struct line) + this_length); thisline-&gt;length = this_length; 如上例所示，struct line数据结构定义了一个int length变量和一个变长数组contents[0]，这个struct line数据结构的大小只包含int类型的大小，不包含contents的大小，也就是sizeof (struct line) = sizeof (int)。
创建结构体对象时，可根据实际的需要指定这个可变长数组的长度，并分配相应的空间，如上述实例代码分配了this_length 字节的内存，并且可以通过contents[index]来访问第index个地址的数据。
case范围 GNU C语言支持指定一个case的范围作为一个标签，如：
case low ...high: case &#39;A&#39; ...&#39;Z&#39;: 这里low到high表示一个区间范围，在ASCII字符代码中也非常有用。下面是Linux内核中的代码例子。
&lt;arch/x86/platform/uv/tlb_uv.c&gt; static int local_atoi(const char *name){ int val = 0; for (;; name++) { switch (*name) { case &#39;0&#39; ...&#39;9&#39;: val = 10*val+(*name-&#39;0&#39;); break; default: return val; } } } 另外，还可以用整形数来表示范围，但是这里需要注意在“&hellip;”两边有空格，否则编译会出错。
&lt;drivers/usb/gadget/udc/at91_udc.c&gt; static int at91sam9261_udc_init(struct at91_udc *udc){ for (i = 0; i &lt; NUM_ENDPOINTS; i++) { ep = &amp;udc-&gt;ep[i]; switch (i) { case 0: ep-&gt;maxpacket = 8; break; case 1 ... 3: ep-&gt;maxpacket = 64; break; case 4 ... 5: ep-&gt;maxpacket = 256; break; } } } 标号元素 GNU C语言可以通过指定索引或结构体成员名来初始化，不必按照原来的固定顺序进行初始化。
结构体成员的初始化在 Linux 内核中经常使用，如在设备驱动中初始化file_operations数据结构：
&lt;drivers/char/mem.c&gt; static const struct file_operations zero_fops = { .llseek = zero_lseek, .read = new_sync_read, .write = write_zero, .read_iter = read_iter_zero, .aio_write = aio_write_zero, .mmap = mmap_zero, }; 如上述代码中的zero_fops的成员llseek初始化为zero_lseek函数，read成员初始化为new_sync_read函数，依次类推。当file_operations数据结构的定义发生变化时，这种初始化方法依然能保证已知元素的正确性，对于未初始化成员的值为0或者NULL。
可变参数宏 在GNU C语言中，宏可以接受可变数目的参数，主要用在输出函数里。例如：
&lt;include/linux/printk.h&gt; #define pr_debug(fmt, ...) \ dynamic_pr_debug(fmt, ##__VA_ARGS__) “&hellip;”代表一个可以变化的参数表，“VA_ARGS”是编译器保留字段，预处理时把参数传递给宏。当宏的调用展开时，实际参数就传递给dynamic_pr_debug函数了。
UL 的使用 在Linux内核代码中，我们经常会看到一些数字的定义使用了UL后缀修饰。
数字常量会被隐形定义为int类型，两个int类型相加的结果可能会发生溢出。
因此使用UL强制把int类型数据转换为unsigned long类型，这是为了保证运算过程不会因为int的位数不同而导致溢出。
 1 ：表示有符号整型数字1 UL：表示无符号长整型数字1 ]]></content>
  </entry>
  
  <entry>
    <title>单片机基础概念：指令、数位、字节、存储器、总线</title>
    <url>/post/mcu/basic-concept-of-micro-processor-unit.html</url>
    <categories><category>MCU</category>
    </categories>
    <tags>
      <tag>ROM</tag>
      <tag>BUS</tag>
      <tag>Data Bit</tag>
    </tags>
    <content type="html"><![CDATA[本文介绍单片机基础概念：指令、数位、字节、存储器、总线
执行指令 我们来思考一个问题，当我们在编程器中把一条指令写进单片机内部，然后取下单片机，单片机就可以执行这条指令。
那么这条指令一定保存在单片机的某个地方，并且这个地方在单片机掉电后依然可以保持这条指令不会丢失，这是个什么地方呢？这个地方就是单片机内部的只读存储器即ROM(READ ONLY MEMORY)。
为什么称它为只读存储器呢？刚才我们不是明明把两个数字写进去了吗？原来在89C51中的ROM是一种电可擦除的ROM，称为FLASH ROM，刚才我们是用的编程器，在特殊的条件下由外部设备对ROM进行写的操作，在单片机正常工作条件下，只能从那面读，不能把数据写进去，所以我们还是把它称为ROM。
数的本质和物理现象 我们知道，计算机可以进行数学运算，这令我们非常难以理解，它们只是一些电子元器件，怎么可以进行数学运算呢？
我们人类做数学题如37+45是这样做的，先在纸上写37，然后在下面写45，然后大脑运算最后写出结果，运算的原材料是37和45，结果是82都是写在纸上的，计算机中又是放在什么地方呢？
为了解决这个问题，先让我们做一个实验：这里有一盏灯，我们知道灯要么亮，要么不亮，就有两种状态，我们可以用‘0’和‘1’来代替这两种状态：规定亮为‘1’、不亮为‘0’。
现在放上三盏灯，一共有几种状态呢？我们列表来看一下：000 / 001 / 010 / 011 / 100 / 101 / 110 / 111。我们来看，这个000 / 001 / 101 不就是我们学过的的二进制数吗？本来，灯的亮和灭只是一种物理现象，可当我们把它们按一定的顺序排好后，灯的亮和灭就代表了数字了。让我们再抽象一步，灯为什么会亮呢？是因为输出电路输出高电平，给灯通了电。因此，灯亮和灭就可以用电路的输出是高电平还是低电平来替代了。这样，数字就和电平的高、低联系上了。
数位的含义 通过上面的实验我们已经知道：一盏灯亮或者说一根线的电平的高低，可以代表两种状态：0和1，实际上这就是一个二进制位。
因此我们就把一根线称之为一“位”，用BIT表示。
一根线可以表示0和1，两根线可以表达00 / 01 / 10 / 11四种状态，也就是可以表达0~3，而三根可以表达0~7，计算机中通常用8根线放在一起，同时计数，就可以表示0~255一共256种状态。
这8根线或者8位就称之为一个字节(BYTE)。
存储器的构造 存储器就是用来存放数据的地方。它是利用电平的高低来存放数据的，也就是说，它存放的实际上是电平的高、低，而不是我们所习惯认为的1234这样的数字，这样，我们的一个谜团就解开了。
一个存储器就象一个个的小抽屉，一个小抽屉里有八个小格子，每个小格子就是用来存放“电荷”的，电荷通过与它相连的电线传进来或释放掉。至于电荷在小格子里是怎样存的，就不用我们操心了，你可以把电线想象成水管，小格子里的电荷就象是水，那就好理解了。存储器中的每个小抽屉就是一个放数据的地方，我们称之为一个“单元”。
有了这么一个构造，我们就可以开始存放数据了，想要放进一个数据12，也就是00001100，我们只要把第二号和第三号小格子里存满电荷，而其它小格子里的电荷给放掉就行了。
可问题出来了，一个存储器有好多单元，线是并联的，在放入电荷的时候，会将电荷放入所有的单元中，而释放电荷的时候，会把每个单元中的电荷都放掉。这样的话，不管存储器有多少个单元，都只能放同一个数，这当然不是我们所希望的。因此，要在结构上稍作变化。
需要在每个单元上有个控制线，想要把数据放进哪个单元，就把一个信号给这个单元的控制线，这个控制线就把开关打开，这样电荷就可以自由流动了。而其它单元控制线上没有信号，所以开关不打开，不会受到影响。
这样，只要控制不同单元的控制线，就可以向各单元写入不同的数据了。同样，如果要从某个单元中取数据，也只要打开相应的控制开关就行了。
存储器的译码 那么，我们怎样来控制各个单元的控制线呢？这个还不简单，把每个单元的控制线都引到集成电路的外面不就行了吗？
事情可没那么简单，一片27512存储器中有65536个单元，把每根线都引出来，这个集成电路就得有6万多个脚？不行，怎么办？要想法减少线的数量。
有一种方法称这为译码，简单介绍一下：一根线可以代表2种状态，2根线可以代表4种状态，3根线可以代表8种，256种状态又需要几根线代表？8根线，所以65536种状态我们只需要16根线就可以代表了。
存储器的选片概念 至此，译码的问题解决了，让我们再来关注另外一个问题。送入每个单元的八根线是用从什么地方来的呢？它就是从计算机上接过来的，一般地，这八根线除了接一个存储器之外，还要接其它的器件。
这样问题就出来了，这八根线既然不是存储器和计算机之间专用的，如果总是将某个单元接在这八根线上，就有问题出现了：比如这个存储器单元中的数值是0FFH另一个存储器的单元是00H，那么这根线到底是处于高电平，还是低电平？怎样分辨？
办法很简单，当外面的线接到集成电路的引脚进来后，不直接接到各单元去，中间再加一组开关就行了。平时我们让开关打开着，如果确实是要向这个存储器中写入数据，或要从存储器中读出数据，再让开关接通就行了。
这组开关由三根引线选择：读控制端、写控制端和片选端。要将数据写入片中，先选中该片，然后发出写信号，开关就合上了，并将传过来的数据(电荷)写入片中。如果要读，先选中该片，然后发出读信号，开关合上，数据就被送出去了。
读和写信号同时还接入到另一个存储器，但是由于片选端不同，所以虽有读或写信号，但没有片选信号，所以另一个存储器不会“误会”而开门，造成冲突。那么会不同时选中两片芯片呢？
只要是设计好的系统就不会，因为它是由计算控制的，而不是我们人来控制的，如果真的出现同时出现选中两片的情况，那就是电路出了故障了，这不在我们的讨论之列。
总线概念 从上面的介绍中我们已经看到，用来传递数据的八根线并不是专用的，而是很多器件大家共用的。
所以我们称之为数据总线，总线英文名为BUS，总即公交车道，谁也可以走。而十六根地址线也是连在一起的，称之为地址总线。
]]></content>
  </entry>
  
  <entry>
    <title>CPU、MPU、MCU和SOC的简介</title>
    <url>/post/mcu/introduction-of-cpu-mpu-mcu-soc.html</url>
    <categories><category>MCU</category>
    </categories>
    <tags>
      <tag>CPU</tag>
      <tag>MPU</tag>
      <tag>MCU</tag>
      <tag>SOC</tag>
    </tags>
    <content type="html"><![CDATA[在嵌入式开发中，我们会经常看到或接触一些专业术语，例如CPU、MPU、MCU和SOC等，并且这些专业术语出现的频率也是非常之高，在面试中也常常会作为提问的知识点，下面我们就来看一下他们之间的特点和区别。
CPU CPU是Central Processing Unit的缩写，计算机的运算控制核心就是CPU。CPU是由运算器、控制器和寄存器及相应的总线构成。众所周知的三级流水线：取址、译码、执行的对象就是CPU，CPU从存储器或高速缓冲存储器中取出指令，放入指令寄存器，并对指令译码，然后执行指令。而计算机的可编程性其实就是指对CPU的编程。
MPU MPU是Micro Processor Unit的缩写，指微处理器（这里要注意不是微控制器，很多人会把微处理器和微控制器混淆），微处理器通常代表功能强大的CPU（可理解为增强型的CPU），这种芯片往往是计算机和高端系统的核心CPU。例如嵌入式开发者最熟悉的 ARM  的Cortex-A芯片，他们都属于MPU。
MCU MCU是Micro Control Unit的缩写，指微控制器。随着大规模集成电路的出现及发展，把计算机的CPU、RAM、ROM、定时器和输入输出I/O引脚集成在一个芯片上，比如51，STC、Cortex-M这些芯片，它们的内部除了CPU外还包含了RAM和ROM，可直接添加简单的器件（电阻，电容）等构成最小系统就可以运行代码了。而像ARM（Cortex-A系列）直接放代码是运行不了的，因为它本质上只是增强版的CPU，必须添加相应的RAM和ROM。
SOC SOC是System on Chip的缩写，指的是片上系统。可以这样对比来看：MCU只是芯片级的芯片，而SOC是系统级的芯片，它集成了MCU和MPU的优点，即拥有内置RAM和ROM的同时又像MPU那样强大，它可以存放并运行系统级别的代码，也就是说可以运行操作系统（以Linux OS为主）
另外，SOPC也是一个值得了解的概念，与上述几项概念相比，SOPC的出现频率并不是那么高，但这并不影响它的重要性。SOPC是System On a Programmable Chip的缩写，即 可编程片上系统，SOPC与MCU、MPU、SOC最明显的区别在于：可更改硬件配置，也就是说自己构造芯片。
举个例子说明便于理解，单片机的硬件配置是固化好了的， 我们能够编程修改的就是软件配置，本来是串口通信功能，通过修改代码变成AD采样功能，也就是说硬件配置是固定了的，我们只能通过修改软件来选择其中的一项或多项功能；而SOPC可以修改硬件配置信息使其成为相应的芯片，可以是MCU，也可以是SOC。
结语 在嵌入式开发中，接触频率较多的一般是MCU和SOC，而现在STM32也几乎成为了MCU的代名词，SOC目前则以Cortex-A系列为主，开发难度也有所差异，对于嵌入式从业者来说，弄清楚这些专业概念是必备的。
]]></content>
  </entry>
  
  <entry>
    <title>嵌入式C语言之结构体封装函数</title>
    <url>/post/programming/embedded-c-programming-language-struct-pack-function.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>C</tag>
      <tag>Embedded</tag>
      <tag>Struct</tag>
    </tags>
    <content type="html"><![CDATA[在嵌入式系统中，结构体封装函数可以用于对于嵌入式硬件资源进行抽象和封装，从而提高软件的可维护性和可移植性。结构体封装函数通常包含数据和行为，并提供了对数据的访问和操作方法。
比如可以将硬件驱动函数封装在结构体中，方便对外提供统一的API接口，同时也便于代码的移植和扩展。另外，结构体封装函数还可以用于实现状态机、任务调度等复杂的系统功能。
在C语言中，结构体不仅可以封装数据，还可以封装函数指针。这种方式可以用于实现回调函数、状态机等，提高代码的复用性和可维护性。特别是在嵌入式当中，应用是非常多的。
结构体封装函数的作用  将函数指针和参数打包成一个结构体，实现了代码的模块化和可复用性。 在结构体中可以定义多个函数指针，实现了对函数的分类管理和调用。 结构体可以作为函数的参数或返回值，传递和返回函数指针和参数。  结构体封装函数的应用  回调函数：将函数指针和参数打包成一个结构体，传递给API函数，在API函数内部执行该函数。 状态机：将每个状态对应的处理函数封装成一个结构体，根据当前状态调用相应的处理函数。 事件驱动：将事件处理函数封装成一个结构体，通过事件触发调用相应的处理函数。 线程池：将任务处理函数封装成一个结构体，加入任务队列后由线程池调用执行  结构体封装函数的好处  更好的隐藏实现细节：结构体封装函数使得函数的实现细节被封装在结构体内部，只有结构体暴露给外部的函数指针，实现了良好的封装和信息隐藏。 更加灵活的函数调用：函数指针可以被动态修改，从而实现动态的函数调用。例如，在状态机中，根据不同的状态，可以将相应的处理函数指针赋值给一个函数指针变量，从而实现状态的转换和函数的调用。 更加方便的扩展性：结构体封装函数可以轻松地添加新的函数指针，从而扩展功能。在需要添加新功能时，只需要定义一个新的函数指针，并添加到结构体中，就可以实现功能的扩展，而不需要修改原有的代码。 更加通用的代码：结构体封装函数可以使用于各种不同的编程范式，例如面向对象编程（OOP）和函数式编程（FP），从而实现通用的代码。例如，在OOP中，结构体可以被看作是一个对象，函数指针可以被看作是对象的方法，从而实现OOP编程的思想。 更加易于维护：结构体封装函数使得代码更加清晰、易于维护和修改。由于函数指针的定义和使用都在结构体内部，因此修改或调整代码时，只需要修改结构体中的函数指针定义或调用方式，而不需要修改其他部分的代码，从而使得代码更加健壮、易于维护和修改 模块化：通过结构体封装函数，可以将多个函数和数据结构组合成一个模块，以便于模块化设计和维护。这种方法可以将代码的复杂性分解到不同的模块中，降低了代码的耦合性，提高了代码的可读性和可维护性。 代码复用：结构体封装的函数可以通过传递结构体的方式重用同一个函数。这种方式可以大大减少代码量，提高代码的复用性和可维护性。 可扩展性：当需要增加新的功能时，只需增加新的函数和数据结构，而不需要修改现有代码。这种方式可以大大减少代码的修改和调试时间，提高代码的可扩展性和可维护性。 保护数据：通过结构体封装函数，可以将数据和函数封装在一个结构体中，防止外部代码对数据的非法访问和修改。 提高安全性：将函数和数据封装在一个结构体中，可以防止其他函数对数据的非法操作，从而提高程序的安全性。  举例1 /* 定义封装函数结构体由外部调用*/ typedef struct { int x; int y; void (*move_up)(int steps); void (*move_down)(int steps); void (*move_left)(int steps); void (*move_right)(int steps); } Point; // 定义结构体中的函数 void move_up(int steps) { // 向上移动steps个单位  // ... } void move_down(int steps) { // 向下移动steps个单位  // ... } void move_left(int steps) { // 向左移动steps个单位  // ... } void move_right(int steps) { // 向右移动steps个单位  // ... } int main() { // 初始化结构体  Point point = { .x = 0, .y = 0, .move_up = move_up, .move_down = move_down, .move_left = move_left, .move_right = move_right }; // 调用结构体中的函数  point.move_up(10); point.move_right(5); return 0; } 在上面的示例代码中，我们定义了一个结构体Point，其中包含了两个整型变量x和y，以及四个函数指针move_up、move_down、move_left和move_right。每个函数指针指向一个移动函数，用于在平面坐标系中移动点的位置。通过使用结构体封装函数，我们可以将函数和数据封装在一起，方便地进行操作和管理。
在main()函数中，我们首先通过初始化的方式，将结构体中的成员变量和函数指针初始化。然后，我们使用结构体中的函数指针，调用了move_up()和move_right()函数，分别将点向上移动10个单位和向右移动5个单位。
值得注意的是，在实际应用中，我们需要根据实际情况修改函数的实现，以及结构体中的成员变量和函数指针的数量和类型。同时避免滥用。
举例2 typedef struct { void (*init)(void); void (*write)(uint8_t data); uint8_t (*read)(void); } spi_t; void spi_init(void) { /* SPI初始化代码 */ } void spi_write(uint8_t data) { /* SPI写入数据 */ } uint8_t spi_read(void) { /* SPI读取数据 */ } int main(void) { spi_t spi = {spi_init, spi_write, spi_read}; spi.init(); spi.write(0xAA); uint8_t data = spi.read(); return 0; } 在举例2这个例子中，我们定义了一个spi_t类型的结构体，它包含了三个成员函数指针，分别对应SPI总线的初始化、写入和读取操作。在main函数中，我们定义了一个spi结构体变量，并且初始化它的函数指针成员。接下来，我们通过spi结构体变量的函数指针成员，分别调用了SPI总线的初始化、写入和读取操作。
使用结构体封装函数可以使代码更加清晰明了，减少了代码的冗余和重复，同时也方便代码的扩展和维护。
举例3 假设我们需要控制一个LED灯的亮度，可以使用PWM（脉冲宽度调制）技术来实现。为了方便控制，我们可以使用一个结构体来封装控制LED灯的函数和变量。
typedef struct { uint8_t duty_cycle; // 占空比  void (*set_duty_cycle)(uint8_t duty_cycle); // 设置占空比的函数指针  void (*start)(void); // 启动PWM输出的函数指针  void (*stop)(void); // 停止PWM输出的函数指针 } pwm_control_t; // 设置占空比 void set_duty_cycle(uint8_t duty_cycle) { // 设置占空比的代码 } // 启动PWM输出 void start_pwm(void) { // 启动PWM输出的代码 } // 停止PWM输出 void stop_pwm(void) { // 停止PWM输出的代码 } int main(void) { pwm_control_t pwm; pwm.duty_cycle = 50; // 设置占空比为50%  pwm.set_duty_cycle = set_duty_cycle; pwm.start = start_pwm; pwm.stop = stop_pwm; pwm.set_duty_cycle(pwm.duty_cycle); // 设置占空比  pwm.start(); // 启动PWM输出  while (1) { // 循环执行其他任务  } } 在上面的代码中，我们定义了一个名为pwm_control_t的结构体，其中包含了一个占空比成员变量duty_cycle和三个函数指针set_duty_cycle、start和stop。set_duty_cycle函数用于设置占空比，start函数用于启动PWM输出，stop函数用于停止PWM输出。
在main函数中，我们创建了一个pwm_control_t类型的结构体变量pwm，并分别给结构体的成员变量和函数指针赋值。接着，我们调用了set_duty_cycle和start函数来设置占空比和启动PWM输出。
结构体封装函数的好处在于，我们可以通过创建不同的结构体变量来控制多个LED灯，而且不同的LED灯可以使用不同的PWM参数。此外，如果需要修改PWM输出的实现方式，只需要修改start和stop函数即可，而不需要修改每个LED灯。
]]></content>
  </entry>
  
  <entry>
    <title>Linux驱动IO篇——mmap操作</title>
    <url>/post/linux/linux-io-device-driver-mmap-operation.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Device Driver</tag>
      <tag>mmap</tag>
      <tag>IO</tag>
    </tags>
    <content type="html"><![CDATA[平时我们写Linux驱动和用户空间交互时，都是通过copy_from_user把用户空间传过来的数据进行拷贝，为什么要这么做呢？
前言 因为用户空间是不能直接内核空间数据的，他们映射的是不同的地址空间，只能先将数据拷贝过来，然后再操作。
如果用户空间需要传几MB的数据给内核，那么原来的拷贝方式显然效率特别低，也不太现实，那怎么办呢？
想想，之所以要拷贝是因为用户空间不能直接访问内核空间，那如果可以直接访问内核空间的buffer，是不是就解决了。
简单来说，就是让一块物理内存拥有两份映射，即拥有两个虚拟地址，一个在内核空间，一个在用户空间。关系如下：
通过mmap映射就可以实现。
应用层 应用层代码很简单，主要就是通过mmap系统调用进行映射，然后就可以对返回的地址进行操作。
char * buf; /* 1. 打开文件 */ fd = open(&#34;/dev/hello&#34;, O_RDWR); if (fd == -1) { printf(&#34;can not open file /dev/hello\n&#34;); return -1; } /* 2. mmap * MAP_SHARED : 多个APP都调用mmap映射同一块内存时, 对内存的修改大家都可以看到。 * 就是说多个APP、驱动程序实际上访问的都是同一块内存 * MAP_PRIVATE : 创建一个copy on write的私有映射。 * 当APP对该内存进行修改时，其他程序是看不到这些修改的。 * 就是当APP写内存时, 内核会先创建一个拷贝给这个APP, * 这个拷贝是这个APP私有的, 其他APP、驱动无法访问。 */ buf = mmap(NULL, 1024*8, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0); mmap的第一个参数是想要映射的起始地址，通常设置为NULL，表示由内核来决定该起始地址。
第二参数是要映射的内存空间的大小。
第三个参数PROT_READ | PROT_WRITE表示映射后的空间是可读可写的。
第四个参数可填MAP_SHARED或MAP_PRIVATE：
 MAP_SHARED：多个APP都调用mmap映射同一块内存时, 对内存的修改大家都可以看到。就是说多个APP、驱动程序实际上访问的都是同一块内存。 MAP_PRIVATE：创建一个copy on write的私有映射。当APP对该内存进行修改时，其他程序是看不到这些修改的。就是当APP写内存时, 内核会先创建一个拷贝给这个APP，这个拷贝是这个APP私有的, 其他APP、驱动无法访问。  驱动层 驱动层主要是实现mmap接口，而mmap接口的实现，主要是调用了remap_pfn_range函数，函数原型如下：
int remap_pfn_range( struct vm_area_struct *vma, unsigned long addr, unsigned long pfn, unsigned long size, pgprot_t prot); vma：描述一片映射区域的结构体指针 addr：要映射的虚拟地址起始地址 pfn：物理内存所对应的页框号，就是将物理地址除以页大小得到的值 size：映射的大小 prot：该内存区域的访问权限
驱动主要步骤：
  使用kmalloc或者kzalloc函数分配一块内存kernel_buf，因为这样分配的内存物理地址是连续的，mmap后应用层会对这一个基地址去访问这块内存。
  实现mmap函数
  static int hello_drv_mmap(struct file *file, struct vm_area_struct *vma) { /* 获得物理地址 */ unsigned long phy = virt_to_phys(kernel_buf);//kernel_buf是内核空间分配的一块虚拟地址空间  /* 设置属性：cache, buffer*/ vma-&gt;vm_page_prot = pgprot_writecombine(vma-&gt;vm_page_prot); /* map */ if(remap_pfn_range(vma, vma-&gt;vm_start, phy&gt;&gt;PAGE_SHFIT, vma-&gt;vm_end - vma-&gt;start, vma-&gt;vm_page_prot)){ printk(&#34;mmap remap_pfn_range failed\n&#34;); return -ENOBUFS; } return 0; } static struct file_operations my_fops = { .mmap = hello_drv_mmap, };  通过virt_to_phys将虚拟地址转为物理地址，这里的kernel_buf是内核空间的一块虚拟地址空间 设置属性：不使用cache，使用buffer 映射：通过remap_pfn_range函数映射，phy&raquo;PAGE_SHIFT其实就是按page映射，除了这个参数，其他的起始地址、大小和权限都可以由用户在系统调用函数中指定。  当应用层调用mmap后，就会调用到驱动层的mmap函数，最终应用层的虚拟地址和驱动中的物理地址就建立了映射关系，应用层也就可以直接访问驱动的buffer了。
]]></content>
  </entry>
  
  <entry>
    <title>Linux下SPI驱动详解</title>
    <url>/post/linux/linux-spi-device-driver-detailed-explanation.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Linux</tag>
      <tag>SPI</tag>
      <tag>Device Driver</tag>
    </tags>
    <content type="html"><![CDATA[SPI，是英语Serial Peripheral interface的缩写，顾名思义就是串行外围设备接口。
SPI总线 SPI总线概述 SPI是Motorola首先在其MC68HCXX系列处理器上定义的。SPI接口主要应用在 EEPROM，FLASH，实时时钟，AD转换器，还有数字信号处理器和数字信号解码器之间。SPI，是一种高速的，全双工，同步的通信总线，并且在芯片的管脚上只占用四根线，节约了芯片的管脚，同时为PCB的布局上节省空间，提供方便，正是出于这种简单易用的特性，现在越来越多的芯片集成了这种通信协议。SPI总线的构成及信号类型如图1-1所示：
 MOSI – 主设备数据输出，从设备数据输入 对应MOSI master output slave input MISO – 主设备数据输入，从设备数据输出 对应MISO master input slave output CLK – 时钟信号，由主设备产生 nCS – 从设备使能信号，由主设备控制  图1-1 SPI总线模型
SPI总线时序 SPI接口在Master控制下产生的从设备使能信号和时钟信号，两个双向移位寄存器按位传输进行数据交换，传输数据高位在前（MSB first），低位在后。如下图所示，在CLK的下降沿上数据改变，上升沿一位数据被存入移位寄存器。
图1-2 spi传输时序图
在一个SPI时钟周期内，会完成如下操作：（1）Master通过MOSI线发送1位数据，同时Slave通过MOSI线读取这1位数据；（2）Slave通过MISO线发送1位数据，同时Master通过MISO线读取这1位数据。Master和Slave各有一个移位寄存器，如图1-3所示，而且这两个移位寄存器连接成环状。依照CLK的变化，数据以MSB first的方式依次移出Master寄存器和Slave寄存器，并且依次移入Slave寄存器和Master寄存器。当寄存器中的内容全部移出时，相当于完成了两个寄存器内容的交换。
SPI总线传输模式 SPI总线传输一共有4种模式，这4种模式分别由时钟极性(CPOL，Clock Polarity)和时钟相位(CPHA，Clock Phase)来定义，其中CPOL参数规定了SCK时钟信号空闲状态的电平，CPHA规定了数据是在SCK时钟的上升沿被采样还是下降沿被采样。这四种模式的时序图如下图1-4所示：
 模式0：CPOL= 0，CPHA=0。CLK串行时钟线空闲是为低电平，数据在SCK时钟的上升沿被采样，数据在CLK时钟的下降沿切换 模式1：CPOL= 0，CPHA=1。CLK串行时钟线空闲是为低电平，数据在SCK时钟的下降沿被采样，数据在CLK时钟的上升沿切换 模式2：CPOL= 1，CPHA=0。CLK串行时钟线空闲是为高电平，数据在SCK时钟的下降沿被采样，数据在CLK时钟的上升沿切换 模式3：CPOL= 1，CPHA=1。CLK串行时钟线空闲是为高电平，数据在SCK时钟的上升沿被采样，数据在CLK时钟的下降沿切换 其中比较常用的模式是模式0和模式3。为了更清晰的描述SPI总线的时序，下面展现了模式0下的SPI时序图1-5：  图1-5 mode0下的SPI时序图
SPI总线的优缺点  在点对点的通信中，SPI接口不需要进行寻址操作，且为全双工通信，显得简单高效。 SPI接口没有指定的流控制，没有应答机制确认是否接收到数据。  Linux SPI 框架 软件架构 Linux系统对spi设备具有很好的支持，linux系统下的spi驱动程序从逻辑上可以分为3个部分：
 spi核心（SPI Core）：SPI Core是Linux内核用来维护和管理spi的核心部分，SPI Core提供操作接口函数，允许一个spi master，spi driver和spi device初始化时在SPI Core中进行注册，以及退出时进行注销。 spi控制器驱动（SPI Master Driver）：SPI Master针对不同类型的spi控制器硬件，实现spi总线的硬件访问操作。SPI Master通过接口函数向SPI Core注册一个控制器。 spi设备驱动（SPI Device Driver）：SPI Driver是对应于spi设备端的驱动程序，通过接口函数向SPI Core进行注册，SPI Driver的作用是将spi设备挂接到spi总线上；Linux的软件架构图如图2-1所示：  图2-1 spi软件架构图
初始化及退出流程 注册spi控制器 注册spi控制器到内核分为两个阶段：第一个阶段，使用spi_alloc_master,分配一个spi_master的空间，具体流程如图2-2所示：
第二阶段，使用spi_register_master将第一阶段分配的spi_master注册到内核中，具体流程如2-3所示：
注销spi控制器 spi控制器注销的流程如图2-4所示：
关键数据结构 spi_device struct spi_device { struct device dev; /*spi控制器对应的device结构 struct spi_master *master; /*设备使用的master结构，挂在哪个主控制器下*/ u32 max_speed_hz; /*通讯时钟最大频率*/ u8 chip_select; /*片选号，每个master支持多个spi_device */ u8 mode; #define SPI_CPHA 0x01 /* clock phase */#define SPI_CPOL 0x02 /* clock polarity */#define SPI_MODE_0 (0|0) /* (original MicroWire) */#define SPI_MODE_1 (0|SPI_CPHA) #define SPI_MODE_2 (SPI_CPOL|0) #define SPI_MODE_3 (SPI_CPOL|SPI_CPHA) #define SPI_CS_HIGH 0x04 /* chipselect active high? */#define SPI_LSB_FIRST 0x08 /* per-word bits-on-wire */#define SPI_3WIRE 0x10 /* SI/SO signals shared */#define SPI_LOOP 0x20 /* loopback mode */#define SPI_NO_CS 0x40 /* 1 dev/bus, no chipselect */#define SPI_READY 0x80 /* slave pulls low to pause */ u8 bits_per_word; /*每个字长的比特数，默认是8*/ int irq; void *controller_state; /*控制器状态*/ void *controller_data; /*控制器数据*/ char modalias[SPI_NAME_SIZE]; /* 设备驱动的名字 */ int cs_gpio; /* chip select gpio */ /* * likely need more hooks for more protocol options affecting how * the controller talks to each chip, like: * - memory packing (12 bit samples into low bits, others zeroed) * - priority * - drop chipselect after each word * - chipselect delays * - ... */ }; spi_device代表一个外围spi设备，由master controller driver注册完成后扫描BSP中注册设备产生的设备链表并向spi_bus注册产生。在内核中，每个spi_device代表一个物理的spi设备。
spi_driver struct spi_driver { const struct spi_device_id *id_table; /*支持的spi_device设备表*/ int (*probe)(struct spi_device *spi); int (*remove)(struct spi_device *spi); void (*shutdown)(struct spi_device *spi); int (*suspend)(struct spi_device *spi, pm_message_t mesg); int (*resume)(struct spi_device *spi); struct device_driver driver; }; spi_driver代表一个SPI protocol drivers，即外设驱动
struct spi_master struct spi_master { struct device dev; /*spi控制器对应的device结构*/ struct list_head list; /*链表 /* other than negative (== assign one dynamically), bus_num is fully * board-specific. usually that simplifies to being SOC-specific. * example: one SOC has three SPI controllers, numbered 0..2, * and one board&#39;s schematics might show it using SPI-2. software * would normally use bus_num=2 for that controller. */ s16 bus_num; /*总线（或控制器编号）*/ /* chipselects will be integral to many controllers; some others * might use board-specific GPIOs. */ u16 num_chipselect; /*片选数量*/ /* some SPI controllers pose alignment requirements on DMAable * buffers; let protocol drivers know about these requirements. */ u16 dma_alignment; /* spi_device.mode flags understood by this controller driver */ u16 mode_bits; /* master支持的设备模式 */ /* bitmask of supported bits_per_word for transfers */ u32 bits_per_word_mask; /* other constraints relevant to this driver */ u16 flags; /*用于限定某些限制条件的标志位 #define SPI_MASTER_HALF_DUPLEX BIT(0) /* can&#39;t do full duplex */ #define SPI_MASTER_NO_RX BIT(1) /* can&#39;t do buffer read */#define SPI_MASTER_NO_TX BIT(2) /* can&#39;t do buffer write */ /* lock and mutex for SPI bus locking */ spinlock_t bus_lock_spinlock; struct mutex bus_lock_mutex; /* flag indicating that the SPI bus is locked for exclusive use */ bool bus_lock_flag; /* Setup mode and clock, etc (spi driver may call many times). * * IMPORTANT: this may be called when transfers to another * device are active. DO NOT UPDATE SHARED REGISTERS in ways * which could break those transfers. */ int (*setup)(struct spi_device *spi); /*根据spi设备更新硬件配置。设置spi工作模式、时钟等*/ /* bidirectional bulk transfers * * + The transfer() method may not sleep; its main role is * just to add the message to the queue. * + For now there&#39;s no remove-from-queue operation, or * any other request management * + To a given spi_device, message queueing is pure fifo * * + The master&#39;s main job is to process its message queue, * selecting a chip then transferring data * + If there are multiple spi_device children, the i/o queue * arbitration algorithm is unspecified (round robin, fifo, * priority, reservations, preemption, etc) * * + Chipselect stays active during the entire message * (unless modified by spi_transfer.cs_change != 0). * + The message transfers use clock and SPI mode parameters * previously established by setup() for this device */ int (*transfer)(struct spi_device *spi, struct spi_message *mesg); /*添加消息到队列的方法，此函数不可睡眠。它的职责是安排发生的传送并且调用注册的回调函数complete()*/ /* called on release() to free memory provided by spi_master */ void (*cleanup)(struct spi_device *spi);/*cleanup函数会在spidev_release函数中被调用，spidev_release被登记为spi dev的release函数。*/ /* * These hooks are for drivers that want to use the generic * master transfer queueing mechanism. If these are used, the * transfer() function above must NOT be specified by the driver. * Over time we expect SPI drivers to be phased over to this API. */ bool queued; struct kthread_worker kworker; /*用于管理数据传输消息队列的工作队列线程*/ struct task_struct *kworker_task; struct kthread_work pump_messages; /*具体实现数据传输队列的工作队列*/ spinlock_t queue_lock; struct list_head queue; /*该控制器的消息队列，所有等待传输的队列挂在该链表下*/ struct spi_message *cur_msg;/*当前正在处理的消息队列*/ bool busy; /忙状态*/ bool running; /*正在跑*/ bool rt; int (*prepare_transfer_hardware)(struct spi_master *master); /*回调函数，正式发起传输前会被调用，用于准备硬件资源*/ int (*transfer_one_message)(struct spi_master *master, struct spi_message *mesg); /*单个消息的原子传输回调函数，队列中每个消息都会回调一次该回调来完成传输工作*/ int (*unprepare_transfer_hardware)(struct spi_master *master); /*清理回调函数*/ /* gpio chip select */ int *cs_gpios; }; spi_master代表一个spi控制器。
struct spi_message 和spi_transfer 要完成和SPI设备的数据传输工作，我们还需要另外两个数据结构：spi_message和spi_transfer。
spi_message包含了一个的spi_transfer结构序列，一旦控制器接收了一个spi_message，其中的spi_transfer应该按顺序被发送，并且不能被其它spi_message打断，所以我们认为spi_message就是一次SPI数据交换的原子操作。下面我们看看这两个数据结构的定义：
struct spi_message ：
struct spi_message { struct list_head transfers; /*spi_transfer链表队列，此次消息的传输段队列，一个消息可以包含多个传输段。*/ struct spi_device *spi; /*传输的目的设备*/ unsigned is_dma_mapped:1; /*如果为真，此次调用提供dma和cpu虚拟地址。*/ /* REVISIT: we might want a flag affecting the behavior of the * last transfer ... allowing things like &#34;read 16 bit length L&#34; * immediately followed by &#34;read L bytes&#34;. Basically imposing * a specific message scheduling algorithm. * * Some controller drivers (message-at-a-time queue processing) * could provide that as their default scheduling algorithm. But * others (with multi-message pipelines) could need a flag to * tell them about such special cases. */ /* completion is reported through a callback */ void (*complete)(void *context);/*异步调用完成后的回调函数*/ void *context; /*回调函数的参数*/ unsigned actual_length; /*实际传输的长度*/ int status; /*该消息的发送结果，成功被置0，否则是一个负的错误码。*/ /* for optional use by whatever driver currently owns the * spi_message ... between calls to spi_async and then later * complete(), that&#39;s the spi_master controller driver. */ struct list_head queue; void *state; }; 链表字段queue用于把该结构挂在代表控制器的spi_master结构的queue字段上，控制器上可以同时被加入多个spi_message进行排队。另一个链表字段transfers则用于链接挂在本message下的spi_tranfer结构。complete回调函数则会在该message下的所有spi_transfer都被传输完成时被调用，以便通知协议驱动处理接收到的数据以及准备下一批需要发送的数据。我们再来看看spi_transfer结构：spi_transfer
struct spi_transfer { /* it&#39;s ok if tx_buf == rx_buf (right?) * for MicroWire, one buffer must be null * buffers must work with dma_*map_single() calls, unless * spi_message.is_dma_mapped reports a pre-existing mapping */ const void *tx_buf; /*发送缓冲区*/ void *rx_buf; /*接收缓冲区*/ unsigned len; /*缓冲区长度，tx和rx的大小（字节数）。指它们各自的大小*/ dma_addr_t tx_dma; /*tx的dma地址*/ dma_addr_t rx_dma; /*rx的dma地址*/ unsigned cs_change:1; /*当前spi_transfer发送完成之后重新片选*/ u8 bits_per_word; /*每个字长的比特数，0代表使用spi_device中的默认值8*/ u16 delay_usecs; /*发送完成一个spi_transfer后的延时时间，此次传输结束和片选改变之间的延时，之后就会启动另一个传输或者结束整个消息*/ u32 speed_hz; /*通信时钟。如果是0，使用默认值*/ #ifdef CONFIG_SPI_LOMBO  struct lombo_spi_operate_para *esop; #endif  struct list_head transfer_list; /*用于链接到spi_message，用来连接的双向链接节点*/ }; 首先，transfer_list链表字段用于把该transfer挂在一个spi_message结构中，tx_buf和rx_buf提供了非dma模式下的数据缓冲区地址，len则是需要传输数据的长度，tx_dma和rx_dma则给出了dma模式下的缓冲区地址。原则来讲，spi_transfer才是传输的最小单位，之所以又引进了spi_message进行打包，我觉得原因是：有时候希望往spi设备的多个不连续的地址（或寄存器）一次性写入，如果没有spi_message进行把这样的多个spi_transfer打包，因为通常真正的数据传送工作是在另一个内核线程（工作队列）中完成的，不打包的后果就是会造成更多的进程切换，效率降低，延迟增加，尤其对于多个不连续地址的小规模数据传送而言就更为明显。
spi_board_info struct spi_board_info { /* the device name and module name are coupled, like platform_bus; * &#34;modalias&#34; is normally the driver name. * * platform_data goes to spi_device.dev.platform_data, * controller_data goes to spi_device.controller_data, * irq is copied too */ char modalias[SPI_NAME_SIZE]; /*名字*/ const void *platform_data; /*平台数据*/ void *controller_data; /*控制器数据*/ int irq; /* slower signaling on noisy or low voltage boards */ u32 max_speed_hz; /*最大速率*/ /* bus_num is board specific and matches the bus_num of some * spi_master that will probably be registered later. * * chip_select reflects how this chip is wired to that master; * it&#39;s less than num_chipselect. */ u16 bus_num; /*spi总线编号*/ u16 chip_select; /*片选*/ /* mode becomes spi_device.mode, and is essential for chips * where the default of SPI_CS_HIGH = 0 is wrong. */ u8 mode; /*模式 */ /* ... may need additional spi_device chip config data here. * avoid stuff protocol drivers can set; but include stuff * needed to behave without being bound to a driver: * - quirks like clock rate mattering when not selected */ }; 数据传输流程 整体的数据传输流程大致上是这样的:
 定义一个spi_message结构； 用spi_message_init函数初始化spi_message； 定义一个或数个spi_transfer结构，初始化并为数据准备缓冲区并赋值给spi_transfer相应的字段（tx_buf，rx_buf等）； 通过spi_message_init函数把这些spi_transfer挂在spi_message结构下； 如果使用同步方式，调用spi_sync()，如果使用异步方式，调用spi_async();(我调试外设时，只使用过spi_sync  传输示意图如图2-5所示：
数据准备 spi_message_init static inline void spi_message_init(struct spi_message *m) { memset(m, 0, sizeof *m); INIT_LIST_HEAD(&amp;m-&gt;transfers); } 初始化spi_message：清空message，初始化transfers链表头。
spi_message_add_tail static inline void spi_message_add_tail(struct spi_transfer *t, struct spi_message *m) { list_add_tail(&amp;t-&gt;transfer_list, &amp;m-&gt;transfers); } 将spi_transfer加入到spi_message的链表尾部。
数据传输 SPI数据传输可以有两种方式：同步方式和异步方式。所谓同步方式是指数据传输的发起者必须等待本次传输的结束，期间不能做其它事情，用代码来解释就是，调用传输的函数后，直到数据传输完成，函数才会返回。而异步方式则正好相反，数据传输的发起者无需等待传输的结束，数据传输期间还可以做其它事情，用代码来解释就是，调用传输的函数后，函数会立刻返回而不用等待数据传输完成，我们只需设置一个回调函数，传输完成后，该回调函数会被调用以通知发起者数据传送已经完成。同步方式简单易用，很适合处理那些少量数据的单次传输。但是对于数据量大、次数多的传输来说，异步方式就显得更加合适。对于SPI控制器来说，要支持异步方式必须要考虑以下两种状况：
 对于同一个数据传输的发起者，既然异步方式无需等待数据传输完成即可返回，返回后，该发起者可以立刻又发起一个message，而这时上一个message还没有处理完。 对于另外一个不同的发起者来说，也有可能同时发起一次message传输请求 首先分析spi_sync()接口的实现流程，如图2-6：  其次分析spi_async_locked接口的实现流程，如图2-7所示：
spi_queued_transfer接口的实现流程如图3-8所示：
spi_pump_messages函数的处理流程如图3-9所示：
图中transfer_one_message是spi控制器驱动要实现的，主要功能是处理spi_message中的每个spi_transfer。
关键函数解析 spi_alloc_master 原型：
struct spi_master *spi_alloc_master(struct device *dev, unsigned size) 功能：分配一个spi_master结构体指针。
参数：dev:spi控制器device指针 size ：分配的driver-private data大小
返回值 ：成功，返回spi_master指针；否则返回NULL
spi_register_master 原型：
int spi_register_master(struct spi_master *master) 功能 注册spi控制器驱动到内核。
参数 master：spi_master指针
返回值 成功，返回0；否则返回错误码
spi_unregister_master 原型：
void spi_unregister_master(struct spi_master *master) 功能 注销spi控制器驱动。
参数 master：spi_master指针
返回值 无
实例Demo #include &lt;linux/types.h&gt;#include &lt;linux/kernel.h&gt;#include &lt;linux/delay.h&gt;#include &lt;linux/ide.h&gt;#include &lt;linux/init.h&gt;#include &lt;linux/module.h&gt;#include &lt;linux/errno.h&gt;#include &lt;linux/gpio.h&gt;#include &lt;linux/cdev.h&gt;#include &lt;linux/device.h&gt;#include &lt;linux/of_gpio.h&gt;#include &lt;linux/semaphore.h&gt;#include &lt;linux/timer.h&gt;#include &lt;linux/i2c.h&gt;#include &lt;linux/spi/spi.h&gt;#include &lt;linux/of.h&gt;#include &lt;linux/of_address.h&gt;#include &lt;linux/of_gpio.h&gt;#include &lt;linux/platform_device.h&gt;#include &lt;asm/mach/map.h&gt;#include &lt;asm/uaccess.h&gt;#include &lt;asm/io.h&gt;#include &#34;icm20608reg.h&#34;/*************************************************************** Copyright © ALIENTEK Co., Ltd. 1998-2029. All rights reserved. 文件名 : icm20608.c 作者 : 左工 版本 : V1.0 描述 : ICM20608 SPI驱动程序 其他 : 无 论坛 : 日志 : 初版V1.0 2019/9/2 左工创建 ***************************************************************/ #define ICM20608_CNT 1 #define ICM20608_NAME &#34;icm20608&#34;  struct icm20608_dev { dev_t devid; /* 设备号 */ struct cdev cdev; /* cdev */ struct class *class; /* 类 */ struct device *device; /* 设备 */ struct device_node *nd; /* 设备节点 */ int major; /* 主设备号 */ void *private_data; /* 私有数据 */ int cs_gpio; /* 片选所使用的GPIO编号 */ signed int gyro_x_adc; /* 陀螺仪X轴原始值 */ signed int gyro_y_adc; /* 陀螺仪Y轴原始值 */ signed int gyro_z_adc; /* 陀螺仪Z轴原始值 */ signed int accel_x_adc; /* 加速度计X轴原始值 */ signed int accel_y_adc; /* 加速度计Y轴原始值 */ signed int accel_z_adc; /* 加速度计Z轴原始值 */ signed int temp_adc; /* 温度原始值 */ }; static struct icm20608_dev icm20608dev; /* * @description : 从icm20608读取多个寄存器数据 * @param - dev: icm20608设备 * @param - reg: 要读取的寄存器首地址 * @param - val: 读取到的数据 * @param - len: 要读取的数据长度 * @return : 操作结果 */ static int icm20608_read_regs(struct icm20608_dev *dev, u8 reg, void *buf, int len) { int ret; unsigned char txdata[len]; struct spi_message m; struct spi_transfer *t; struct spi_device *spi = (struct spi_device *)dev-&gt;private_data; gpio_set_value(dev-&gt;cs_gpio, 0); /* 片选拉低，选中ICM20608 */ t = kzalloc(sizeof(struct spi_transfer), GFP_KERNEL); /* 申请内存 */ /* 第1次，发送要读取的寄存地址 */ txdata[0] = reg | 0x80; /* 写数据的时候寄存器地址bit8要置1 */ t-&gt;tx_buf = txdata; /* 要发送的数据 */ t-&gt;len = 1; /* 1个字节 */ spi_message_init(&amp;m); /* 初始化spi_message */ spi_message_add_tail(t, &amp;m);/* 将spi_transfer添加到spi_message队列 */ ret = spi_sync(spi, &amp;m); /* 同步发送 */ /* 第2次，读取数据 */ txdata[0] = 0xff; /* 随便一个值，此处无意义 */ t-&gt;rx_buf = buf; /* 读取到的数据 */ t-&gt;len = len; /* 要读取的数据长度 */ spi_message_init(&amp;m); /* 初始化spi_message */ spi_message_add_tail(t, &amp;m);/* 将spi_transfer添加到spi_message队列 */ ret = spi_sync(spi, &amp;m); /* 同步发送 */ kfree(t); /* 释放内存 */ gpio_set_value(dev-&gt;cs_gpio, 1); /* 片选拉高，释放ICM20608 */ return ret; } /* * @description : 向icm20608多个寄存器写入数据 * @param - dev: icm20608设备 * @param - reg: 要写入的寄存器首地址 * @param - val: 要写入的数据缓冲区 * @param - len: 要写入的数据长度 * @return : 操作结果 */ static s32 icm20608_write_regs(struct icm20608_dev *dev, u8 reg, u8 *buf, u8 len) { int ret; unsigned char txdata[len]; struct spi_message m; struct spi_transfer *t; struct spi_device *spi = (struct spi_device *)dev-&gt;private_data; t = kzalloc(sizeof(struct spi_transfer), GFP_KERNEL); /* 申请内存 */ gpio_set_value(dev-&gt;cs_gpio, 0); /* 片选拉低 */ /* 第1次，发送要读取的寄存地址 */ txdata[0] = reg &amp; ~0x80; /* 写数据的时候寄存器地址bit8要清零 */ t-&gt;tx_buf = txdata; /* 要发送的数据 */ t-&gt;len = 1; /* 1个字节 */ spi_message_init(&amp;m); /* 初始化spi_message */ spi_message_add_tail(t, &amp;m);/* 将spi_transfer添加到spi_message队列 */ ret = spi_sync(spi, &amp;m); /* 同步发送 */ /* 第2次，发送要写入的数据 */ t-&gt;tx_buf = buf; /* 要写入的数据 */ t-&gt;len = len; /* 写入的字节数 */ spi_message_init(&amp;m); /* 初始化spi_message */ spi_message_add_tail(t, &amp;m);/* 将spi_transfer添加到spi_message队列 */ ret = spi_sync(spi, &amp;m); /* 同步发送 */ kfree(t); /* 释放内存 */ gpio_set_value(dev-&gt;cs_gpio, 1);/* 片选拉高，释放ICM20608 */ return ret; } /* * @description : 读取icm20608指定寄存器值，读取一个寄存器 * @param - dev: icm20608设备 * @param - reg: 要读取的寄存器 * @return : 读取到的寄存器值 */ static unsigned char icm20608_read_onereg(struct icm20608_dev *dev, u8 reg) { u8 data = 0; icm20608_read_regs(dev, reg, &amp;data, 1); return data; } /* * @description : 向icm20608指定寄存器写入指定的值，写一个寄存器 * @param - dev: icm20608设备 * @param - reg: 要写的寄存器 * @param - data: 要写入的值 * @return : 无 */ static void icm20608_write_onereg(struct icm20608_dev *dev, u8 reg, u8 value) { u8 buf = value; icm20608_write_regs(dev, reg, &amp;buf, 1); } /* * @description : 读取ICM20608的数据，读取原始数据，包括三轴陀螺仪、 * : 三轴加速度计和内部温度。 * @param - dev : ICM20608设备 * @return : 无。 */ void icm20608_readdata(struct icm20608_dev *dev) { unsigned char data[14]; icm20608_read_regs(dev, ICM20_ACCEL_XOUT_H, data, 14); dev-&gt;accel_x_adc = (signed short)((data[0] &lt;&lt; 8) | data[1]); dev-&gt;accel_y_adc = (signed short)((data[2] &lt;&lt; 8) | data[3]); dev-&gt;accel_z_adc = (signed short)((data[4] &lt;&lt; 8) | data[5]); dev-&gt;temp_adc = (signed short)((data[6] &lt;&lt; 8) | data[7]); dev-&gt;gyro_x_adc = (signed short)((data[8] &lt;&lt; 8) | data[9]); dev-&gt;gyro_y_adc = (signed short)((data[10] &lt;&lt; 8) | data[11]); dev-&gt;gyro_z_adc = (signed short)((data[12] &lt;&lt; 8) | data[13]); } /* * @description : 打开设备 * @param - inode : 传递给驱动的inode * @param - filp : 设备文件，file结构体有个叫做privateate_data的成员变量 * 一般在open的时候将private_data似有向设备结构体。 * @return : 0 成功;其他 失败 */ static int icm20608_open(struct inode *inode, struct file *filp) { filp-&gt;private_data = &amp;icm20608dev; /* 设置私有数据 */ return 0; } /* * @description : 从设备读取数据 * @param - filp : 要打开的设备文件(文件描述符) * @param - buf : 返回给用户空间的数据缓冲区 * @param - cnt : 要读取的数据长度 * @param - offt : 相对于文件首地址的偏移 * @return : 读取的字节数，如果为负值，表示读取失败 */ static ssize_t icm20608_read(struct file *filp, char __user *buf, size_t cnt, loff_t *off) { signed int data[7]; long err = 0; struct icm20608_dev *dev = (struct icm20608_dev *)filp-&gt;private_data; icm20608_readdata(dev); data[0] = dev-&gt;gyro_x_adc; data[1] = dev-&gt;gyro_y_adc; data[2] = dev-&gt;gyro_z_adc; data[3] = dev-&gt;accel_x_adc; data[4] = dev-&gt;accel_y_adc; data[5] = dev-&gt;accel_z_adc; data[6] = dev-&gt;temp_adc; err = copy_to_user(buf, data, sizeof(data)); return 0; } /* * @description : 关闭/释放设备 * @param - filp : 要关闭的设备文件(文件描述符) * @return : 0 成功;其他 失败 */ static int icm20608_release(struct inode *inode, struct file *filp) { return 0; } /* icm20608操作函数 */ static const struct file_operations icm20608_ops = { .owner = THIS_MODULE, .open = icm20608_open, .read = icm20608_read, .release = icm20608_release, }; /* * ICM20608内部寄存器初始化函数 * @param : 无 * @return : 无 */ void icm20608_reginit(void) { u8 value = 0; icm20608_write_onereg(&amp;icm20608dev, ICM20_PWR_MGMT_1, 0x80); mdelay(50); icm20608_write_onereg(&amp;icm20608dev, ICM20_PWR_MGMT_1, 0x01); mdelay(50); value = icm20608_read_onereg(&amp;icm20608dev, ICM20_WHO_AM_I); printk(&#34;ICM20608 ID = %#X\r\n&#34;, value); icm20608_write_onereg(&amp;icm20608dev, ICM20_SMPLRT_DIV, 0x00); /* 输出速率是内部采样率 */ icm20608_write_onereg(&amp;icm20608dev, ICM20_GYRO_CONFIG, 0x18); /* 陀螺仪±2000dps量程 */ icm20608_write_onereg(&amp;icm20608dev, ICM20_ACCEL_CONFIG, 0x18); /* 加速度计±16G量程 */ icm20608_write_onereg(&amp;icm20608dev, ICM20_CONFIG, 0x04); /* 陀螺仪低通滤波BW=20Hz */ icm20608_write_onereg(&amp;icm20608dev, ICM20_ACCEL_CONFIG2, 0x04); /* 加速度计低通滤波BW=21.2Hz */ icm20608_write_onereg(&amp;icm20608dev, ICM20_PWR_MGMT_2, 0x00); /* 打开加速度计和陀螺仪所有轴 */ icm20608_write_onereg(&amp;icm20608dev, ICM20_LP_MODE_CFG, 0x00); /* 关闭低功耗 */ icm20608_write_onereg(&amp;icm20608dev, ICM20_FIFO_EN, 0x00); /* 关闭FIFO */ } /* * @description : spi驱动的probe函数，当驱动与 * 设备匹配以后此函数就会执行 * @param - client : spi设备 * @param - id : spi设备ID * */ static int icm20608_probe(struct spi_device *spi) { int ret = 0; /* 1、构建设备号 */ if (icm20608dev.major) { icm20608dev.devid = MKDEV(icm20608dev.major, 0); register_chrdev_region(icm20608dev.devid, ICM20608_CNT, ICM20608_NAME); } else { alloc_chrdev_region(&amp;icm20608dev.devid, 0, ICM20608_CNT, ICM20608_NAME); icm20608dev.major = MAJOR(icm20608dev.devid); } /* 2、注册设备 */ cdev_init(&amp;icm20608dev.cdev, &amp;icm20608_ops); cdev_add(&amp;icm20608dev.cdev, icm20608dev.devid, ICM20608_CNT); /* 3、创建类 */ icm20608dev.class = class_create(THIS_MODULE, ICM20608_NAME); if (IS_ERR(icm20608dev.class)) { return PTR_ERR(icm20608dev.class); } /* 4、创建设备 */ icm20608dev.device = device_create(icm20608dev.class, NULL, icm20608dev.devid, NULL, ICM20608_NAME); if (IS_ERR(icm20608dev.device)) { return PTR_ERR(icm20608dev.device); } /* 获取设备树中cs片选信号 */ icm20608dev.nd = of_find_node_by_path(&#34;/soc/aips-bus@02000000/spba-bus@02000000/ecspi@02010000&#34;); if(icm20608dev.nd == NULL) { printk(&#34;ecspi3 node not find!\r\n&#34;); return -EINVAL; } /* 2、 获取设备树中的gpio属性，得到BEEP所使用的BEEP编号 */ icm20608dev.cs_gpio = of_get_named_gpio(icm20608dev.nd, &#34;cs-gpio&#34;, 0); if(icm20608dev.cs_gpio &lt; 0) { printk(&#34;can&#39;t get cs-gpio&#34;); return -EINVAL; } /* 3、设置GPIO1_IO20为输出，并且输出高电平 */ ret = gpio_direction_output(icm20608dev.cs_gpio, 1); if(ret &lt; 0) { printk(&#34;can&#39;t set gpio!\r\n&#34;); } /*初始化spi_device */ spi-&gt;mode = SPI_MODE_0; /*MODE0，CPOL=0，CPHA=0*/ spi_setup(spi); icm20608dev.private_data = spi; /* 设置私有数据 */ /* 初始化ICM20608内部寄存器 */ icm20608_reginit(); return 0; } /* * @description : spi驱动的remove函数，移除spi驱动的时候此函数会执行 * @param - client : spi设备 * @return : 0，成功;其他负值,失败 */ static int icm20608_remove(struct spi_device *spi) { /* 删除设备 */ cdev_del(&amp;icm20608dev.cdev); unregister_chrdev_region(icm20608dev.devid, ICM20608_CNT); /* 注销掉类和设备 */ device_destroy(icm20608dev.class, icm20608dev.devid); class_destroy(icm20608dev.class); return 0; } /* 传统匹配方式ID列表 */ static const struct spi_device_id icm20608_id[] = { {&#34;alientek,icm20608&#34;, 0}, {} }; /* 设备树匹配列表 */ static const struct of_device_id icm20608_of_match[] = { { .compatible = &#34;alientek,icm20608&#34; }, { /* Sentinel */ } }; /* SPI驱动结构体 */ static struct spi_driver icm20608_driver = { .probe = icm20608_probe, .remove = icm20608_remove, .driver = { .owner = THIS_MODULE, .name = &#34;icm20608&#34;, .of_match_table = icm20608_of_match, }, .id_table = icm20608_id, }; /* * @description : 驱动入口函数 * @param : 无 * @return : 无 */ static int __init icm20608_init(void) { return spi_register_driver(&amp;icm20608_driver); } /* * @description : 驱动出口函数 * @param : 无 * @return : 无 */ static void __exit icm20608_exit(void) { spi_unregister_driver(&amp;icm20608_driver); } module_init(icm20608_init); module_exit(icm20608_exit); MODULE_LICENSE(&#34;GPL&#34;); MODULE_AUTHOR(yikoulinux&#34;); ]]></content>
  </entry>
  
  <entry>
    <title>什么是 nftables ? 它与 iptables 的区别是什么？</title>
    <url>/post/linux/difference-between-nftables-and-iptables.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>nftables, iptables</tag>
    </tags>
    <content type="html"><![CDATA[几乎每个 Linux 管理员都使用过 iptables，它是一个 Linux 系统的防火墙。
什么是 nftables ? 它与 iptables 的区别是什么？ 但是你可能还不太熟悉 nftables，这是一个新的防火墙，可为我们提供一些必需的升级，还有可能会取代 iptables。
为什么要使用 nftables 呢？ nftables 是由 Netfilter 开发的，该组织目前维护 iptables。nftables 的创建是为了解决 iptables 的一些性能和扩展问题。
除了新的语法和一些升级以外，nftables 的功能与 iptables 几乎是一样的。之所以推出 nftables 的另一个原因，是因为 iptables 的框架变的有点复杂，iptables, ip6tables, arptables 以及 ebtables 都有不同但相似的功能。
比如，在 iptables 中创建 IPv4 规则和在 ip6tables 中创建 IPv6 规则并保持两者同步是非常低效的。Nftables 旨在取代所有这些，成为一个集中的解决方案。
尽管自 2014 年以来，nftables 就被包含在 Linux 内核中，但随着采用范围的扩大，它最近越来越受欢迎。Linux 世界的变化很慢，过时的实用程序通常需要几年或更长的时间才能逐步淘汰，取而代之的是升级后的实用程序。
今天我们就简单介绍一下 nftables 和 iptables 之间的差异，并展示在新的 nftables 语法中配置防火墙规则的例子。
nftables 中的链（chains）和规则 在 iptables 中，有三个默认的链：输入、输出和转发。这三个“链”（以及其他链）包含“规则”，iptables 通过将网络流量与 链中的规则列表匹配进行工作。如果正在检查的流量与任何规则都不匹配，则链的默认策略将用于流量（即ACCEPT、DROP）。
Nftables的工作原理与此类似，也有“链”和“规则”。然而，它一开始没有任何基础链，这使得配置更加灵活。
iptables 效率低下的一个方面是，即使流量与任何规则都不匹配，所有网络数据也必须遍历上述链中的一个或多个。无论你是否配置了链，iptables仍然会根据它们检查你的网络数据。
在 Linux 中安装 nftables nftables 在所有主要的 Linux 发行版中都可用，可以使用发行版的包管理器安装。
在 Ubuntu 或基于 Debian 的系统中可使用如下命令：
sudo apt install nftables 设置 nftables在系统重启的时候自动启动，可执行如下操作：
sudo systemctl enable nftables.service iptables 和 nftables 之间的语法差异 与 iptables 相比，nftables 的语法更加简单，不过对于 iptables 中的语法，在 nftables 中也能用。
大家可使用 iptables-translate 工具，该工具接受 iptables 命令并将其转为等效的 nftables 命令，这是了解两种语法差异的一种简单方法。
使用以下命令在 Ubuntu 和基于 Debian 的发行版上安装 iptables-translate：
sudo apt install iptables-nftables-compat 安装后，你可以将 iptables 语法传递给 iptables-translate 命令，它将返回 nftables 等效命令。
下面我们看一些具体的语法示例。
阻止传入连接 下述命令将阻止来自IP地址192.168.2.1的传入连接：
$ iptables-translate -A INPUT -s 192.168.2.1 -j DROP nft add rule ip filter INPUT ip saddr 192.168.2.1 counter drop 允许传入SSH连接 放开 ssh 连接权限：
$ iptables-translate -A INPUT -p tcp --dport 22 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT nft add rule ip filter INPUT tcp dport 22 ct state new,established counter accept 允许来自特定 IP 范围的传入SSH连接 如果只想允许来自192.168.1.0/24的传入SSH连接：
$ iptables-translate -A INPUT -p tcp -s 192.168.1.0/24 --dport 22 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT nft add rule ip filter INPUT ip saddr 192.168.1.0/24 tcp dport 22 ct state new,established counter accept 允许MySQL连接到eth0网络接口 $ iptables-translate -A INPUT -i eth0 -p tcp --dport 3306 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT nft add rule ip filter INPUT iifname eth0 tcp dport 3306ct state new,established counter accept 允许传入HTTP和HTTPS流量 为了允许特定类型的流量，以下是这两个命令的语法：
$ iptables-translate -A INPUT -p tcp -m multiport --dports 80,443 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT nft add rule ip filter INPUT ip protocol tcp tcp dport { 80,443} ct state new,established counter accept 从这些例子中可以看出，nftables 语法与 iptables 非常相似，但命令更直观一些。
nftables 日志 上述nft命令示例中的“counter”选项告诉nftables统计规则被触碰的次数，就像默认情况下使用的iptables一样。
在nftables中，需要指定：
nft add rule ip filter INPUT ip saddr 192.168.2.1 counter accept nftables内置了用于导出配置的选项。它目前支持XML和JSON。
nft export xml ]]></content>
  </entry>
  
  <entry>
    <title>风河支持NXP S32G3——信心百倍、加速创新</title>
    <url>/post/news/wind-river-supports-nxp-s32g3.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Wind River</tag>
      <tag>NXP</tag>
      <tag>S32G3</tag>
    </tags>
    <content type="html"><![CDATA[风河公司与NXP公司的合作伙伴关系深厚且长远，由此不断地为我们的共同客户带来新的效益。 风河公司非常兴奋地宣布其运行时（Runtime）产品组合和Wind River Studio Linux 服务都已经支持下一代NXP S32G3处理器，从而有力地促进企业客户加速智能边缘设备的创建与部署。
具体而言，风河将为VxWorks、Wind River Linux和Helix虚拟化平台提供S32G3 BSP（板级支持包），以及配套的设计、全生命周期安全与维护服务。例如，企业客户希望充分利用S32G3处理器独特的车辆场景功能，而风河的实时产品阵容以及久经验证的嵌入式专业知识都将在相应的软件支持平台上助力汽车解决方案的设计与构建。
以上都是风河与NXP长期合作伙伴关系的基本组成部分，双方将共同致力于为嵌入式行业提供成熟可靠的硬件和软件解决方案。两家公司密切合作，其系统化效益就是形成了紧密集成的解决方案，获得更高的开发人员生产力、更佳的成本效率，还有更高的可靠性以及更快的上市速度。
无论您属于哪个行业，无论您的企业正在为智能边缘创建什么样的系统或设备，风河广泛的软件产品阵容和DevOps成套工具都可以帮助您充分发挥NXP硬件潜能，加速您的项目进程。以下是风河为NXP客户提供支持的几个典型代表：
 BSP 支持：风河为NXP系列处理器提供众多BSP，包括S32G、I.MX和Layerscape系列。 NXP LINUX SDK支持：针对NXP基于Yocto的Linux软件开发工具包，Wind River Studio Linux Services提供安全扫描、漏洞缓解和补救以及全生命周期管理和维护服务。 汽车解决方案：VxWorks和Wind River Linux为汽车网关、域和区域控制器提供高性能实时解决方案。在运行Linux或Android的i.MX应用处理器上构建信息娱乐解决方案方面，风河也拥有丰富的经验。 工业、医疗和机器人解决方案：风河拥有大量的NXP BSP ，非常适合用于关键基础设施、医疗和机器人的设计，因为它们都需要依赖于VxWorks的实时和确定性或Wind River Linux的开源灵活性。风河还为多个i.MX应用处理器提供VxWorks和Wind River Linux BSP，包括最新的i.MX x 93以及Helix Virtualization Platform虚拟化平台。 航空航天和国防解决方案：凡是需要安全认证的嵌入式设计都将得益于我们对NXP处理器的支持，从传统的Power架构到最新的Arm架构解决方案，如LX2160。借助于我们的Helix Virtualization Platform虚拟化平台和VxWorks和Wind River Linux的BSP，可以将NXP解决方案作为当今许多飞行系统的基石。 面向NXP平台的数字孪生和模拟仿真：帮助测试前移且提前启动软件项目并更快地推进开发工作，更快地构建更高质量且更安全的软件系统，并与DevOps开发实践完全集成。模拟仿真基于NXP硬件和软件的完整系统，支持ARM和PowerPC架构。 风河与NXP合作的广度和深度以及对最新产品的支持，都将为企业客户带来了巨大的价值，例如简化整个团队的开发流程以及加快产品上市速度。我们期待在S32G3上看到令人兴奋的新产品。  原文连接 VxWorks俱乐部  
]]></content>
  </entry>
  
  <entry>
    <title>Linux中pdf转word的工具你知道几个</title>
    <url>/post/linux/five-tools-to-convert-pdf-to-word-in-linux.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>PDF</tag>
      <tag>Word</tag>
    </tags>
    <content type="html"><![CDATA[下面推荐5款Linux中pdf转word的工具
LibreOffice LibreOffice是一个免费的开源Office套件，可以将PDF文件转换为Word文档。
# 安装LibreOffice sudo apt-get install libreoffice # 将PDF转换为Word文档 libreoffice --convert-to docx filename.pdf 其中，“docx”表示要将PDF转换为的输出格式，“filename.pdf”是要转换的PDF文件的文件名。该工具可以在Linux系统中使用，并且命令简单易用。
Pdf2doc Pdf2doc是一个基于Python的命令行工具，可以将PDF文件转换为Microsoft Word文档。
# 安装Pdf2doc sudo apt-get install pdf2doc # 将PDF转换为Word文档 pdf2doc filename.pdf 在这个命令中，“filename.pdf”是要转换的PDF文件的文件名。该工具可以在Linux中使用，只需在终端输入相应的命令即可。
GImageReader GImageReader是一款免费的OCR（光学字符识别）软件，可以将扫描的PDF文件转换为Word文档。
# 安装GImageReader sudo apt-get install gimagereader # 将PDF转换为Word文档 gimagereader filename.pdf 在这个命令中，“filename.pdf”是要转换的PDF文件的文件名。该工具支持多种语言的OCR识别，可以在Linux中使用。
Pandoc Pandoc是一个跨平台的文档格式转换器，支持将PDF文件转换为Word文档。
# 安装Pandoc sudo apt-get install pandoc # 将PDF转换为Word文档 pandoc -s -o output.docx input.pdf 在这个命令中，“-s”表示生成带有样式的Word文档，“-o”后面跟输出文件的名称和保存的路径，“input.pdf”是要转换的PDF文件的文件名。该工具可以在Linux中进行使用。
Unoconv Unoconv是一个用于文件格式转换的Python包，可以将PDF文件转换为Word文档。
# 安装Unoconv sudo apt-get install unoconv # 将PDF转换为Word文档 unoconv -f docx filename.pdf 在这个命令中，“docx”表示要将PDF文件转换成的输出格式，“filename.pdf”表明要转换的PDF文件的文件名。这个工具也非常简单易用。
总结：以上是5个将PDF文件转换为Word文档的Linux工具，它们都是免费的工具，不需要任何商业版本。这些工具均可以在Linux平台上使用，并且可以快速并且容易地将PDF文件转换为Word文档。
]]></content>
  </entry>
  
  <entry>
    <title>Linux系统内核概述</title>
    <url>/post/linux/linux-kernel-overview.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Kernel</tag>
    </tags>
    <content type="html"><![CDATA[Linux 内核是一种开源的类 Unix 操作系统宏内核。
Linux 内核是 Linux 操作系统的主要组件，也是计算机硬件与其进程之间的核心接口。它负责两者之间的通信，还要尽可能高效地管理资源。之所以称为内核，是因为它在操作系统中就像果实硬壳中的种子一样，并且控制着硬件的所有主要功能。内核的用途主要有以下 4 项工作：
  内存管理：追踪记录有多少内存存储了什么以及存储在哪里
  进程管理：确定哪些进程可以使用中央处理器、何时使用以及持续多长时间
  设备驱动程序：充当硬件与进程之间的调解程序/解释程序
  系统调用和安全防护：从流程接受服务请求
  在正确实施的情况下，内核对于用户是不可见的，它在自己的小世界(称为内核空间)中工作，并从中分配内存和跟踪所有内容的存储位置。用户所看到的内容则被称为用户空间。这些应用通过系统调用接口(SCI)与内核进行交互。
内核简介 单内核体系设计、但充分借鉴了微内核设计体系的优点，为内核引入模块化机制。
Linux 内核的重要组成部分，主要有以下几部分：
kernel内核核心，一般为 bzImage通常在 /boot 目录下，名称为 vmlinuz-VERSION-RELEASE kernel object内核对象，一般放置于 /lib/modules/VERSION-RELEASE/[ ] ==&gt; N ==&gt; 不编译进内核[M] ==&gt; M ==&gt; 编译为模块文件[*] ==&gt; Y ==&gt; 编译进内核 辅助文件(ramdisk)initrdinitramfs 内核模块 uname 命令 使用格式 uname [OPTION]&hellip;
参数解释  -n 显示节点名称 -r 显示VERSION-RELEASE -s 内核名称 -v 内核版本 -n 节点名 -m 硬件名称 -i 硬件平台 -p 处理器类型 -o 操作系统  # uname -m i686 # uname -r 2.6.32-573.22.1.el6.i686 # uname -a Linux MyServer 2.6.32-573.22.1.el6.i686 ... i686 i386 GNU/Linux lsmod 命令 显示由核心已经装载的内核模块
命令定义 显示的内容来自于: /proc/modules 文件。
使用 lsmod 命令时，常会采用类似 lsmod | grep -i ext4 这样的命令来查询系统是否加载了某些模块。
# cat /proc/modules iptable_filter 2173 0 - Live 0xed9b2000 ip_tables 9567 1 iptable_filter, Live 0xed9a9000 ext3 203718 1 - Live 0xed962000 jbd 65315 1 ext3, Live 0xed904000 xenfs 4360 1 - Live 0xed8e6000 ipv6 271097 14 - Live 0xed88e000 xen_netfront 15871 0 - Live 0xed7d9000 ext4 339812 2 - Live 0xed764000 jbd2 75927 1 ext4, Live 0xed6d9000 mbcache 6017 2 ext3,ext4, Live 0xed6b7000 xen_blkfront 19209 5 - Live 0xed69f000 dm_mirror 11969 0 - Live 0xed68d000 dm_region_hash 9644 1 dm_mirror, Live 0xed67e000 dm_log 8322 2 dm_mirror,dm_region_hash, Live 0xed672000 dm_mod 84711 11 dm_mirror,dm_log, Live 0xed64e000 # lsmod | grep ext4 ext4 339812 2 jbd2 75927 1 ext4 mbcache 6017 2 ext3,ext4 字段含义  第 1 列：表示模块的名称 第 2 列：表示模块的大小 第 3 列：表示依赖模块的个数 第 4 列：表示依赖模块的内容  # lsmod Module Size Used by iptable_filter 2173 0 ip_tables 9567 1 iptable_filter ext3 203718 1 jbd 65315 1 ext3 xenfs 4360 1 ipv6 271097 14 xen_netfront 15871 0 ext4 339812 2 jbd2 75927 1 ext4 mbcache 6017 2 ext3,ext4 xen_blkfront 19209 5 dm_mirror 11969 0 dm_region_hash 9644 1 dm_mirror dm_log 8322 2 dm_mirror,dm_region_hash dm_mod 84711 11 dm_mirror,dm_log modinfo 命令 显示模块的详细描述信息
命令定义 modinfo 列出 Linux 内核中命令行指定的模块的信息。
modinfo 能够查询系统中未安装的模块信息。
若模块名不是一个文件名，则会在 /lib/modules/version 目录中搜索，就像 modprobe 一样。
modinfo 默认情况下，为了便于阅读，以下面的格式列出模块的每个属性：fieldname : value。
语法 modinfo [选项] [ modulename|filename&hellip; ]
选项  -n 只显示模块文件路径 -p 显示模块参数 -a author -d description -l license -0 使用’\0’字符分隔 field 值，而不是一个新行，对脚本比较有用  实战演示 # modinfo ext4 filename: /lib/modules/2.6.32-573.22.1.el6.i686/kernel/fs/ext4/ext4.ko license: GPL description: Fourth Extended Filesystem author: Remy Card, Stephen Tweedie, Andrew Morton, Andreas Dilger, Theodore and others srcversion: CB1B990F5A758DFB0FB12F1 depends: mbcache,jbd2 vermagic: 2.6.32-573.22.1.el6.i686 SMP mod_unload modversions 686 # modinfo btrfs filename: /lib/modules/2.6.32-573.22.1.el6.i686/kernel/fs/btrfs/btrfs.ko license: GPL alias: devname:btrfs-control alias: char-major-10-234 srcversion: B412C18B0F5BF7F1B3C941A depends: libcrc32c,zlib_deflate,lzo_compress,lzo_decompress vermagic: 2.6.32-573.22.1.el6.i686 SMP mod_unload modversions 686 modprobe 命令 装载或卸载内核模块
命令定义 配置文件/etc/modprobe.conf/etc/modprobe.d/*.conf
解决依赖modprobe需要一个最新的modules.dep文件，可以用depmod来生成该文件列出了每一个模块需要的其他模块，modprobe使用这个去自动添加或删除模块的依赖
bash # modules.dep为解决依赖的配置文件，modules.dep.bin二进制文件运行 # ls /lib/modules/2.6.32-358.6.1.el6.i686/ build modules.block modules.ieee1394map modules.ofmap modules.symbols.bin weak-updates extra modules.ccwmap modules.inputmap modules.order modules.usbmap kernel modules.dep modules.isapnpmap modules.pcimap source modules.alias modules.dep.bin modules.modesetting modules.seriomap updates modules.alias.bin modules.drm modules.networking modules.symbols vdso 语法 modprobe [ -c ]
modprobe [ -l ] [ -t dirname ] [ wildcard ]
modprobe [ -r ] [ -v ] [ -n ] [ -i ] [ modulename … ]
选项  -v显示程序在干什么，通常在出问题的情况下，modprobe 才显示信息 -C重载，默认配置文件(/etc/modprobe.conf 或 /etc/modprobe.d) -c输出配置文件并退出 -n可以和 -v 选项一起使用，调试非常有用 -i该选项会使得 modprobe 忽略配置文件中的，在命令行上输入的 install 和 remove -q一般 modprobe 删除或插入一个模块时，若没有找到会提示错误。使用该选项，会忽略指定的模块，并不提示任何错误信息。 -r该选项会导致 modprobe 去删除，而不是插入一个模块通常没有没有理由去删除内核模块，除非是一些有 bug 的模块 -f使用该选项是比较危险的和同时使用 –force-vermagic，–force-modversion 一样 -l列出所有模块 -a插入所有命令行中的模块 -t强制 -l 显示 dirname 中的模块 -s错误信息写入 syslog  depmod 命令 内核模块依赖关系文件及系统信息映射文件的生成工具
语法 depmod [-adeisvV][-m &lt;文件&gt;][&ndash;help][模块名称]
参数  -a 分析所有可用的模块 -d 执行排错模式 -e 输出无法参照的符号 -i 不检查符号表的版本 -m&lt;文件&gt; 使用指定的符号表文件 -s 在系统记录中记录错误 -v 执行时显示详细的信息 -V 显示版本信息 &ndash;help 显示帮助  insmod 和 rmmod 命令 装载或卸载内核模块
不解决依赖关系，需要自己手动卸载
insmod命令 向 Linux 内核中插入一个模块
insmod 是一个向内核插入模块的小程序
大多数用户使用 modprobe 因为它比较智能化
insmod [ filename ] [ module options&hellip; ]
rmmod命令 命令解析删除内核中的一模块rmmod 是一个可以从内核中删除模块的小程序，大多数用户使用modprobe -r去删除模块
语法格式rmmod [ modulename ]
参数选项-f除非编译内核时 CONFIG_MODULE_FORCE_UNLOAD 被设置该命令才有效果，否则没效果用该选项可以删除正在被使用的模块，设计为不能删除的模块，或者标记为 unsafe 的模块-wrmmod 拒绝删除正在被使用的模块使用该选项后，指定的模块会被孤立起来，直到不被使用-s将错误信息写入 syslog，而不是标准错误(stderr)
/proc 目录 内核把自己内部状态信息及统计信息，以及可配置参数通过 proc 伪文件系统加以输出。
# ls /proc/ 1 1173 22 29855 35 47 60 973 filesystems loadavg scsi version 10 12 23 3 36 48 600 buddyinfo fs locks self vmallocinfo 1071 13 232 30 37 49 61 bus interrupts mdstat slabinfo vmstat 1082 14 234 31 38 5 62 cgroups iomem meminfo softirqs xen 1085 15 24 31314 39 528 7 cmdline ioports misc stat zoneinfo 11 16 25 317 4 531 739 cpuinfo irq modules swaps 1150 17 252 318 40 543 8 crypto kallsyms mounts sys 1162 18 253 32 41 56 808 devices kcore mtd sysrq-trigger 1163 19 26 320 42 566 830 diskstats keys net sysvipc 1165 1908 27 33 43 567 853 dma key-users pagetypeinfo timer_list 1167 2 28 330 44 57 9 driver kmsg partitions timer_stats 1169 20 29 334 45 59 94 execdomains kpagecount sched_debug tty 1171 21 29853 34 46 6 95 fb kpageflags schedstat uptime sysctl 命令 语法格式 sysctl(选项)(参数)
命令参数  -n 打印值时不打印关键字 -e 忽略未知关键字错误 -N 仅打印名称 -w 当改变 sysctl 设置时使用此项 -p 从配置文件 /etc/sysctl.conf 加载内核参数设置 -a 打印当前所有可用的内核参数变量和值 -A 以表格方式打印当前所有可用的内核参数变量和值  默认配置文件 /etc/sysctl.conf
命令使用方式 (1) 设置某参数sysctl -w parameter=VALUE (2) 通过读取配置文件设置参数sysctl -p [/path/to/conf_file]
参数说明 只读：输出信息
可写：可接受用户指定“新值”来实现对内核某功能或特性的配置/proc/sys
两种修改方式 (1) sysctl 命令用于查看或设定此目录中诸多参数sysctl -w path.to.parameter=VALUEsysctl -w kernel.hostname=mail.escapelife.com
(2) echo 命令通过重定向的方式也可以修改大多数参数的值echo &ldquo;VALUE&rdquo; &gt; /proc/sys/path/to/parameterecho &ldquo; www.escapelife.com  &rdquo; &gt; /proc/sys/kernel/hostname
配置文件中常用的几个参数
net.ipv4.ip_forward /proc/sys/net/ipv4/ip_forward vm.drop_caches /proc/sys/vm/drop_caches kernel.hostname /proc/sys/kernel/hostname 修改配置文件 # cat /etc/sysctl.conf # Kernel sysctl configuration file for Red Hat Linux # Controls IP packet forwarding net.ipv4.ip_forward = 0 # Controls source route verification net.ipv4.conf.default.rp_filter = 1 # Do not accept source routing net.ipv4.conf.default.accept_source_route = 0 # Controls the System Request debugging functionality of the kernel kernel.sysrq = 0 # Controls whether core dumps will append the PID to the core filename. # Useful for debugging multi-threaded applications. kernel.core_uses_pid = 1 # Controls the use of TCP syncookies net.ipv4.tcp_syncookies = 1 # Disable netfilter on bridges. net.bridge.bridge-nf-call-ip6tables = 0 net.bridge.bridge-nf-call-iptables = 0 net.bridge.bridge-nf-call-arptables = 0 # Controls the default maxmimum size of a mesage queue kernel.msgmnb = 65536 # Controls the maximum size of a message, in bytes kernel.msgmax = 65536 # Controls the maximum shared segment size, in bytes kernel.shmmax = 4294967295 # Controls the maximum number of shared memory segments, in pages kernel.shmall = 268435456 # Auto-enabled by xs-tools:install.sh net.ipv4.conf.all.arp_notify = 1 实战演示 # 查看所有可读变量 sysctl -a # 修改对应参数 sysctl -w kernel.sysrq=0 sysctl -w kernel.core_uses_pid=1 sysctl -w net.ipv4.conf.default.accept_redirects=0 # 如果希望屏蔽别人 ping 你的主机，配置文件修改 net.ipv4.icmp_echo_ignore_all = 1 # 编辑完成后，请执行以下命令使变动立即生效 /sbin/sysctl -p /sbin/sysctl -w net.ipv4.route.flush=1 /sys 目录 sysfs 伪文件系统，输出内核识别出的各硬件设备的相关属性信息，也有内核对硬件特性的设定信息。有些参数是可以修改的，用于调整硬件工作特性。
udev  udev 是运行用户空间程序。 udev 通 /sys/ 路径下输出的信息动态为各设备创建所需要设备文件。 udev 是 Linux 内核的设备管理器，它取代了 udevadmin 和 hotplug，负责管理 /dev 中的设备节点。 udev 也处理所有用户空间发生的硬件添加、删除事件，以及某些特定设备所需的固件加载。 udev 为设备创建设备文件时，会读取其事先定义好的规则文件，一般在 /etc/udev/rules.d 及 /usr/lib/udev/rules.d 目录下。  ramdisk 文件的制作 方法一 mkinitrd 命令
为当前正在使用的内核重新制作 ramdisk 文件
mkinitrd /boot/initramfs-$(uname -r).img $(uname -r)
# 移动ramdisk文件到/root目录下 mv /boot/initramfs-2.6.32...img /root # 为当前正在使用的内核重新制作ramdisk文件 mkinitrd /boot/initramfs-$(uname -r).img $(uname -r) 方法二 dracut 命令
为当前正在使用的内核重新制作 ramdisk 文件
dracut /boot/initramfs-$(uname -r).img $(uname -r)
# 移动ramdisk文件到/root目录下 mv /boot/initramfs-2.6.32...img /root # 为当前正在使用的内核重新制作ramdisk文件 dracut /boot/initramfs-$(uname -r).img $(uname -r) 查看 ramdisk # 使用file命令查看ramdisk文件发现是以gz压缩存放的 file /boot/initramfs-2.6.32-504.el6.x86_64.img # 改名称，解压 cd /boot/ mv initramfs-2.6.32-504.el6.x86_64.img initramfs-2.6.32-504.el6.x86_64.img.gz gzip -d initramfs-2.6.32-504.el6.x86_64.img.gz # 使用file命令查看发现是以cpio存放的文本文件 file initramfs-2.6.32-504.el6.x86_64.img # 解压这个文本文件 # 之后会在initrd目录下生成相应的文件，一个微型的/root mkdir initrd cd initrd cpio -id &lt; ../initramfs-2.6.32-504.el6.x86_64.img # 这个时候就可以查看init脚本文件了 cat init # 在sbin文件中存放着相关的命令 ls sbin 编译内核 前提准备 (1) 准备好开发环境
包组(CentOS 6)
Server Platform Development
Development Tools
(2) 获取目标主机上硬件设备的相关信息
CPUcat /proc/cpuinfox86info -alscpu
PCI 设备lspci-v-vvlsusb-v-vvlsblk
了解全部硬件设备信息hal-device
(3) 获取到目标主机系统功能的相关信息
(4) 获取内核源代码包
 www.kernel.org  
简易安装内核 简易安装 获取当前系统的安装文件作为模块安装较为方便
修改相应的参数即可
只适用于当前特定的内核版本
当前系统的安装文件在 config-2.6.32-504.el6.x86_64
简单依据模板文件的制作内核 # 下载对应的Linux内核版本进行解压缩 # 会在/usr/src目录下创建debug、kernels和linux-3.10.67目录 tar xf linux-3.10.67.tar.xz -C /usr/src # 为了方便多内核共存，使用连接指向 # 会在当前目录下创建一个链接文件 linux -&gt; linux-3.10.67 cd /usr/src ln -sv linux-3.10.67 linux # 创建模板 cd linux # 查看链接指向的文件内容 ls # 拷贝系统自带的模板文件 cp /boot/config-$(uname -r) .config # 打开图形界面配置内核选项，选择添加、删除内核模块 # 添加的默认选项来自.config配置文件 make menuconfig # 使用screen来不中断安装 screen # 采用几个线程进行编译 make -j n # 安装内核 make modules_install # make install中将会安装内容 # 安装bzImage为/boot/vmlinuz-VERSION-RELEASE # 生成initramfs文件 # 编辑grub的配置文件 make install # 重启系统，并测试使用新内核，不是默认启动内核 init 6 详解编译内核 (1) 配置内核选项 支持“更新”模式进行配置  (a) make config：基于命令行以遍历的方式去配置内核中可配置的每个选项 (b) make menuconfig：基于 curses 的文本窗口界面 (c) make gconfig：基于 GTK 开发环境的窗口界面 (d) make xconfig：基于 Qt 开发环境的窗口界面  支持“全新配置”模式进行配置  (a) make defconfig：基于内核为目标平台提供的“默认”配置进行配置 (b) make allnoconfig: 所有选项均回答为”no“  (2) 编译 - make [-j #] 如何只编译内核中的一部分功能
# (a)只编译某子目录中的相关代码 cd /usr/src/linux make dir/ # (b)只编译一个特定的模块 cd /usr/src/linux make dir/file.ko # 例如：只为e1000编译驱动 make drivers/net/ethernet/intel/e1000/e1000.ko 如何交叉编译内核
# 编译的目标平台与当前平台不相同； make ARCH=arch_name # 要获取特定目标平台的使用帮助 make ARCH=arch_name help 如何在已经执行过编译操作的内核源码树做重新编译
# 事先清理操作 # 清理大多数编译生成的文件，但会保留config文件等 make clean # 清理所有编译生成的文件、config及某些备份文件 make mrproper # mrproper、patches以及编辑器备份文件 make distclean ]]></content>
  </entry>
  
  <entry>
    <title>如何在 Linux 使用 pv 命令监控数据传输速度与进度</title>
    <url>/post/linux/using-pv-cmd-to-monitor-data-transfer-speed-and-progress.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>pv</tag>
    </tags>
    <content type="html"><![CDATA[pv 命令是一个在 Linux/Unix 系统的实用工具，用于监控数据的流动。pv 命令可以用于计算数据传输的速度，显示进度条以及估算剩余时间等。
pv命令可以通过管道监控数据流的进度，但是无法直接监控mv命令的进度。这是因为mv命令实际上是将文件从一个位置移动到另一个位置，而不是在管道中传输数据。
默认情况下，pv 命令只会显示一个进度条，以及传输速度和估算的剩余时间等信息。除了默认的进度条外，pv 命令还支持许多选项。
在本教程中，我们将会说明如何在 Linux 使用 pv 命令监控数据的传输速度并估算剩余时间等信息。
pv 命令 pv 命令的使用非常简单，只需在终端输入 pv 命令，后面是需要监视的文件或数据流即可。
pv file.txt &gt; /dev/null pv 命令选项 这些选项可以用于更好地控制 pv 命令的输出。下面是一些常用的选项：
 -f，&ndash;force：  强制 pv 命令执行，即使标准错误不是终端也输出。
 -n，&ndash;numeric：  显示数字的百分比和速度而不是进度条。
 -q，&ndash;quiet：  不输出错误和警告信息。
 -s，&ndash;size SIZE：  指定输入流的大小。
 -t，&ndash;timer：  显示时间估计。
 -h，&ndash;help：  显示 pv 命令的帮助信息。
 -V，&ndash;version：  显示 pv 命令的版本信息。
下面是一些使用pv命令的示例：
监控文件传输 在这个示例中，pv 命令将 file.txt 文件的内容输出到 /dev/null 空设备中，这是一个类似于垃圾桶的设备，只可写入但无法读取的设备。
可以将其用于丢弃不需要的输出。pv 命令会计算 file.txt 文件的大小并显示一个进度条，以及估算剩余时间和传输速度等信息。
pv file.txt &gt; /dev/null 监控标准输入流向标准输出的数据 在这个示例中，pv 命令将 file.txt 文件的内容通过管道传递给 gzip 命令进行压缩，然后将压缩后的数据写入到 file.txt.gz文件。
pv 命令会监控管道中的数据流，并显示一个进度条，以及估算剩余时间和传输速度等信息。
cat file.txt | pv | gzip &gt; file.txt.gz 评估数据传输时间 在这个示例中，pv 命令将 iso 镜像文件的内容通过管道传递给 dd 命令进行写入，pv命令会计算 iso 文件的大小并显示一个进度条，以及估算剩余时间和传输速度等信息。
pv -pteb file.iso | dd of=/dev/sdb 使用场景 以上示例只是 pv 命令的基本用法，但实际上，pv 命令可以在许多场景中发挥重要的作用。例如，在备份文件或复制大量文件时，pv 命令可以帮助用户跟踪数据的传输速度和进度，以及估算剩余时间。
对于网络传输或云存储等场景中，pv 命令可以帮助用户监视数据流，确保传输的可靠性和效率。
此外，在编写脚本或命令行工具时，pv 命令也可以用于监视数据流并提供更好的用户体验。
除了以上介绍的选项和示例外，pv 命令还具有许多其他功能。例如，pv 命令可以与其他命令和工具结合使用，如tar、rsync、scp等，实现更复杂的数据传输和备份操作。此外，pv 命令还支持在终端显示颜色，以便用户更容易地识别不同类型的信息。
限制 需要注意的是，pv 命令虽然非常实用，但也有一些局限性。首先，pv 命令只能监控单个数据流，而不能同时监控多个数据流。其次，pv 命令无法解密加密的数据流，因此无法直接监视加密的数据流。
最后，pv 命令会消耗一定的 CPU 资源和内存，因此在处理大文件或大量数据时，可能会对系统性能产生一定的影响。
总的来说，pv 命令是一个非常实用的 Linux/Unix 工具，可以帮助用户监控数据流，计算传输速度和估算剩余时间等信息。
通过结合不同的选项和示例，用户可以充分利用 pv 命令的功能，以更好地管理和监视数据传输和备份操作。
如果您有任何问题或反馈，请随时发表评论。点击下方阅读原文获取更好排版格式与文章参考引用。
]]></content>
  </entry>
  
  <entry>
    <title>关于extern C的的详细剖析</title>
    <url>/post/programming/extern-c-detailed-analysis.html</url>
    <categories><category>Programming</category>
    </categories>
    <tags>
      <tag>C</tag>
      <tag>extern</tag>
    </tags>
    <content type="html"><![CDATA[在你工作过的系统里，不知能否看到类似下面的代码。
这好像没有什么问题，你应该还会想 “嗯，是啊，我们的代码都是这样写的，从来没有因此碰到过什么麻烦啊～”。
你说的没错，如果你的头文件从来没有被任何C++程序引用过的话。
这与C++有什么关系呢? 看看__cplusplus（注意前面是两个下划线）的名字你就应该知道它与C++有很大关系。__cplusplus是一个C++规范规定的预定义宏。你可以信任的是：所有的现代C++编译器都预先定义了它；而所有C语言编译器则不会。另外，按照规范__cplusplus的值应该等于1 9 9 7 1 1 L ，然而不是所有的编译器都照此实现，比如g++编译器就将它的值定义为1。
所以，如果上述代码被C语言程序引用的话，它的内容就等价于下列代码。
在这种情况下，既然extern &ldquo;C&rdquo; { }经过预处理之后根本就不存在，那么它和#include指令之间的关系问题自然也就是无中生有。
extern &ldquo;C&quot;的前世今生 在C++编译器里，有一位暗黑破坏神，专门从事一份称作“名字粉碎”(name mangling)的工作。当把一个C++的源文件投入编译的时候，它就开始工作，把每一个它在源文件里看到的外部可见的名字粉碎的面目全非，然后存储到二进制目标文件的符号表里。
之所以在C++的世界里存在这样一个怪物，是因为C++允许对一个名字给予不同的定义，只要在语义上没有二义性就好。比如，你可以让两个函数是同名的，只要它们的参数列表不同即可，这就是函数重载(function overloading)；甚至，你可以让两个函数的原型声明是完全相同的，只要它们所处的名字空间(namespace)不一样即可。事实上，当处于不同的名字空间时，所有的名字都是可以重复的，无论是函数名，变量名，还是类型名。
另外，C++程序的构造方式仍然继承了C语言的传统：编译器把每一个通过命令行指定的源代码文件看做一个独立的编译单元，生成目标文件；然后，链接器通过查找这些目标文件的符号表将它们链接在一起生成可执行程序。
编译和链接是两个阶段的事情；事实上，编译器和链接器是两个完全独立的工具。编译器可以通过语义分析知道那些同名的符号之间的差别；而链接器却只能通过目标文件符号表中保存的名字来识别对象。
所以，编译器进行名字粉碎的目的是为了让链接器在工作的时候不陷入困惑，将所有名字重新编码，生成全局唯一，不重复的新名字，让链接器能够准确识别每个名字所对应的对象。
但 C语言却是一门单一名字空间的语言，也不允许函数重载，也就是说，在一个编译和链接的范围之内，C语言不允许存在同名对象。比如，在一个编译单元内部，不允许存在同名的函数，无论这个函数是否用static修饰；在一个可执行程序对应的所有目标文件里，不允许存在同名对象，无论它代表一个全局变量，还是一个函数。所以，C语言编译器不需要对任何名字进行复杂的处理（或者仅仅对名字进行简单一致的修饰（decoration），比如在名字前面统一的加上单下划线_）。
C++的缔造者Bjarne Stroustrup在最初就把——能够兼容C，能够复用大量已经存在的C库——列为C++语言的重要目标。但两种语言的编译器对待名字的处理方式是不一致的，这就给链接过程带来了麻烦。
例如，现有一个名为my_handle.h的头文件，内容如下：
然后使用C语言编译器编译my_handle.c，生成目标文件my_handle.o。
由于C语言编译器不对名字进行粉碎，所以在my_handle.o的符号表里，这三个函数的名字和源代码文件中的声明是一致的。
随后，我们想让一个C++程序调用这些函数，所以，它也包含了头文件my_handle.h。
假设这个C++源代码文件的名字叫my_handle_client.cpp，其内容如下：
其中，粗体的部分就是那三个函数的名字被粉碎后的样子。
然后，为了让程序可以工作，你必须将my_handle.o和my_handle_client.o放在一起链接。由于在两个目标文件对于同一对象的命名不一样，链接器将报告相关的“符号未定义”错误。
为了解决这一问题，C++引入了链接规范(linkage specification)的概念，表示法为extern&quot;language string&rdquo;，C++编译器普遍支持的&quot;language string&quot;有&quot;C&quot;和&quot;C++&quot;，分别对应C语言和C++语言。
链接规范的作用是告诉C++编译：对于所有使用了链接规范进行修饰的声明或定义，应该按照指定语言的方式来处理，比如名字，调用习惯（calling convention）等等。
链接规范的用法有两种：  单个声明的链接规范，比如：  extern &#34;C&#34; void foo(); 一组声明的链接规范，比如：  extern &#34;C&#34; { void foo(); int bar(); } 对我们之前的例子而言，如果我们把头文件my_handle.h的内容改成：
然后使用C++编译器重新编译my_handle_client.cpp，所生成目标文件my_handle_client.o中的符号表就变为：
从中我们可以看出，此时，用extern &ldquo;C&rdquo; 修饰了的声明，其生成的符号和C语言编译器生成的符号保持了一致。这样，当你再次把my_handle.o和my_handle_client.o放在一起链接的时候，就不会再有之前的“符号未定义”错误了。
但此时，如果你重新编译my_handle.c，C语言编译器将会报告“语法错误”，因为extern&quot;C&quot;是C++的语法，C语言编译器不认识它。此时，可以按照我们之前已经讨论的，使用宏__cplusplus来识别C和C++编译器。修改后的my_handle.h的代码如下：
小心门后的未知世界 在我们清楚了 extern &ldquo;C&rdquo; 的来历和用途之后，回到我们本来的话题上，为什么不能把#include 指令放置在 extern &ldquo;C&rdquo; { &hellip; } 里面？
我们先来看一个例子，现有a.h，b.h，c.h以及foo.cpp，其中foo.cpp包含c.h，c.h包含b.h，b.h包含a.h，如下：
现使用C++编译器的预处理选项来编译foo.cpp，得到下面的结果：
正如你看到的，当你把#include指令放置在extern &ldquo;C&rdquo; { }里的时候，则会造成extern &ldquo;C&rdquo; { } 的嵌套。这种嵌套是被C++规范允许的。当嵌套发生时，以最内层的嵌套为准。比如在下面代码中，函数foo会使用C++的链接规范，而函数bar则会使用C的链接规范。
如果能够保证一个C语言头文件直接或间接依赖的所有头文件也都是C语言的，那么按照C++语言规范，这种嵌套应该不会有什么问题。
但具体到某些编译器的实现，比如MSVC2005，却可能由于 extern &ldquo;C&rdquo; { } 的嵌套过深而报告错误。
不要因此而责备微软，因为就这个问题而言，这种嵌套是毫无意义的。你完全可以通过把#include指令放置在extern &ldquo;C&rdquo; { }的外面来避免嵌套。
拿之前的例子来说，如果我们把各个头文件的 #include 指令都移到extern &ldquo;C&rdquo; { } 之外，然后使用C++编译器的预处理选项来编译foo.cpp，就会得到下面的结果：
这样的结果肯定不会引起编译问题的结果——即便是使用MSVC。
把 #include 指令放置在extern &ldquo;C&rdquo; { }里面的另外一个重大风险是，你可能会无意中改变一个函数声明的链接规范。比如：有两个头文件a.h，b.h，其中b.h包含a.h，如下：
按照a.h作者的本意，函数foo是一个C++自由函数，其链接规范为&quot;C++&quot;。但在b.h中，由于#include &ldquo;a.h&quot;被放到了extern &ldquo;C&rdquo; { }的内部，函数foo的链接规范被不正确地更改了。
由于每一条 #include 指令后面都隐藏这一个未知的世界，除非你刻意去探索，否则你永远都不知道，当你把一条条#include指令放置于extern &ldquo;C&rdquo; { }里面的时候，到底会产生怎样的结果，会带来何种的风险。
或许你会说，“我可以去查看这些被包含的头文件，我可以保证它们不会带来麻烦”。但，何必呢？毕竟，我们完全可以不必为不必要的事情买单，不是吗？
Q &amp; A Q: 难道任何# i n c l u d e指令都不能放在e x t e r n &ldquo;C&quot;里面吗？
A: 正像这个世界的大多数规则一样，总会存在特殊情况。
有时候，你可能利用头文件机制“巧妙”的解决一些问题。比如，#pragma pack的问题。这些头文件和常规的头文件作用是不一样的，它们里面不会放置C的函数声明或者变量定义，链接规范不会对它们的内容产生影响。这种情况下，你可以不必遵守这些规则。
更加一般的原则是，在你明白了这所有的原理之后，只要你明白自己在干什么，那就去做吧。
Q: 你只说了不应该放入e x t e r n &ldquo;C&quot;的，但什么可以放入呢？
A: 链接规范仅仅用于修饰函数和变量，以及函数类型。所以，严格的讲，你只应该把这三种对象放置于extern &ldquo;C&quot;的内部。
但，你把C语言的其它元素，比如非函数类型定义（结构体，枚举等）放入extern &ldquo;C&quot;内部，也不会带来任何影响。更不用说宏定义预处理指令了。
所以，如果你更加看重良好组织和管理的习惯，你应该只在必须使用extern &ldquo;C&quot;声明的地方使用它。即使你比较懒惰，绝大多数情况下，把一个头件自身的所有定义和声明都放置在extern&quot;C&quot;里面也不会有太大的问题。
Q: 如果一个带有函数/变量声明的C头文件里没有e x t e r n &ldquo;C&quot;声明怎么办？
A: 如果你可以判断，这个头文件永远不可能让C++代码来使用，那么就不要管它。
但现实是，大多数情况下，你无法准确的推测未来。你在现在就加上这个extern &ldquo;C&rdquo;，这花不了你多少成本，但如果你现在没有加，等到将来这个头文件无意中被别人的C++程序包含的时候，别人很可能需要更高的成本来定位错误和修复问题。
Q: 如果我的C+ +程序想包含一个C头文件a . h，它的内容包含了C的函数/变量声明，但它们却没有使用e x t e r n &ldquo;C&quot;链接规范，该怎么办？
A: 在a.h里面加上它。
某些人可能会建议你，如果a.h没有extern &ldquo;C&rdquo;，而b.cpp包含了a.h，可以在b.cpp里加上 ：
extern &#34;C&#34; { #include &#34;a.h&#34;} 这是一个邪恶的方案，原因在之前我们已经阐述。但值得探讨的是，这种方案这背后却可能隐含着一个假设，即我们不能修改a.h。不能修改的原因可能来自两个方面：
  头文件代码属于其它团队或者第三方公司，你没有修改代码的权限；
  虽然你拥有修改代码的权限，但由于这个头文件属于遗留系统，冒然修改可能会带来不可预知的问题。
  对 于第一种情况，不要试图自己进行workaround，因为这会给你带来不必要的麻烦。正确的解决方案是，把它当作一个bug，发送缺陷报告给相应的团队 或第三方公司。
如果是自己公司的团队或你已经付费的第三方公司，他们有义务为你进行这样的修改。如果他们不明白这件事情的重要性，告诉他们。如果这些头文 件属于一个免费开源软件，自己进行正确的修改，并发布patch给其开发团队。
在 第二种情况下，你需要抛弃掉这种不必要的安全意识。
因为，首先，对于大多数头文件而言，这种修改都不是一种复杂的，高风险的修改，一切都在可控的范围之 内；
其次，如果某个头文件混乱而复杂，虽然对于遗留系统的哲学应该是：“在它还没有带来麻烦之前不要动它”，但现在麻烦已经来了，逃避不如正视，所以上策 是，将其视作一个可以整理到干净合理状态的良好机会。
Q: 我们代码中关于e x t e r n &ldquo;C&quot;的写法如下，这正确吗?
A: 不确定。
按照C++的规范定义，__cplusplus 的值应该被定义为199711L，这是一个非零的值；尽管某些编译器并没有按照规范来实现，但仍然能够保证__cplusplus的值为非零——至少我到目前为止还没有看到哪款编译器将其实现为0。
这种情况下，#if __cplusplus &hellip; #endif完全是冗余的。
但，C++编译器的厂商是如此之多，没有人可以保证某款编译器，或某款编译器的早期版本没有将__cplusplus的值定义为0。
但即便如此，只要能够保证宏__cplusplus只在C++编译器中被预先定义 ，那么，仅仅使用#ifdef __cplusplus ⋯ #endif就足以确保意图的正确性；额外的使用#if __cplusplus &hellip; #endif反而是错误的。
只有在这种情况下：即某个厂商的C语言和C++语言编译器都预先定义了__cplusplus ，但通过其值为0和非零来进行区分，使用#if __cplusplus &hellip; #endif才是正确且必要的。
既然现实世界是如此复杂，你就需要明确自己的目标，然后根据目标定义相应的策略。比如：如果你的目标是让你的代码能够使用几款主流的、正确遵守了规范的编译器进行编译，那么你只需要简单的使用#ifdef __cplusplus &hellip; #endif就足够了。
但如果你的产品是一个雄心勃勃的，试图兼容各种编译器的（包括未知的）跨平台产品， 我们可能不得不使用下述方法来应对各种情况 ，其中__ALIEN_C_LINKAGE__是为了标识那些在C和C++编译中都定义了__cplusplus宏的编译器。
这应该可以工作，但在每个头文件中都写这么一大串，不仅有碍观瞻，还会造成一旦策略进行修改，就会到处修改的状况。违反了DRY(Don&rsquo;t Repeat Yourself)原则，你总要为之付出额外的代价。解决它的一个简单方案是，定义一个特定的头文件——比如clinkage.h，在其中增加这样的定义：
以下举例中c的函数声明和定义分别在cfun.h 和 cfun.c 中，函数打印字符串 “this is c fun call”，c++函数声明和定义分别在cppfun.h 和 cppfun.cpp中，函数打印字符串 &ldquo;this is cpp fun call&rdquo;, 编译环境vc2010
c++ 调用 c 的方法（关键是要让c的函数按照c的方式编译，而不是c++的方式）
（1） cfun.h如下：
#ifndef _C_FUN_H_ #define _C_FUN_H_  void cfun(); #endif cppfun.cpp 如下：
//#include &#34;cfun.h&#34; 不需要包含cfun.h #include &#34;cppfun.h&#34;#include &lt;iostream&gt;using namespace std; extern &#34;C&#34; void cfun(); //声明为 extern void cfun(); 错误  void cppfun() { cout&lt;&lt;&#34;this is cpp fun call&#34;&lt;&lt;endl; } int main() { cfun(); return 0; } （2）cfun.h同上，cppfun.cpp 如下：
extern &#34;C&#34; { #include &#34;cfun.h&#34;//注意include语句一定要单独占一行;} #include &#34;cppfun.h&#34;#include &lt;iostream&gt;using namespace std; void cppfun() { cout&lt;&lt;&#34;this is cpp fun call&#34;&lt;&lt;endl; } int main() { cfun(); return 0; } （3）cfun.h如下：
#ifndef _C_FUN_H_ #define _C_FUN_H_  #ifdef __cplusplus extern &#34;C&#34; { #endif  void cfun(); #ifdef __cplusplus } #endif  #endif cppfun.cpp如下：
#include &#34;cfun.h&#34;#include &#34;cppfun.h&#34;#include &lt;iostream&gt;using namespace std; void cppfun() { cout&lt;&lt;&#34;this is cpp fun call&#34;&lt;&lt;endl; } int main() { cfun(); return 0; } c调用c++（关键是C++ 提供一个符合 C 调用惯例的函数）
在vs2010上测试时，没有声明什么extern等，只在在cfun.c中包含cppfun.h，然后调用cppfun()也可以编译运行，在gcc下就编译出错，按照c++/c的标准这种做法应该是错误的。以下方法两种编译器都可以运行
cppfun.h如下：
#ifndef _CPP_FUN_H_ #define _CPP_FUN_H_  extern &#34;C&#34; void cppfun(); #endif cfun.c如下：
//#include &#34;cppfun.h&#34; //不要包含头文件，否则编译出错 #include &#34;cfun.h&#34;#include &lt;stdio.h&gt; void cfun() { printf(&#34;this is c fun call\n&#34;); } extern void cppfun(); int main() { #ifdef __cplusplus  cfun(); #endif  cppfun(); return 0; } ]]></content>
  </entry>
  
  <entry>
    <title>风河 Helix 虚拟化平台和 VxWorks 653 提供扩展的安全认证证据支持以加速下一代 A&D 系统的开发</title>
    <url>/post/news/wind-river-helix-virtualization-platform-and-vxWorks-653-deliver-expanded-safety-certification.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Wind River</tag>
      <tag>Helix Virtualization</tag>
      <tag>VxWorks 653</tag>
    </tags>
    <content type="html"><![CDATA[为关键任务智能系统提供软件的全球领导者 Wind River® 今天宣布扩展其行业领先平台的架构支持，以进一步满足高度计算的苛刻需求 - 重型任务关键型应用，特别是那些与航空航天和国防工业相关的应用。
DO-178C DAL A 认证证据现已可用于 Armv8-A 架构的 Wind River Helix™ 虚拟化平台。 最新版本的 VxWorks  ® 653 为 PowerPC 提供更新的 DO-178C DAL A 认证证据。
具有 DO-178C DAL A 证据的 Helix 平台是一个安全认证的多核、多租户平台，支持多个独立的关键级别。 它专为各种关键任务行业用例而设计，例如商业和军事航空电子设备，允许客户运行不安全的软件以及经过最高级别认证的软件，这些软件涉及航空电子设备 (DO-178C)、汽车 (ISO 26262)、工业 ( IEC 61508) 和其他类似标准。 Helix Platform 也符合 ARINC 653 标准，并在最新的硬件平台上提供强大的时间和空间分区，以确保故障遏制以及以最少的测试和集成需求升级应用程序的能力。
VxWorks 653，具有DO-178C DAL A证明，是一个安全可靠的多核、多租户平台，可用于PowerPC架构。 VxWorks 653 也符合 ARINC 653 标准，具有与 Helix 平台相同的稳健性。
“智能边缘的发展为航空航天和国防等行业提出了一系列独特的要求和挑战，包括与认证需求相关的复杂性增加。 随着 Helix 平台和 VxWorks 653 的更新安全认证证据的可用性，我们正在为我们的客户提供更多灵活性和加速创新的机会，以创建下一代安全可靠的多操作系统智能设备，”首席产品 Avijit Sinha 说 军官，风河。
风河技术已在最具挑战性的安全关键型应用中得到验证，使组织能够更轻松、更经济地满足 EN 50128、IEC 61508、ISO 26262、DO-178C 和 ED-12C 的严格安全认证要求。
凭借在 120 多架民用和军用飞机的 880 多个安全项目中得到 400 多家客户验证的技术，风河正在推动航空航天和国防领域向软件定义系统的过渡。
]]></content>
  </entry>
  
  <entry>
    <title>三星电子研发出其首款支持CXL 2.0的CXL DRAM</title>
    <url>/post/news/samsung-develops-first-CXL-DRAM-supporting-CXL-2.0.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Samsung</tag>
      <tag>CXL 2.0</tag>
      <tag>CXL Memory</tag>
    </tags>
    <content type="html"><![CDATA[基于先进CXL 2.0的128GB CXL DRAM将于今年量产，加速下一代存储器解决方案的商用化，三星将继续与全球数据中心、服务器、芯片企业合作，扩大CXL生态系统
三星电子今日宣布，研发出其首款支持Compute Express Link™（CXL™）2.0的128GB DRAM。同时，三星与英特尔密切合作，在英特尔®至强®平台上取得了具有里程碑意义的进展。
继2022年五月，三星电子研发出其首款基于CXL 1.1的CXL DRAM（内存扩展器）后，又继续推出支持CXL 2.0的128GB CXL DRAM，预计将加速下一代存储解决方案的商用化。该解决方案支持PCIe 5.0（x8通道），提供高达每秒35GB的带宽。
三星电子新业务企划副总裁Jangseok Choi表示： 作为CXL联盟的董事会成员，三星电子在CXL技术上一直处于前沿地位，这一突破性的进展强化了我们通过与全球各地的数据中心、企业级服务器和芯片公司合作，进一步扩大CXL生态系统的决心。
英特尔公司技术创新总监Jim Pappas表示： 英特尔很高兴与三星合作，共同投资一个充满活力的CXL生态系统。英特尔将继续与三星携手，促进创新CXL产品在整个行业的发展和采用。
澜起科技总裁Stephen Tai表示： 澜起科技很高兴能够量产第一批支持CXL 2.0的控制器，我们期待与三星继续加强合作，推进CXL技术发展并扩大其生态系统。
CXL 2.0是三星有史以来第一个支持内存池（Pooling）的产品。内存池是一种内存管理技术，它将服务器平台上的多个CXL内存块绑定在一起，形成一个内存池，使多个主机能够根据需要从池中动态分配内存。这项新技术使客户尽可能的降本增效，从而帮助企业将有限的资源重新投资于增强服务器内存中去。
三星电子计划于今年年底之前开始量产这一最新的CXL 2.0 DRAM，并准备推出多种容量的产品，以满足快速变化的下一代计算市场，进一步加速扩大CXL生态系统。
CXL作为下一代，能够为高性能服务器系统中与CPU一起使用的加速器、DRAM和存储设备提高效率。由于它与主内存（main DRAM）共同使用时可扩大带宽和容量，该技术的进步有望在人工智能（AI）和机器学习（ML）等核心技术，对处理高速数据的需求极大增加的下一代计算市场引起轰动。
]]></content>
  </entry>
  
  <entry>
    <title>苦战15年，谷歌云业务终于实现首次盈利</title>
    <url>/post/news/google-cloud-business-is-finally-profitable-for-the-first-time.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Cloud</tag>
      <tag>Google</tag>
    </tags>
    <content type="html"><![CDATA[谷歌云业务（Google Cloud）是谷歌母公司 Alphabet 旗下的一个部门，负责提供云计算、数据分析、机器学习等服务。
自从 2008 年推出以来，谷歌云业务一直处于亏损状态，直到今年第一季度才终于首次实现了盈利。
根据 Alphabet 公布的财报，截至 3 月 31 日的季度，谷歌云业务的收入为 74 亿美元，运营利润为 1.91 亿美元（约 13.24 亿元人民币），盈利率为 2.5%。虽然这是一个历史性的突破，但与其竞争对手相比，谷歌云业务仍然显得弱小。
亚马逊旗下的 AWS（Amazon Web Services）在 2022 年的收入达到了 800 亿美元，运营利润为 228 亿美元，盈利率为 28%。微软旗下的 Azure 也在不断增长，虽然没有公布具体的数字，但据估计其市场份额已经超过了谷歌云业务的两倍。
谷歌云业务为什么一直亏损？主要原因是其投入了大量的资金来扩大基础设施、开发新产品、招募新客户和人才。在过去的三年里，谷歌云业务累计亏损了 146 亿美元。
Alphabet 的 CEO Sundar Pichai 并不在意这些亏损，他认为这是为了未来的增长而做出的必要投资，并且谷歌本身有足够的利润来支撑这些投资。他对这次盈利表示满意，并称谷歌云业务已经成为全球最大的企业软件公司之一。
谷歌云业务能否在未来缩小与领先者之间的差距，还要看它能否继续保持创新和竞争力。目前，谷歌云业务正在开发一些新技术，如生成式人工智能（generative AI），以提升其搜索服务的质量和效率。Pichai 表示，他认为这些技术不会增加太多的基础设施成本，并且会给用户提供更多的选择。
同时，谷歌云业务也面临着一些挑战和压力。首先是市场环境的变化，由于新冠疫情和芯片短缺等因素，导致整个行业的需求和收入都有所下降。Alphabet 本季度的总收入只增长了 3%，达到 698 亿美元。其次是成本控制的问题，Alphabet 本季度承担了 26 亿美元的费用，主要包括裁员和取消闲置办公空间等项目。这些费用导致其运营收入和利润、利润率都有所下降。最后是战略调整的问题，Alphabet 的首席财务官 Ruth Porat 表示，该公司将致力于实现长期增长，并通过优化成本结构来为最有前景的领域创造投资空间。这意味着谷歌云业务可能会面临更多的内部竞争和审查，而不是无限制的支持和扶持。
 Google AI TPU  
]]></content>
  </entry>
  
  <entry>
    <title>Linux内核设计与实现—进程调度</title>
    <url>/post/linux/linux-kernel-design-and-implementation-process-schedule.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Linux</tag>
      <tag>Kernel</tag>
    </tags>
    <content type="html"><![CDATA[最近开始学习 Linux  内核，主要参考书籍为《Linux内核设计与实现》，所以本系列大章节和小章节会遵从原书结构，再辅以其他书籍或网上资料对其未理解部分进行补充。
概述 本系列定位为初级文档，不会详细阐述实现原理，只讲解概念和逻辑。
调度程序的定义 调度程序决定将哪个进程投入运行，何时运行以及运行多长时间。调度程序可看做在可运行态进程之间分配有限的处理器时间资源的内核子系统。
只要有可以执行的进程，那么就总会有进程正在执行。但是只要系统中可运行的进程的数目比处理器的个数多，就注定某一给定时刻会有一些进程不能执行。
在一组处于可运行状态的进程中选择一个来执行，是调度程序所需完成的基本工作。
调度策略的分类 策略决定调度程序在何时让什么进程运行。不同策略的调度器对系统呈现的风格不同。
###IO消耗型和处理器消耗型
进程可以被分为I/O 消耗型和处理器消耗型。
I/O消耗型指的是进程大部分时间用来提交I/O或者等待I/O请求。这样的进程进程处于可运行状态，一般都是运行一小会，更多时间是处于阻塞状态。常见的IO消耗型进程通常有图像界面，鼠标或键盘类。
处理器消耗型是把大多数时间用于执行代码，除非被抢占，否则他们一直不停的运行。常见的消耗型进程通常有算法，业务类。
进程优先级 调度算法中最基本的一类就是基于优先级的调度。通常做法是高优先级先运行，低优先级后运行，优先级一样则轮转运行。
Linux 采用了两种不同的优先级范围：
第一种是 Nice 值，Nice值的范围是-20~+19，拥有Nice值越大的进程的实际优先级越小（即Nice值为+19的进程优先级最小，为-20的进程优先级最大），默认的Nice值是0。
“Nice值”这个名称来自英文单词nice，意思为友好。Nice值越高，这个进程越“友好”，就会让给其他进程越多的时间。
第二种范围是实时优先级PRI值，其值是可配。默认情况下它的变化范围是从0 到99 （包括0 和99) 。与nice 值意义相反，越高的实时优先级数值意味着进程优先级越高。
最终的进程优先级为PRI(now) = PRI(start) + NI(now)。Linux操作系统中，我们是通过修改NI值的方式，来修改PRI优先级的大小。
在Linux中使用ps -efl来查看PRI和NI值。
mark@vxworks:~/github/2beanet$ ps -elf F S UID PID PPID C PRI NI ADDR SZ WCHAN STIME TTY TIME CMD 4 S root 1 0 0 80 0 - 42022 - Apr15 ? 00:00:40 /lib/systemd/systemd splash --system --deserialize 56 1 S root 2 0 0 80 0 - 0 - Apr15 ? 00:00:00 [kthreadd] 1 I root 3 2 0 60 -20 - 0 - Apr15 ? 00:00:00 [rcu_gp] 1 I root 4 2 0 60 -20 - 0 - Apr15 ? 00:00:00 [rcu_par_gp] 1 I root 5 2 0 60 -20 - 0 - Apr15 ? 00:00:00 [slub_flushwq] 1 I root 6 2 0 60 -20 - 0 - Apr15 ? 00:00:00 [netns] 1 I root 8 2 0 60 -20 - 0 - Apr15 ? 00:00:00 [kworker/0:0H-events_highpri] 1 I root 10 2 0 60 -20 - 0 - Apr15 ? 00:00:00 [mm_percpu_wq] 1 I root 11 2 0 80 0 - 0 - Apr15 ? 00:00:00 [rcu_tasks_kthread] 1 I root 12 2 0 80 0 - 0 - Apr15 ? 00:00:00 [rcu_tasks_rude_kthread] 1 I root 13 2 0 80 0 - 0 - Apr15 ? 00:00:00 [rcu_tasks_trace_kthread] 1 S root 14 2 0 80 0 - 0 - Apr15 ? 00:00:08 [ksoftirqd/0] 1 I root 15 2 0 80 0 - 0 - Apr15 ? 00:05:23 [rcu_preempt] 1 S root 16 2 0 -40 - - 0 - Apr15 ? 00:00:04 [migration/0] 1 S root 17 2 0 9 - - 0 - Apr15 ? 00:00:00 [idle_inject/0] 1 S root 19 2 0 80 0 - 0 - Apr15 ? 00:00:00 [cpuhp/0] 5 S root 20 2 0 80 0 - 0 - Apr15 ? 00:00:00 [cpuhp/1] 1 S root 21 2 0 9 - - 0 - Apr15 ? 00:00:00 [idle_inject/1] 1 S root 22 2 0 -40 - - 0 - Apr15 ? 00:00:04 [migration/1] 1 S root 23 2 0 80 0 - 0 - Apr15 ? 00:00:04 [ksoftirqd/1] 1 I root 25 2 0 60 -20 - 0 - Apr15 ? 00:00:00 [kworker/1:0H-events_highpri] 5 S root 26 2 0 80 0 - 0 - Apr15 ? 00:00:00 [cpuhp/2] 1 S root 27 2 0 9 - - 0 - Apr15 ? 00:00:00 [idle_inject/2] 1 S root 28 2 0 -40 - - 0 - Apr15 ? 00:00:04 [migration/2] 1 S root 29 2 0 80 0 - 0 - Apr15 ? 00:01:44 [ksoftirqd/2] 1 I root 31 2 0 60 -20 - 0 - Apr15 ? 00:00:00 [kworker/2:0H-events_highpri] 5 S root 32 2 0 80 0 - 0 - Apr15 ? 00:00:00 [cpuhp/3] 1 S root 33 2 0 9 - - 0 - Apr15 ? 00:00:00 [idle_inject/3] 1 S root 34 2 0 -40 - - 0 - Apr15 ? 00:00:04 [migration/3] 1 S root 35 2 0 80 0 - 0 - Apr15 ? 00:00:14 [ksoftirqd/3] 1 I root 37 2 0 60 -20 - 0 - Apr15 ? 00:00:00 [kworker/3:0H-events_highpri] 时间片 时间片是一个数值，它表明进程在被抢占前所能持续运行的时间。
调度策略必须规定一个默认的时间片，时间片过长会导致系统对交互的响应表现欠佳，让人觉得系统无法并发执行应用程序；
时间片太短会明显增大进程切换带来的处理器耗时，因为肯定会有相当一部分系统时间用在进程切换上，而这些进程能够用来运行的时间片却很短。
任何长时间片都将导致系统交互表现欠佳。很多操作系统中都特别重视这一点，所以默认的时间片很短，如10ms。
调度策略的活动 根据系统的进程自动分配CPU。
比如说现在两个可运行的进程，文字编辑程序和视频编码程序。文本编辑程序是I/O消耗型，大部分时间用于阻塞等待用户键盘输入，视频编码程序属于处理器消耗型，大量的时间用于视频解码运算。
用户希望按下按键能立马响应，而对视频编码没有严格的要求，用户分辨不出是立刻就运行还是延迟后在执行。
理想情况是调度器应该给予文本编辑程序相比视频编码程序更多的处理器时间，因为它属于交互式应用。
对文本编辑器而言，我们有两个目标：
 我们希望系统给它更多的处理器时间，这并非因为它需要更多的处理器时间（其实它不需要），是因为我们希望在它需要时总是能得到处理器； 我们希望文本编辑器能在其被唤醒时（也就是当用户打字时）抢占视频解码程序。这样才能确保文本编辑器具有很好的交互性能，以便能响应用户输入。  在多数操作系统中，上述目标的达成是要依靠系统分配给文本编辑器比视频解码程序更高的优先级和更多的时间片。
在Linux中，默认调度器为CFS（Completely Fair Scheduler，完全公平调度器），它是通过分配一个给定的处理器使用比来实现这个目的。假如文本编辑器和视频解码程序是仅有的两个运行进程，并且又具有同样的nice 值，那么处理器的使用比将都是50％——它们平分了处理器时间。
但因为文本编辑器将更多的时间用于等待用户输人，因此它肯定不会用到处理器的50% 。同时，视频解码程序无疑将能有机会用到超过50％的处理器时间，以便它能更快速地完成解码任务。
在上述场景中， 一旦文本编辑器被唤醒， CFS注意到给它的处理器使用比是50% ，但是其实它却用得少之又少。特别是， CFS 发现文本编辑器比视频解码器运行的时间短得多。
这种情况下，为了兑现让所有进程能公平分享处理器的承诺，它会立刻抢占视频解码程序，让文本编辑器投入运行。文本编辑器运行后，立即处理了用户的击键输入后，又一次进入睡眠等待用户下一次输入。因为文本编辑器井没有消费掉承诺给它的50％处理器使用比，因此情况依旧， CFS 总是会毫不犹豫地让文本编辑器在需要时被投入运行，而让视频处理程序只能在剩下的时刻运行。
Linux的调度策略 Linux内核支持的调度策略如下：
（1）限期进程使用限期调度策略（SCHED_DEADLINE）。
限期调度策略有3个参数：运行时间runtime、截止期限deadline和周期period。
每个周期运行一次，在截止期限之前执行完，一次运行的时间长度是runtime。
（2）实时进程支持两种调度策略：先进先出调度（SCHED_FIFO）和轮流调度（SCHED_RR）。
SCHED_FIFO 实现了一种简单的、先人先出的调度算法。
一但一个SCHED_FIFO 级进程处于可执行状态，就会一直执行，直到它自己受阻塞或显式地释放处理器为止。只有更高优先级的SCHED_FIFO 或者SCHED_RR任务才能抢占SCHED_FIFO 任务。如果有两个或者更多的同优先级的SCHED_FIFO 级进程，它们会轮流执行，但是依然只有在它们愿意让出处理器时才会退出。
SCHED_RR 与SCHED_FIFO 大体相同， 只是SCHED_RR 级的进程在耗尽事先分配给它的 时间后就不能再继续执行了。也就是说， SCHED_RR 是带有时间片的SCHED_FIFO。当SCHED_RR 任务耗尽它的时间片时，在同一优先级的其他实时进程被轮流调度。
（3）普通进程支持两种调度策略：标准轮流分时（SCHED_NORMAL，在POSIX中叫做SCHED_OTHER）和空闲（SCHED_IDLE）。
标准轮流分时策略使用完全公平调度算法，把处理器时间公平地分配给每个进程。
空闲调度策略用来执行优先级非常低的后台作业，优先级比使用标准轮流分时策略和相对优先级为19的普通进程还要低，进程的相对优先级对空闲调度策略没有影响。
Linux调度器类 这5种调度类的优先级从高到低依次为：停机调度类、限期调度类、实时调度类、公平调度类和空闲调度类。
 停机调度类  停机调度类是优先级最高的调度类，停机进程（stop-task）是优先级最高的进程，可以抢占所有其他进程，其他进程不可以抢占停机进程。
停机（stop是指stop machine）的意思是使处理器停下来，做更紧急的事情。
停机进程没有时间片，如果它不主动让出处理器，那么它将一直霸占处理器。
限期调度类  限期调度类使用最早期限优先算法，使用红黑树（一种平衡的二叉树）把进程按照绝对截止期限从小到大排序，每次调度时选择绝对截止期限最小的进程。如果限期进程用完了它的运行时间，它将让出处理器，并且把它从运行队列中删除。在下一个周期的开始，重新把它添加到运行队列中。
实时调度器  实时调度类为每个调度优先级维护一个队列，使用位图法用来快速查找第一个非空队列，然后用链表数组串联任务，跟FreeRTOS类似。
公平调度类  CFS（Completely Fair Scheduler）是 Linux 内置（也是目前默认）的一个内核调度器 ， 如名字所示，它实现了所谓的“完全公平”调度算法，将 CPU 资源均匀地分配给各进程（ 在内核代码中称为“任务”，task）。简单来说，如果一台机器有一个 CPU 多个（计算密集型）进程，那采用 CFS 调度器。
 两个进程：每个进程会各占 50% CPU 时间 四个进程：每个进程会各占 25% CPU 时间  空闲调度器  每个处理器上有一个空闲线程，即0号线程。空闲调度类的优先级最低，仅当没有其他进程可以调度的时候，才会调度空闲线程。
抢占与上下文切换 上下文切换， 也就是从一个可执行进程切换到另一个可执行进程。
上下文切换调用schedule() 函数，schedule() 在调用context_switch()进行处理。它完成了最基本的两项工作：
 调用声明在中的switch_mm(), 该函数负责把虚拟内存从上一个进程映射切换到新进程中。 调用声明在中的switch_to(), 该函数负责从上一个进程的处理器状态切换到新进程的处理器状态。这包括保存、恢复栈信息和寄存器信息，还有其他任何与体系结  构相关的状态信息，都必须以每个进程为对象进行管理和保存。
内核提供need_resched()函数用于检测TIF_NEED_RESCHED是否被置位，用于判断需要重新调度。
在system_tick中断中和主动调用schedule()切换线程时，会对TIF_NEED_RESCHED 置位。
当返回用户空间以及从中断返回的时候，内核也会检查TIF_NEED_RESCHED 标志。如果已被设置， 会重新选择进程执行。
用户抢占 内核在即将返回用户空间的时候，如果need_resched 标志被设置，会导致schedule() 被调用，此时就会发生用户抢占。内核无论是在中断处理程序还是在系统调用后返回，都会检查need_resched 标志，如果它被设置了， 那么，内核会选择一个其他（更合适的）进程运行。
简而言之，用户抢占在以下情况时产生：
 从系统调用返回用户空间时 从中断处理程序返回用户空间时  内核抢占 Linux 在2.6版本后通过config PREEMPT 配置为内核抢占。
在不支持内核抢占的内核中，内核代码可以一直执行，到它完成为止。在内核空间运行的进程不具备抢占性。内核代码一直要执行到完成（返回用户空间）或明显的阻塞为止。现在，只要重新调度是安全的（没有持有锁），内核就可以在任何时间抢占正在执行的任务。
内核抢占会发生在：
 中断处理程序正在执行，且返回内核空间之前 内核代码再一次具有可抢占性的时候 如果内核中的任务显式地调用schedule() 如果内核中的任务阻塞（ 这同样也会导致调用schedule() ）  简单理解就是被抢占之前是用户空间，就是用户抢占，被抢占之前是内核空间就是内核抢占。
我们在看一下代码就更明确这两个概念了。
我们知道当中断发生在用户空间，即 USR mode 时执行 __irq_usr，当中断发生在内核空间即 SVC mode 时执行 __irq_svc。
// 抢占之前是运行在用户空间，比如说调用文件系统接口(open, read, write)等 __irq_usr: usr_entry kuser_cmpxchg_check irq_handler get_thread_info tsk mov why, #0 b ret_to_user_from_irq UNWIND(.fnend ) ENDPROC(__irq_usr) // 抢占之前运行在内核空间 **__irq_svc**: svc_entry irq_handler **#ifdef CONFIG_PREEMPT // 判断是否支持内核抢占** ldr r8, [tsk, #TI_PREEMPT] @ get preempt count ldr r0, [tsk, #TI_FLAGS] @ get flags teq r8, #0 @ if preempt count != 0 movne r0, #0 @ force flags to 0 tst r0, #_TIF_NEED_RESCHED blne svc_preempt **#endif** 总结 这一章主要描述了调度器的基本概念，以及常见的调度策略和Linux支持的调度类，在最后讲解了一些进程切换相关的知识。如果对RTOS比较熟悉的同学，虽然RTOS简单很多，但毕竟概念相通，阅读起来会比较轻松。如果没有RTOS基础的同学，可以读一下我之前写过的FreeRTOS源码分析相关章节，相信一定大有裨益。
]]></content>
  </entry>
  
  <entry>
    <title>5G和边缘计算</title>
    <url>/post/hardware/5g-and-edge-computing.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>Edge</tag>
      <tag>Computing</tag>
    </tags>
    <content type="html"><![CDATA[第五代 (5G) 蜂窝网络和边缘计算是近年来出现的两项最具创新性的技术。两者都准备好彻底改变企业的运营方式，并且潜在的好处是非常巨大的。
5G 和边缘计算在企业中的优势 5G是一种无线电信技术，有望提供比其前身（4G）更快的速度、更广的覆盖范围和更低的延迟。这意味着可以更快地传输和接收数据，这对于为连接的设备和应用供电至关重要。5G还具有处理更多设备和数据流量的潜力，从而实现更大的可扩展性。
边缘计算是一种分布式计算模型，可使数据处理和分析更靠近源头。通过让计算能力更接近用户，边缘计算可以减少延迟，提高应用的速度，并提供更可靠和安全的数据处理。
5G和边缘计算的结合是一个强大的组合。通过实现更快的数据传输和处理，5G和边缘计算可以为企业提供改进的性能和更高的效率。5G和边缘计算可以结合使用来创建更高效的通信和协作系统，以及可以改进决策制定和简化流程的自治系统。此外，5G和边缘计算可以协同工作，提高数据和应用的安全性。
企业将从许多方面受益于5G和边缘计算。它可以降低成本、改善客户体验、提高流程效率并创造新的收入来源。随着5G和边缘计算变得越来越普遍，企业应该做好充分利用其潜力的准备。
5G和边缘计算的挑战是什么？ 5G和边缘计算的引入带来了一系列新挑战。5G是一种移动通信标准，可实现更高的数据传输速度和更低的延迟。边缘计算是一种分布式计算范式，可以在靠近数据源的地方进行数据处理，而不是依赖集中式云计算资源。5G和边缘计算都将改变我们使用技术的方式，但也存在许多与之相关的挑战。
5G和边缘计算的最大挑战之一是安全性。随着越来越多的数据被边缘计算设备处理，数据泄露的风险也在增加。为了确保数据的安全，需要有一个强大的系统来保护数据免受潜在威胁。此外，边缘计算设备需要正确配置，以确保数据不会落入坏人之手。
5G和边缘计算的另一个挑战是可扩展性。边缘计算设备通常在计算能力方面受到限制，这使得在需要处理更多数据时难以扩展操作。此外，随着 5G 的普及，有必要确保边缘计算网络能够在不影响性能的情况下处理更高的数据传输速度。
最后，5G和边缘计算的成本是另一个挑战。与任何新技术一样，实施成本可能很高。此外，还需要考虑维护和升级 5G和边缘计算网络的成本。
总体而言，5G和边缘计算带来了一系列新的可能性和机遇，但也带来了一系列新的挑战。安全性、可扩展性和成本都是需要解决的领域，以确保可以安全、可靠且经济高效地使用 5G 和边缘计算。
探索5G和边缘计算的不同部署选项 随着世界继续朝着更强大的连接和更先进的技术发展，5G和边缘计算的部署变得越来越重要。5G和边缘计算在数据速度、可靠性和可扩展性方面提供了一系列可能性，使其成为一系列应用的理想选择。然而，为了最大限度地发挥其潜力，探索可用的不同部署选项至关重要。
5G和边缘计算最常见的部署选项是通过蜂窝网络。这涉及定期安装基站，为5G网络提供覆盖和容量。此选项非常适合需要高水平连接的人口稠密地区，因为它提供可靠的覆盖范围和数据速度。
另一种选择是通过固定无线系统部署5G和边缘计算。这涉及使用一系列天线和接收器来提供覆盖范围和容量。此部署选项非常适合无法安装基站的地区，例如农村地区。它还具有能够快速有效地覆盖广阔区域的优势。
第三个部署选项是使用网状网络。这涉及创建一个相互无线连接的节点网络。该选项非常适合在固定无线系统不可行的地区提供覆盖和容量，因为它可以在很短的时间内部署。
最后，5G和边缘计算也可以通过卫星部署。这涉及使用一系列卫星为偏远地区提供覆盖和容量。此选项非常适合提供广域覆盖，以及需要低延迟和高数据速度的应用。
随着5G和边缘计算的应用越来越广泛，探索可用的不同部署选项非常重要。每个选项都有自己的优点和缺点，因此必须考虑哪个选项最适合特定应用。通过正确的部署选项，5G和边缘计算可以为一系列应用提供可靠、高速的数据连接。
在 5G 和边缘计算环境中保护数据 5G 技术和边缘计算正在彻底改变我们访问和存储数据的方式。随着这些新技术变得越来越普遍，保护数据变得越来越重要。公司必须确保他们存储的数据不受任何恶意行为者的影响，无论他们是黑客、政府还是竞争对手。
为确保 5G 和边缘计算环境中的数据安全，企业需要结合使用加密、虚拟专用网络 (VPN) 和访问控制机制等技术。加密是对数据进行加扰的过程，以便它只能由授权用户读取。VPN 允许用户通过不安全的公共网络（例如 Internet）访问安全网络。访问控制机制有助于规范谁可以访问特定系统以及他们在那里可以做什么。
除了这些技术之外，公司还应该考虑使用数据治理和数据隐私计划。数据治理是在组织内管理、保护和利用数据的过程。数据隐私计划旨在保护组织存储和访问其数据的个人的隐私。公司还应确保定期更新和修补其系统，以确保快速解决任何漏洞。
最后，公司需要确保其员工接受数据安全最佳实践方面的培训。员工应了解数据安全的重要性以及为确保数据安全而需要采取的步骤。这包括使用强密码和避免访问公共网络上的机密数据。
通过实施这些措施，公司可以确保其数据在 5G 和边缘计算环境中安全可靠。这将有助于保护客户的隐私并防止恶意行为者滥用他们的数据。
为企业释放5G和边缘计算的潜力 5G网络和边缘计算的出现使企业能够释放这些技术的潜力，最大限度地提高效率和生产力。5G和边缘计算的结合可以为企业提供更快的数据访问速度、更高的可扩展性和更高的安全性。
借助5G网络，企业将能够更快、更可靠地访问数据，从而快速响应客户需求。此外，企业可以利用更高的速度来支持更复杂的应用程序和流程。通过利用边缘，企业可以减少延迟并提高其应用的可扩展性。边缘计算还有助于降低数据成本，因为它无需将数据发送到云端。
5G和边缘计算的结合，通过利用 5G 网络的更高速度和更高的可靠性，企业可以更快、更可靠地访问数据。此外，边缘计算有助于减少延迟并提高可扩展性。这些进步可以使企业改善运营并在全球市场上更具竞争力。通过利用5G和边缘计算的强大功能，企业可以释放这些技术的潜力，最大限度地提高效率和生产力。
]]></content>
  </entry>
  
  <entry>
    <title>边缘计算的5类最佳应用场景</title>
    <url>/post/news/5-best-use-cases-for-edge-computing.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Edge</tag>
      <tag>Computing</tag>
    </tags>
    <content type="html"><![CDATA[当你认可时间等同于金钱或安全时；当你面对数据合规性问题时，边缘计算就是你最好的选择。本文将带给你5个边缘计算的应用场景，从而帮助大家思考如何进行边缘化设计。
开篇 边缘计算是指将基础设施定位在靠近数据产生或消费的地方。与其将数据推送到公共或私有云进行存储和计算，不如在 “边缘”进行就地处理，处理数据的基础设施可以是简单的商品服务器，也可以是复杂的平台，如AWS for the Edge、Azure Stack Edge或Google Distributed Cloud。
边缘计算的第二层含义，包括性能、可靠性、安全性和操作的合规性。为了支持这些要求边缘计算会将计算、存储和带宽转移到边缘的基础设施上执行，因为这些功能在集中式云架构上是不能执行的。
Edgevana的首席执行官Mark Thiele说：“边缘计算为企业领导者提供了一个新的途径，可以与客户和合作伙伴发展更深的关系，并获得实时的洞察力”。
当开发团队开发规模还不大，并处于概念验证的早期时，可能很难认识到最佳基础设施。但是，随着团队规模的扩大以及项目进度的推进，大家会逐渐认识到对边缘基础设施的需求，这就会迫使团队重新架构甚至重构应用程序。从而增加开发成本，放慢开发进度，甚至阻碍企业的交付。
随着应用程序变得越来越现代化和集成化，企业应该在开发的早期考虑边缘技术和集成，以防止开发企业级应用程序时出现的性能和安全挑战。Devops团队应该在平台的基础设施要求被准确建模之前寻找响应的指标。以下是考虑边缘的五个理由。
提高效率与安全性 在制造业，当延迟可能导致工人受伤时，几秒钟的价值是什么？如果制造需要昂贵的材料，而提前几百毫秒发现缺陷可以节省大量资金，那又如何呢？
在制造业中，有效利用边缘计算可以减少浪费，提高效率，减少工伤，并提高设备的可用性。
架构师要考虑的一个关键因素是由于决策失败或延迟而导致的失败成本。如果存在重大风险或成本，如制造系统、手术平台或自动驾驶汽车，边缘计算可能为需要更大安全性的应用提供更高的性能和可靠性。
减少延时 亚秒级的响应时间是大多数金融交易平台的基本要求，现在许多应用都希望有这样的性能，缩短从感觉到发现问题的时间，缩短发现机会到做出行动的时间，总之在不断加速决定的周期。
咨询公司的高级副总裁Amit Patel说：“如果实时决策对你的业务很重要，那么提高速度或减少延迟就很关键，特别是在企业使用所有连接设备收集数据的情况下”。
当有成千上万的数据源和决策节点时，提供低延迟技术就显得尤为重要。这方面的例子包括连接数以千计的拖拉机和农场机器，并在边缘设备上部署机器学习（ML），或实现元数据或其他大规模企业对消费者的体验。
如果需要实时采取行动，就从边缘计算开始，“Akamai高级产品经理Pavel Despot说。”边缘基础设施适合于低延迟、高弹性和高吞吐量的应用场景，从而处理分布在不同地理位置用户的工作负载，这一技术涉及到流媒体、银行、电子商务、物联网设备等不同领域。
LaunchDarkly的开发者关系总监Cody De Arkland表示，在全球都分布有办公地点的企业或支持大规模的混合工作的企业就是一个典型的例子。边缘工作的价值在于，你能将工作分配到离你最近的人身上，这些人会对工作进行分担。如果应用程序对数据传输时间敏感的话，你应该考虑边缘基础设施，并考虑哪些工作应该在边缘运行。
提高应用程序的可靠性 Scale Computing的首席执行官Jeff Ready表示，我们看到制造业、零售业和运输业对边缘基础设施的兴趣很大，在这些行业中，设备根本不可能停机，数据的实时访问和利用数据的需求已经成为差异化竞争的要素。
因此，当停机成本高，维修时间长，以及集中式基础设施故障影响多个业务时，应考虑边缘基础设施。
Ready分享了两个例子。例如在海洋中间的一艘货船，它不能依靠断断续续的卫星连接来运行其船上系统，或者一家杂货店需要从店内收集数据来创造个性化的购物体验。如果一个集中式系统发生故障，可能会影响到多艘船和物流，而高度可靠的边缘基础设施可以减少停机的风险和造成的影响。
本地数据处理和法规支持 如果性能、延迟和可靠性不是主要的设计考虑因素，那么根据有关数据收集和消费地点的规定，可能仍然需要边缘基础设施的支持。
AWS物联网副总裁Yasser Alsaied认为，边缘基础设施对本地数据处理和数据驻留要求很重要。例如，它有利于那些远程操作工作负载的公司，这些公司由于连接性的原因而无法将数据上传到云端，该企业的特点是数据会驻留在某个特定的区域内，并对数据进行高度管制，或者拥有需要本地处理的大量数据。
开发团队应该回答的一个基本问题是，数据将在哪里被收集和消费？合规部门应提供关于数据限制的监管指南，并应就物理和地理限制咨询运营职能部门的领导。
对大数据集带宽的成本优化 带有视频监控、设施管理系统和能源跟踪系统的智能建筑，都会以每秒的速度捕获大量的数据。在建筑中本地处理这些数据比在云端集中处理数据要便捷得多。
ScaleFlux的营销副总裁JB Baker表示，所有行业都在经历数据的激增，要适应这种复杂性，需要一种完全不同的思维方式来利用巨大数据集的潜力。边缘计算是解决方案的一部分，因为它使计算和存储更接近数据的起源。
MinIO的首席执行官和联合创始人AB Periasamy提出了这样的建议：“随着数据在网络边缘的产生，在应用和基础设施架构方面产生了独特的挑战。将带宽作为模型中成本最高的项目，而资本和运营支出在边缘的运作方式有所不同。”
总之，当开发团队看到应用程序需要在性能、可靠性、延迟、安全、监管或规模方面的优势时，那么在开发的早期对边缘基础设施进行建模可以考虑更智能的架构。
]]></content>
  </entry>
  
  <entry>
    <title>自制STM32的下载器</title>
    <url>/post/hardware/smt32-download-fixture-how-to.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>SMT32, STLINK-V2</tag>
    </tags>
    <content type="html"><![CDATA[本文介绍制作一个STM32下载器的过程。
原理图 STLINK-V2下载器电路原理图如下。
上图中，H5接口是固件下载口。H4接口是STLINK-V2下载口（实现下载功能的接口 T_JTCK就是SWCLK, T_JTMS就是 SWDIO）。其他都是测试接口。
制作过程 首先我们焊接完的板子是不能用的，需要往里面下载固件。前提是你手头有一块好的ST-LinkV2下载器，不然就白搞了。
使用好的STLINK-V2下载器给我们自己制作的STLINK-V2板子下载固件，将STLINK-V2下载器连接角与板子上的H5固件下载接口对应连接，再将STLINK-V2连接到电脑，
安装ST的ST-LINK Utility软件，使用STM32 ST-LINK Utility 软件下载固件（这个软件官网可以下载）。先连接后下载，要是软件识别不了你的芯片，那你得仔细检查下你的板子啦！
连接成功后，然后开始下载固件。
图中框1打开固件STLinkV2.J16.S4.bin文件（这个最新固件官网可以下载的到）。
点击框2下载，弹出下载窗口。
点击框3开始下载。
下载完后，将自制的ST-Link插上电脑，然后，更新固件。点击框1，弹出窗口然后点击框2连接自制的ST-Link,识别出来后，点击框3开始更新固件。如果没有识别出来，可能是你同时插上了2个ST-Link，或者是你的板子有问题，得耐心排查。到这里，就算完成啦！接下来你可以体验你自制的ST-Link任意下载程序啦！这个电路我测试的是keil，IAR都支持，STM8也能下载。
]]></content>
  </entry>
  
  <entry>
    <title>PCB板上镀金与镀银有什么区别</title>
    <url>/post/hardware/difference-between-gold-plating-and-silver-plating-on-PCB.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>PCB, Gold Plating, Silver Plating</tag>
    </tags>
    <content type="html"><![CDATA[很多DIY玩家会发现，市场中各种各样的板卡产品所使用的PCB颜色五花八门，令人眼花缭乱。比较常见的PCB颜色有黑色、绿色、蓝色、黄色、紫色、红色、棕色。
除此之外，一些厂商还别出心裁地开发了白色、粉色等不同色彩的PCB。
在传统的印象中，黑色PCB似乎定位着高端，而红色、黄色等则是低端专用，那是不是这样呢？
没有涂覆阻焊漆的PCB铜层暴露在空气中极易氧化 我们知道PCB正反两面都是铜层，在PCB的生产中，铜层无论采用加成法还是减成法制造，都会得到光滑无保护的表面。
铜的化学性质虽然不如铝、铁、镁等活泼，但在有水的条件下，纯铜和氧气接触极易被氧化；因为空气中存在氧气和水蒸气，所以纯铜表面在和空气接触后很快会发生氧化反应。
由于PCB中铜层的厚度很薄，因此氧化后的铜将成为电的不良导体，会极大地损害整个PCB的电气性能。
为了阻止铜氧化，也为了在焊接时PCB的焊接部分和非焊接部分分开，还为了保护PCB表层，工程师们发明了一种特殊的涂料。这种涂料能够轻松涂刷在PCB表面，形成具有一定厚度的保护层，并阻断铜和空气的接触。这层涂层叫做阻焊层，使用的材料为阻焊漆。
既然叫漆，那肯定有不同的颜色。没错，原始的阻焊漆可以做成无色透明的，但PCB为了维修和制造方便，往往需要在板上面印制细小的文字。
透明阻焊漆只能露出PCB底色，这样无论是制造、维修还是销售，外观都不够好看。因此工程师们在阻焊漆中加入了各种各样的颜色，就形成了黑色或者红色、蓝色的PCB。
黑色的PCB难以看清走线，为维修带来了困难 从这一点来看，PCB的颜色和PCB的质量是没有任何关系的。黑色的PCB和蓝色PCB、黄色PCB等其他颜色PCB的差别在于刷上的阻焊漆颜色不同。
如果PCB设计、制造过程完全一样，颜色不会对性能产生任何影响，也不会对散热产生任何影响。
关于黑色的PCB，由于其表层走线几乎全部遮住，导致对后期的维修造成很大困难，所以是不太方便制造和使用的一种颜色。
因此近年来人们渐渐改革，放弃使用黑色阻焊漆，转而使用深绿色、深棕色、深蓝色等阻焊漆，目的就是为了方便制造和维修。
说到这里，大家已经基本清楚了PCB颜色的问题。关于之所以出现“颜色代表或低档”的说法，那是因为厂商喜爱使用黑色PCB来制造高端产品，用红色、蓝色、绿色、黄色等制造低端产品所导致。
总结一句话就是：产品赋予了颜色含义，而不是颜色赋予了产品含义。
金、银等贵金属用在PCB上有什么好处？ 颜色说清楚了，再来说说PCB上的贵重金属！一些厂商在宣传自己的产品时，会特别提到自己的产品采用了镀金、镀银等特殊工艺。那么这种工艺究竟有什么用处呢？
PCB表面需要焊接元件，就要求有一部分铜层暴露在外用于焊接。这些暴露在外的铜层被称为焊盘，焊盘一般都是长方形或者圆形，面积很小。
在上文中，我们知道PCB中使用的铜极易被氧化，因此刷上了阻焊漆后，暴露在空气中的就是焊盘上的铜了。
如果焊盘上的铜被氧化了，不仅难以焊接，而且电阻率大增，严重影响终产品性能。所以，工程师们才想出了各种各样的办法来保护焊盘。比如镀上惰性金属金，或在表面通过化学工艺覆盖一层银，或用一种特殊的化学薄膜覆盖铜层，阻止焊盘和空气的接触。
PCB上暴露出来的焊盘，铜层直接裸露在外。这部分需要保护，阻止它被氧化。
从这个角度来说，无论是金还是银，工艺本身的目的都是阻止被氧化、保护焊盘，使其在接下来的焊接工艺中确保良品率。
不过采用不同的金属，会对生产工厂使用的PCB的存放时间和存放条件提出要求。因此PCB厂一般会在PCB生产完成，交付客户使用前，利用真空塑封机器包装PCB，限度地确保PCB不发生氧化损害。
而在元件上机焊接之前，板卡生产厂商还要检测PCB的氧化程度，剔除氧化PCB，保证良品率。终消费者拿到的板卡，是已经过了各种检测，即使长时间使用后的氧化也几乎只会发生在插拔连接部位，且对焊盘和已经焊接好的元件，没有什么影响。
由于银和金的电阻更低，那么在采用了银和金等特殊金属后，会不会减少PCB使用时的发热量呢？
我们知道，影响发热量的因素是电阻；电阻又和导体本身材质、导体的横截面积、长度相关。焊盘表面金属材质厚度甚至远低于0.01毫米，如果采用OST（有机保护膜）方式处理的焊盘，根本不会有多余厚度产生。如此微小的厚度所表现出来的电阻几乎为0，甚至无法计算，当然不会影响到发热量了。
]]></content>
  </entry>
  
  <entry>
    <title>dBm-Vpp-W转换</title>
    <url>/post/hardware/dBm-Vpp-convert.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>dBm, Vpp</tag>
    </tags>
    <content type="html"><![CDATA[在不同的应用领域，大家对一个信号的表示各不相同，如RF领域喜欢使用dBm来描述一个信号的功率，而实际的调试或测试过程中，大家更喜欢使用示波器来测试一个信号的峰峰值
所以工程实践中经常需要对各个单位进行转换，如dBm转Vpp，dBm转W(功率瓦特)。本文就是介绍如何进行这些转换。  典型的测试测量系统 如下是一个典型的射频源和频谱仪/示波器组成的一个测量系统，射频源等效输出阻抗Rs为50欧姆，输出一个标准正弦波，频谱仪/示波器等效输入阻抗RL为50欧姆，则频谱仪测试到的正弦波功率为一个以dBm为单位的功率值，而示波器则测试到的是落在输入50欧姆电阻上的Vpp电压值：
频谱仪看到的功率值为：
$$ P_{dBm} = 10*lg \left(\frac{P_{w}}{1mW}\right) $$
示波器看到的电压值为：
$$ V_{pp} = 20* \sqrt{P_{w}} $$
dBm-Vpp-W转换表 下表是50欧姆系统中的正弦波信号参数的转换关系：
小程序实现 使用电路设计小程序可以很方便的实现dBm-Vpp-W的转换：
]]></content>
  </entry>
  
  <entry>
    <title>动图演示常用通信协议原理</title>
    <url>/post/hardware/animation-demonstrates-principles-of-common-communication-protocols.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>SPI</tag>
      <tag>UART</tag>
      <tag>I2C</tag>
      <tag>PWM</tag>
    </tags>
    <content type="html"><![CDATA[本文分享电子系统中信号波形的动图，有助于帮助我们理解传输的机理。
SPI传输 图1 SPI 数据传输
图1.2 SPI数据传输（2）
图1.3 SPI时序信号
I²C传输 图1.2.1 I2C总线以及寻址方式
UART传输 图1.3.1 PC 上通过UART来调试MCU
图1.3.2 RS-232通过电平转换芯片与MCU通讯
串口通信相关文章: VxWorks下的串口测试程序设计和源码  
红外控制 图1.4.1 红外控制信号也是一个串行通讯信号
图1.4.2 红外信号接收与放大整形电路
图1.4.3 一个使用红外接收光电管控制继电器进行鱼食投喂电路
串并转换电路 图1.5.1 串入、并出移位寄存器
图1.5.2 由八个D寄存器组成的移位寄存器
图1.5.3 串行传输示意图
其他波形动画 图1.6.1 PWM控制LED亮度
图1.6.2 PWM控制LED亮度
图1.6.3 调幅与调频信号
图1.6.4 相位调制信号
图1.6.5 方波边沿抖动波形
原文链接: 动图演示常用通信协议原理  
]]></content>
  </entry>
  
  <entry>
    <title>6个与戈登・摩尔相关的冷知识</title>
    <url>/post/news/6-trivia-about-Gordon-moore.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Intel</tag>
      <tag>Integrated Electronics</tag>
      <tag>Gordon Moore</tag>
    </tags>
    <content type="html"><![CDATA[英特尔的创始人戈登・摩尔于上周五（2023年3月24日）逝世，享年 94 岁，他的一生对计算机科学和半导体工业的发展做出了巨大的贡献。
今天给大家分享6个与戈登・摩尔相关的“冷知识”。
]]></content>
  </entry>
  
  <entry>
    <title>Linux 进程概念: 冯 • 诺依曼体系结构</title>
    <url>/post/linux/linux-process-concept-von-neumann-architecture.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Process</tag>
      <tag>Von Neumann Architecture</tag>
    </tags>
    <content type="html"><![CDATA[在 1945 年冯诺依曼和其他计算机科学家们提出了计算机具体实现的报告，其遵循了[图灵机]的设计，而且还提出用电子元件构造计算机，并约定了用二进制进行计算和存储。
冯诺依曼体系结构 冯诺依曼体系，最重要的是定义计算机基本结构为 5 个部分，分别是[运算器]、控制器、存储器、输入设备、输出设备，这 5 个部分也被称为冯诺依曼模型。
下图为冯 • 诺依曼体系结构流程图：
运算器、控制器是在中央处理器里的，存储器就是我们常见的内存，输入输出设备则是计算机外接的设备，比如键盘就是输入设备，显示器就是输出设备。 
我们常见的计算机，如笔记本。我们不常见的计算机，如服务器，大部分都遵守冯诺依曼体系，截至目前，我们所认识的计算机，都是有一个个的硬件组件组成。
存储单元和输入输出设备要与中央处理器打交道的话，离不开总线。所以，它们之间的关系如下图：
接下来，分别介绍内存、中央处理器、总线、输入输出设备。
输入、输出设备 输入设备向计算机输入数据，计算机经过计算后，把数据输出给输出设备。  常见的输入和输出设备有：
 输入设备：键盘，话筒，摄像头，磁盘，网卡等等… 输出设备：显示器，音响，磁盘，网卡，显卡等等…  注意：同种设备在不同场景下可能属于输入设备，也可能属于输入设备。
中央处理器 中央处理器也就是我们常说的 CPU，它是由运算器和控制器组成。
CPU 内部还有一些组件，常见的有寄存器、控制单元 和 逻辑运算单元 等。其中，控制单元负责控制 CPU 工作，逻辑运算单元负责计算，而寄存器可以分为多种类，每种寄存器的功能又不尽相同。
CPU 中的寄存器主要作用是存储计算时的数据，你可能好奇为什么有了内存还需要寄存器？原因很简单，因为内存离 CPU 太远了，而寄存器就在 CPU 里，还紧挨着控制单元和逻辑运算单元，自然计算时速度会很快。
常见的寄存器种类：
 通用寄存器，用来存放需要进行运算的数据，比如需要进行加和运算的两个数据。 程序计数器，用来存储 CPU 要执行下一条指令「所在的内存地址」，注意不是存储了下一条要执行的指令，此时指令还在内存中，程序计数器只是存储了下一条指令「的地址」。 指令寄存器，用来存放当前正在执行的指令，也就是指令本身，指令被执行完成之前，指令都存储在这里。  内存 内存，也就是所谓的存储器。
我们的程序和数据都是存储在内存，存储的区域是线性的。
在计算机数据存储中，存储数据的基本单位是字节（byte），1 字节等于 8 位（8 bit）。每一个字节都对应一个内存地址。
内存的地址是从 0 开始编号的，然后自增排列，最后一个地址为内存总字节数 - 1，这种结构好似我们程序里的数组，所以内存的读写任何一个数据的速度都是一样的。
思考一个问题： 当我们的体系结构中，有了输入、输出设备和 CPU 以后，就能正常工作了，那么为什么还需要内存呢？
从技术角度来说 CPU 的运算速度 &gt; &raquo; 寄存器的速度 &gt; &raquo; L1~L3Cache &gt; &raquo; 内存 &gt; &raquo; 外设（磁盘）&gt; &raquo; 光盘磁带
也就是说，输入设备和输出设备相对于 CPU 来说是非常慢的。
如果没有内存的话，那么当前这个体系整体呈现出来的就是：输入设备和输出设备很慢，而 CPU 很快。
相信大家知道木桶原理吧，那么最终整个体系结构所呈现出来的速度将会是很慢的。
所以，从数据角度出发，外设几乎不和 CPU 打交道，它是直接和内存打交道，CPU 也同样如此。
进言之，内存在我们看来，就是体系结构的一个大的缓存，用来适配外设和 CPU 速度不均的问题！
从成本角度来说 既然上面说了内存是用来适配外设和 CPU 速度不均的问题，那么为什么不直接在 CPU 里面开发一个类似于内存的东西呢？
这个想法可以，但是如果真要去实现的话，那么一台计算机的成本起码得 10W+，而计算机它是蔓延全世界的，也就是说人人都能用得起的！
寄存器的价格 &gt; &raquo; 内存 &gt; &raquo; 外设 (磁盘)
所以内存就是方便我们使用较低的成本，获得较高的性能。
总线 总线是用于 CPU 和内存以及其他设备之间的通信，总线可分为 3 种：   地址总线，用于指定 CPU 将要操作的内存地址； 数据总线，用于读写内存的数据； 控制总线，用于发送和接收信号，比如中断、设备复位等信号，CPU 收到信号后自然进行响应，这时也需要控制总线；  当 CPU 要读写内存数据的时候，一般需要通过下面这三个总线：
 首先要通过「地址总线」来指定内存的地址； 然后通过「控制总线」控制是读或写命令； 最后通过「数据总线」来传输数据；  局部性原理 我相信大家应该还有个疑惑：就是，先将输入设备的数据交给内存，再由内存将数据交给 CPU，这个过程真的比 CPU 直接从输入设备获取数据更快吗？
说明这个问题之前，我们首先需要知道：内存具有数据存储的能力。虽然内存的大小只有 4G/8G，但是既然内存有大小，那么它就有预装数据的能力，而这就是提高该体系结构效率的秘诀。
这里不得不说到的就是 局部性原理：根据统计学原理，当一个数据正在被访问时，那么下一次有很大可能会访问其周围的数据。所以当 CPU 需要获取某一行数据时，内存可以将该行数据之后的数据一同加载进来，而 CPU 处理数据和内存加载数据是可以同时进行的，这样下次 CPU 就可以直接从内存当中获取数据。
输出数据的时候也一样，CPU 处理完数据后直接将数据放到内存当中，当输出设备需要时再在内存当中获取即可，这也就有了我们平常所说的缓冲区的概念。
例如，缓冲区满了才将数据打印到屏幕上，使用 fflush 函数将缓冲区当中的数据直接输出之类的，都是将内存当中的数据直接拿到输出设备当中进行显示输出。
总结 冯 • 诺依曼体系结构核心原理为：用户输入的数据先放到内存当中，CPU 读取数据的时候就直接从内存当中读取，CPU 处理完数据后又写回内存当中，然后内存再将数据输出到输出设备当中，最后由输出设备进行输出显示。
我们可以知道，站在硬件角度或是数据层面上，CPU 和外设不能直接交互，而是通过内存，也就是说，所有设备都只能和内存打交道。
由此可以说明一个问题：为什么程序运行之前必须先加载到内存？
因为可执行程序（文件）是在硬盘（外设）上的，而 CPU 只能从内存当中获取数据，所以必须先将硬盘上的数据加载到内存，也就是必须先将程序加载到内存。
数据的流动过程 对冯诺依曼的理解，不能停留在概念上，要深入到对软件数据流理解上。
从你登录上 QQ 和某位朋友聊天开始，数据的流动过程是怎样的呢？从你打开窗口，开始给他发消息，到他的到消息之后的数据流动过程。
要使用 QQ，首先需要联网，而你和你的朋友的电脑都是冯诺依曼体系结构，在你向朋友发送消息这个过程中，你的电脑当中的键盘充当输入设备，显示器和网卡充当输出设备，你朋友的电脑当中的网卡充当输入设备，显示器充当输出设备。
刚开始你在键盘当中输入消息，键盘将消息加载到内存，此时你的显示器就可以从内存获取消息进而显示在你自己的显示器上，此时你就能在你自己的电脑上看到你所发的消息了。
在键盘将消息加载到内存后，CPU 从内存获取到消息后对消息进行各种封装，然后再将其写回内存，此时你的网卡就可以从内存获取已经封装好的消息，然后在网络当中经过一系列处理（这里忽略网络处理细节）。
之后你朋友的网卡从网络当中获取到你所发的消息后，将该消息加载到内存当中，你朋友的 CPU 再从内存当中获取消息并对消息进行解包操作，然后将解包好的消息写回内存，最后你朋友的显示器从内存当中获取消息并显示在他的电脑上。
那么如果是在 QQ 上发送文件呢？
首先你的文件最开始是在你本地的磁盘上的，先从磁盘上把文件读到内存中，文件里面的东西其实还是数据，把数据再经过 CPU 封装成报文，然后刷新到我们的内存中，定期再经过网卡，把数据刷新到网卡上，然后再发出去。
传文件的本质就是：两端的磁盘进行通信。 ]]></content>
  </entry>
  
  <entry>
    <title>40个简单但有效的Linux Shell脚本示例</title>
    <url>/post/linux/40-simple-and-useful-linux-shell-script.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>linux</tag>
      <tag>shell script</tag>
    </tags>
    <content type="html"><![CDATA[历史上，shell一直是类Unix系统的本地命令行解释器。它已被证明是Unix的主要功能之一，并发展成为一个全新的主题。Linux提供了各种功能强大的shell，包括Bash、Zsh、Tcsh和Ksh。这些外壳最令人惊讶的特性之一是其可编程性。创建简单而有效的Linux shell脚本来处理日常工作非常容易。
Hello World 程序员经常通过学习hello world程序来学习新语言。这是一个简单的程序，将字符串“HelloWorld”打印到标准输出中。然后，使用vim或nano等编辑器创建hello-world.sh文件，并将以下行复制到其中。
#!/bin/bash echo &#34;Hello World&#34; 保存并退出文件。接下来，您需要使用以下命令使该文件可执行。
$ chmod a+x hello-world.sh 可以使用以下两个命令中的任何一个来运行此命令。
$ bash hello-world.sh $ ./hello-world.sh 它将打印出传递给脚本内部回显的字符串。
使用echo打印 echo命令用于在bash中打印信息。它类似于C函数“printf”，并提供了许多常见选项，包括转义序列和重定向。将以下行复制到名为echo.sh的文件中，并使其可执行，如上所述。
#!/bin/bash echo &#34;Printing text&#34; echo -n &#34;Printing text without newline&#34; echo -e &#34;\nRemoving \t special \t characters\n&#34; 运行脚本以查看其功能。-e选项用于告诉echo传递给它的字符串包含特殊字符，需要扩展功能。
使用注释 注释对文档很有用，是高质量代码库的要求。将注释放在处理关键逻辑的代码中是一种常见的做法。要注释掉一行，只需在其前面使用#（hash）字符。例如，请查看下面的bash脚本示例。
#!/bin/bash  # Adding two values ((sum=25+35)) #Print the result echo $sum 此脚本将输出数字60。首先，在某些行之前使用#检查注释的使用方式。不过，第一行是一个例外。它被称为shebang，让系统知道在运行这个脚本时要使用哪个解释器。
多行注释 许多人使用多行注释来记录他们的shell脚本。在下一个名为comment.sh的脚本中检查这是如何完成的。
#!/bin/bash : &#39; This script calculates the square of 5. &#39; ((area=5*5)) echo $area 注意多行注释是如何放置在内部的：“和”字符。
While循环 while循环构造用于多次运行某些指令。查看以下名为while.sh的脚本，以更好地理解此概念。
#!/bin/bash #!/bin/bash i=0 while [ $i -le 2 ] do echo Number: $i ((i++)) done 因此，while循环采用以下形式。
#!/bin/bash while [ condition ] do commands 1 commands n done 方括号周围的空格是必填的。
For循环 for循环是另一种广泛使用的bashshell构造，它允许用户高效地迭代代码。下面演示了一个简单的示例。
#!/bin/bash  for (( counter=1; counter&lt;=10; counter++ )) do echo -n &#34;$counter&#34; done printf &#34;\n&#34; 接收用户输入 #!/bin/bash  echo -n &#34;Enter Something:&#34; read something echo &#34;You Entered: $something&#34; If语句 if CONDITION then STATEMENTS fi 只有当条件为真时，才会执行这些语句。fi关键字用于标记if语句的结尾。下面显示了一个快速示例。
#!/bin/bash  echo -n &#34;Enter a number: &#34; read num if [[ $num -gt 10 ]] then echo &#34;Number is greater than 10.&#34; fi 如果通过输入提供的数字大于10，上述程序将仅显示输出。-gt表示大于；类似地-lt表示小于-le表示小于等于；且-ge表示大于等于。此外，还需要[[]]。
使用If Else进行更多控制 将else构造与if结合起来，可以更好地控制脚本的逻辑。下面显示了一个简单的示例。
#!/bin/bash  read n if [ $n -lt 10 ]; then echo &#34;It is a one digit number&#34; else echo &#34;It is a two digit number&#34; fi 其他部分需要放在if的动作部分之后和fi之前。
使用AND运算符 AND运算符允许我们的程序检查是否同时满足多个条件。由AND运算符分隔的所有部分必须为true。否则，包含AND的语句将返回false。查看下面的bash脚本示例，以更好地了解AND的工作原理。
#!/bin/bash  echo -n &#34;Enter Number:&#34; read num if [[ ( $num -lt 10 ) &amp;&amp; ( $num%2 -eq 0 ) ]]; then echo &#34;Even Number&#34; else echo &#34;Odd Number&#34; fi AND运算符由&amp;&amp;符号表示。
使用OR运算符 OR运算符是另一个关键的构造，它允许我们在脚本中实现复杂、健壮的编程逻辑。与AND相反，当OR运算符的任一操作数为真时，由OR运算符组成的语句返回真。仅当由OR分隔的每个操作数为假时，它才返回假。
#!/bin/bash  echo -n &#34;Enter any number:&#34; read n if [[ ( $n -eq 15 || $n -eq 45 ) ]] then echo &#34;You won&#34; else echo &#34;You lost!&#34; fi 这个简单的示例演示了OR运算符如何在Linuxshell脚本中工作。只有当用户输入数字15或45时，它才会宣布用户为获胜者。||符号表示OR运算符。
使用El if elif语句代表else if，并为实现链逻辑提供了一种方便的方法。通过评估以下示例，了解elif的工作原理。
#!/bin/bash  echo -n &#34;Enter a number: &#34; read num if [[ $num -gt 10 ]] then echo &#34;Number is greater than 10.&#34; elif [[ $num -eq 10 ]] then echo &#34;Number is equal to 10.&#34; else echo &#34;Number is less than 10.&#34; fi 上面的程序是不言自明的，所以我们不会逐行剖析它。相反，更改脚本中的变量名称和值等部分，以检查它们如何一起工作。
case 条件 switch构造是Linux bash脚本提供的另一个强大功能。它可以用于需要嵌套条件的地方，但不希望使用复杂的if-else elif链。看看下一个例子。
#!/bin/bash  echo -n &#34;Enter a number: &#34; read num case $num in 100) echo &#34;Hundred!!&#34; ;; 200) echo &#34;Double Hundred!!&#34; ;; *) echo &#34;Neither 100 nor 200&#34; ;; esac 条件写在case和esac关键字之间。*）用于匹配除100和200以外的所有输入。
命令行参数 在许多情况下，直接从命令shell获取参数是有益的。下面的示例演示了如何在bash中执行此操作。
#!/bin/bash echo &#34;Total arguments : $#&#34; echo &#34;First Argument = $1&#34; echo &#34;Second Argument = $2&#34; 运行此脚本时，在其名称后添加两个附加参数。我将其命名为test.sh，调用过程概述如下。
$ ./test.sh Hey Howdy 因此，$1用于访问第一个参数，$2用于访问第二个参数，依此类推。最后，$#用于获取参数总数。
使用名称获取参数 下面的示例显示了如何获取带有名称的命令行参数。
#!/bin/bash  for arg in &#34;$@&#34; do index=$(echo $arg | cut -f1 -d=) val=$(echo $arg | cut -f2 -d=) case $index in X) x=$val;; Y) y=$val;; *) esac done ((result=x+y)) echo &#34;X+Y=$result&#34; 将此脚本命名为test.sh，并按如下所示调用它。
$ ./test.sh X=44 Y=100 它应该返回X+Y=144。这里的参数存储在“$@”中，脚本使用Linuxcut命令获取它们。
连接字符串 字符串处理对于广泛的现代bash脚本来说非常重要。值得庆幸的是，它在bash中更加舒适，并允许以更精确、简洁的方式实现这一点。请参见下面的示例，了解bash字符串连接。
#!/bin/bash  string1=&#34;Ubuntu&#34; string2=&#34;Pit&#34; string=$string1$string2 echo &#34;$stringis a great resource for Linux beginners.&#34; 字符串截取 与许多编程语言不同，bash不提供任何用于剪切字符串部分的内置函数。然而，下面的示例演示了如何使用参数展开来实现这一点。
#!/bin/bash Str=&#34;Learn Bash Commands from UbuntuPit&#34; subStr=${Str:0:20} echo $subStr 该脚本应打印出“学习Bash命令”作为其输出。参数展开形式为${VAR_NAME:S:L}。这里，S表示起始位置，L表示长度。
使用cut 做截取 可以在脚本中使用Linux cut命令来截取字符串的一部分，也就是子字符串。下一个示例显示了如何做到这一点。
#!/bin/bash Str=&#34;Learn Bash Commands from UbuntuPit&#34; #subStr=${Str:0:20} subStr=$(echo $Str| cut -d &#39; &#39; -f 1-3) echo $subStr 添加两个值 在Linux shell脚本中执行算术运算非常容易。下面的示例演示了如何从用户接收两个数字作为输入并将它们相加。
#!/bin/bash echo -n &#34;Enter first number:&#34; read x echo -n &#34;Enter second number:&#34; read y (( sum=x+y )) echo &#34;The result of addition=$sum&#34; 如您所见，在bash中添加数字相当简单。
添加多个值 您可以使用循环获取多个用户输入并将其添加到脚本中。以下示例显示了这一点。
#!/bin/bash sum=0 for (( counter=1; counter&lt;5; counter++ )) do echo -n &#34;Enter Your Number:&#34; read n (( sum+=n )) #echo -n &#34;$counter &#34; done printf &#34;\n&#34; echo &#34;Result is: $sum&#34; 但是，省略(())将导致字符串串联而不是相加。所以，在你的程序中检查类似的情况。
Bash中的函数 与任何编程方言一样，函数在Linux shell脚本中扮演着重要角色。它们允许管理员创建自定义代码块以供频繁使用。下面的演示将概述函数如何在Linux bash脚本中工作。
#!/bin/bash function Add() { echo -n &#34;Enter a Number: &#34; read x echo -n &#34;Enter another Number: &#34; read y echo &#34;Adiition is: $(( x+y ))&#34; } Add 这里我们像以前一样添加了两个数字。但在这里，我们使用了一个名为Add的函数来完成这项工作。因此，每当您需要再次添加时，只需调用此函数，而不必再次编写该部分。
具有返回值的函数 最神奇的功能之一是允许数据从一个函数传递到另一个函数。它在各种场景中都很有用。查看下一个示例。
#!/bin/bash  function Greet() { str=&#34;Hello $name, what brings you to UbuntuPit.com?&#34; echo $str } echo &#34;-&gt; what&#39;s your name?&#34; read name val=$(Greet) echo -e &#34;-&gt; $val&#34; 这里，输出包含从Greet（）函数接收的数据。
从Bash脚本创建目录 使用shell脚本运行系统命令的能力使开发人员的工作效率大大提高。下面的简单示例将向您展示如何在shell脚本中创建目录。
#!/bin/bash echo -n &#34;Enter directory name -&gt;&#34; read newdir cmd=&#34;mkdir $newdir&#34; eval $cmd 该脚本只需调用标准shell命令mkdir，并在仔细查看时将目录名传递给它。这个程序应该在文件系统中创建一个目录。您还可以传递命令以在backticks（“）内部执行，如下所示。
`mkdir $newdir` 确认存在后创建目录 如果当前工作目录中已包含同名文件夹，则上述程序将无法运行。例如，下面的程序将检查是否存在名为$dir的文件夹，如果找不到，则只创建一个。
#!/bin/bash echo -n &#34;Enter directory name -&gt;&#34; read dir if [ -d &#34;$dir&#34; ] then echo &#34;Directory exists&#34; else `mkdir $dir` echo &#34;Directory created&#34; fi 使用eval编写此程序以提高bash脚本编写技能。 读取文件 Bash脚本允许用户非常有效地读取文件。下面的示例将展示如何使用shell脚本读取文件。首先，创建一个名为editors.txt的文件，其中包含以下内容。
1. Vim 2. Emacs 3. ed 4. nano 5. Code 此脚本将输出上述5行中的每一行。
#!/bin/bash file=&#39;editors.txt&#39; while read line; do echo $line done &lt; $file 删除文件 以下程序将演示如何在Linux shell脚本中删除文件。程序将首先要求用户提供文件名作为输入，如果文件名存在，则将其删除。Linux rm命令在此处执行删除操作。
#!/bin/bash echo -n &#34;Enter filename -&gt;&#34; read name rm -i $name 让我们输入editors.txt作为文件名，并在要求确认时按y。它应该删除该文件。
附加到文件 下面的shell脚本示例将向您展示如何使用bash脚本将数据附加到文件系统上的文件。它向早期的editors.txt文件添加了一行。
#!/bin/bash echo &#34;Before appending the file&#34; cat editors.txt echo &#34;6. NotePad++&#34; &gt;&gt; editors.txt echo &#34;After appending the file&#34; cat editors.txt 现在您应该注意到，我们直接从Linux bash脚本使用日常终端命令。
测试文件存在 下一个shell脚本示例显示如何检查bash程序中文件的存在。
#!/bin/bash filename=$1 if [ -f &#34;$filename&#34; ]; then echo &#34;File exists&#34; else echo &#34;File does not exist&#34; fi 我们直接从命令行传递文件名作为参数。
从Shell脚本发送邮件 从bash脚本发送电子邮件非常简单。下面的简单示例将演示一种从bash应用程序执行此操作的方法。
#!/bin/bash recipient=”admin@example.com” subject=”Greetings” message=”Welcome to UbuntuPit” `mail -s $subject $recipient &lt;&lt;&lt; $message` 它将向收件人发送包含给定主题和消息的电子邮件。
解析日期和时间 下一个bash脚本示例将向您展示如何使用脚本处理日期和时间。同样，Linuxdate命令用于获取必要的信息，我们的程序执行解析。
#!/bin/bash year=`date +%Y` month=`date +%m` day=`date +%d` hour=`date +%H` minute=`date +%M` second=`date +%S` echo `date` echo &#34;Current Date is: $day-$month-$year&#34; echo &#34;Current Time is: $hour:$minute:$second&#34; 运行此程序以了解其工作原理。此外，尝试从终端运行date命令。
sleep命令 sleep命令允许shell脚本在指令之间暂停。它在许多场景中都很有用，例如执行系统级作业。下一个示例显示了shell脚本中的sleep命令。
#!/bin/bash echo &#34;How long to wait?&#34; read time sleep $time echo &#34;Waited for $timeseconds!&#34; 该程序暂停最后一条指令的执行，直到$time秒，在本例中，用户提供了这一点。
wait命令 wait命令用于暂停Linux bash脚本中的系统进程。查看下面的示例，详细了解这在bash中的工作方式。
#!/bin/bash echo &#34;Testing wait command&#34; sleep 5 &amp; pid=$! kill $pid wait $pid echo $pid was terminated. 显示上次更新的文件 有时，您可能需要为某些操作查找最后更新的文件。下面的简单程序向我们展示了如何在bash中使用awk命令执行此操作。它将列出当前工作目录中最近更新或创建的文件。
#!/bin/bash  ls -lrt | grep ^- | awk &#39;END{print $NF}&#39; 为了简单起见，我们将避免在本示例中描述awk的功能。相反，您可以简单地复制此代码来完成任务。
添加批处理扩展 下面的示例将对目录中的所有文件应用自定义扩展名。创建一个新目录，并将一些文件放在其中以供演示。我的文件夹共有五个文件，每个文件名为test，后跟（0-4）。我已将此脚本编程为在文件末尾添加（.UP）。您可以添加所需的任何扩展名。
#!/bin/bash dir=$1 for file in `ls $1/*` do mv $file $file.UP done 首先，不要从任何常规目录尝试此脚本；相反，请从测试目录运行此命令。此外，您需要提供文件的目录名作为命令行参数。对当前工作目录使用句点（.）。
打印文件或目录的数量 下面的Linuxbash脚本查找给定目录中存在的文件或文件夹的数量。它使用Linux find命令来执行此操作。首先，需要传递目录名以从命令行搜索文件。
#!/bin/bash  if [ -d &#34;$@&#34; ]; then echo &#34;Files found: $(find &#34;$@&#34; -type f | wc -l)&#34; echo &#34;Folders found: $(find &#34;$@&#34; -type d | wc -l)&#34; else echo &#34;[ERROR] Please retry with another folder.&#34; exit 1 fi 如果指定的目录不可用或存在权限问题，程序将要求用户重试。
清理日志文件 下一个简单的示例演示了在现实生活中使用shell脚本的简便方法。该程序只需删除/var/log目录中的所有日志文件。您可以更改保存此目录的变量以清理其他日志。
#!/bin/bash LOG_DIR=/var/log cd $LOG_DIR cat /dev/null &gt; messages cat /dev/null &gt; wtmp echo &#34;Logs cleaned up.&#34; 请记住以root身份运行此Linuxshell脚本。
使用Bash备份脚本 Shell脚本提供了一种强大的方法来备份文件和目录。以下示例将备份过去24小时内修改的每个文件或目录。该程序使用find命令执行此操作。
#!/bin/bash  BACKUPFILE=backup-$(date +%m-%d-%Y) archive=${1:-$BACKUPFILE} find . -mtime -1 -type f -print0 | xargs -0 tar rvf &#34;$archive.tar&#34; echo &#34;Directory $PWDbacked up in archive file \&#34;$archive.tar.gz\&#34;.&#34; exit 0 备份过程成功后，它将打印文件和目录的名称。
检查你是否是root用户 下面的示例演示了通过Linux bash脚本快速确定用户是否为root用户的方法。
#!/bin/bash ROOT_UID=0 if [ &#34;$UID&#34; -eq &#34;$ROOT_UID&#34; ] then echo &#34;You are root.&#34; else echo &#34;You are not root&#34; fi exit 0 此脚本的输出取决于运行它的用户。它将根据$UID匹配根用户。
从文件中删除重复行 文件处理需要相当长的时间，并在许多方面阻碍了管理员的工作效率。例如，在文件中搜索重复项可能会成为一项艰巨的任务。幸运的是，您可以使用一个简短的shell脚本来完成此操作。
#! /bin/sh  echo -n &#34;Enter Filename-&gt; &#34; read filename if [ -f &#34;$filename&#34; ]; then sort $filename | uniq | tee sorted.txt else echo &#34;No $filenamein $pwd...try again&#34; fi exit 0 上面的脚本逐行遍历文件并删除所有重复的行。然后，它将新内容放入新文件，并保持原始文件的完整性。
系统维护 我经常使用一个小的Linuxshell脚本来升级我的系统，而不是手动升级。下面的简单shell脚本将向您展示如何做到这一点。
#!/bin/bash  echo -e &#34;\n$(date &#34;+%d-%m-%Y --- %T&#34;)--- Starting work\n&#34; apt-get update apt-get -y upgrade apt-get -y autoremove apt-get autoclean echo -e &#34;\n$(date &#34;+%T&#34;)\t Script Terminated&#34; 该脚本还处理不再需要的旧包。您需要使用sudo运行此脚本，否则它将无法正常工作。
]]></content>
  </entry>
  
  <entry>
    <title>Linux中touch命令的8个实际例子</title>
    <url>/post/linux/8-examples-of-touch-cmd-in-linux.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>touch</tag>
    </tags>
    <content type="html"><![CDATA[在本文中，我们将介绍一些有用的 Linux   实际示例touch command。这touch command是一个标准程序Unix/Linux操作系统，用于创建、更改和修改文件的时间戳。在开始接触命令示例之前，请查看以下选项。
touch命令选项   -a, 只更改访问时间 -c, 如果文件不存在，不创建 -d, 更新访问和修改时间 -m, 只更改修改时间 -r, 使用文件的访问和修改次数 -t, 使用指定时间创建文件  如何创建一个空文件  以下 touch 命令创建一个名为的空（零字节）新文件sheena.
 # touch sheena 如何创建多个文件  通过使用 touch 命令，您还可以创建多个文件。例如，以下命令将创建 3 个名为的文件，sheena,meena和temp.
 # touch sheena meena temp 如何更改文件访问和修改时间 更改或更新名为的文件的上次访问和修改时间temp， 使用-a选项如下。以下命令设置文件的当前时间和日期。如果temp文件不存在，它将创建具有名称的新空文件。  # touch -a temp  find 命令使用时间戳来列出和查找文件。
 如何避免创建新文件 使用-c带有 touch 命令的选项可避免创建新文件。例如，以下命令不会创建名为temp如果它不存在。  # touch -c temp ##如何更改文件修改时间
如果您想更改名为的文件的唯一修改时间temp，然后使用-m带有触摸命令的选项。请注意，它只会更新文件的最后修改时间（而不是访问时间）。  # touch -m temp ##明确设置访问和修改时间
 您可以使用显式设置时间-c和-t带有触摸命令的选项。格式如下。
 # touch -c -t YYDDHHMM temp  例如，以下命令设置文件的访问和修改日期和时间temp作为17:30(17:30 p.m.)August 10当年（2021）。
 # touch -c -t 12101730 temp  接下来验证文件的访问和修改时间temp， 和ls -l命令。
 # ls -l total 2 -rw-r--r--. 1 root root 0 Dec 10 17:30 temp 如何使用另一个文件的时间戳  以下触摸命令与-r选项，将更新文件的时间戳meena带有时间戳temp文件。因此，两个文件都拥有相同的时间戳。
 # touch -r temp meena 使用指定时间创建文件  如果你想创建一个指定时间而不是当前时间的文件，那么格式应该是。
 # touch -t YYMMDDHHMM.SS rumenz  例如下面的命令 touch 命令-t选项将给出rumenz归档时间戳18:30:55 p.m.在August 5,2021.
 # touch -t 202108051830.55 rumenz ]]></content>
  </entry>
  
  <entry>
    <title>C语言回调函数，提升C技巧必备</title>
    <url>/post/linux/c-programming-callback-function.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>c</tag>
      <tag>callback</tag>
    </tags>
    <content type="html"><![CDATA[本文简要介绍了C语言编程中的回调函数
函数指针 在讲回调函数之前，我们需要了解函数指针。
我们都知道，C语言的灵魂是指针，我们经常使用整型指针，字符串指针，结构体指针等。
int *p1; char *p2; STRUCT *p3; // STRUCT为我们定义的结构体 但是好像我们一般很少使用函数指针，我们一般使用函数都是直接使用函数调用。
下面我们来了解一下函数指针的概念和使用方法。
概念 函数指针是指向函数的指针变量。
通常我们说的指针变量是指向一个整型、字符型或数组等变量，而函数指针是指向函数。
函数指针可以像一般函数一样，用于调用函数、传递参数。
函数指针的定义方式为：
函数返回值类型 (* 指针变量名) (函数参数列表);  “函数返回值类型”表示该指针变量可以指向具有什么返回值类型的函数；“函数参数列表”表示该指针变量可以指向具有什么参数列表的函数。这个参数列表中只需要写函数的参数类型即可。
我们看到，函数指针的定义就是将“函数声明”中的“函数名”改成“（指针变量名）”。但是这里需要注意的是：“（指针变量名）”两端的括号不能省略，括号改变了运算符的优先级。如果省略了括号，就不是定义函数指针而是一个函数声明了，即声明了一个返回值类型为指针型的函数。
那么怎么判断一个指针变量是指向变量的指针变量还是指向函数的指针变量呢？首先看变量名前面有没有“”，如果有“”说明是指针变量；其次看变量名的后面有没有带有形参类型的圆括号，如果有就是指向函数的指针变量，即函数指针，如果没有就是指向变量的指针变量。
最后需要注意的是，指向函数的指针变量没有 ++ 和 – 运算。
一般为了方便使用，我们会选择：
typedef 函数返回值类型 (* 指针变量名) (函数参数列表);  比如：
typedef int (*Fun1)(int); //声明也可写成int (*Fun1)(int x)，但习惯上一般不这样。 typedef int (*Fun2)(int, int); //参数为两个整型，返回值为整型 typedef void (*Fun3)(void); //无参数和返回值 typedef void* (*Fun4)(void*); //参数和返回值都为void*指针 如何用函数指针调用函数 给大家举一个例子：
int Func(int x); /*声明一个函数*/ int (*p) (int x); /*定义一个函数指针*/ p = Func; /*将Func函数的首地址赋给指针变量p*/ p = &amp;Func; /*将Func函数的首地址赋给指针变量p*/ 赋值时函数 Func 不带括号，也不带参数。由于函数名 Func 代表函数的首地址，因此经过赋值以后，指针变量 p 就指向函数 Func() 代码的首地址了。
下面来写一个程序，看了这个程序你们就明白函数指针怎么使用了：
#include &lt;stdio.h&gt;int Max(int, int); //函数声明 int main(void) { int(*p)(int, int); //定义一个函数指针  int a, b, c; p = Max; //把函数Max赋给指针变量p, 使p指向Max函数  printf(&#34;please enter a and b:&#34;); scanf(&#34;%d%d&#34;, &amp;a, &amp;b); c = (*p)(a, b); //通过函数指针调用Max函数  printf(&#34;a = %d\nb = %d\nmax = %d\n&#34;, a, b, c); return 0; } int Max(int x, int y) //定义Max函数 { int z; if (x &gt; y) { z = x; } else { z = y; } return z; } 特别注意的是，因为函数名本身就可以表示该函数地址（指针），因此在获取函数指针时，可以直接用函数名，也可以取函数的地址。
p = Max可以改成 p = &amp;Max c = (*p)(a, b) 可以改成 c = p(a, b)  函数指针作为某个函数的参数 既然函数指针变量是一个变量，当然也可以作为某个函数的参数来使用的。示例：
#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt; typedef void(*FunType)(int); //前加一个typedef关键字，这样就定义一个名为FunType函数指针类型，而不是一个FunType变量。 //形式同 typedef int* PINT; void myFun(int x); void hisFun(int x); void herFun(int x); void callFun(FunType fp,int x); int main() { callFun(myFun,100);//传入函数指针常量，作为回调函数  callFun(hisFun,200); callFun(herFun,300); return 0; } void callFun(FunType fp,int x) { fp(x);//通过fp的指针执行传递进来的函数，注意fp所指的函数有一个参数 } void myFun(int x) { printf(&#34;myFun: %d\n&#34;,x); } void hisFun(int x) { printf(&#34;hisFun: %d\n&#34;,x); } void herFun(int x) { printf(&#34;herFun: %d\n&#34;,x); } 输出：
myFun: 100 hisFun: 200 herFun: 300 函数指针作为函数返回类型 有了上面的基础，要写出返回类型为函数指针的函数应该不难了，下面这个例子就是返回类型为函数指针的函数：
void (* func5(int, int, float ))(int, int) { ... } 在这里， func5 以 (int, int, float) 为参数，其返回类型为 void (\*)(int, int) 。在C语言中，变量或者函数的声明也是一个大学问，想要了解更多关于声明的话题，可以参考我之前的文章 - C专家编程》读书笔记(1-3章)。这本书的第三章花了整整一章的内容来讲解如何读懂C语言的声明。
函数指针数组 在开始讲解回调函数前，最后介绍一下函数指针数组。既然函数指针也是指针，那我们就可以用数组来存放函数指针。下面我们看一个函数指针数组的例子：
/* 方法1 */ void (*func_array_1[5])(int, int, float); /* 方法2 */ typedef void (*p_func_array)(int, int, float); p_func_array func_array_2[5]; 上面两种方法都可以用来定义函数指针数组，它们定义了一个元素个数为5，类型是 * void (\*)(int, int, float) * 的函数指针数组。
函数指针总结 函数指针常量 ：Max；函数指针变量：p；
数名调用如果都得如(*myFun)(10)这样，那书写与读起来都是不方便和不习惯的。所以C语言的设计者们才会设计成又可允许myFun(10)这种形式地调用（这样方便多了，并与数学中的函数形式一样）。
在函数指针变量也可以存入一个数组内。数组的声明方法：int (*fArray[10]) ( int );
 回调函数 什么是回调函数 我们先来看看百度百科是如何定义回调函数的：
回调函数就是一个通过函数指针调用的函数。如果你把函数的指针（地址）作为参数传递给另一个函数，当这个指针被用来调用其所指向的函数时，我们就说这是回调函数。回调函数不是由该函数的实现方直接调用，而是在特定的事件或条件发生时由另外的一方调用的，用于对该事件或条件进行响应。  这段话比较长，也比较绕口。下面我通过一幅图来说明什么是回调：
假设我们要使用一个排序函数来对数组进行排序，那么在主程序(Main program)中，我们先通过库，选择一个库排序函数(Library function)。但排序算法有很多，有冒泡排序，选择排序，快速排序，归并排序。同时，我们也可能需要对特殊的对象进行排序，比如特定的结构体等。库函数会根据我们的需要选择一种排序算法，然后调用实现该算法的函数来完成排序工作。这个被调用的排序函数就是回调函数(Callback function)。
结合这幅图和上面对回调函数的解释，我们可以发现，要实现回调函数，最关键的一点就是要将函数的指针传递给一个函数(上图中是库函数)，然后这个函数就可以通过这个指针来调用回调函数了。注意，回调函数并不是C语言特有的，几乎任何语言都有回调函数。在C语言中，我们通过使用函数指针来实现回调函数。
我的理解是：把一段可执行的代码像参数传递那样传给其他代码，而这段代码会在某个时刻被调用执行，这就叫做回调。
如果代码立即被执行就称为同步回调，如果过后再执行，则称之为异步回调。
回调函数就是一个通过函数指针调用的函数。如果你把函数的指针（地址）作为参数传递给另一个函数，当这个指针被用来调用其所指向的函数时，我们就说这是回调函数。
回调函数不是由该函数的实现方直接调用，而是在特定的事件或条件发生时由另外的一方调用的，用于对该事件或条件进行响应。
为什么要用回调函数？ 因为可以把调用者与被调用者分开，所以调用者不关心谁是被调用者。它只需知道存在一个具有特定原型和限制条件的被调用函数。
简而言之，回调函数就是允许用户把需要调用的方法的指针作为参数传递给一个函数，以便该函数在处理相似事件的时候可以灵活的使用不同的方法。
int Callback() // /&lt; 回调函数 { // TODO  return 0; } int main() // /&lt; 主函数 { // TODO  Library(Callback); // /&lt; 库函数通过函数指针进行回调  // TODO  return 0; } 回调似乎只是函数间的调用，和普通函数调用没啥区别。
但仔细看，可以发现两者之间的一个关键的不同：在回调中，主程序把回调函数像参数一样传入库函数。
这样一来，只要我们改变传进库函数的参数，就可以实现不同的功能，这样有没有觉得很灵活？并且当库函数很复杂或者不可见的时候利用回调函数就显得十分优秀。
怎么使用回调函数？ int Callback_1(int a) // /&lt; 回调函数1 { printf(&#34;Hello, this is Callback_1: a = %d &#34;, a); return 0; } int Callback_2(int b) // /&lt; 回调函数2 { printf(&#34;Hello, this is Callback_2: b = %d &#34;, b); return 0; } int Callback_3(int c) // /&lt; 回调函数3 { printf(&#34;Hello, this is Callback_3: c = %d &#34;, c); return 0; } int Handle(int x, int (*Callback)(int)) // /&lt; 注意这里用到的函数指针定义 { Callback(x); } int main() { Handle(4, Callback_1); Handle(5, Callback_2); Handle(6, Callback_3); return 0; } 如上述代码：可以看到，Handle() 函数里面的参数是一个指针，在 main() 函数里调用 Handle() 函数的时候，给它传入了函数 Callback_1()/Callback_2()/Callback_3() 的函数名，这时候的函数名就是对应函数的指针，也就是说，回调函数其实就是函数指针的一种用法。
下面是一个四则运算的简单回调函数例子： #include &lt;stdio.h&gt;#include &lt;stdlib.h&gt; /**************************************** * 函数指针结构体 ***************************************/ typedef struct _OP { float (*p_add)(float, float); float (*p_sub)(float, float); float (*p_mul)(float, float); float (*p_div)(float, float); } OP; /**************************************** * 加减乘除函数 ***************************************/ float ADD(float a, float b) { return a + b; } float SUB(float a, float b) { return a - b; } float MUL(float a, float b) { return a * b; } float DIV(float a, float b) { return a / b; } /**************************************** * 初始化函数指针 ***************************************/ void init_op(OP *op) { op-&gt;p_add = ADD; op-&gt;p_sub = SUB; op-&gt;p_mul = &amp;MUL; op-&gt;p_div = &amp;DIV; } /**************************************** * 库函数 ***************************************/ float add_sub_mul_div(float a, float b, float (*op_func)(float, float)) { return (*op_func)(a, b); } int main(int argc, char *argv[]) { OP *op = (OP *)malloc(sizeof(OP)); init_op(op); /* 直接使用函数指针调用函数 */ printf(&#34;ADD = %f, SUB = %f, MUL = %f, DIV = %f\n&#34;, (op-&gt;p_add)(1.3, 2.2), (*op-&gt;p_sub)(1.3, 2.2), (op-&gt;p_mul)(1.3, 2.2), (*op-&gt;p_div)(1.3, 2.2)); /* 调用回调函数 */ printf(&#34;ADD = %f, SUB = %f, MUL = %f, DIV = %f\n&#34;, add_sub_mul_div(1.3, 2.2, ADD), add_sub_mul_div(1.3, 2.2, SUB), add_sub_mul_div(1.3, 2.2, MUL), add_sub_mul_div(1.3, 2.2, DIV)); return 0; } 回调函数实例（很有用） 一个 GPRS 模块联网的小项目，使用过的同学大概知道 2G、4G、NB 等模块要想实现无线联网功能都需要经历模块上电初始化、注册网络、查询网络信息质量、连接服务器等步骤，这里的的例子就是，利用一个状态机函数（根据不同状态依次调用不同实现方法的函数），通过回调函数的方式依次调用不同的函数，实现模块联网功能，如下：
/********* 工作状态处理 *********/ typedef struct { uint8_t mStatus; uint8_t (* Funtion)(void); //函数指针的形式 } M26_WorkStatus_TypeDef; //M26的工作状态集合调用函数  /********************************************** ** &gt;M26工作状态集合函数 ***********************************************/ M26_WorkStatus_TypeDef M26_WorkStatus_Tab[] = { {GPRS_NETWORK_CLOSE, M26_PWRKEY_Off }, //模块关机  {GPRS_NETWORK_OPEN, M26_PWRKEY_On }, //模块开机  {GPRS_NETWORK_Start, M26_Work_Init }, //管脚初始化  {GPRS_NETWORK_CONF, M26_NET_Config }, //AT指令配置  {GPRS_NETWORK_LINK_CTC, M26_LINK_CTC }, //连接调度中心  {GPRS_NETWORK_WAIT_CTC, M26_WAIT_CTC }, //等待调度中心回复  {GPRS_NETWORK_LINK_FEM, M26_LINK_FEM }, //连接前置机  {GPRS_NETWORK_WAIT_FEM, M26_WAIT_FEM }, //等待前置机回复  {GPRS_NETWORK_COMM, M26_COMM }, //正常工作  {GPRS_NETWORK_WAIT_Sig, M26_WAIT_Sig }, //等待信号回复  {GPRS_NETWORK_GetSignal, M26_GetSignal }, //获取信号值  {GPRS_NETWORK_RESTART, M26_RESET }, //模块重启 } /********************************************** ** &gt;M26模块工作状态机，依次调用里面的12个函数 ***********************************************/ uint8_t M26_WorkStatus_Call(uint8_t Start) { uint8_t i = 0; for(i = 0; i &lt; 12; i++) { if(Start == M26_WorkStatus_Tab[i].mStatus) { return M26_WorkStatus_Tab[i].Funtion(); } } return 0; } 所以，如果有人想做个 NB 模块联网项目，可以 copy 上面的框架，只需要修改回调函数内部的具体实现，或者增加、减少回调函数，就可以很简洁快速的实现模块联网。
]]></content>
  </entry>
  
  <entry>
    <title>DDR、DDR2、DDR3、DDR4、LPDDR区别</title>
    <url>/post/hardware/difference-between-different-generation-DDRs.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>DDR</tag>
    </tags>
    <content type="html"><![CDATA[今天我们和大家介绍一下各代DDR的区别
什么是DDR DDR是Double Data Rate的缩写，即“双比特翻转”。DDR是一种技术，中国大陆工程师习惯用DDR称呼用了DDR技术的SDRAM，而在中国台湾以及欧美，工程师习惯用DRAM来称呼。
DDR的核心要义是在一个时钟周期内，上升沿和下降沿都做一次数据采样，这样400MHz的主频可以实现800Mbps的数据传输速率。
每一代DDR的基本区别 关键技术解释 VTT VTT为DDR的地址线，控制线等信号提供上拉电源，上拉电阻是50Ω左右。VTT=1/2VDDQ，并且VTT要跟随VDDQ，因此需要专用的电源同时提供VDDQ和VTT。例如芯片TPS51206DSQT，LP2996。用专门的电源芯片，还有一个重要的原因，在Fly-by的拓扑中，VTT提供电流，增强DDR信号线的驱动能力。
DDR的接收器是一个比较器，其中一端是VREF，另一端是信号，例如地址线A2在有VTT上拉的时候，A2的信号在0和1.8V间跳动，当A2电压高于VTT时，电流流向VTT。当A2低于VTT时，VTT流向DDR。因此VTT需要有提供电流和吸收电流的能力，一般的开关电源不能作为VTT的提供者。此外，VTT电源相当于DDR接收器信号输入端的直流偏执，且这个偏执等于VREF，因此VTT的噪声要越小越好，否则当A2的状态为高阻态时，DDR接收器的比较器容易产生误触发。
上文说过，VTT相当于DDR接收器的直流偏执，其实如果没有VTT，这个直流偏执也存在，它在芯片的内部，提供电流的能力很弱。如果只有1个或2个DDR芯片，走Fly-by拓扑，那么不需要外部的VTT上拉。如果有2个以上的DDR芯片，则一定需要VTT上拉。
Prefetch Prefetch字面意思就是预存取，每一代的DDR预存取大小不同，详见第2章中表格。以DDR3为例，它的Prefetch=8n，相当于DDR的每一个IO都有一个宽度为8的buffer，从IO进来8个数据后，在第8个数据进来后，才把这8个数据一次性的写入DDR内部的存储单元。下图是一个形象的解释，同时我们关注一下几个速率。DDR3的时钟是800MHz，Data Rate是1600Mbps，由于这个Buffer的存在，DDR内部的时钟只需要200MHz就可以了（注意DDR内部不是双比特翻转采样）。
我们来做一个频率对照表，如下：
DDR内部的最小存储单元（1bit）是一个晶体管+一个电容，电容会放电，需要不断的“刷新”（充电）才能保持正常的工作状态，由于电容充放电需要时间，DDR内部的频率受限于此，很难提高，目前技术一般在100~200MHz。因此需要用Prefetch技术来提内部数据高吞吐率（其实就是串并转换原理）。Prefetch位宽的提高，是DDR2,3,4非常显著的变化。
第一段提到，对于DDR3，在第8个数据进来后，FIFO满了，然后才把这8个数据一次性的写入DDR内部的存储单元，那么必须要求DDR的内部时钟和外部时钟有一定的约束关系，FIFO满的时候一定是以DQS下降沿采样结束的，数据手册中对DQS的下降沿与clk有一个建立时间和保持时间的约束要求的目的原来是这样。
SSTL SSTL（Stub Series Terminated Logic）接口标准也是JEDEC所认可的标准之一。该标准专门针对高速内存(特别是SDRAM)接口。SSTL规定了开关特点和特殊的端接方案。
SSTL标准规定了IC供电，IO的DC和AC输入输出门限，差分信号门限，Vref电压等。SSTL_3是3.3V标准，SSTL_2是2.5V标准，SSTL_18是1.8V标准，SSTL_15是1.5V。
SSTL最大的特点是需要终端匹配电阻，也叫终端终结电阻，上拉到VTT（1/2VDDQ）。这个短接电阻最大的作用是为了信号完整性，特别是在1拖多的Fly-by走线拓扑下，还能增强驱动能力。
Bank 以下图为例，一个Bank中包含若干个Array，Array相当于一个表单，选中“行地址”和“列地址”后，表单中的一个单元格就被选中，这个单元格就是一个bit。Bank中的所有Array的行地址是连在一起的，列地址也是。那么选中“行地址”和“列地址”后，将一起选中所有Array的bit。有多少个array，就有多少个bit被选中。以DDR3为例，Data线宽度是32，prefetch是8，那么Array就有32x8=256.内部一次操作会选中256bit的数据。
Bank数量越多，需要的Bank选择线越多，DDR3有8个bank，需要3个BA信号BA0~2。BA，行地址，列地址共同组成了存储单元的访问地址，缺一不可。
DDR的容量计算 下图是DDR3 1Gb的寻址配置，以其中128Mbx8为例说明，其中x8表示IO数据（DQ）位宽度。
 DDD容量=2Bank Addressx2Row Addressx2Col Addressx位宽=23x214x210x8=1Gb Page Site=2Col Addressx位宽➗8（Byte)  我的理解是，这个page size更像是逻辑上的一个页，并不是一个bank中，一行的所有bit，因为一行的所有bit要考虑prefetch宽度。
上表是JESD-3D中的表格，Row Address和Column Address都是真实需要寻址的地址，其他用途的地址比如A10，A12或者A11等并没有计算在内。在计算时，不要因为有A13，就认为Column Address就是A0~A13。
Burst Burst字面意思是突发，DDR的访问都是以突发的方式连续访问同一行的相邻几个单元。进行Brust时，需要有几个参数：
 Burst Length：一次突发访问几个列地址。 Read/Write: 是读还是写 Starting Column：从哪一列开始Burst Burst：突发的顺序。  下图是DDR3中突发类型和顺序，Burst是通过A12/BC#选择的。但对于DDR，DDR2和DDR4，不一定就是通过A12/BC#，详见PIN定义章节。
DDR的tRDC，CL，tAC 在实际工作中，Bank地址与相应的行地址是同时发出的，此时这个命令称之为“行激活”（Row Active）。在此之后，将发送列地址寻址命令与具体的操作命令（是读还是写），这两个命令也是同时发出的，所以一般都会以“读/写命令”来表示列寻址。根据相关的标准，从行有效到读/写命令发出之间的间隔被定义为tRCD，即RAS to CAS Delay（RAS至CAS延迟，RAS就是行地址选通脉冲，CAS就是列地址选通脉冲），我们可以理解为行选通周期。tRCD是DDR的一个重要时序参数，广义的tRCD以时钟周期（tCK，Clock Time）数为单位，比如tRCD=3，就代表延迟周期为两个时钟周期，具体到确切的时间，则要根据时钟频率而定，DDR3-800，tRCD=3，代表30ns的延迟。
接下来，相关的列地址被选中之后，将会触发数据传输，但从存储单元中输出到真正出现在内存芯片的 I/O 接口之间还需要一定的时间（数据触发本身就有延迟，而且还需要进行信号放大），这段时间就是非常著名的 CL（CAS Latency，列地址脉冲选通潜伏期）。CL 的数值与 tRCD 一样，以时钟周期数表示。如 DDR3-800，时钟频率为 100MHz，时钟周期为 10ns，如果 CL=2 就意味着 20ns 的潜伏期。不过CL只是针对读取操作。
由于芯片体积的原因，存储单元中的电容容量很小，所以信号要经过放大来保证其有效的识别性，这个放大/驱动工作由S-AMP负责，一个存储体对应一个S- AMP通道。但它要有一个准备时间才能保证信号的发送强度（事前还要进行电压比较以进行逻辑电平的判断），因此从数据I/O总线上有数据输出之前的一个时钟上升沿开始，数据即已传向S-AMP，也就是说此时数据已经被触发，经过一定的驱动时间最终传向数据I/O总线进行输出，这段时间我们称之为 tAC（Access Time from CLK，时钟触发后的访问时间）。
目前内存的读写基本都是连续的，因为与CPU交换的数据量以一个Cache Line（即CPU内Cache的存储单位）的容量为准，一般为64字节。而现有的Rank位宽为8字节（64bit），那么就要一次连续传输8次，这就涉及到我们也经常能遇到的突发传输的概念。突发（Burst）是指在同一行中相邻的存储单元连续进行数据传输的方式，连续传输的周期数就是突发长度（Burst Lengths，简称BL）。
在进行突发传输时，只要指定起始列地址与突发长度，内存就会依次地自动对后面相应数量的存储单元进行读/写操作而不再需要控制器连续地提供列地址。这样，除了第一笔数据的传输需要若干个周期（主要是之前的延迟，一般的是tRCD+CL）外，其后每个数据只需一个周期的即可获得。
突发连续读取模式：只要指定起始列地址与突发长度，后续的寻址与数据的读取自动进行，而只要控制好两段突发读取命令的间隔周期（与BL相同）即可做到连续的突发传输。
谈到了突发长度时。如果BL=4，那么也就是说一次就传送4×64bit的数据。但是，如果其中的第二笔数据是不需要的，怎么办？还都传输吗？为了屏蔽不需要的数据，人们采用了数据掩码（Data I/O Mask，简称DQM）技术。通过DQM，内存可以控制I/O端口取消哪些输出或输入的数据。这里需要强调的是，在读取时，被屏蔽的数据仍然会从存储体传出，只是在“掩码逻辑单元”处被屏蔽。DQM由北桥控制，为了精确屏蔽一个P-Bank位宽中的每个字节，每个DIMM有8个DQM 信号线，每个信号针对一个字节。这样，对于4bit位宽芯片，两个芯片共用一个DQM信号线，对于8bit位宽芯片，一个芯片占用一个DQM信号，而对于 16bit位宽芯片，则需要两个DQM引脚。
在数据读取完之后，为了腾出读出放大器以供同一Bank内其他行的寻址并传输数据，内存芯片将进行预充电的操作来关闭当前工作行。还是以上面那个Bank示意图为例。当前寻址的存储单元是B1、R2、C6。如果接下来的寻址命令是B1、R2、C4，则不用预充电，因为读出放大器正在为这一行服务。但如果地址命令是B1、R4、C4，由于是同一Bank的不同行，那么就必须要先把R2关闭，才能对R4寻址。从开始关闭现有的工作行，到可以打开新的工作行之间的间隔就是tRP（Row Precharge command Period，行预充电有效周期），单位也是时钟周期数。
ODT ODT是内建核心的终结电阻，它的功能是让一些信号在终结电阻处消耗完，防止这些信号在电路上形成反射。换句话说就是在片内设置合适的上下拉电阻，以获得更好的信号完整性。被ODT校准的信号包括：
 DQ, DQS, DQS# and DM for x4 configuration DQ, DQS, DQS#, DM, TDQS and TDQS# for X8 configuration DQU, DQL, DQSU, DQSU#, DQSL, DQSL#, DMU and DML for X16 configuration  当一个CPU挂了很多个DDR芯片的时候，他们是共用控制线，地址线的，走线肯定要分叉，如果没有中端匹配电阻，肯定会产生信号完整性问题。那么如果只有一个DDR芯片的时候，需不需要呢？正常情况下，走线很短，有符合规则，是不需要的。
下图是DDR中的IO上下拉电阻，RON是DDR的输出结构的上下拉电阻，RTT是DDR输入结构的上下拉电阻。这两个电阻的阻值都是可调的。
下图是RON的调节，注意这不是ODT的任务，调节是通过寄存器实现。
下图是RTT的调节，是ODT要做的事情，而且RTT的档位要多，也是通过寄存器调节的。
注意，DDR3的PIN定义上有一个引脚是ODT，如果ODT=0，DRAM Termination State功能关闭；ODT=1，DRAM Termination State的功能参考寄存器设置。如下是一个真值表。因为DRAM Termination State非常耗电，所以不用的时候最好不要打开。
DDR3的ZQ ZQ信号在DDR3时代开始引入，要求在ZQ引脚放置一个240Ω±1%的高精度电阻到地，注意必须是高精度。而且这个电阻是必须的，不能省略的。进行ODT时，是以这个引脚上的阻值为参考来进行校准的。
校准需要调整内部电阻，以获得更好的信号完整性，但是内部电阻随着温度会有些细微的变化，为了将这个变化纠正回来，就需要一个外部的精确电阻作为参考。详细来讲，就是为RTT和RON提供参考电阻。
OCD OCD 是在 DDR-II 开始加入的新功能，而且这个功能是可选的，有的资料上面又叫离线驱动调整。OCD的主要作用在于调整 I/O 接口端的电压，来补偿上拉与下拉电阻值， 从而调整DQS 与 DQ 之间的同步确保信号的完整与可靠性。调校期间，分别测试 DQS 高电平和 DQ高电平，以及 DQS 低电平和 DQ 高电平的同步情况。如果不满足要求，则通过设定突发长度的地址线来传送上拉 / 下拉电阻等级（加一档或减一档），直到测试合格才退出 OCD 操作，通过 OCD 操作来减少 DQ 、 DQS的倾斜从而提高信号的完整性及控制电压来提高信号品质。由于在一般情况下对应用环境稳定程度要求并不太高，只要存在差分 DQS时就基本可以保证同步的准确性， 而且 OCD 的调整对其他操作也有一定影响， 因此 OCD 功能在普通台式机上并没有什么作用，其优点主要体现在对数据完整性非常敏感的服务器等高端产品领域。
DDR3的PIN定义 下面是三星K4B4G0446Q/K4B4G0846Q的PIN定义，每一个都有很详细的解释。
以x8的配置为例，如下是其Ball Map。
  一对时钟线CK和CKn
  数据线DQ0~DQ7共8位。
  一对差分对DQS和DQSn
  地址线A0~A15，其中，A10和A12有特殊用途。
  行选中信号RASn
  列选中信号CASn
  写使能Wen
  片选CSn
  Bank选择BA0~2
  一个Reset信号，是DDR3新增的一项重要功能，并为此专门准备了一个引脚。这一引脚将使DDR3的初始化处理变得简单。当Reset命令有效时，DDR3 内存将停止所有的操作，并切换至最少量活动的状态，以节约电力。在Reset期间，DDR3内存将关闭内在的大部分功能，所有数据接收与发送器都将关闭，且所有内部的程序装置将复位，DLL（延迟锁相环路）与时钟电路将停止工作，甚至不理睬数据总线上的任何动静。这样一来，该功能将使DDR3达到最节省电力的目的。
  ZQ和ODT PIN上文已经说明。
  DDR的走线规则 DDR的信号线需要分组：
数据线一组（DQ,DQS,DQM），误差控制在20mil以内；
控制线一组（Address，控制线，时钟），以时钟为中心，误差控制在100mil以内。
]]></content>
  </entry>
  
  <entry>
    <title>什么是DSP</title>
    <url>/post/dsp/what-is-dsp.html</url>
    <categories><category>DSP</category>
    </categories>
    <tags>
      <tag>DSP</tag>
    </tags>
    <content type="html"><![CDATA[嵌入式工程师都知道什么是CPU、MCU，还有一位成员——DSP，DSP到底是什么？
DSP概述 DSP（digital signal processor）是一种独特的微处理器，有自己的完整指令系统，是以数字信号来处理大量信息的器件。其最大特点是内部有专用的硬件乘法器和哈佛总线结构对大量的数字信号处理的速度快。一个数字信号处理器在一块不大的芯片内包括有控制单元、运算单元、各种寄存器以及一定数量的存储单元等等，在其外围还可以连接若干存储器，并可以与一定数量的外部设备互相通信，有软、硬件的全面功能，本身就是一个微型计算机。
DSP采用的是哈佛设计，即数据总线和地址总线分开，使程序和数据分别存储在两个分开的空间，允许取指令和执行指令完全重叠。也就是说在执行上一条指令的同时就可取出下一条指令，并进行译码，这大大的提高了微处理器的速度。另外还允许在程序空间和数据空间之间进行传输，因为增加了器件的灵活性。
当今的数字化时代背景下，DSP己成为通信、计算机、消费类电子产品等领域的基础器件。
根据数字信号处理的要求，DSP芯片一般具有如下的一些主要特点：
 在一个指令周期内可完成一次乘法和一次加法。 程序和数据空间分开，可以同时访问指令和数据。 片内具有快速RAM，通常可通过独立的数据总线在两块中同时访问。 具有低开销或无开销循环及跳转的硬件支持。 快速的中断处理和硬件I/O支持。 具有在单周期内操作的多个硬件地址产生器。 可以并行执行多个操作。 支持流水线操作，使取指、译码和执行等操作可以重叠执行。  与通用微处理器相比，DSP芯片的其他通用功能相对较弱些。
DSP 芯片的诞生过程 DSP 芯片的诞生是时代所需。20世纪60年代以来，随着计算机和信息技术的飞速发展，数字信号处理技术应运而生并得到迅速的发展。在 DSP 芯片出现之前数字信号处理只能依靠微处理器来完成。但由于微处理器较低的处理速度不快，根本就无法满足越来越大的信息量的高速实时要求。
上世纪 70 年代，DSP芯片的理论和算法基础已成熟。但那时的DSP仅仅停留在教科书上，即使是研制出来的 DSP 系统也是由分立元件组成的，其应用领域仅局限于军事、航空航天部门。
 1978 年， AMI 公司发布世界上第一个单片 DSP 芯片 S2811，但没有现代 DSP芯片所必须有的硬件乘法器； 1979 年， 美国 Intel 公司发布的商用可编程器件 2920 是 DSP 芯片的一个主要里程碑，但其依然没有硬件乘法器； 1980 年，日本 NEC 公司推出的 MPD7720 是第一个具有硬件乘法器的商用 DSP芯片，从而被认为是第一块单片 DSP 器件； 1982 年世界上诞生了第一代 DSP 芯片 TMS32010 及其系列产品。这种 DSP 器件采用微米工艺 NMOS 技术制作，虽功耗和尺寸稍大，但运算速度却比微处理器快了几十倍。  DSP 芯片的问世是个里程碑，它标志着 DSP 应用系统由大型系统向小型化迈进了一大步。至 80 年代中期，随着 CMOS 工艺的 DSP 芯片应运而生，其存储容量和运算速度都得到成倍提高，成为语音处理、图像硬件处理技术的基础。
80 年代后期，第三代 DSP 芯片问世，运算速度进一步提高，其应用范围逐步扩大到通信、计算机领域；
90 年代 DSP 发展最快，相继出现了第四代和第五代 DSP 芯片。第五代与第四代相比系统集成度更高，将 DSP 芯核及外围元件综合集成在单一芯片上。
进入 21 世纪后，第六代 DSP 芯片横空出世。第六代芯片在性能上全面碾压第五代芯片，同时基于商业目的的不同发展出了诸多个性化的分支，并开始逐渐拓展新的领域。
DSP 芯片的应用领域 如今，各种各样的DSP器件已相当丰富。大大小小封装形式的DSP器件，已广泛应用于各种产品的生产领域，而且DSP的应用领域仍在不断地扩大，发展迅速异常。
DSP芯片强调数字信号处理的实时性。DSP作为数字信号处理器将模拟信号转换成数字信号，用于专用处理器的高速实时处理。它具有高速，灵活，可编程，低功耗的界面功能，在图形图像处理，语音处理，信号处理等通信领域起到越来越重要的作用。
根据美国的权威资讯公司统计，目前 DSP 芯片在市场上应用最多的是通信领域，其次是计算机领域。
DSP芯片的应用领域 1）DSP芯片在多媒体通信领域的应用。 媒体数据传输产生的信息量是巨大的，多媒体网络终端在整个过程中需要对获取的信息量进行快速分析和处理，因此 DSP 被运用在语音编码，图像压缩和减少语音通信上。如今 DSP 对于语音解码计算产生实时效果，设计协议要求已经成为最基本的一条国际标准。
2）DSP芯片在工业控制领域的应用。 在工业控制领域， 工业机器人被广泛应用，对机器人控制系统的性能要求也越来越高。机器人控制系统重中之重就是实时性，在完成一个动作的同时会产生较多的数据和计算处理，这里可以采用高性能的 DSP。DSP通过应用到机器人的控制系统后，充分利用自身的实时计算速度特性，使得机器人系统可以快速处理问题，随着不断提高 DSP 数字信号芯片速度，在系统中容易构成并行处理网络，大大提高控制系统的性能，使得机器人系统得到更为广泛的发展。
3）DSP芯片在仪器仪表领域的应用。 DSP 丰富的片内资源可以大大简化仪器仪表的硬件电路，实现仪器仪表的 SOC 设计。器仪表的测量精度和速度是一项重要的指标，使用 DSP 芯片开发产品可使这两项指标大大提高。例如 TI 公司的 TMS320F2810 具有高效的 32 位 CPU 内核，12 位 A/D 转换器，丰富的片上存储器和灵活的指挥系统，为高精密仪器搭建了广阔的平台。高精密仪器现在已经发展成为 DSP 的一个重要应用，正处于快速传播时期，将推动产业的技术创新。
4）DSP芯片在汽车安全与无人驾驶领域的应用。 汽车电子系统日益兴旺发达起来，诸如装设红外线和毫米波雷达，将需用 DSP 进行分析。如今，汽车愈来愈多，防冲撞系统已成为研究热点。而且，利用摄像机拍摄的图像数据需要经过 DSP 处理，才能在驾驶系统里显示出来，供驾驶人员参考。
5）DSP芯片在军事领域的应用。 DSP 的功耗低、体积小、实时性反应速度都是武器装备中特别需要的。如机载空空导弹，在有限的体积内装有红外探测仪和相应的 DSP信号处理器等部分，完成目标的自动锁定与跟踪。先进战斗机上装备的目视瞄准器和步兵个人携带的头盔式微光仪，需用 DSP 技术完成图像的滤波与增强，智能化目标搜索捕获。DSP 技术还用于自动火炮控制、巡航导弹、预警飞机、相控阵天线等雷达数字信号处理中。
未来DSP技术将向以下几个方向继续发展： 1）DSP芯核集成度越来越高。 缩小 DSP 芯片尺寸一直是 DSP 技术的发展趋势，当前使用较多的是基于 RISC 结构，随着新工艺技术的引入，越来越多的制造商开始改进DSP 芯核，并且把多个 DSP 芯核、 MPU 芯核以及外围的电路单元集成在一个芯片上，实现了 DSP 系统级的集成电路。
2）可编程DSP芯片将是未来主导产品。 随着个性化发展的需要， DSP 的可编程化为生产厂商提供了更多灵活性，满足厂家在同一个 DSP 芯片上开发出更多不同型号特征的系列产品，也使得广大用户对于 DSP 的升级换代。例如冰箱、洗衣机，这些原来装有微控制器的家电如今已换成可编程 DSP 来进行大功率电机控制。
3）定点DSP占据主流。 目前，市场上所销售的 DSP 器件中，占据主流产品的依然是16 位的定点可编程 DSP 器件，随着 DSP 定点运算器件成本的不断低，能耗越来越小的优势日渐明显，未来定点 DSP 芯片仍将是市场的主角。
DSP芯片的分类 DSP的芯片可以按照以下的三种方式进行分类。
（1）按基础特性分 这是根据DSP芯片的工作时钟和指令类型来分类的。如果DSP芯片在某时钟频率范围内的任何频率上能正常工作，除计算速度有变化外，没有性能的下降，这类DSP芯片一般称之为静态DSP芯片。
如果有两种或两种以上的DSP芯片,它们的指令集和相应的机器代码机管脚结构相互兼容,则这类DSP芯片称之为一致性的DSP芯片。
（2）按数据格式分 这是根据DSP芯片工作的数据格式来分类的。数据以定点格式工作的DSP芯片称之为定点DSP芯片。以浮点格式工作的称为DSP芯片。不同的浮点DSP芯片所采用的浮点格式不完全一样，有的DSP芯片采用自定义的浮点格式，有的DSP芯片则采用IEEE的标准浮点格式。
（3）按用途分 按照DSP芯片的用途来分，可分为通用型DSP芯片和专用型的DSP芯片。通用型DSP芯片适合普通的DSP应用，如TI公司的一系列DSP芯片。专用型DSP芯片为特定的DSP运算而设计，更适合特殊的运算，如数字滤波，卷积和FFT等。
DSP芯片的基本结构 DSP芯片的基本结构包括： （1）哈佛结构。哈佛结构的主要特点是将程序和数据存储在不同的存储空间中，即程序存储器和数据存储器是两个相互独立的存储器，每个存储器独立编址，独立访问。与两个存储器相对应的是系统中设置了程序总线和数据总线，从而使数据的吞吐率提高了一倍。由于程序和存储器在两个分开的空间中，因此取指和执行能完全重叠。
（2）流水线操作。流水线与哈佛结构相关，DSP芯片广泛采用流水线以减少指令执行的时间，从而增强了处理器的处理能力。处理器可以并行处理二到四条指令，每条指令处于流水线的不同阶段。
（3）专用的硬件乘法器。乘法速度越快，DSP处理器的性能越高。由于具有专用的应用乘法器，乘法可在一个指令周期内完成。
（4）特殊的DSP指令。特殊的DSP指令DSP芯片是采用特殊的指令。
（5）快速的指令周期。快速的指令周期哈佛结构、流水线操作、专用的硬件乘法器、特殊的DSP指令再加上集成电路的优化设计可使DSP芯片的指令周期在200ns以下。
DSP系统的特点、构成和设计过程 数字信号处理系统是以数字信号处理为基础，因此具有数字处理的全部特点：
 接口方便。DSP系统与其它以现代数字技术为基础的系统或设备都是相互兼容，这样的系统接口以实现某种功能要比模拟系统与这些系统接口要容易的多。 编程方便。DSP系统中的可编程DSP芯片可使设计人员在开发过程中灵活方便地对软件进行修改和升级。 稳定性好。DSP系统以数字处理为基础，受环境温度以及噪声的影响较小，可靠性高。 精度高。16位数字系统可以达到的精度。 可重复性好。模拟系统的性能受元器件参数性能变化比较大，而数字系统基本上不受影响，因此数字系统便于测试，调试和大规模生产。 集成方便。DSP系统中的数字部件有高度的规范性，便于大规模集成。   DSP系统的设计过程  根据需求确定DSP系统的性能指标 算法研究及模拟实现和功能验证 选择适合的DSP芯片和外围组件 软件设计及调试 硬件设计及调试 系统集成及测试  定点DSP和浮点DSP的区别 一般来说，定点DSP处理器具有速度快，功耗低，价格便宜的特点；而浮点DSP处理器则计算精确，动态范围大，速度快，易于编程，功耗大，价格高。
而它们的区别，还可以从各方面去比较。
 宏观上  从宏观上讲，浮点DSP比定点DSP的动态范围大得多。定点运算中，程序员必须时刻关注溢出的发生，为了防止溢出，要么不断进行移位定标，要么做截尾。前者耗费大量时间和空间，后者则带来精度的损失。相反，浮点运算DSP扩大了动态范围，提高了精度，节省了运算时间和存储空间，因而大大减少了定标，移位和溢出检查。
硬件上  单纯从技术的角度来看，定点与浮点的区别主要在两个方面，即硬件和软件。硬件上的区别来自于：浮点DSP处理器具有浮点/整数乘法器，整数/浮点算术逻辑运算单元ALU，适合存放扩展精度的浮点结果的寄存器等。
软件上  再看看在软件开发上的不同之处，主要有浮点DSP编程的特点以及注意事项；定点DSP进行浮点运算时的定标，移位，检测溢出操作。比较两个浮点数时，永远不要使用操作符==来判断是否相等。即使比较两个相同的数，还是可能有微小的舍入差别。甚至定义精确的0，也不是很安全，尽管C语言中有0的表示，永远不要写这样的代码(x==0)，而应该写成(fabs(x) &lt; TINY)，其中TINY定义为一个很小的值，也就是处理器的浮点格式舍入误差。
应用实例  另外一个比较重要区别涉及应用场合对定点与浮点dsp处理器的选择。设计师关心的是最后的系统性能、成本以及上市时间。
例如，在移动电视中，没必要进行浮点处理。而在军用雷达中，经常用到浮点处理器。
DSP芯片的选择依据  运算速度 运算精度 功耗 价格 硬件资源 开发工具 ]]></content>
  </entry>
  
  <entry>
    <title>如何为Ubuntu 22.04安装拼音输入法</title>
    <url>/post/linux/how-to-install-pinyin-for-ubuntu-22.04.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Input Method</tag>
      <tag>Pinyin</tag>
    </tags>
    <content type="html"><![CDATA[本文的指令原本只是针对Vanilla Ubuntu，而且只是针对Ubuntu 22.04来讲述基于简体汉字的基本拼音输入法。
Ubuntu到目前为止都没有提供一个简单，很好的文档化的指南来描述如何添加拼音输入法。但是，为了在Ubuntu 22.04上获得基本的拼音输入法的支持，你可以简单地：
依次打开 Settings(设定) -&gt; Region &amp; Language(区域和语言) -&gt; Manage Installed Languages(管理安装的语言) -&gt; Install / Remove languages(安装/删除语言)。
选择Chinese(Simplified)，确保键盘输入法系统已经选择了Ibus，然后点Apply(应用)，重启系统。
重新登陆系统，再次打开Settings(设定)，依次选择到键盘。
点击输入源的&quot;+&ldquo;图标，选择中文(中国)，然后中文(智能拼音)。
现在你应该可以看到一个小的&quot;en&quot;图表(或者你的Ubuntu安装的任何语言代码)在你的主屏幕的右上角，你可以点击然后看到可用的输入法的列表，包含中文(智能拼音)。
选中中文(智能拼音)，然后打开任何可以接受输入的应用(比如gedit, openoffice, vim, etc.)，你也可以通过快捷键 Win+space来进行输入法的切换。
再次重新启动系统，确保输入法的图表还在那儿。
]]></content>
  </entry>
  
  <entry>
    <title>Linux下大文件切割与合并</title>
    <url>/post/linux/linux-big-file-cut-and-cat.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>linux</tag>
      <tag>file</tag>
    </tags>
    <content type="html"><![CDATA[往往是因为网络传输的限制，导致很多时候，我们需要在 Linux 系统下进行大文件的切割。这样将一个大文件切割成为多个小文件，进行传输，传输完毕之后进行合并即可。
文件切割split 在 Linux 系统下使用 split 命令进行大文件切割很方便  命令语法 split [-a] [-d] [-l &lt;行数&gt;] [-b &lt;字节&gt;] [-C &lt;字节&gt;] [要切割的文件] [输出文件名] 使用实例 $ split -l 300000 users.sql /data/users_ $ split -d -l 300000 users.sql /data/users_ $ split -d -b 100m users.sql /data/users_ 帮助信息 $ split --help 文件合并 - cat 在 Linux 系统下使用 cat 命令进行多个小文件的合并也很方便  命令语法 cat [-n] [-e] [-t] [输出文件名] 使用实例 $ cat /data/users_* &gt; users.sql 帮助信息 $ cat --h ]]></content>
  </entry>
  
  <entry>
    <title>从实用的角度聊聊MOS管</title>
    <url>/post/hardware/mosfet-tube-application-introduction.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>MOS Tube</tag>
    </tags>
    <content type="html"><![CDATA[说起MOS管，有些人的脑子里可能是一团浆糊。大部分的教材都会告诉你长长的一段话：MOS管全称金属氧化半导体场效应晶体管
英文名Metal-Oxide-Semiconductor Field-Effect Transistor，属于绝缘栅极场效晶体管，以硅片为秤体，利用扩散工艺制作&hellip;&hellip;.有N沟道和P沟道两个型。不仅如此，它还有两个兄弟，分别是结型场效应管以及晶体场效应管&hellip;
面对这么大一段话，我不知道你有没有搞明白，反正我大学里是完全没有搞明白，学了一个学期就学了个寂寞。
那么，为什么这些教材要这么的反人类，他们难道就不能好好写说人话吗？
我大概分析了一下，因为同一本教材他需要面对不同专业的学生，所以教材最重要的是严谨。和全面相比是不是通俗易懂就没有那么重要了。而且一般的教材也不会告诉你学了有什么用，这就导致了在学习中你很容易迷失在这些概念中，抓不到重点。
那本文呢，我想根据自己的工作学习经历，抛开书本上这些教条的框架，从应用侧出发来给大家介绍一下MOS管里面最常见的，也是最容易使用的一种：增强型NMOS管，简称NMOS。当你熟悉了这个NMOS的使用之后，再回过头去看这个教材上的内容，我相信就会有不同的体会了。
NMOS的用法 首先来看这么一张简单的图（图1），我们可以用手去控制这个开关的开合，以此来控制这个灯光的亮灭。
图1
那如果我们想要用Arduino或者单片机去控制这个灯泡的话，就需要使用MOS管来替换掉这个开关了。为了更加符合我们工程的实际使用习惯呢，我们需要把这张图稍微转换一下，就像如图2这样子。
图2
那这两张图是完全等价的，我们可以看到MOS管是有三个端口，也就是有三个引脚，分别是GATE、DRAIN和SOURCE。至于为啥这么叫并不重要，只要记住他们分别简称G、D、S就可以。
图3
我们把单片机的一个IO口接到MOS管的gate端口，就可以控制这个灯泡的亮灭了。当然别忘了供电。当这个单片机的IO口输出为高的时候，NMOS就等效为这个被闭合的开关，指示灯光就会被打开；那输出为低的时候呢，这个NMOS就等效为这个开关被松开了，那此时这个灯光就被关闭，是不很简单。
那如果我们不停的切换这个开关，那灯光就会闪烁。如果切换的这个速度再快一点，因为人眼的视觉暂留效应，灯光就不闪烁了。此时我们还能通过调节这个开关的时间来调光，这就是所谓的PWM波调光，以上就是MOS管最经典的用法，它实现了单片机的IO口控制一个功率器件。当然你完全可以把灯泡替换成其他的器件。器件比如说像水泵、电机、电磁铁这样的东西。
图4：PWM波调光
如何选择NMOS 明白了NMOS的用法之后呢，我们来看一下要如何选择一个合适的NMOS，也就是NMOS是如何选型的。
那对于一个初学者来说，有四个比较重要的参数需要来关注一下。第一个是封装，第二个是Vgs(th)，第三个是Rds(on)上，第四个是Cgs。
封装比较简单，它指的就是一个MOS管这个外形和尺寸的种类也有很多。一般来说封装越大，它能承受的电流也就越大。为了搞明白另外三个参数呢，我们先要来介绍一下NMOS的等效模型。
图5：NMOS等效模型
MOS其实可以看成是一个由电压控制的电阻。这个电压指的是G、S的电压差，电阻指的是D、S之间的电阻。这个电阻的大小会随着G、S电压的变化而变化。当然它们不是线性对应的关系，实际的关系差不多像这样的，横坐标是G、S电压差。
图6：Rds与Vgs关系图
纵坐标是电阻的值，当G、S的电压小于一个特定值的时候呢，电阻基本上是无穷大的。然后这个电压值大于这个特定值的时候，电阻就接近于零，至于说等于这个值的时候会怎么样，我们先不用管这个临界的电压值，我们称之为Vgs(th)，也就是打开MOS管需要的G、S电压，这是每一个MOS管的固有属性，我们可以在MOS管的数据手册里面找到它。
图7：MOS管数据手册
显然，Vgs(th)一定要小于这个高电平的电压值，否则就没有办法被正常的打开。所以在你选择这个MOS管的时候，如果你的高电平是对应的5V，那么选3V左右的Vgs(th)是比较合适的。太小的话会因为干扰而误触发，太大的话又打不开这个MOS管。
接下来，我们再来看看NMOS的第二个重要参数Rdson，刚才有提到NMOS被完全打开的时候，它的电阻接近于零。但是无论多小，它总归是有一个电阻值的，这就是所谓的Rds(on)。它指的是NMOS被完全打开之后，D、S之间的电阻值。同样的你也可以在数据手册上找到它。这个电阻值当然是越小越好。越小的话呢，它分压分的少，而且发热也相对比较低。但实际情况一般Rds(on)越小，这个NMOS的价格就越高，而且一般对应的体积也会比较大。所以还是要量力而行，选择恰好合适。
最后说一下Cgs，这个是比较容易被忽视的一个参数，它指的是G跟S之间的寄生电容。所有的NMOS都有，这是一个制造工艺的问题，没有办法被避免。
那它会影响到NMOS打开速度，因为加载到gate端的电压，首先要给这个电容先充电，这就导致了G、S的电压并不能一下子到达给定的一个数值。
图8
它有一个爬升的过程。当然因为Cgs比较小，所以一般情况下我们感觉不到它的存在。但是当我们把这个时间刻度放大的时候，我们就可以发现这个上升的过程了。对于这个高速的PWM波控制场景是致命的。当PWM波的周期接近于这个爬升时间时，这个波形就会失真。一般来说Cgs大小和Rds(on)是成反比的关系。Rds(on)越小，Cgs就越大。所以大家要注意平衡他们之间的关系。
以上就是关于NMOS大家需要初步掌握的知识了，希望能对大家有所帮助。
]]></content>
  </entry>
  
  <entry>
    <title>MOS管驱动电路设计</title>
    <url>/post/hardware/the-drive-circuit-design-of-MOS-tube.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>MOS Tube</tag>
    </tags>
    <content type="html"><![CDATA[MOS管因为其导通内阻低，开关速度快，因此被广泛应用在开关电源上。而用好一个MOS管，其驱动电路的设计就很关键。下面分享几种常用的驱动电路。
电源IC直接驱动 电源IC直接驱动是最简单的驱动方式，应该注意几个参数以及这些参数的影响。
 查看电源IC手册的最大驱动峰值电流，因为不同芯片，驱动能力很多时候是不一样的。 了解MOS管的寄生电容，如图C1、C2的值，这个寄生电容越小越好。如果C1、C2的值比较大，MOS管导通的需要的能量就比较大，如果电源IC没有比较大的驱动峰值电流，那么管子导通的速度就比较慢，就达不到想要的效果。  推挽驱动 当电源IC驱动能力不足时，可用推挽驱动。 这种驱动电路好处是提升电流提供能力，迅速完成对于栅极输入电容电荷的充电过程。这种拓扑增加了导通所需要的时间，但是减少了关断时间，开关管能快速开通且避免上升沿的高频振荡。
加速关断驱动 MOS管一般都是慢开快关。在关断瞬间驱动电路能提供一个尽可能低阻抗的通路供MOSFET栅源极间电容电压快速泄放，保证开关管能快速关断。 为使栅源极间电容电压的快速泄放，常在驱动电阻上并联一个电阻和一个二极管，如上图所示，其中D1常用的是快恢复二极管。这使关断时间减小，同时减小关断时的损耗。Rg2是防止关断的时电流过大，把电源IC给烧掉。
如上图，是我之前用的一个电路，量产至少上万台，推荐使用。 用三极管来泄放栅源极间电容电压是比较常见的。如果Q1的发射极没有电阻，当PNP三极管导通时，栅源极间电容短接，达到最短时间内把电荷放完，最大限度减小关断时的交叉损耗。 还有一个好处，就是栅源极间电容上的电荷泄放时电流不经过电源IC，提高了可靠性。
隔离驱动 为了满足高端MOS管的驱动，经常会采用变压器驱动。其中R1目的是抑制PCB板上寄生的电感与C1形成LC振荡，C1的目的是隔开直流，通过交流，同时也能防止磁芯饱和。
]]></content>
  </entry>
  
  <entry>
    <title>SPI总线详解</title>
    <url>/post/hardware/spi-bus-introduction.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>SPI</tag>
    </tags>
    <content type="html"><![CDATA[SPI是串行外设接口（Serial Peripheral Interface）的缩写，是一种高速的，全双工，同步的通信总线，并且在芯片的管脚上只占用四根线，节约了芯片的管脚，同时为PCB的布局上节省空间，提供方便，正是出于这种简单易用的特性，越来越多的芯片集成了这种通信协议，比如AT91RM9200。
什么是SPI？ SPI是串行外设接口(Serial Peripheral Interface)的缩写，是 Motorola 公司推出的一种同步串行接口技术，是一种高速、全双工、同步的通信总线。
SPI优点  支持全双工通信 通信简单 数据传输速率块 SPI电路图  缺点 没有指定的流控制，没有应答机制确认是否接收到数据，所以跟IIC总线协议比较在数据可靠性上有一定的缺陷。
特点  高速、同步、全双工、非差分、总线式 主从机通信模式  SPI电路连接  SPI的通信原理很简单，它以主从方式工作，这种模式通常有一个主设备和一个或多个从设备，有三线制和四线制之分。信号线包括SDI(串行数据输入 Serial Digital IN)、SDO(串行数据输出 Serial Digital OUT)、SCLK(时钟)、CS(片选)。 SDO/MOSI – 主设备数据输出，从设备数据输入 SDI/MISO – 主设备数据输入，从设备数据输出 SCLK – 时钟信号，由主设备产生; CS/SS – 从设备使能信号，由主设备控制。当有多个从设备的时候，因为每个从设备上都有一个片选引脚接入到主设备机中，当主设备和某个从设备通信时将需要将从设备对应的片选引脚电平拉低(一般低有效)。  SPI通信模式分析 SPI通信有4种不同的模式，不同的从设备在出厂时配置模式已经固定， 这是不能改变的，但通信双方设备必须工作在同一模式下，所以可以对主设备的SPI模式进行配置，通过CPOL（时钟极性）和CPHA（时钟相位）来控制主设备的通信模式。
具体模式具体如下：
 Mode0：CPOL=0，CPHA=0 Mode1：CPOL=0，CPHA=1 Mode2：CPOL=1，CPHA=0 Mode3：CPOL=1，CPHA=1     模式 CPOL CPHA     Mode0 0 0   Mode1 0 1   Mode2 1 0   Mode3 1 1    时钟极性CPOL是用来配置SCLK电平的有效态的;
时钟相位CPHA是用来配置数据采样是发生在第几个边沿的。
 CPOL=0表示当SCLK=0时处于空闲态，所以SCLK处于高电平时有效； CPOL=1表示当SCLK=1时处于空闲态，所以SCLK处于低电平时有效； CPHA=0表示数据采样是在第1个边沿，数据发送在第2个边沿； CPHA=1表示数据采样是在第2个边沿，数据发送在第1个边沿； SPI主模块和与之通信的外设通信时，两者的时钟相位和极性应该保持一致。  SPI 时序详解 CPOL=0，CPHA=0：此时空闲态时，SCLK处于低电平，数据采样是在第1个边沿，也就是SCLK由低电平到高电平的跳变，所以数据采样是在上升沿，数据发送是在下降沿。
CPOL=0，CPHA=1：此时空闲态时，SCLK处于低电平，数据发送是在第1个边沿，也就是SCLK由低电平到高电平的跳变，所以数据采样是在下降沿，数据发送是在上升沿。
CPOL=1，CPHA=0：此时空闲态时，SCLK处于高电平，数据采集是在第1个边沿，也就是SCLK由高电平到低电平的跳变，所以数据采集是在下降沿，数据发送是在上升沿。
CPOL=1，CPHA=1：此时空闲态时，SCLK处于高电平，数据发送是在第1个边沿，也就是SCLK由高电平到低电平的跳变，所以数据采集是在上升沿，数据发送是在下降沿。
注意：SPI主设备能够控制时钟信号，因为SPI通信并不像UART或者IIC通信那样有专门的通信周期、通信起始信号、通信结束信号；所以SPI协议只能通过控制时钟信号线，在没有数据交流的时候，时钟线要么是保持高电平，要么是保持低电平。
例如：工作在模式0这种时序（CPOL＝0，CPHA＝0），如下：
我们来关注SCK的第一个时钟周期，在时钟的前沿采样数据（上升沿，第一个时钟沿），在时钟的后沿输出数据（下降沿，第二个时钟沿）。首先来看主器件，主器件的输出口（MOSI）输出的数据bit1，在时钟的前沿被从器件采样，那主器件是在何时刻输出bit1的呢？bit1的输出时刻实际上在SCK信号有效以前，比SCK的上升沿还要早半个时钟周期。bit1的输出时刻与SSEL信号没有关系。再来看从器件，主器件的输入口MISO同样是在时钟的前沿采样从器件输出的bit1的，那从器件又是在何时刻输出bit1的呢。从器件是在SSEL信号有效后，立即输出bit1，尽管此时SCK信号还没有起效。
从这张图就可以很清楚的看出主从器件的bit1是怎样输出的。
]]></content>
  </entry>
  
  <entry>
    <title>10个超赞的C语言开源项目</title>
    <url>/post/linux/ten-great-c-language-open-source-project.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>C language</tag>
      <tag>open source</tag>
    </tags>
    <content type="html"><![CDATA[今天给大家分享10个超赞的C语言开源项目，希望这些内容能对大家有所帮助！
Webbench Webbench是一个在 Linux 下使用的非常简单的网站压测工具。
它使用fork()模拟多个客户端同时访问我们设定的URL，测试网站在压力下工作的性能。
最多可以模拟 3 万个并发连接去测试网站的负载能力。Webbench使用C语言编写，代码非常简洁，源码加起来不到 600 行。
项目地址 http://home.tiscali.cz/~cz210552/webbench.html Tinyhttpd tinyhttpd是一个超轻量型Http Server，使用C语言开发，全部代码只有 502 行（包括注释），附带一个简单的 Client
可以通过阅读这段代码理解一个 Http Server 的本质。
项目地址 http://sourceforge.net/projects/tinyhttpd/ cJSON cJSON是C语言中的一个JSON编解码器，非常轻量级，C文件只有 500 多行，速度也非常理想。
虽然cJSON功能不是非常强大，但cJSON的小身板和速度是最值得赞赏的。
其代码被非常好地维护着，结构也简单易懂，可以作为一个非常好的C语言项目进行学习。
项目主页 http://sourceforge.net/projects/cjson/ CMockery CMockery是google发布的用于C单元测试的一个轻量级的框架。
它很小巧，对其他开源包没有依赖，对被测试代码侵入性小。
CMockery 的源代码行数不到3K，阅读一下will_return和mock的源代码就一目了然了。
主要特点  免费且开源，google 提供技术支持； 轻量级的框架，使测试更加快速简单； 避免使用复杂的编译器特性，对老版本的编译器来讲，兼容性好； 并不强制要求待测代码必须依赖 C99 标准，这一特性对许多嵌入式系统的开发很有用。   项目地址 http://code.google.com/p/cmockery/downloads/list Libev libev 是一个开源的事件驱动库，基于 epoll、kqueue 等 OS 提供的基础设施。
其以高效出名，它可以将 IO 事件、定时器、和信号统一起来，统一放在事件处理这一套框架下处理。
基于 Reactor 模式，效率较高，并且代码精简（4.15 版本 8000 多行），是学习事件驱动编程的很好的资源。
项目地址 http://software.schmorp.de/pkg/libev.html Memcached Memcached 是一个高性能的分布式内存对象缓存系统，用于动态 Web 应用以减轻数据库负载。
它通过在内存中缓存数据和对象来减少读取数据库的次数，从而提供动态数据库驱动网站的速度。
Memcached 基于一个存储键/值对的 hashmap。Memcached-1.4.7 的代码量还是可以接受的，只有 10K 行左右。
项目地址 http://memcached.org/ Lua Lua 很棒，在任何支持 ANSI C 编译器的平台上都可以轻松编译通过。 Lua 的代码数量足够小，5.1.4 仅仅 1.5W 行，去掉空白行和注释估计能到 1W 行。
项目地址 http://www.lua.org/ SQLite SQLite 是一个开源的嵌入式关系数据库，实现自包容、零配置、支持事务的 SQL 数据库引擎。其特点是高度便携、使用方便、结构紧凑、高效、可靠。
足够小，大致 3 万行C代码，250K。
项目地址 http://www.sqlite.org/ UNIX v6 UNIX V6 的内核源代码包括设备驱动程序在内约有 1 万行，这个数量的源代码，初学者是能够充分理解的。有一种说法是一个人所能理解的代码量上限为 1 万行，UNIX V6 的内核源代码从数量上看正好在这个范围之内。
看到这里，大家是不是也有“如果只有 1 万行的话没准儿我也能学会”的想法呢？
另一方面，最近的操作系统，例如 Linux   最新版的内核源代码据说超过了 1000 万行。
就算不是初学者，想完全理解全部代码基本上也是不可能的。
项目地址 http://minnie.tuhs.org/cgi-bin/utree.pl?file=V6 NETBSD NetBSD 是一个免费的，具有高度移植性的 UNIX-like 操作系统。
NetBSD 计划的口号是：“Of course it runs NetBSD”。
它设计简洁，代码规范，拥有众多先进特性，使得它在业界和学术界广受好评。
由于简洁的设计和先进的特征，使得它在生产和研究方面，都有卓越的表现，而且它也有受使用者支持的完整的源代码。
许多程序都可以很容易地通过 NetBSD Packages Collection 获得。
]]></content>
  </entry>
  
  <entry>
    <title>Linux环境监控工具基础参考</title>
    <url>/post/linux/linux-environment-monitor-tool.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>linux</tag>
      <tag>monitor tool</tag>
    </tags>
    <content type="html"><![CDATA[Linux 操作系统有很多自带和第三方监控工具，这篇文章从不同维度整理了一些，但仅限基础了解，因为，单独讲任何一个指令，都可以成篇文章，具体指令参数，可以检索 man，从中理解。
CPU top(任务管理工具)
top -n 1 -b vmstat(展现给定时间间隔的服务器的状态值，包括服务器的 CPU 使用率，内存使用)
vmstat 1 10 #每1秒采集一次共采集10次 pidstat(进程实时监控)
pidstat -u 1 -p pid mpstat(多 CPU 实时监控工具)
mpstat -P ALL 1 5 sar(性能监控和瓶颈检查)
sar -u dstat(dstat 是一个可以取代 vmstat，iostat，netstat 和 ifstat 这些命令的多功能产品)
dstat 2 10 #每2秒采集一次共采集10次 内存 top
top -n 1 -b pidstat
pidstat -r free(查看当前系统的物理内存使用情况)
free -mh sar(性能监控和瓶颈检查)
sar -r 10 3 #每10秒采样一次，连续采样3次 vmstat
vmstat 2 1 磁盘 IO iostat(IO 实时监控)
iostat -d -x -k 1 10 iotop(监控系统中各个进程对 IO 的使用量)
iotop pidstat
示例: pidstat -d sar
sar -d vmstat
vmstat 2 1 网络 netstat(监控 TCP/IP 网络)
netstat -nltup iftop(实时流量监控工具)
iftop -i em2 ss(获取 socket 统计信息，他可以显示和 netstat 类似的内容)
ss -aA tcp sar
sar -n EDEV 1 5 tcpdump(抓包工具)
tcpdump -i em1 host 192.168.1.1 and port 80 tcpflow(分析网络流量)
tcpflow -i em1 port 80 nload(用于查看 Linux 网络流量状况，实时输出)
nload -t 200 -i 1024 -o 128 -U M 系统负载 (1) CPU 负载说明
 如果某个程序频繁的进行计算、逻辑判断等操作，那么此类程序主要依赖于 CPU 的处理速度，故称之为 &ldquo;计算密集型程序&rdquo;。
 (2) IO 负载说明
 如果某个程序频繁的从磁盘中读取写入文件，那么这种类型的操作主要依赖于磁盘的读取速度，也就是输入输出 (input/output)，简写为 I/O。此类 I/O 负载的程序，称为 I/O 密集型程序。
 top
top uptime
uptime sar
sar -q 1 20 其他工具  htop(类似 top，比 top 更加人性化) glances(类似 top，基于 Python 的系统遥测监控工具) strace(常用来跟踪进程执行时的系统调用和所接收的信号) dtrace(动态跟踪) valgrind(内存泄漏检测) dmesg(内核信息) ]]></content>
  </entry>
  
  <entry>
    <title>详解TCP和UDP协议的原理和区别</title>
    <url>/post/linux/tcp-and-udp-principle-and-difference.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>tcp</tag>
      <tag>udp</tag>
    </tags>
    <content type="html"><![CDATA[ 最近重新认知了一下TCP和UDP的原理以及区别，做一个简单的总结。
 作用 首先，tcp和udp都是工作再传输层，用于程序之间传输数据的。数一般包含：文件类型，视频类型，jpg图片等。
区别 TCP是基于连接的，而UDP是基于非连接的。
tcp传输数据稳定可靠，适用于对网络通讯质量要求较高的场景，需要准确无误的传输给对方，比如，传输文件，发送邮件，浏览网页等等。
udp的优点是速度快，但是可能产生丢包，所以适用于对实时性要求较高但是对少量丢包并没有太大要求的场景。比如：域名查询，语音通话，视频直播等。udp还有一个非常重要的应用场景就是隧道网络，比如：vpn，VXLAN。
以人与人之间的通信为例：UDP协议就相当于是写信给对方，寄出去信件之后不能知道对方是否收到信件，信件内容是否完整，也不能得到及时反馈，而TCP协议就像是打电话通信，在这一系列流程都能得到及时反馈，并能确保对方及时接收到。如下图：
TCP通信的过程 tcp是如何保证以上过程的:分为三个步骤，三次握手，传输确认，四次挥手。三次握手是建立连接的过程。
三次握手 当客户端向服务端发起连接时，会先发一包连接请求数据，过去询问一下，能否与你建立连接？这包数据称之为SYN包，如果对端同意连接，则回复一包SYN+ACK包，客户端收到之后，发送一包ACK包，连接建立，因为这个过程中互相发送了三包数据，所以称之为三次握手。
为什么要三次握手而不是两次握手？
**这是为了防止，因为已失效的请求报文，突然又传到服务器，引起错误，**这是什么意思？
假设采用两次握手建立连接，客户端向服务端发送一个syn包请求建立连接，因为某些未知的原因，并没有到达服务器，在中间某个网络节点产生了滞留，为了建立连接，客户端会重发syn包，这次的数据包正常送达，服务端发送syn+ack之后就建立起了连接，但是第一包数据阻塞的网络突然恢复，第一包syn包又送达到服务端，这是服务端会认为客户端又发起了一个新的连接，从而在两次握手之后进入等待数据状态，服务端认为是两个连接，而客户端认为是一个连接，造成了状态不一致，如果在三次握手的情况下，服务端收不到最后的ack包，自然不会认为连接建立成功，所以三次握手本质上来说就是为了解决网络信道不可靠的问题，为了在不可靠的信道上建立起可靠的连接，经过三次握手之后，客户端和服务端都进入了数据传输状态。
数据传输 数据传输： 一包数据可能会被拆成多包发送,如何处理丢包问题，这些数据包到达的先后顺序不同，如何处理乱序问题？针对这些问题，tcp协议为每一个连接建立了发送缓冲区，从建立链接后的第一个字节的序列号为0，后面每个字节的序列号就会增加1，发送数据时，从数据缓冲区取一部分数据组成发送报文，在tcp协议头中会附带序列号和长度，接收端在收到数据后需要回复确认报文，确认报文中的ack等于接受序列号加长度，也就是下包数据发送的起始序列号，这样一问一答的发送方式，能够使发送端确认发送的数据已经被对方收到，发送端也可以发送一次的连续的多包数据，接受端只需要回复一次ack就可以了如图：
四次挥手 处于连接状态的客户端和服务端，都可以发起关闭连接请求，此时需要四次挥手来进行连接关闭，假设客户端主动发起连接关闭请求，他给服务端发起一包FIN包，标识要关闭连接，自己进入终止等待1装填，服务端收到FIN包，发送一包ACK包，标识自己进入了关闭等待状态，客户端进入终止等待2状态。这是第二次挥手，服务端此时还可以发送未发送的数据，而客户端还可以接受数据，待服务端发送完数据之后，发送一包FIN包，最后进入确认状态，这是第3次挥手，客户端收到之后恢复ACK包，进入超时等待状态，经过超时时间后关闭连接，而服务端收到ACK包后，立即关闭连接，这是第四次挥手。为什么客户端要等待超时时间这是为了保证对方已经收到ACK包，因为假设客户端发送完最后一包ACK包后释放了连接，一旦ACK包在网络中丢失，服务端将一直停留在 最后确认状态，如果等待一段时间，这时服务端会因为没有收到ack包重发FIN包，客户端会响应 这个FIN包进行重发ack包，并刷新超时时间，这个机制跟第三次握手一样。也是为了保证在不可靠的网络链路中进行可靠的连接断开确认。
UDP协议 udp:首先udp协议是非连接的，发送数据就是把简单的数据包封装一下，然后从网卡发出去就可以了，数据包之间并没有状态上的联系，正因为udp这种简单的处理方式，导致他的性能损耗非常少，对于cpu,内存资源的占用也远小于tcp,但是对于网络传输过程中产生的丢包，udp并不能保证，所以udp在传输稳定性上要弱于tcp，所以，tcp和udp的主要却别：tcp传输数据稳定可靠，适用于对网络通讯质量要求较高的场景，需要准确无误的传输给对方，比如，传输文件，发送邮件，浏览网页等等，udp的优点是速度快，但是可能产生丢包，所以适用于对实时性要求较高但是对少量丢包并没有太大要求的场景。比如：域名查询，语音通话，视频直播等。udp还有一个非常重要的应用场景就是隧道网络，比如：vpn，VXLAN。
]]></content>
  </entry>
  
  <entry>
    <title>分享一种通信协议的应用编程原理和思路</title>
    <url>/post/linux/communication-protocol-programming-principle-and-idea.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>programming</tag>
      <tag>uart</tag>
      <tag>can</tag>
      <tag>usb</tag>
    </tags>
    <content type="html"><![CDATA[ 嵌入式开发过程中，UART、 CAN、 USB等通信基本离不开通信协议。
 下面给大家分享一种通信协议（MAVLink）在应用编程中的编程原理和思路。
应用编程主要内容 发送和接收说明 利用MAVLink通信协议进行编程，主要实现的功能就是：
发送端 将需要发送的数据（如：SysState, BatVol），添加MAVLink通信协议，通过硬件（如：UART、CAN）发送出去。
接收端 硬件（如：UART、CAN）接收到的数据，通过MAVLink协议解析，得到一帧完整的MAVLink数据包，提取发送端发送的数据（如：SysState, BatVol），将得到的数据应用到我们程序中。
主要流程：数据 -&gt; MAVLink封装 -&gt; 发送 -&gt; 接收 -&gt; MAVLink解析 -&gt;数据
发送和接收流程图 该流程图是结合我上一篇文章提供的源代码例程画出来，包含的只是主要内容，更多细节没有在流程图中呈现。
提示：
我提供例程是针对初学者提供比较单一发送和接收例程（MDK-ARM和EWARM包含各自的发送和接收工程）。
而实际项目可能会：
  发送和接收在一个工程；
  包含操作系统；
  发送、接收数据FIFO（队列）处理；
  所以，实际项目，请按需修改我提供的源码。
MAVLink函数接口详细说明 这一章节讲述发送和接收主要用到的函数接口，请参考我提供的源代码例程理解。 为方便初学者理解，我将其分为发送和接收两个部分来讲述。
发送主要函数接口 上面是我提供例程的代码，主要讲4个接口。
MAVLink_SendTest 这个接口是根据自己情况进行封装函数，用于应用程序调用，这里不多说。
mavlink_msg_sys_info_pack 这个函数接口主要目的：将变量信息（SysID、CompID、SysState、BatVol）打包，最终得到MAVLink_Msg这个消息包。
mavlink_msg_to_send_buffer 将上一步得到的MAVLink_Msg转换成我们要发送的数据BUF缓存。
MAV_USART_SendNByte 这个函数接口也是我自己根据硬件（UART）封装的，如果你是其它硬件通信，只需要封装一个类似的接口（参数具有BUF，LEN）即可。
发送数据的流程：从应用代码 -&gt; 底层硬件（发送出去）。
如果要深入了解，可以先熟悉软件流程，再结合源代码工程，同时参看接口函数具体实现。相信你很快就明白了。
接收主要函数接口 上面是我提供例程的代码（方便截图，去掉了部分），主要讲以上4点内容。
MAV_USART_GetByte 该函数接口也是硬件底层通信接口，请根据自己情况修改，只需要传递数据（流）进来即可。
mavlink_parse_char MAVLink解析是按照一个一个字符进行解析，我们接收到一个字符，就对其进行解析，直到解析完（根据返回标志判断）一帧数据为止。
if(MAVLINK_MSG_ID_SYS_INFO == MAVLinkMsg.msgid) 这里就是对解析好的一包完整消息进行分类判断吧。其实，我是想说，这个地方还有两个ID需要进行判断，SysID系统ID和CompID部件ID。
我提供例程为方便初学者快速理解，未提供SysID和CompID判断，在后续应用编程中会用到。
mavlink_msg_sys_info_get_voltage_battery 通过该接口获取消息变量，看图中说明文字，前面是消息，后面是消息变量。
接收数据的流程：从底层硬件（接收数据） -&gt; 应用代码。
以上就是发送和接收的主要函数接口，如果你只是简单的进行通信，这几个接口就够你使用了。当然，更高级的编程应用还需要你进一步掌握其中的内容。
]]></content>
  </entry>
  
  <entry>
    <title>Linux之awk使用技巧</title>
    <url>/post/linux/awk-usage-skills.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>linux</tag>
      <tag>awk</tag>
    </tags>
    <content type="html"><![CDATA[AWK 是一种处理文本文件的语言，是一个强大的文本分析工具。
之所以叫 AWK 是因为其取了三位创始人 Alfred Aho，Peter Weinberger, 和 Brian Kernighan 的 Family Name 的首字符。
打印文件的第一列
awk &#39;{print $1}&#39; vxworks.txt 打印文件的前两列
awk &#39;{print $1,$2}&#39; vxworks.txt 打印文件的最后一列
awk &#39;{print $NF}&#39; vxworks.txt 打印文件的总行数
awk &#39;END{print NR}&#39; vxworks.txt 打印文件的第一行
awk &#39;NR==1{print}&#39; vxworks.txt NR是指awk正在处理的记录位于文件中的位置（行号）
打印文件的第3行第2列
sed -n &#39;3,1p&#39; vxworks.txt | awk &#39;{print $2}&#39; 删除空行
awk &#39;NF&#39; vxworks.txt 打印奇数行
awk &#39;b=!b&#39; vxworks.txt 打印文件按#分割后,行长度为3的所有行
awk -F &#39;#&#39; &#39;if(NF==3){print}&#39; vxworks.txt NF是指awk正在处理的记录包含几个域（字段），这与域分隔符有关，默认为空
统计Linux系统中每个用户所用的shell
cat /etc/passwd | awk -F &#34;:&#34; &#39;{print $1&#34; : &#34;$7}&#39; 用awk统计linux系统中所有的用户数
cat /etc/passwd | awk &#39;{count++}END{ print count}&#39; 统计某个文件夹下文件所占的字节数
ls -l | awk &#39;BEGIN{size=0}{size=size+$5}END{print size}&#39; 统计某个文件夹下文件所占的字节数,按M显示
ls -l | awk &#39;BEGIN{size=0}{size=size+$5}END{print size}&#39; netstat结合awk统计TCP连接数
netstat -tunlp | awk &#39;/^tcp/{++a[$6]}END{for(i in a) print i,a[i]}&#39; 过滤空行
awk &#39;/^[^$]/ {print $0}&#39; vxworks.txt 列运算
awk &#39;/^[^$]/ {print $0}&#39; vxworks.txt cat 1.txt 1 2 3 求和
cat 1.txt | awk &#39;{a+=$1}END{print a}&#39; 求平均值
cat 1.txt | awk &#39;{a+=$1}END{print a/NR}&#39; 求列的最大值
cat 1.txt | awk &#39;BEGIN{a=0}{if($1&gt;a) a=$1 fi}END{print a}&#39; ]]></content>
  </entry>
  
  <entry>
    <title>关于Linux下的crontab，你不知道的那些知识点</title>
    <url>/post/linux/linux-crontab-knowledge.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>linux</tag>
      <tag>crontab</tag>
    </tags>
    <content type="html"><![CDATA[实际工作中，crontab出现的问题是多种多样的，下面就深入介绍下crontab在具体工作中容易出现的问题和解决问题的办法。
crontab能干啥 crond是linux下用来周期性的执行某种任务或等待处理某些事件的一个守护进程，与windows下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动crond进程，crond进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。
Linux下的任务调度分为两类，系统任务调度和用户任务调度。
 系统任务调度：系统周期性所要执行的工作，比如写缓存数据到硬盘、日志清理等。 用户任务调度：用户定期要执行的工作，比如用户数据备份、定时邮件提醒等。用户可以使用 crontab 工具来定制自己的计划任务。所有用户定义的crontab 文件都被保存在 /var/spool/cron目录中。其文件名与用户名一致。  关于crontab的用途，在企业实际应用中非常广泛，常见的有定时数据备份、定时系统检测、定时数据收集、定时更新配置、定时生成报表等等。
crontab应用实例 crontab使用格式 crontab常用的使用格式有如下两种：
crontab [-u user] [file] crontab [-u user] [-e|-l|-r |-i] 选项含义如下：
  -u user：用来设定某个用户的crontab服务，例如，“-u ixdba”表示设定ixdba用户的crontab服务，此参数一般有root用户来运行。
  file：file是命令文件的名字,表示将file做为crontab的任务列表文件并载入crontab。如果在命令行中没有指定这个文件，crontab命令将接受标准输入（键盘）上键入的命令，并将它们载入crontab。
  -e：编辑某个用户的crontab文件内容。如果不指定用户，则表示编辑当前用户的crontab文件。
  -l：显示某个用户的crontab文件内容，如果不指定用户，则表示显示当前用户的crontab文件内容。
  -r：从/var/spool/cron目录中删除某个用户的crontab文件，如果不指定用户，则默认删除当前用户的crontab文件。
  -i：在删除用户的crontab文件时给确认提示。
  crontab文件语法 用户所建立的crontab文件中，每一行都代表一项任务，每行的每个字段代表一项设置，它的格式共分为六个字段，前五段是时间设定段，第六段是要执行的命令段，格式如下：
minute hour day month week command 其中：
? minute：表示分钟，可以是从0到59之间的任何整数。 ? hour：表示小时，可以是从0到23之间的任何整数。 ? day：表示日期，可以是从1到31之间的任何整数。 ? month：表示月份，可以是从1到12之间的任何整数。 ? week：表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日。 ? command：要执行的命令，可以是系统命令，也可以是自己编写的脚本文件。 在以上各个字段中，还可以使用以下特殊字符：
? 星号（）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。 ? 逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9” ? 中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6” ? 正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如/10，如果用在minute字段，表示每十分钟执行一次。 几个crontab例子 0 /3 /usr/local/apache2/apachectl restart 表示每隔3个小时重启apache服务一次。
30 3 6 /webdata/bin/backup.sh 表示每周六的3点30分执行/webdata/bin/backup.sh脚本的操作。
0 0 1,20 fsck /dev/sdb8 表示每个月的1号和20号检查/dev/sdb8磁盘设备。
10 5 /5 * echo &#34;&#34;&gt;/usr/local/apache2/log/access_log 表示每个月的5号、10号、15号、20号、25号、30号的5点10分执行清理apache日志操作。
系统级任务调度/etc/crontab 在/etc目录下有一个crontab文件，这个就是系统任务调度的配置文件。
/etc/crontab文件包括下面几行：
SHELL=/bin/bash PATH=/sbin:/bin:/usr/sbin:/usr/bin MAILTO=root HOME=/ # run-parts 01 * * * * root run-parts /etc/cron.hourly 02 4 * * * root run-parts /etc/cron.daily 22 4 * * 0 root run-parts /etc/cron.weekly 42 4 1 * * root run-parts /etc/cron.monthly 从上面的示例文件可看出，crontab的任务列表主要由两部分组成：环境变量配置与定时任务配置。可能大家在工作中更多是只用到了任务配置部分。
前四行是用来配置crond任务运行的环境变量，第一行SHELL变量指定了系统要使用哪个shell，这里是bash，第二行PATH变量指定了系统执行命令的路径，第三行MAILTO变量指定了crond的任务执行信息将通过电子邮件发送给root用户，如果MAILTO变量的值为空，则表示不发送任务执行信息给用户，第四行的HOME变量指定了在执行命令或者脚本时使用的主目录。第六至九行就是crontab执行格式的具体写法。
##crontab调试解析神器
通常在使用crontab添加任务时，我们会依靠自己已有知识编写定时语句。当需要测试语句是否正确时，还需要在服务器上不断调试，，这种方式太不高效了。有没有一款工具，只要我们给出语句，就能告诉具体执行时间以及对错呢？还真有，下面介绍一款老外开发的crontab在线解析工具。
工具地址：https://crontab.guru
给出这个工具的截图如下：
好用不好用，你试试就知道。
crontab使用的各种坑 环境变量问题 当我们刚使用crontab时，运维老鸟们一般会告知所有命令尽量都使用绝对路径，以防错误。这是为什么？这就和我们下面要谈的环境变量有关了。
首先，获取shell终端环境变量，内容如下：
[root@SparkWorker1 dylogs]# env XDG_SESSION_ID=1629 HOSTNAME=SparkWorker1 TERM=linux SHELL=/bin/bash HISTSIZE=1000 SSH_CLIENT=172.16.213.132 50080 22 HADOOP_PREFIX=/opt/hadoop/current CATALINA_BASE=/opt/hadoop/current/share/hadoop/httpfs/tomcat SSH_TTY=/dev/pts/1 QT_GRAPHICSSYSTEM_CHECKED=1 USER=root MAIL=/var/spool/mail/root PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/java/default/bin:/opt/hadoop/current/bin:/opt/hadoop/current/sbin:/root/bin PWD=/data/dylogs LANG=zh_CN.UTF-8 HOME=/root 要获取crontab环境变量信息，可以设置如下计划任务：
* * * * * /usr/bin/env &gt; /tmp/env.txt 等待片刻，env.txt输出内容如下：
[root@SparkWorker1 dylogs]# cat /tmp/env.txt XDG_SESSION_ID=1729 SHELL=/bin/sh USER=root PATH=/usr/bin:/bin PWD=/root LANG=zh_CN.UTF-8 SHLVL=1 HOME=/root LOGNAME=root XDG_RUNTIME_DIR=/run/user/0 _=/usr/bin/env 从上面输出结果可知，shell命令行的PATH值为
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/java/default/bin:/opt/hadoop/current/bin:/opt/hadoop/current/sbin:/root/bin 而crontab中的PATH值为：
PATH=/usr/bin:/bin 对比crontab环境变量与shell终端环境变量的输出，可以发现两者的差异很大。大家可能遇到过，在shell命令行执行脚本都没有问题，而放到crontab后却执行异常，或者执行失败，此时，我们就需要考虑是否命令涉及的环境变量在crontab和shell命令行间存在差异。
例如，我们在crontab中执行了如下定时任务：
20 16 * * * php autosave.php 而如果我们的php是安装在/usr/local/bin/目录下的话，那么上面这个定时任务由于无法找到php命令，会运行失败。
那么，知道了环境变量问题，可能导致计划任务无法正常执行，怎么才能避免这个问题呢，这个交给大家一个终极大招，可以在crontab中加入如下配置，保证你的计划任务执行不会出现环境变量问题：
* * * * * source /$HOME/.bash_profile &amp;&amp; command 这个其实是在执行计划任务命令之前，先加载了用户环境变量信息，由此可保证所有环境变量都可正常加载。
定时时间配置误区 时间是crontab的核心，稍微配置不当，就会出现问题，先看在整点时间设置时可能出现的错误，例如，设定每天2点执行一次任务，很多朋友可能这么写过：
* 2 * * * command 很明显，这个时间写法是错误的，当我们听到每天2点执行一次某任务时，很多人会把重点放在2点，而忽略了执行一次的需求。上面这个定时任务他会在2点开始执行，每分钟执行一次，总共执行60次。
正确的写法应该是这样的：
0 2 * * * command 这个才表示每天2点0分执行command对应的任务。
特殊符号%问题 %在crontab中是特殊符号，具体含义如下：
第一个%表示标准输入的开始，其余%表示换行符，看下面两个例子：
* * * * * cat &gt;&gt; /tmp/cat.txt 2&gt;&amp;1 % stdin out 查看/tmp/cat.txt的内容为：
stdin out 再看下面这个例子：
* * * * * cat &gt;&gt; /tmp/cat1.txt 2&gt;&amp;1 % stdin out 1 % stdin out 2 % stdin out 3 查看 /tmp/cat1.txt的内容如下：
stdin out 1 stdin out 2 stdin out 3 有输出内容可知，第一个%表示标准输入的开始，其余%表示换行符。
既然&quot;%&ldquo;是特殊字符,那么在crontab中使用时，就要特别注意，怎么使用这些特殊字符呢，很明显，使用转移字符即可，例如：
* * * * * cat &gt;&gt; /tmp/cat2.txt 2&gt;&amp;1 % Special character escape \%. 查看输出/tmp/cat2.txt 输出内容如下：
Special character escape %. 可以看到，执行成功了，并成功避开这个坑了。
关于crontab的输出重定向 在crontab执行的计划任务中，有些任务如果不做输出重定向，那么原本会输出到屏幕的信息，会以邮件的形式输出到某个文件中，例如，执行下面这个计划任务：
* * * * * /bin/date 这个计划任务是没有做输出重定向的，他的主要用途是输出时间，由于没有配置输出重定向，那么这个时间信息默认将以邮件的形式输出到/var/spool/mail/$USER（这个$USER对应的是系统用户，这里是root用户）文件中，大致内容如下：
From root@SparkWorker1.localdomain Fri Sep 21 12:58:02 2022 Return-Path: &lt;root@SparkWorker1.localdomain&gt; X-Original-To: root Delivered-To: root@SparkWorker1.localdomain Received: by SparkWorker1.localdomain (Postfix, from userid 0) id F2745192AE; Fri, 21 Sep 2022 12:58:01 +0800 (CST) From: &#34;(Cron Daemon)&#34; &lt;root@SparkWorker1.localdomain&gt; To: root@SparkWorker1.localdomain Subject: Cron &lt;root@SparkWorker1&gt; /bin/date Content-Type: text/plain; charset=UTF-8 Auto-Submitted: auto-generated Precedence: bulk X-Cron-Env: &lt;XDG_SESSION_ID=1820&gt; X-Cron-Env: &lt;XDG_RUNTIME_DIR=/run/user/0&gt; X-Cron-Env: &lt;LANG=zh_CN.UTF-8&gt; X-Cron-Env: &lt;SHELL=/bin/sh&gt; X-Cron-Env: &lt;HOME=/root&gt; X-Cron-Env: &lt;PATH=/usr/bin:/bin&gt; X-Cron-Env: &lt;LOGNAME=root&gt; X-Cron-Env: &lt;USER=root&gt; Message-Id: &lt;20220921045801.F2745192AE@SparkWorker1.localdomain&gt; Date: Fri, 21 Sep 2022 12:58:01 +0800 (CST) 2022年 09月 21日 星期五 12:58:01 CST 由此可见，输出内容还是很多的，如遇到任务有大量输出的话，会占用大量磁盘空间，显然，这个邮件输出最好关闭，怎么关闭呢，只需设置MAILTO环境变量为空即可，上面的计划任务，可做如下修改：
MAILTO=&#34;&#34; * * * * * /bin/date 这样，就不会发邮件信息到/var/spool/mail/$USER下了，但是问题并没有彻底解决，关闭mail功能后，输出内容将继续写入到/var/spool/clientmqueue中，长期下去，可能占满分区的inode资源，导致任务无法执行。
为了避免此类问题发生，建议任务都加上输出重定向，例如，可以在crontab文件中设置如下形式，忽略日志输出：
0 */3 * * * /usr/local/apache2/apachectl restart &gt;/dev/null 2&gt;&amp;1 其中，“/dev/null 2&gt;&amp;1”表示先将标准输出重定向到/dev/null，然后将标准错误重定向到标准输出，由于标准输出已经重定向到了/dev/null，因此标准错误也会重定向到/dev/null，这样日志输出问题就解决了。
调试crontab问题的一般思路 要解决crontab相关异常问题，可按照如下思路进行调试：
（1）、通过/var/log/cron日志确认任务是否执行
（2）、如未执行则分析定时语句，是否是环境变量问题、特殊字符问题、时间配置问题、权限问题等。
（3）、确认crond服务开启，如果定时语句也正确，检查crond服务是否开启。
Systemd方式(centos7及以上)
[root@SparkWorker1 spool]# systemctl status crond.service SysVinit方式(centos7以下)
[root@SparkWorker1 spool]# service crond status （4）确认定时任务中命令是否执行成功
这个问题可通过输出获取错误信息进行调试，方法就是利用重定向获取输出，然后进行分析。举例如下：
* * * * * python /usr/local/dyserver/dypos.py &gt;&gt; /tmp/dypos.log 2&gt;&amp;1 通过加上“/tmp/dypos.log 2&gt;&amp;1”，就可以很快定位问题，因为这个dypos.py脚本在执行的时候会把错误信息都输出到dypos.log 中，接着查看dypos.log文件，问题一目了然：
[root@SparkWorker1 spool]# cat /tmp/dypos.log /bin/sh: python: 未找到命令 /bin/sh: python: 未找到命令 显示python命令没有找到，很明显的就可以确定是环境变量的问题。这种方式定位问题非常有效。
]]></content>
  </entry>
  
  <entry>
    <title>I2C总线接上拉电阻的原因</title>
    <url>/post/hardware/why-add-pull-up-resistor-for-i2c.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>I2C</tag>
    </tags>
    <content type="html"><![CDATA[I2C为什么要接上拉电阻？因为它是开漏输出。
为什么是开漏输出？ I2C协议支持多个主设备与多个从设备在一条总线上，如果不用开漏输出，而用推挽输出，会出现主设备之间短路的情况。所以总线一般会使用开漏输出。
为什么要接上拉电阻？ 接上拉电阻是因为I2C通信需要输出高电平的能力。一般开漏输出无法输出高电平，如果在漏极接上拉电阻，则可以进行电平转换。
I2C由两条总线SDA和SCL组成。连接到总线的器件的输出级必须是漏极开路，都通过上拉电阻连接到电源，这样才能够实现“线与”功能。当总线空闲时，这两条线路都是高电平。
上拉电阻阻值怎么确定？ 一般IO端口的驱动能力在2mA～4mA量级。
考虑到功耗问题，阻值不能过小
如果上拉阻值过小，VDD灌入端口的电流将较大，功耗会很大，导致端口输出的低电平值增大(I2C协议规定，端口输出低电平的最高允许值为0.4V)。故通常上拉电阻应选取不低于1K的电阻（当VDD＝3V时，灌入电流不超过3mA）。
考虑到速度问题，阻值不能过大
它取决于上拉电阻和线上电容形成的RC延时，RC延时越大，波形越偏离方波趋向于正弦波，数据读写正确的概率就越低，所以上拉电阻不能过大。
I2C总线上的负载电容不能超过400pF。当I2C总线上器件逐渐增多时，总线负载电容也相应增加。当总的负载电容大于400pF时，就不能可靠的工作。这也是I2C的局限性。
建议上拉电阻可选用1.5K，2.2K，4.7K。
I2C总线基本操作 根据I2C总线规范，总线空闲时两根线都必须为高。假设主设备A需要启动I2C，他需要在SCL高电平时，将SDA由高电平转换为低电平作为启动信号。
主设备A在把SDA拉高后，它需要再检查一下SDA的电平。为什么? 因为线与，如果主设备A拉高SDA时，已经有其他主设备将SDA拉低了，由于 1 &amp; 0 = 0 那么主设备A在检查SDA电平时, 会发现不是高电平，而是低电平。说明其他主设备抢占总线的时间比它早，主设备A只能放弃占用总线。如果SDA是高电平，说明主设备A可以占用总线，然后主设备A将SDA拉低，开始通信。
因此，模拟I2C一定要将GPIO端口设置为开漏输出并加上拉电阻。
]]></content>
  </entry>
  
  <entry>
    <title>Linux编程之经典多级时间轮定时器</title>
    <url>/post/linux/linux-programming-multiple-time-wheel-timer.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>linux</tag>
      <tag>timer</tag>
    </tags>
    <content type="html"><![CDATA[mmap用于把文件映射到内存空间中，简单说mmap就是把一个文件的内容在内存里面做一个映像。
多级时间轮实现框架 上图是5个时间轮级联的效果图。中间的大轮是工作轮，只有在它上的任务才会被执行；其他轮上的任务时间到后迁移到下一级轮上，他们最终都会迁移到工作轮上而被调度执行。
多级时间轮的原理也容易理解：就拿时钟做说明，秒针转动一圈分针转动一格；分针转动一圈时针转动一格；同理时间轮也是如此：当低级轮转动一圈时，高一级轮转动一格，同时会将高一级轮上的任务重新分配到低级轮上。从而实现了多级轮级联的效果。
多级时间轮对象 多级时间轮应该至少包括以下内容：
 每一级时间轮对象 轮子上指针的位置 关于轮子上指针的位置有一个比较巧妙的办法：那就是位运算。比如定义一个无符号整型的数：  通过获取当前的系统时间便可以通过位操作转换为时间轮上的时间，通过与实际时间轮上的时间作比较，从而确定时间轮要前进调度的时间，进而操作对应时间轮槽位对应的任务。
为什么至少需要这两个成员呢？
 定义多级时间轮，首先需要明确的便是级联的层数，也就是说需要确定有几个时间轮。 轮子上指针位置，就是当前时间轮运行到的位置，它与真实时间的差便是后续时间轮需要调度执行，它们的差值是时间轮运作起来的驱动力。  多级时间轮对象的定义
//实现5级时间轮 范围为0~ (2^8 * 2^6 * 2^6 * 2^6 *2^6)=2^32 struct tvec_base { unsigned long current_index; pthread_t thincrejiffies; pthread_t threadID; struct tvec_root tv1; /*第一个轮*/ struct tvec tv2; /*第二个轮*/ struct tvec tv3; /*第三个轮*/ struct tvec tv4; /*第四个轮*/ struct tvec tv5; /*第五个轮*/ }; 时间轮对象 我们知道每一个轮子实际上都是一个哈希表，上面我们只是实例化了五个轮子的对象，但是五个轮子具体包含什么，有几个槽位等等没有明确(即struct tvec和struct tvec_root)。
#define TVN_BITS 6 #define TVR_BITS 8 #define TVN_SIZE (1&lt;&lt;TVN_BITS) #define TVR_SIZE (1&lt;&lt;TVR_BITS) struct tvec { struct list_head vec[TVN_SIZE];/*64个格子*/ }; struct tvec_root{ struct list_head vec[TVR_SIZE];/*256个格子*/ }; 此外，每一个时间轮都是哈希表，因此它的类型应该至少包含两个指针域来实现双向链表的功能。这里我们为了方便使用通用的struct list_head的双向链表结构。
定时任务对象 定时器的主要工作是为了在未来的特定时间完成某项任务，而这个任务经常包含以下内容：
 任务的处理逻辑(回调函数) 任务的参数 双向链表节点 到时时间  定时任务对象的定义
typedef void (*timeouthandle)(unsigned long ); struct timer_list{ struct list_head entry; //将时间连接成链表  unsigned long expires; //超时时间  void (*function)(unsigned long); //超时后的处理函数  unsigned long data; //处理函数的参数  struct tvec_base *base; //指向时间轮 }; 在时间轮上的效果图：
双向链表 在时间轮上我们采用双向链表的数据类型。采用双向链表的除了操作上比单链表复杂，多占一个指针域外没有其他不可接收的问题。而多占一个指针域在今天大内存的时代明显不是什么问题。至于双向链表操作的复杂性，我们可以通过使用通用的struct list结构来解决，因为双向链表有众多的标准操作函数，我们可以通过直接引用list.h头文件来使用他们提供的接口。
struct list可以说是一个万能的双向链表操作框架，我们只需要在自定义的结构中定义一个struct list对象即可使用它的标准操作接口。同时它还提供了一个类似container_of的接口，在应用层一般叫做list_entry，因此我们可以很方便的通过struct list成员找到自定义的结构体的起始地址。
关于应用层的log.h, 我将在下面的代码中附上该文件。如果需要内核层的实现，可以直接从linux源码中获取。
联结方式 多级时间轮效果图：
多级时间轮C语言实现 双向链表头文件: list.h 提到双向链表，很多的源码工程中都会实现一系列的统一的双向链表操作函数。它们为双向链表封装了统计的接口，使用者只需要在自定义的结构中添加一个struct list_head结构，然后调用它们提供的接口，便可以完成双向链表的所有操作。这些操作一般都在list.h的头文件中实现。Linux源码中也有实现（内核态的实现）。他们实现的方式基本完全一样，只是实现的接口数量和功能上稍有差别。可以说这个list.h文件是学习操作双向链表的不二选择，它几乎实现了所有的操作：增、删、改、查、遍历、替换、清空等等。这里我拼凑了一个源码中的log.h函数，终于凑够了多级时间轮中使用到的接口。
#if !defined(_BLKID_LIST_H) &amp;&amp; !defined(LIST_HEAD) #define _BLKID_LIST_H #ifdef __cplusplus extern &#34;C&#34; { #endif /* * Simple doubly linked list implementation. * * Some of the internal functions (&#34;__xxx&#34;) are useful when * manipulating whole lists rather than single entries, as * sometimes we already know the next/prev entries and we can * generate better code by using them directly rather than * using the generic single-entry routines. */ struct list_head { struct list_head *next, *prev; }; #define LIST_HEAD_INIT(name) { &amp;(name), &amp;(name) } #define LIST_HEAD(name) \ struct list_head name = LIST_HEAD_INIT(name) #define INIT_LIST_HEAD(ptr) do { \ (ptr)-&gt;next = (ptr); (ptr)-&gt;prev = (ptr); \ } while (0) static inline void __list_add(struct list_head *entry, struct list_head *prev, struct list_head *next) { next-&gt;prev = entry; entry-&gt;next = next; entry-&gt;prev = prev; prev-&gt;next = entry; } /** * Insert a new element after the given list head. The new element does not * need to be initialised as empty list. * The list changes from: * head → some element → ... * to * head → new element → older element → ... * * Example: * struct foo *newfoo = malloc(...); * list_add(&amp;newfoo-&gt;entry, &amp;bar-&gt;list_of_foos); * * @param entry The new element to prepend to the list. * @param head The existing list. */ static inline void list_add(struct list_head *entry, struct list_head *head) { __list_add(entry, head, head-&gt;next); } /** * Append a new element to the end of the list given with this list head. * * The list changes from: * head → some element → ... → lastelement * to * head → some element → ... → lastelement → new element * * Example: * struct foo *newfoo = malloc(...); * list_add_tail(&amp;newfoo-&gt;entry, &amp;bar-&gt;list_of_foos); * * @param entry The new element to prepend to the list. * @param head The existing list. */ static inline void list_add_tail(struct list_head *entry, struct list_head *head) { __list_add(entry, head-&gt;prev, head); } static inline void __list_del(struct list_head *prev, struct list_head *next) { next-&gt;prev = prev; prev-&gt;next = next; } /** * Remove the element from the list it is in. Using this function will reset * the pointers to/from this element so it is removed from the list. It does * NOT free the element itself or manipulate it otherwise. * * Using list_del on a pure list head (like in the example at the top of * this file) will NOT remove the first element from * the list but rather reset the list as empty list. * * Example: * list_del(&amp;foo-&gt;entry); * * @param entry The element to remove. */ static inline void list_del(struct list_head *entry) { __list_del(entry-&gt;prev, entry-&gt;next); } static inline void list_del_init(struct list_head *entry) { __list_del(entry-&gt;prev, entry-&gt;next); INIT_LIST_HEAD(entry); } static inline void list_move_tail(struct list_head *list, struct list_head *head) { __list_del(list-&gt;prev, list-&gt;next); list_add_tail(list, head); } /** * Check if the list is empty. * * Example: * list_empty(&amp;bar-&gt;list_of_foos); * * @return True if the list contains one or more elements or False otherwise. */ static inline int list_empty(struct list_head *head) { return head-&gt;next == head; } /** * list_replace - replace old entry by new one * @old : the element to be replaced * @new : the new element to insert * * If @old was empty, it will be overwritten. */ static inline void list_replace(struct list_head *old, struct list_head *new) { new-&gt;next = old-&gt;next; new-&gt;next-&gt;prev = new; new-&gt;prev = old-&gt;prev; new-&gt;prev-&gt;next = new; } /** * Retrieve the first list entry for the given list pointer. * * Example: * struct foo *first; * first = list_first_entry(&amp;bar-&gt;list_of_foos, struct foo, list_of_foos); * * @param ptr The list head * @param type Data type of the list element to retrieve * @param member Member name of the struct list_head field in the list element. * @return A pointer to the first list element. */ #define list_first_entry(ptr, type, member) \ list_entry((ptr)-&gt;next, type, member) static inline void list_replace_init(struct list_head *old, struct list_head *new) { list_replace(old, new); INIT_LIST_HEAD(old); } /** * list_entry - get the struct for this entry * @ptr: the &amp;struct list_head pointer. * @type: the type of the struct this is embedded in. * @member: the name of the list_struct within the struct. */ #define list_entry(ptr, type, member) \ ((type *)((char *)(ptr)-(unsigned long)(&amp;((type *)0)-&gt;member))) /** * list_for_each - iterate over elements in a list * @pos: the &amp;struct list_head to use as a loop counter. * @head: the head for your list. */ #define list_for_each(pos, head) \ for (pos = (head)-&gt;next; pos != (head); pos = pos-&gt;next) /** * list_for_each_safe - iterate over elements in a list, but don&#39;t dereference * pos after the body is done (in case it is freed) * @pos: the &amp;struct list_head to use as a loop counter. * @pnext: the &amp;struct list_head to use as a pointer to the next item. * @head: the head for your list (not included in iteration). */ #define list_for_each_safe(pos, pnext, head) \ for (pos = (head)-&gt;next, pnext = pos-&gt;next; pos != (head); \ pos = pnext, pnext = pos-&gt;next) #ifdef __cplusplus } #endif #endif /* _BLKID_LIST_H */这里面一般会用到一个重要实现：container_of, 它的原理这里不叙述
调试信息头文件: log.h 这个头文件实际上不是必须的，我只是用它来添加调试信息(代码中的errlog(), log()都是log.h中的宏函数)。它的效果是给打印的信息加上颜色，效果如下：
log.h的代码如下：
#ifndef _LOG_h_ #define _LOG_h_ #include &lt;stdio.h&gt;#define COL(x) &#34;\033[;&#34; #x &#34;m&#34; #define RED COL(31) #define GREEN COL(32) #define YELLOW COL(33) #define BLUE COL(34) #define MAGENTA COL(35) #define CYAN COL(36) #define WHITE COL(0) #define GRAY &#34;\033[0m&#34; #define errlog(fmt, arg...) do{ \ printf(RED&#34;[#ERROR: Toeny Sun:&#34;GRAY YELLOW&#34; %s:%d]:&#34;GRAY WHITE fmt GRAY, __func__, __LINE__, ##arg);\ }while(0) #define log(fmt, arg...) do{ \ printf(WHITE&#34;[#DEBUG: Toeny Sun: &#34;GRAY YELLOW&#34;%s:%d]:&#34;GRAY WHITE fmt GRAY, __func__, __LINE__, ##arg);\ }while(0) #endif 时间轮代码: timewheel.c /* *毫秒定时器 采用多级时间轮方式 借鉴linux内核中的实现 *支持的范围为1 ~ 2^32 毫秒(大约有49天) *若设置的定时器超过最大值 则按最大值设置定时器 **/ #include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;#include &lt;pthread.h&gt;#include &lt;sys/time.h&gt;#include &#34;list.h&#34;#include &#34;log.h&#34; #define TVN_BITS 6 #define TVR_BITS 8 #define TVN_SIZE (1&lt;&lt;TVN_BITS) #define TVR_SIZE (1&lt;&lt;TVR_BITS)  #define TVN_MASK (TVN_SIZE - 1) #define TVR_MASK (TVR_SIZE - 1)  #define SEC_VALUE 0 #define USEC_VALUE 2000  struct tvec_base; #define INDEX(N) ((ba-&gt;current_index &gt;&gt; (TVR_BITS + (N) * TVN_BITS)) &amp; TVN_MASK)  typedef void (*timeouthandle)(unsigned long ); struct timer_list{ struct list_head entry; //将时间连接成链表  unsigned long expires; //超时时间  void (*function)(unsigned long); //超时后的处理函数  unsigned long data; //处理函数的参数  struct tvec_base *base; //指向时间轮 }; struct tvec { struct list_head vec[TVN_SIZE]; }; struct tvec_root{ struct list_head vec[TVR_SIZE]; }; //实现5级时间轮 范围为0~ (2^8 * 2^6 * 2^6 * 2^6 *2^6)=2^32 struct tvec_base { unsigned long current_index; pthread_t thincrejiffies; pthread_t threadID; struct tvec_root tv1; /*第一个轮*/ struct tvec tv2; /*第二个轮*/ struct tvec tv3; /*第三个轮*/ struct tvec tv4; /*第四个轮*/ struct tvec tv5; /*第五个轮*/ }; static void internal_add_timer(struct tvec_base *base, struct timer_list *timer) { struct list_head *vec; unsigned long expires = timer-&gt;expires; unsigned long idx = expires - base-&gt;current_index; #if 1  if( (signed long)idx &lt; 0 ) /*这里是没有办法区分出是过时还是超长定时的吧?*/ { vec = base-&gt;tv1.vec + (base-&gt;current_index &amp; TVR_MASK);/*放到第一个轮的当前槽*/ } else if ( idx &lt; TVR_SIZE ) /*第一个轮*/ { int i = expires &amp; TVR_MASK; vec = base-&gt;tv1.vec + i; } else if( idx &lt; 1 &lt;&lt; (TVR_BITS + TVN_BITS) )/*第二个轮*/ { int i = (expires &gt;&gt; TVR_BITS) &amp; TVN_MASK; vec = base-&gt;tv2.vec + i; } else if( idx &lt; 1 &lt;&lt; (TVR_BITS + 2 * TVN_BITS) )/*第三个轮*/ { int i = (expires &gt;&gt; (TVR_BITS + TVN_BITS)) &amp; TVN_MASK; vec = base-&gt;tv3.vec + i; } else if( idx &lt; 1 &lt;&lt; (TVR_BITS + 3 * TVN_BITS) )/*第四个轮*/ { int i = (expires &gt;&gt; (TVR_BITS + 2 * TVN_BITS)) &amp; TVN_MASK; vec = base-&gt;tv4.vec + i; } else /*第五个轮*/ { int i; if (idx &gt; 0xffffffffUL) { idx = 0xffffffffUL; expires = idx + base-&gt;current_index; } i = (expires &gt;&gt; (TVR_BITS + 3 * TVN_BITS)) &amp; TVN_MASK; vec = base-&gt;tv5.vec + i; } #else  /*上面可以优化吧*/; #endif  list_add_tail(&amp;timer-&gt;entry, vec); } static inline void detach_timer(struct timer_list *timer) { struct list_head *entry = &amp;timer-&gt;entry; __list_del(entry-&gt;prev, entry-&gt;next); entry-&gt;next = NULL; entry-&gt;prev = NULL; } static int __mod_timer(struct timer_list *timer, unsigned long expires) { if(NULL != timer-&gt;entry.next) detach_timer(timer); internal_add_timer(timer-&gt;base, timer); return 0; } //修改定时器的超时时间外部接口 int mod_timer(void *ptimer, unsigned long expires) { struct timer_list *timer = (struct timer_list *)ptimer; struct tvec_base *base; base = timer-&gt;base; if(NULL == base) return -1; expires = expires + base-&gt;current_index; if(timer-&gt;entry.next != NULL &amp;&amp; timer-&gt;expires == expires) return 0; if( NULL == timer-&gt;function ) { errlog(&#34;timer&#39;s timeout function is null\n&#34;); return -1; } timer-&gt;expires = expires; return __mod_timer(timer,expires); } //添加一个定时器 static void __ti_add_timer(struct timer_list *timer) { if( NULL != timer-&gt;entry.next ) { errlog(&#34;timer is already exist\n&#34;); return; } mod_timer(timer, timer-&gt;expires); } /*添加一个定时器 外部接口 *返回定时器 */ void* ti_add_timer(void *ptimewheel, unsigned long expires,timeouthandle phandle, unsigned long arg) { struct timer_list *ptimer; ptimer = (struct timer_list *)malloc( sizeof(struct timer_list) ); if(NULL == ptimer) return NULL; bzero( ptimer,sizeof(struct timer_list) ); ptimer-&gt;entry.next = NULL; ptimer-&gt;base = (struct tvec_base *)ptimewheel; ptimer-&gt;expires = expires; ptimer-&gt;function = phandle; ptimer-&gt;data = arg; __ti_add_timer(ptimer); return ptimer; } /* *删除一个定时器 外部接口 * * */ void ti_del_timer(void *p) { struct timer_list *ptimer =(struct timer_list*)p; if(NULL == ptimer) return; if(NULL != ptimer-&gt;entry.next) detach_timer(ptimer); free(ptimer); } /*时间轮级联*/ static int cascade(struct tvec_base *base, struct tvec *tv, int index) { struct list_head *pos,*tmp; struct timer_list *timer; struct list_head tv_list; /*将tv[index]槽位上的所有任务转移给tv_list,然后清空tv[index]*/ list_replace_init(tv-&gt;vec + index, &amp;tv_list);/*用tv_list替换tv-&gt;vec + index*/ list_for_each_safe(pos, tmp, &amp;tv_list)/*遍历tv_list双向链表，将任务重新添加到时间轮*/ { timer = list_entry(pos,struct timer_list,entry);/*struct timer_list中成员entry的地址是pos, 获取struct timer_list的首地址*/ internal_add_timer(base, timer); } return index; } static void *deal_function_timeout(void *base) { struct timer_list *timer; int ret; struct timeval tv; struct tvec_base *ba = (struct tvec_base *)base; for(;;) { gettimeofday(&amp;tv, NULL); while( ba-&gt;current_index &lt;= (tv.tv_sec*1000 + tv.tv_usec/1000) )/*单位：ms*/ { struct list_head work_list; int index = ba-&gt;current_index &amp; TVR_MASK;/*获取第一个轮上的指针位置*/ struct list_head *head = &amp;work_list; /*指针指向0槽时，级联轮需要更新任务列表*/ if(!index &amp;&amp; (!cascade(ba, &amp;ba-&gt;tv2, INDEX(0))) &amp;&amp;( !cascade(ba, &amp;ba-&gt;tv3, INDEX(1))) &amp;&amp; (!cascade(ba, &amp;ba-&gt;tv4, INDEX(2))) ) cascade(ba, &amp;ba-&gt;tv5, INDEX(3)); ba-&gt;current_index ++; list_replace_init(ba-&gt;tv1.vec + index, &amp;work_list); while(!list_empty(head)) { void (*fn)(unsigned long); unsigned long data; timer = list_first_entry(head, struct timer_list, entry); fn = timer-&gt;function; data = timer-&gt;data; detach_timer(timer); (*fn)(data); } } } } static void init_tvr_list(struct tvec_root * tvr) { int i; for( i = 0; i&lt;TVR_SIZE; i++ ) INIT_LIST_HEAD(&amp;tvr-&gt;vec[i]); } static void init_tvn_list(struct tvec * tvn) { int i; for( i = 0; i&lt;TVN_SIZE; i++ ) INIT_LIST_HEAD(&amp;tvn-&gt;vec[i]); } //创建时间轮 外部接口 void *ti_timewheel_create(void ) { struct tvec_base *base; int ret = 0; struct timeval tv; base = (struct tvec_base *) malloc( sizeof(struct tvec_base) ); if( NULL==base ) return NULL; bzero( base,sizeof(struct tvec_base) ); init_tvr_list(&amp;base-&gt;tv1); init_tvn_list(&amp;base-&gt;tv2); init_tvn_list(&amp;base-&gt;tv3); init_tvn_list(&amp;base-&gt;tv4); init_tvn_list(&amp;base-&gt;tv5); gettimeofday(&amp;tv, NULL); base-&gt;current_index = tv.tv_sec*1000 + tv.tv_usec/1000;/*当前时间毫秒数*/ if( 0 != pthread_create(&amp;base-&gt;threadID,NULL,deal_function_timeout,base) ) { free(base); return NULL; } return base; } static void ti_release_tvr(struct tvec_root *pvr) { int i; struct list_head *pos,*tmp; struct timer_list *pen; for(i = 0; i &lt; TVR_SIZE; i++) { list_for_each_safe(pos,tmp,&amp;pvr-&gt;vec[i]) { pen = list_entry(pos,struct timer_list, entry); list_del(pos); free(pen); } } } static void ti_release_tvn(struct tvec *pvn) { int i; struct list_head *pos,*tmp; struct timer_list *pen; for(i = 0; i &lt; TVN_SIZE; i++) { list_for_each_safe(pos,tmp,&amp;pvn-&gt;vec[i]) { pen = list_entry(pos,struct timer_list, entry); list_del(pos); free(pen); } } } /* *释放时间轮 外部接口 * */ void ti_timewheel_release(void * pwheel) { struct tvec_base *base = (struct tvec_base *)pwheel; if(NULL == base) return; ti_release_tvr(&amp;base-&gt;tv1); ti_release_tvn(&amp;base-&gt;tv2); ti_release_tvn(&amp;base-&gt;tv3); ti_release_tvn(&amp;base-&gt;tv4); ti_release_tvn(&amp;base-&gt;tv5); free(pwheel); } /************demo****************/ struct request_para{ void *timer; int val; }; void mytimer(unsigned long arg) { struct request_para *para = (struct request_para *)arg; log(&#34;%d\n&#34;,para-&gt;val); mod_timer(para-&gt;timer,3000); //进行再次启动定时器  sleep(10);/*定时器依然被阻塞*/ //定时器资源的释放是在这里完成的  //ti_del_timer(para-&gt;timer); } int main(int argc,char *argv[]) { void *pwheel = NULL; void *timer = NULL; struct request_para *para; para = (struct request_para *)malloc( sizeof(struct request_para) ); if(NULL == para) return 0; bzero(para,sizeof(struct request_para)); //创建一个时间轮  pwheel = ti_timewheel_create(); if(NULL == pwheel) return -1; //添加一个定时器  para-&gt;val = 100; para-&gt;timer = ti_add_timer(pwheel, 3000, &amp;mytimer, (unsigned long)para); while(1) { sleep(2); } //释放时间轮  ti_timewheel_release(pwheel); return 0; } 编译运行 peng@ubuntu:/mnt/hgfs/timer/4. timerwheel/2. 多级时间轮$ ls a.out list.h log.h mutiTimeWheel.c toney@ubantu:/mnt/hgfs/timer录/4. timerwheel/2. 多级时间轮$ gcc mutiTimeWheel.c -lpthread toney@ubantu:/mnt/hgfs/timer/4. timerwheel/2. 多级时间轮$ ./a.out [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 从结果可以看出：如果添加的定时任务是比较耗时的操作，那么后续的任务也会被阻塞，可能一直到超时，甚至一直阻塞下去，这个取决于当前任务是否耗时。
这个理论上是绝不能接受的：一个任务不应该也不能去影响其他的任务吧。但是目前没有对此问题进行改进和完善，以后有机会再继续完善吧。
]]></content>
  </entry>
  
  <entry>
    <title>Linux mmap内存映射详解</title>
    <url>/post/linux/linux-mmap-explanation.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>linux</tag>
      <tag>device driver</tag>
    </tags>
    <content type="html"><![CDATA[mmap用于把文件映射到内存空间中，简单说mmap就是把一个文件的内容在内存里面做一个映像。
mmap基础概念 mmap是一种内存映射的方法，这一功能可以用在文件的处理上，即将一个文件或者其它对象映射到进程的地址空间，实现文件磁盘地址和进程虚拟地址空间中一段虚拟地址的一一对映关系。在编程时可以使某个磁盘文件的内容看起来像是内存中的一个数组。如果文件由记录组成，而这些记录又能够用结构体来描述的话，可以通过访问结构体来更新文件的内容。
实现这样的映射关系后，进程就可以采用指针的方式读写操作这一段内存，而系统会自动回写到页面到对应的文件磁盘上，即完成了对文件的操作而不必再调用read,write等系统调用函数。内核空间对这段区域的修改也直接反映用户空间，从而可以实现不同进程间的文件共享。如图所示：
进程的虚拟地址空间，由多个虚拟内存区域构成。虚拟内存区域是进程的虚拟地址空间中的一个同质区间，即具有同样特性的连续地址范围。上图中所示的text数据段（代码段）、初始数据段、BSS数据段、堆、栈和内存映射，都是一个独立的虚拟内存区域。而为内存映射服务的地址空间处在堆栈之间的空余部分。
内核为系统中的每个进程维护一个单独的任务结构（task_struct）。任务结构中的元素包含或者指向内核运行该进程所需的所有信息(PID、指向用户栈的指针、可执行目标文件的名字、程序计数器等)。Linux内核使用vm_area_struct结构来表示一个独立的虚拟内存区域，由于每个不同质的虚拟内存区域功能和内部机制都不同，因此一个进程使用多个vm_area_struct结构来分别表示不同类型的虚拟内存区域。各个vm_area_struct结构使用链表或者树形结构链接，方便进程快速访问，如下图所示：
vm_area_struct结构中包含区域起始和终止地址以及其他相关信息，同时也包含一个vm_ops指针，其内部可引出所有针对这个区域可以使用的系统调用函数。这样，进程对某一虚拟内存区域的任何操作需要用要的信息，都可以从vm_area_struct中获得。mmap函数就是要创建一个新的vm_area_struct结构，并将其与文件的物理磁盘地址相连。
mm_struct：描述了虚拟内存的当前状态。pgd指向一级页表的基址（当内核运行这个进程时， pgd会被存放在CR3控制寄存器，也就是页表基址寄存器中），mmap指向一个vm_area_structs 的链表，其中每个vm_area_structs都描述了当前虚拟地址空间的一个区域。 vm_starts 指向这个区域的起始处。 vm_end 指向这个区域的结束处。 vm_prot 描述这个区域内包含的所有页的读写许可权限。 vm_flags 描述这个区域内的页面是与其他进程共享的，还是这个进程私有的以及一些其他信息。 vm_next 指向链表的下一个区域结构。  mmap内存映射原理 mmap内存映射的实现过程，总的来说可以分为三个阶段：
(一)启动映射过程，并在虚拟地址空间中为映射创建虚拟映射区域    进程在用户空间调用库函数mmap，原型：void *mmap(void *start, size_t length, int prot, int flags, int fd, off_t offset);
  在当前进程的虚拟地址空间中，寻找一段空闲的满足要求的连续的虚拟地址。
  为此虚拟区分配一个vm_area_struct结构，接着对这个结构的各个域进行了初始化。
  将新建的虚拟区结构（vm_area_struct）插入进程的虚拟地址区域链表或树中。
  (二)调用内核空间的系统调用函数mmap（不同于用户空间函数），实现文件物理地址和进程虚拟地址的一一映射关系   为映射分配了新的虚拟地址区域后，通过待映射的文件指针，在文件描述符表中找到对应的文件描述符，通过文件描述符，链接到内核“已打开文件集”中该文件的文件结构体（struct file），每个文件结构体维护者和这个已打开文件相关的各项信息。
  通过该文件的文件结构体，链接到file_operations模块，调用内核函数mmap，其原型为：int mmap(struct file *filp, struct vm_area_struct *vma)，不同于用户空间库函数。
  内核mmap函数通过虚拟文件系统inode模块定位到文件磁盘物理地址。
  通过remap_pfn_range函数建立页表，即实现了文件地址和虚拟地址区域的映射关系。此时，这片虚拟地址并没有任何数据关联到主存中。
  (三)进程发起对这片映射空间的访问，引发缺页异常，实现文件内容到物理内存（主存）的拷贝  注：前两个阶段仅在于创建虚拟区间并完成地址映射，但是并没有将任何文件数据的拷贝至主存。真正的文件读取是当进程发起读写操作时。
 进程的读或写操作访问虚拟地址空间这一段映射地址，通过查询页表，发现这一段地址并不在物理页面上。因为目前只建立了地址映射，真正的硬盘数据还没有拷贝到内存中，因此引发缺页异常。
  缺页异常进行一系列判断，确定无非法操作后，内核发起请求调页过程。
  调页过程先在交换缓存空间（swap cache）中寻找需要访问的内存页，如果没有则调用nopage函数把所缺的页从磁盘装入到主存中。
  之后进程即可对这片主存进行读或者写的操作，如果写操作改变了其内容，一定时间后系统会自动回写脏页面到对应磁盘地址，也即完成了写入到文件的过程。
  注意：修改过的脏页面并不会立即更新回文件中，而是有一段时间的延迟，可以调用msync()来强制同步, 这样所写的内容就能立即保存到文件里了。
mmap 示例代码 mmap (内存映射)函数的作用是建立一段可以被两个或更多个程序读写的内存。一个程序对它所做出的修改可以被其他程序看见。这要通过使用带有特殊权限集的虚拟内存段来实现。对这类虚拟内存段的读写会使操作系统去读写磁盘文件中与之对应的部分。mmap 函数创建一个指向一段内存区域的指针，该内存区域与可以通过一个打开的文件描述符访问的文件的内容相关联。mmap 函数原型如下：
#include &lt;sys/mman.h&gt;void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset); 可以通过传递 off 参数来改变共享内存段访问的文件中数据的起始偏移值。打开的文件描述符由 fildes 参数给出。可以访问的数据量(即内存段的长度)由 len 参数设置。
可以通过 addr 参数来请求使用某个特定的内存地址。如果它的取值是零，结果指针就将自动分配。这是推荐的做法，否则会降低程序的可移植性，因为不同系统上的可用地址范围是不一样的。
prot 参数用于设置内存段的访问权限。它是下列常数值的按位或的结果：
PROT_READ 内存段可读。 PROT_WRITE 内存段可写。 PROT_EXEC 内存段可执行。 PROT_NONE 内存段不能被访问。 flags 参数控制程序对该内存段的改变所造成的影响：
msync 函数的作用是：把在该内存段的某个部分或整段中的修改写回到被映射的文件中(或者从被映射文件里读出)。
#include &lt;sys/mman.h&gt;int msync(void *addr, size_t len, int flags); 内存段需要修改的部分由作为参数传递过来的起始地址 addr 和长度 len 确定。flags 参数控制着执行修改的具体方式，可以使用的选项如下：
MS_ASYNC 采用异步写方式 MS_SYNC 采用同步写方式 MS_INVALIDATE 从文件中读回数据 munmap 函数的作用是释放内存段：
#include &lt;sys/mman.h&gt;int munmap(void *addr, size_t length); 示例代码：
(1) 定义一个 RECORD 数据结构，然后创建出 NRECORDS 每个记录，每个记录中保存着它们各自的编号。然后把这些记录都追加到文件 records.dat 里去。
(2) 接着，把第 43 记录中的整数值由 43 修改为 143，并把它写入第 43 条记录中的字符串。
(3) 把这些记录映射到内存中，然后访问第 43 条记录，把它的整数值修改为 243 (同时更新该记录中的字符串)，使用的还是内存映射的方法。
可以将上述 (2) (3) 分别编写程序验证结果。
#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/mman.h&gt;#include &lt;fcntl.h&gt;#include &lt;stdlib.h&gt;typedef struct{ int integer; char string[24]; }RECORD; #define NRECORDS (100) int main() { RECORD record, *mapped; int i, f; FILE *fp; fp = fopen(&#34;records.dat&#34;, &#34;w+&#34;); for( i = 0; i &lt; NRECORDS; i++) { record.integer = i; sprintf(record.string, &#34;[RECORD-%d]&#34;, i); fwrite(&amp;record, sizeof(record), 1, fp); } fclose(fp); fp = fopen(&#34;records.dat&#34;, &#34;r+&#34;); fseek(fp, 43 * sizeof(record), SEEK_SET); fread(&amp;record, sizeof(record), 1, fp); record.integer = 143; sprintf(record.string, &#34;[RECORD-%d]&#34;, record.integer); fseek(fp, 43 * sizeof(record), SEEK_SET); fwrite(&amp;record, sizeof(record), 1, fp); fclose(fp); f = open(&#34;records.dat&#34;, O_RDWR); mapped = (RECORD*)mmap(0, NRECORDS * sizeof(record), PROT_READ | PROT_WRITE, MAP_SHARED, f, 0); printf(&#34;f:[%d]\n&#34;, f); //open是系统调用，返回文件描述符。fopen是库函数，返回指针。 	mapped[43].integer = 243; sprintf(mapped[43].string, &#34;[RECORD-%d]&#34;, mapped[43].integer); msync((void *) mapped, NRECORDS * sizeof(record), MS_ASYNC); munmap((void *)mapped, NRECORDS * sizeof(record)); close(f); return 0;	} mmap 和常规文件操作的区别 使用系统调用，函数的调用过程：
  进程发起读文件请求。
  内核通过查找进程文件描述符表，定位到内核已打开文件集上的文件信息，从而找到此文件的inode。
  inode在address_space上查找要请求的文件页是否已经缓存在页缓存中。如果存在，则直接返回这片文件页的内容。
  如果不存在，则通过inode定位到文件磁盘地址，将数据从磁盘复制到页缓存。之后再次发起读页面过程，进而将页缓存中的数据发给用户进程。
  总结来说，常规文件操作为了提高读写效率和保护磁盘，使用了页缓存机制。这样造成读文件时需要先将文件页从磁盘拷贝到页缓存中，由于页缓存处在内核空间，不能被用户进程直接寻址，所以还需要将页缓存中数据页再次拷贝到内存对应的用户空间中。这样，通过了两次数据拷贝过程，才能完成进程对文件内容的获取任务。写操作也是一样，待写入的buffer在内核空间不能直接访问，必须要先拷贝至内核空间对应的主存，再写回磁盘中（延迟写回），也是需要两次数据拷贝。
而使用mmap操作文件中，创建新的虚拟内存区域和建立文件磁盘地址和虚拟内存区域映射这两步，没有任何文件拷贝操作。而之后访问数据时发现内存中并无数据而发起的缺页异常过程，可以通过已经建立好的映射关系，只使用一次数据拷贝，就从磁盘中将数据传入内存的用户空间中，供进程使用。
总而言之，常规文件操作需要从磁盘到页缓存再到用户主存的两次数据拷贝。而mmap操控文件，只需要从磁盘到用户主存的一次数据拷贝过程。说白了，mmap的关键点是实现了用户空间和内核空间的数据直接交互而省去了空间不同、数据不通的繁琐过程。因此mmap效率更高。
由上文讨论可知，mmap优点共有一下几点：
  对文件的读取操作跨过了页缓存，减少了数据的拷贝次数，用内存读写取代I/O读写，提高了文件读取效率。
  实现了用户空间和内核空间的高效交互方式。两空间的各自修改操作可以直接反映在映射的区域内，从而被对方空间及时捕捉。
  提供进程间共享内存及相互通信的方式。不管是父子进程还是无亲缘关系的进程，都可以将自身用户空间映射到同一个文件或匿名映射到同一片区域。从而通过各自对映射区域的改动，达到进程间通信和进程间共享的目的。
  同时，如果进程A和进程B都映射了区域C，当A第一次读取C时通过缺页从磁盘复制文件页到内存中；但当B再读C的相同页面时，虽然也会产生缺页异常，但是不再需要从磁盘中复制文件过来，而可直接使用已经保存在内存中的文件数据。
可用于实现高效的大规模数据传输。内存空间不足，是制约大数据操作的一个方面，解决方案往往是借助硬盘空间协助操作，补充内存的不足。但是进一步会造成大量的文件I/O操作，极大影响效率。这个问题可以通过mmap映射很好的解决。换句话说，但凡是需要用磁盘空间代替内存的时候，mmap都可以发挥其功效。  mmap 使用的细节   使用mmap需要注意的一个关键点是，mmap映射区域大小必须是物理页大小(page_size)的整倍数（32位系统中通常是4k字节）。原因是，内存的最小粒度是页，而进程虚拟地址空间和内存的映射也是以页为单位。为了匹配内存的操作，mmap从磁盘到虚拟地址空间的映射也必须是页。
  内核可以跟踪被内存映射的底层对象（文件）的大小，进程可以合法的访问在当前文件大小以内又在内存映射区以内的那些字节。也就是说，如果文件的大小一直在扩张，只要在映射区域范围内的数据，进程都可以合法得到，这和映射建立时文件的大小无关。
  映射建立之后，即使文件关闭，映射依然存在。因为映射的是磁盘的地址，不是文件本身，和文件句柄无关。同时可用于进程间通信的有效地址空间不完全受限于被映射文件的大小，因为是按页映射。
 ]]></content>
  </entry>
  
  <entry>
    <title>几道简单的Linux驱动相关面试题</title>
    <url>/post/linux/linux-device-driver-questions-and-answers.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>linux</tag>
      <tag>device driver</tag>
    </tags>
    <content type="html"><![CDATA[今天给大家分享几道Linux设备驱动相关的面试题，希望能对需要的网友一些帮助！
Linux基础 任意3种网络操作的Linux命令,并说明他们的含义 ifconfig 命令 ifconfig 用于查看和配置 Linux 系统的网络接口。 查看所有网络接口及其状态：ifconfig -a 。 使用 up 和 down 命令启动或停止某个接口：ifconfig eth0 up 和 ifconfig eth0 down 。
iptables 命令 iptables ，是一个配置 Linux 内核防火墙的命令行工具。功能非常强大，对于我们开发来说，主要掌握如何开放端口即可。
netstat 命令 Linux netstat命令用于显示网络状态。
利用netstat指令可让你得知整个Linux系统的网络情况。
ping 命令 Linux ping命令用于检测主机。
执行ping指令会使用ICMP传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。
telnet 命令 Linux telnet命令用于远端登入。
执行telnet指令开启终端机阶段作业，并登入远端主机。
Linux支持的文件类型  普通文件类型 - 目录文件类型 d 块设备文件类型 b 字符设备类型 c 套接字文件类型 s FIFO管道文件类型 p 链接文件类型 l  Linux系统编程 嵌入式操作系统进程间有哪些同步通信服务？ Linux进程间通信方式主要有  信号(signal) 信号量 管道(pipe)、流管道(s_pipe)、有名管道(FIFO)。 消息队列 共享内存 套接字（本地的还有域套接字）  ARM 请问ARM支持哪几种异常类型？ 异常源分类
要进入异常模式，一定要有异常源，ARM规定有7种异常源：
   异常源 描述     Reset 上电时执行   Undef 当流水线中的某个非法指令到达执行状态时执行   SWI 当一个软中断指令被执行完的时候执行   Prefetch 当一个指令被从内存中预取时，由于某种原因而失败，如果它能到达执行状态这个异常才会产生   Data 如果一个预取指令试图存取一个非法的内存单元，这时异常产生   IRQ 通常的中断   FIQ 快速中断    请简述什么是中断？中断发生后，CPU做了哪些操作 中断：是指CPU在执行程序的过程中，出现了某些突发事件时CPU必须暂停执行当前的程序，转去处理突发事件，处理完毕后CPU又返回源程序被中断的位置并继续执行。
中断发生后，ARM核的操作步骤可以总结为4大步3小步。
4大步3小步  保存执行状态：将CPSR复制到发生的异常模式下SPSR中； 模式切换：   CPSR模式位强制设置为与异常类型相对应的值， 处理器进入到ARM执行模式， 禁止所有IRQ中断，当进入FIQ快速中断模式时禁止FIQ中断；   保存返回地址：将下一条指令的地址（被打断程序）保存在LR(异常模式下LR_excep)中。 跳入异常向量表：强制设置PC的值为相应异常向量地址，跳转到异常处理程序中。  什么是GPIO？ general purpose input/output GPIO是相对于芯片本身而言的，如某个管脚是芯片的GPIO脚，则该脚可作为输入或输出高或低电平使用，当然某个脚具有复用的功能，即可做GPIO也可做其他用途。
也就是说你可以把这些引脚拿来用作任何一般用途的输入输出，例如用一根引脚连到led的一极来控制它的亮灭，也可以用一根（一些）引脚连到一个传感器上以获得该传感器的状态，这给cpu提供了一个方便的控制周边设备的途经。如果没有足够多的gpio管脚，在控制一些外围设备时就会力有不逮，这时可采取的方案是使用CPLD来帮助管理。
IIC引脚名称及功能？  SDA 数据线，用于传输数据 SCL 时钟线，用于同步数据  IIC的S、P信号如何发出？ 每次通信都必须由主设备发起，当主设备决定开始通讯时，需要发送开始（S）信号，需要执行以下动作；
 空闲时SCL默认是高电平； 将SDA线从高压电平切换到低压电平； 然后将SCL从高电平切换到低电平。 在主设备发送开始条件信号之后，所有从机即使处于睡眠模式也将变为活动状态，并等待接收地址位。 当双方决定结束通讯时，需要发送停止（P）信号，需要执行以下动作； 先将SDA、SCL设置为低电平； 然后将SCL从低电平切换到高电平； 将SDA从低电平切换到高电平。 在停止条件信号之后，I2C总线即处于空闲状态。  SPI引脚名称及功能？ 串行时钟线（SCK）、 主机输入/从机输出数据线MISO、 主机输出/从机输入数据线MOSI 从机选择线SS
(有的SPI接口芯片带有中断信号线INT或INT、有的SPI接口芯片没有主机输出/从机输入数据线MOSI)
驱动  查看驱动模块中打印信息应该使用什么命令？如何查看内核中已有的字符设备的信息？如何查看正在使用的有哪些中断号？
 查看驱动模块中打印信息的命令： dmesg 查看加载模块信息可以用 lsmod 已经分配的字符设备块设备号信息可以查看下面文件 cat /proc/devices 内核会为每一个驱动模块建立一个文件夹，如下： ls /sys/module/ 显示当前使用的中断号 cat /proc/interrupts  如何手动创建字符设备？并简述主设备号和次设备号的用途。
 创建字符设备命令如下:
mknod chartest c 4 64， mknod : 创建设备节点 chartest ：设备节点名字 c ： 字符设备， 4 ： 主设备号 64： 次设备号 主设备号：主设备号标识设备对应的驱动程序。虽然现代的linux内核允许多个驱动程序共享主设备号，但我们看待的大多数设备仍然按照“一个主设备对应一个驱动程序”的原则组织。
次设备号：次设备号由内核使用，用于正确确定设备文件所指的设备。依赖于驱动程序的编写方式，我们可以通过次设备号获得一个指向内核设备的直接指针，也可将此设备号当作设备本地数组的索引。
比如：
硬件平台可能又4个串口，他们驱动非常类似，区别仅仅是个字对应的SFR基地址不同， 那么我们可以让着几个串口共用同一个串口设备驱动 通过次设备号来区别具体是哪一个串口  内核中使用共享资源时，为了使之满足互斥条件，通常有哪些方法？
 原子操作，自旋锁，信号量，互斥锁
 Linux内核包括那几个子系统？
 Linux内核主要由进程调度（SCHED）、内存管理（MM）、虚拟文件系统（VFS）、网络接口（NET）和进程间通信（IPC）5个子系统组成
]]></content>
  </entry>
  
  <entry>
    <title>10个Python脚本来自动化你的日常任务</title>
    <url>/post/python/ten-python-script-to-automatically-execute-your-daily-task.html</url>
    <categories><category>Python</category>
    </categories>
    <tags>
      <tag>python</tag>
    </tags>
    <content type="html"><![CDATA[ 在这个自动化时代，我们有很多重复无聊的工作要做。 想想这些你不再需要一次又一次地做的无聊的事情，让它自动化，让你的生活更轻松。 那么在本文中，我将向您介绍 10 个 Python 自动化脚本，以使你的工作更加自动化，生活更加轻松。 因此，没有更多的重复任务将这篇文章放在您的列表中，让我们开始吧。
 解析和提取 HTML  此自动化脚本将帮助你从网页 URL 中提取 HTML，然后还为你提供可用于解析 HTML 以获取数据的功能。这个很棒的脚本对于网络爬虫和那些想要解析 HTML 以获取重要数据的人来说是一种很好的享受。
 # Parse and Extract HTML # pip install gazpacho import gazpacho # Extract HTML from URL url = &#39;https://www.example.com/&#39; html = gazpacho.get(url) print(html) # Extract HTML with Headers headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;} html = gazpacho.get(url, headers=headers) print(html) # Parse HTML parse = gazpacho.Soup(html) # Find single tags tag1 = parse.find(&#39;h1&#39;) tag2 = parse.find(&#39;span&#39;) # Find multiple tags tags1 = parse.find_all(&#39;p&#39;) tags2 = parse.find_all(&#39;a&#39;) # Find tags by class tag = parse.find(&#39;.class&#39;) # Find tags by Attribute tag = parse.find(&#34;div&#34;, attrs={&#34;class&#34;: &#34;test&#34;}) # Extract text from tags text = parse.find(&#39;h1&#39;).text text = parse.find_all(&#39;p&#39;)[0].text 二维码扫描仪  拥有大量二维码图像或只想扫描二维码图像，那么此自动化脚本将帮助你。该脚本使用 Qrtools 模块，使你能够以编程方式扫描 QR 图像。
 # Qrcode Scanner # pip install qrtools from qrtools import Qr def Scan_Qr(qr_img): qr = Qr() qr.decode(qr_img) print(qr.data) return qr.data print(&#34;Your Qr Code is: &#34;, Scan_Qr(&#34;qr.png&#34;)) 截图  现在，你可以使用下面这个很棒的脚本以编程方式截取屏幕截图。使用此脚本，你可以直接截屏或截取特定区域的屏幕截图。
 # Grab Screenshot # pip install pyautogui # pip install Pillow from pyautogui import screenshot import time from PIL import ImageGrab # Grab Screenshot of Screen def grab_screenshot(): shot = screenshot() shot.save(&#39;my_screenshot.png&#39;) # Grab Screenshot of Specific Area def grab_screenshot_area(): area = (0, 0, 500, 500) shot = ImageGrab.grab(area) shot.save(&#39;my_screenshot_area.png&#39;) # Grab Screenshot with Delay def grab_screenshot_delay(): time.sleep(5) shot = screenshot() shot.save(&#39;my_screenshot_delay.png&#39;) 创建有声读物  厌倦了手动将您的 PDF 书籍转换为有声读物，那么这是你的自动化脚本，它使用 GTTS 模块将你的 PDF 文本转换为音频。
 # Create Audiobooks # pip install gTTS # pip install PyPDF2 from PyPDF2 import PdfFileReader as reader from gtts import gTTS def create_audio(pdf_file): read_Pdf = reader(open(pdf_file, &#39;rb&#39;)) for page in range(read_Pdf.numPages): text = read_Pdf.getPage(page).extractText() tts = gTTS(text, lang=&#39;en&#39;) tts.save(&#39;page&#39; + str(page) + &#39;.mp3&#39;) create_audio(&#39;book.pdf&#39;) PDF 编辑器  使用以下自动化脚本使用 Python 编辑 PDF 文件。该脚本使用 PyPDF4 模块，它是 PyPDF2 的升级版本，下面我编写了 Parse Text、Remove pages 等常用功能。当你有大量 PDF 文件要编辑或需要以编程方式在 Python 项目中使用脚本时，这是一个方便的脚本。
 # PDF Editor # pip install PyPDf4 import PyPDF4 # Parse the Text from PDF def parse_text(pdf_file): reader = PyPDF4.PdfFileReader(pdf_file) for page in reader.pages: print(page.extractText()) # Remove Page from PDF def remove_page(pdf_file, page_numbers): filer = PyPDF4.PdfReader(&#39;source.pdf&#39;, &#39;rb&#39;) out = PyPDF4.PdfWriter() for index in page_numbers: page = filer.pages[index] out.add_page(page) with open(&#39;rm.pdf&#39;, &#39;wb&#39;) as f: out.write(f) # Add Blank Page to PDF def add_page(pdf_file, page_number): reader = PyPDF4.PdfFileReader(pdf_file) writer = PyPDF4.PdfWriter() writer.addPage() with open(&#39;add.pdf&#39;, &#39;wb&#39;) as f: writer.write(f) # Rotate Pages def rotate_page(pdf_file): reader = PyPDF4.PdfFileReader(pdf_file) writer = PyPDF4.PdfWriter() for page in reader.pages: page.rotateClockwise(90) writer.addPage(page) with open(&#39;rotate.pdf&#39;, &#39;wb&#39;) as f: writer.write(f) # Merge PDFs def merge_pdfs(pdf_file1, pdf_file2): pdf1 = PyPDF4.PdfFileReader(pdf_file1) pdf2 = PyPDF4.PdfFileReader(pdf_file2) writer = PyPDF4.PdfWriter() for page in pdf1.pages: writer.addPage(page) for page in pdf2.pages: writer.addPage(page) with open(&#39;merge.pdf&#39;, &#39;wb&#39;) as f: writer.write(f) 迷你 Stackoverflow  作为一名程序员，我知道我们每天都需要 StackOverflow，但你不再需要在 Google 上搜索它。现在，在您继续处理项目的同时，在你的 CMD 中获得直接解决方案。通过使用 Howdoi 模块，你可以在命令提示符或终端中获得 StackOverflow 解决方案。你可以在下面找到一些可以尝试的示例。
 # Automate Stackoverflow # pip install howdoi # Get Answers in CMD #example 1 &gt; howdoi how do i install python3 # example 2 &gt; howdoi selenium Enter keys # example 3 &gt; howdoi how to install modules # example 4 &gt; howdoi Parse html with python # example 5 &gt; howdoi int not iterable error # example 6 &gt; howdoi how to parse pdf with python # example 7 &gt; howdoi Sort list in python # example 8 &gt; howdoi merge two lists in python # example 9 &gt;howdoi get last element in list python # example 10 &gt; howdoi fast way to sort list 自动化手机  此自动化脚本将帮助你使用 Python 中的 Android 调试桥 (ADB) 自动化你的智能手机。下面我将展示如何自动执行常见任务，例如滑动手势、呼叫、发送短信等等。您可以了解有关 ADB 的更多信息，并探索更多令人兴奋的方法来实现手机自动化，让您的生活更轻松。
 # Automate Mobile Phones # pip install opencv-python import subprocess def main_adb(cm): p = subprocess.Popen(cm.split(&#39; &#39;), stdout=subprocess.PIPE, shell=True) (output, _) = p.communicate() return output.decode(&#39;utf-8&#39;) # Swipe  def swipe(x1, y1, x2, y2, duration): cmd = &#39;adb shell input swipe {}{}{}{}{}&#39;.format(x1, y1, x2, y2, duration) return main_adb(cmd) # Tap or Clicking def tap(x, y): cmd = &#39;adb shell input tap {}{}&#39;.format(x, y) return main_adb(cmd) # Make a Call def make_call(number): cmd = f&#34;adb shell am start -a android.intent.action.CALL -d tel:{number}&#34; return main_adb(cmd) # Send SMS def send_sms(number, message): cmd = &#39;adb shell am start -a android.intent.action.SENDTO -d sms:{}--es sms_body &#34;{}&#34;&#39;.format(number, message) return main_adb(cmd) # Download File From Mobile to PC def download_file(file_name): cmd = &#39;adb pull /sdcard/{}&#39;.format(file_name) return main_adb(cmd) # Take a screenshot def screenshot(): cmd = &#39;adb shell screencap -p&#39; return main_adb(cmd) # Power On and Off def power_off(): cmd = &#39;&#34;adb shell input keyevent 26&#34;&#39; return main_adb(cmd) 监控 CPU/GPU 温度  你可能使用 CPU-Z 或任何规格监控软件来捕获你的 Cpu 和 Gpu 温度，但你也可以通过编程方式进行。好吧，这个脚本使用 Pythonnet 和 OpenhardwareMonitor 来帮助你监控当前的 Cpu 和 Gpu 温度。你可以使用它在达到一定温度时通知自己，也可以在 Python 项目中使用它来简化日常生活。
 # Get CPU/GPU Temperature # pip install pythonnet import clr clr.AddReference(&#34;OpenHardwareMonitorLib&#34;) from OpenHardwareMonitorLib import * spec = Computer() spec.GPUEnabled = True spec.CPUEnabled = True spec.Open() # Get CPU Temp def Cpu_Temp(): while True: for cpu in range(0, len(spec.Hardware[0].Sensors)): if &#34;/temperature&#34; in str(spec.Hardware[0].Sensors[cpu].Identifier): print(str(spec.Hardware[0].Sensors[cpu].Value)) # Get GPU Temp def Gpu_Temp() while True: for gpu in range(0, len(spec.Hardware[0].Sensors)): if &#34;/temperature&#34; in str(spec.Hardware[0].Sensors[gpu].Identifier): print(str(spec.Hardware[0].Sensors[gpu].Value)) Instagram 上传机器人  Instagram 是一个著名的社交媒体平台，你现在不需要通过智能手机上传照片或视频。你可以使用以下脚本以编程方式执行此操作。
 # Upload Photos and Video on Insta # pip install instabot from instabot import Bot def Upload_Photo(img): robot = Bot() robot.login(user) robot.upload_photo(img, caption=&#34;Medium Article&#34;) print(&#34;Photo Uploaded&#34;) def Upload_Video(video): robot = Bot() robot.login(user) robot.upload_video(video, caption=&#34;Medium Article&#34;) print(&#34;Video Uploaded&#34;) def Upload_Story(img): robot = Bot() robot.login(user) robot.upload_story(img, caption=&#34;Medium Article&#34;) print(&#34;Story Photos Uploaded&#34;) Upload_Photo(&#34;img.jpg&#34;) Upload_Video(&#34;video.mp4&#34;) 视频水印  使用此自动化脚本为你的视频添加水印，该脚本使用 Moviepy，这是一个方便的视频编辑模块。在下面的脚本中，你可以看到如何添加水印并且可以自由使用它。
 # Video Watermark with Python # pip install moviepy from moviepy.editor import * clip = VideoFileClip(&#34;myvideo.mp4&#34;, audio=True) width,height = clip.size text = TextClip(&#34;WaterMark&#34;, font=&#39;Arial&#39;, color=&#39;white&#39;, fontsize=28) set_color = text.on_color(size=(clip.w + text.w, text.h-10), color=(0,0,0), pos=(6,&#39;center&#39;), col_opacity=0.6) set_textPos = set_color.set_pos( lambda pos: (max(width/30,int(width-0.5* width* pos)),max(5*height/6,int(100* pos))) ) Output = CompositeVideoClip([clip, set_textPos]) Output.duration = clip.duration Output.write_videofile(&#34;output.mp4&#34;, fps=30, codec=&#39;libx264&#39;) ]]></content>
  </entry>
  
  <entry>
    <title>FPGA硬核和软核处理器的区别</title>
    <url>/post/fpga/difference-between-hard-core-processor-and-soft-core-processor-of-fpga.html</url>
    <categories><category>FPGA</category>
    </categories>
    <tags>
      <tag>CPU</tag>
      <tag>fpga</tag>
      <tag>processor</tag>
      <tag>Altera</tag>
      <tag>Xilinx</tag>
    </tags>
    <content type="html"><![CDATA[ 从架构的角度来说，SOPC和SoC FPGA是统一的，都是由FPGA部分和处理器部分组成。在SoC FPGA 中，嵌入的是纯硬件基础的硬核处理器，简称HPS(Hardware Processor System)，而SOPC技术中，嵌入的是使用FPGA逻辑资源实现的软核处理器，两者指令集不一样，处理器性能也不一样。
 软核处理器 SOPC技术，即软核处理器，最早是由Altera公司提出来的，它是基于 FPGA  的SOC片上系统设计技术。是使用FPGA的逻辑和资源搭建的一个软核CPU系统，由于是使用FPGA的通用逻辑搭建的CPU，因此具有一定的灵活性，用户可以根据自己的需求对CPU进行定制裁剪，增加一些专用功能，例如除法或浮点运算单元，用于提升CPU在某些专用运算方面的性能，或者删除一些在系统里面使用不到的功能，以节约逻辑资源。
另外也可以根据用户的实际需求，为CPU添加各种标准或定制的外设，例如UART，SPI，IIC等标准接口外设，同时，用户也可以自己使用FPGA的逻辑资源，编写各种专用的外设，然后连接到CPU总线上，由CPU进行控制，以实现软硬件的协同工作，在保证系统性能的同时，增加了系统的灵活性。
而且，如果单个的软核CPU无法满足用户需求，可以添加多个CPU软核，搭建多核系统，通过多核CPU协同工作，让系统拥有更加灵活便捷的控制能力。
由于是使用FPGA资源实现的，所以具有很大的灵活性，可以实现根据需要实现多种处理器，如8051，RISC-V，Xilinx的 MicroBlaze ，Altera的Nios-II等等。
硬核处理器 由于软核CPU是使用FPGA的通用逻辑资源搭建的，相较使用经过布局布线优化的硬核处理器来说，软核处理器够运行的最高实时钟主频要低一些，而且也会相应的消耗较多的FPGA逻辑资源以及片上存储器资源，因此SOPC方案仅适用于对于数处理器整体性能要求不高的应用，例如整个系统的初始化配置，人机交互，多个功能模块间的协调控制等功能。
所以，各大FPGA厂家推出了SoC FPGA技术，是在芯片设计之初，就在内部的硬件电路上添加了硬核处理器，是纯硬件实现的，不会消耗FPGA的逻辑资源，硬核处理器和FPGA逻辑在一定程度上是相互独立的，简单的说，就是SoC FPGA就是把一块ARM处理器和一块FPGA芯片封装成了一个芯片。
例如比较有名的Xilinx的ZYNQ/PYNQ系列集成ARM Cortex-A9处理器，同时具有ARM软件的可编程性和FPGA 的硬件可编程性，不仅可实现重要分析与硬件加速，同时还在单个器件上高度集成 CPU、DSP、ASSP 以及混合信号功能。
ZYNQ开发板 Intel的Cyclone V系列，集成双核Cortex-A9，于2013年发布，在单一芯片上集成了双核的ARM Cortex-A9处理器和FPGA逻辑资源的新型SoC芯片，相较于传统的单一ARM处理器或FPGA芯片，它既拥有了ARM处理器灵活高效的数据运算和事务处理能力，同时又集成了FPGA的高速并行处理优势，同时，基于两者独特的片上互联结构，使用时可以将FPGA上的通用逻辑资源经过配置，映射为ARM处理器的一个或多个具有特定功能的外设，通过高达128位宽的AXI高速总线进行通信，完成数据和控制命令的交互。由于片上的ARM处理器是经过布局布线的硬线逻辑，因此其能工作的时钟主频较高，因此单位时间内能够执行的指令也更多。
区别和联系 从架构的角度来说，SOPC和SoC FPGA是统一的，都是由FPGA部分和处理器部分组成。在SoC FPGA 中，嵌入的是纯硬件基础的硬核处理器，简称HPS(Hardware Processor System)，而SOPC技术中，嵌入的是使用FPGA逻辑资源实现的软核处理器，两者指令集不一样，处理器性能也不一样。
一般来说，硬核处理器的性能要远远高于软核处理器。另外，硬核处理器除了CPU部分，还集成了各种高性能外设，如MMU、DDR3控制器、Nand FLASH控制器等，可以运行成熟的Linux操作系统和应用程序，提供统一的系统API，降低开发者的软件开发难度。而软核CPU虽然可以通过配置，用逻辑资源来搭建相应的控制器以支持相应功能，但是从性能和开发难度上来说，基于SoC FPGA架构进行设计开发是比较好的选择。
ZYNQ内部框图 另外，虽然SoC FPGA芯片上既包含了有ARM，又包含了有FPGA，但是两者一定程度上是相互独立的，SoC芯片上的ARM处理器核并非是包含于FPGA逻辑单元内部的，FPGA和ARM（HPS）处理器只是封装到同一个芯片中，JTAG接口、电源引脚和外设的接口引脚都是独立的，因此，如果使用SoC FPGA芯片进行设计，即使不使用到片上的ARM处理器，ARM处理器部分占用的芯片资源也无法释放出来，不能用作通用的FPGA资源。
而SOPC则是使用FPGA通用逻辑和存储器资源搭建的CPU，当不使用CPU时，CPU部分占用的资源可以被释放，重新用作通用FPGA资源。
]]></content>
  </entry>
  
  <entry>
    <title>详解嵌入式LCD的接口类型</title>
    <url>/post/fpga/embedded-lcd-interface-model.html</url>
    <categories><category>FPGA</category>
    </categories>
    <tags>
      <tag>LCD</tag>
      <tag>RGB</tag>
    </tags>
    <content type="html"><![CDATA[ 从架构的角度来说，SOPC和SoC FPGA是统一的，都是由FPGA部分和处理器部分组成。在SoC FPGA 中，嵌入的是纯硬件基础的硬核处理器，简称HPS(Hardware Processor System)，而SOPC技术中，嵌入的是使用FPGA逻辑资源实现的软核处理器，两者指令集不一样，处理器性能也不一样。
 LCD的接口有多种，分类很细。主要看LCD的驱动方式和控制方式，目前手机上的彩色LCD的连接方式一般有这么几种：MCU模式，RGB模式，SPI模式，VSYNC模式，MDDI模式，DSI模式。MCU模式(也写成MPU模式的)。只有TFT模块才有RGB接口。
但应用比较多的就是MUC模式和RGB模式，区别有以下几点：
MCU接口: 会解码命令，由timing generator产生时序信号，驱动COM和SEG驱器。
RGB接口: 在写LCD register setting时，和MCU接口没有区别。区别只在于图像的写入方式。
用MCU模式时由于数据可以先存到IC内部GRAM后再往屏上写，所以这种模式LCD可以直接接在MEMORY的总线上。
用RGB模式时就不同了，它没有内部RAM，HSYNC，VSYNC，ENABLE，CS，RESET，RS可以直接接在MEMORY的GPIO口上，用GPIO口来模拟波形.
MPU接口方式: 显示数据写入DDRAM，常用于静止图片显示。
RGB接口方式: 显示数据不写入DDRAM，直接写屏，速度快，常用于显示视频或动画用。
主要的区别是: MCU接口方式: 显示数据写入DDRAM，常用于静止图片显示。 RGB接口方式: 显示数据不写入DDRAM，直接写屏，速度快，常用于显示视频或动画用。
MCU模式 因为主要针对单片机的领域在使用,因此得名.后在中低端手机大量使用,其主要特点是价格便宜的。MCU-LCD接口的标准术语是Intel提出的8080总线标准，因此在很多文档中用I80 来指MCU-LCD屏。主要又可以分为8080模式和6800模式，这两者之间主要是时序的区别。数据位传输有8位，9位，16位，18位，24位。连线分为：CS/，RS(寄存器选择)，RD/，WR/，再就是数据线了。优点是：控制简单方便，无需时钟和同步信号。缺点是：要耗费GRAM，所以难以做到大屏(3.8以上)。对于MCU接口的LCM，其内部的芯片就叫LCD驱动器。主要功能是对主机发过的数据/命令，进行变换，变成每个象素的RGB数据，使之在屏上显示出来。这个过程不需要点、行、帧时钟。
MCU接口的LCD的DriverIC都带GRAM，Driver IC作为MCU的一片协处理器，接受MCU发过来的Command/Data，可以相对独立的工作。对于MCU接口的LCM(LCD Module)，其内部的芯片就叫LCD驱动器。主要功能是对主机发过的数据/命令，进行变换，变成每个象素的RGB数据，使之在屏上显示出来。这个过程不需要点、行、帧时钟。
M6800模式 M6800模式支持可选择的总线宽度8/9/16/18-bit(默认为8位)，其实际设计思想是与I80的思想是一样的，主要区别就是该模式的总线控制读写信号组合在一个引脚上(/WR)，而增加了一个锁存信号(E)数据位传输有8位，9位，16位和18位。
I8080模式 I80模式连线分为：CS/，RS(寄存器选择)，RD/，WR/，再就是数据线了。优点是：控制简单方便，无需时钟和同步信号。缺点是：要耗费GRAM，所以难以做到大屏(QVGA以上)。
 MCU接口标准名称是I80，管脚的控制脚有5个： CS 片选信号 RS (置1为写数据,置0为写命令) /WR (为0表示写数据) 数据命令区分信号 /RD (为0表示读数据) RESET 复位LCD( 用固定命令系列 0 1 0来复位)  VSYNC模式 该模式其实就是就是在MCU模式上加了一个VSYNC信号，应用于运动画面更新，这样就与上述两个接口有很大的区别。该模式支持直接进行动画显示的功能，它提供了一个对MCU接口最小的改动，实现动画显示的解决方案。在这种模式下，内部的显示操作与外部VSYNC信号同步。可以实现比内部操作更高的速率的动画显示。但由于其操作方式的不同，该模式对速率有一个限制，那就是对内部SRAM的写速率一定要大于显示读内部SRAM的速率。
RGB模式 大屏采用较多的模式，数据位传输也有6位，16位和18位，24位之分。连线一般有：VSYNC，HSYNC，DOTCLK，CS，RESET，有的也需要RS，剩下就是数据线。它的优缺点正好和MCU模式相反。
MCU-LCD屏它与RGB-LCD屏主要区别在于显存的位置。RGB-LCD的显存是由系统内存充当的，因此其大小只受限于系统内存的大小，这样RGB-LCD可以做出较大尺寸，象现在4.3&quot;只能算入门级，而MID中7&quot;,10&quot;的屏都开始大量使用。而MCU-LCD的设计之初只要考虑单片机的内存较小，因此都是把显存内置在LCD模块内部.然后软件通过专门显示命令来更新显存，因此MCU屏往往不能做得很大。同时显示更新速度也比RGB-LCD慢。显示数据传输模式也有差别。RGB屏只需显存组织好数据。启动显示后，LCD-DMA会自动把显存中的数据通过RGB接口送到LCM。而MCU屏则需要发送画点的命令来修改MCU内部的RAM(即不能直接写MCU屏的RAM)。所以RGB显示速度明显比MCU快，而且播放视频方面，MCU-LCD也比较慢。
对于RGB接口的LCM，主机输出的直接是每个象素的RGB数据，不需要进行变换(GAMMA校正等除外)，对于这种接口，需要在主机部分有个LCD控制器，以产生RGB数据和点、行、帧同步信号。
彩色TFT液晶屏主要有2种接口：TTL接口(RGB颜色接口)， LVDS接口(将RGB颜色打包成差分信号传输)。TTL接口主要用于12.1寸一下的小尺寸TFT屏，LVDS接口主要用于8寸以上的大尺寸TFT屏。TTL接口线多，传输距离短;LVDS接口传输距离长，线的数量少。大屏采用较多的模式，控制脚是VSYNC，HSYNC，VDEN，VCLK， S3C2440最高支持24个数据脚，数据脚是VD[23-0]。
CPU或显卡发出的图像数据是TTL信号(0-5V、0-3.3V、0-2.5V、或0-1.8V)，LCD本身接收的也是TTL信号，由于TTL信号在高速率的长距离传输时性能不佳，抗干扰能力比较差，后来又提出了多种传输模式，比如LVDS、TDMS、GVIF、P&amp;D、DVI和DFP等。他们实际上只是将CPU或显卡发出的TTL信号编码成各种信号以传输，在LCD那边将接收到的信号进行解码得到TTL信号。
但是不管采用何种传输模式，本质的TTL信号是一样的。
注意: TTL/LVDS分别是两种信号的传输模式，TTL是高电平表示1，低电平表示0的模式，LVDS是正负两个对应波形，用两个波形的差值来表示当前是1还是0
SPI模式 采用较少，有3线和4线的，连线为CS/，SLK，SDI，SDO四根线，连线少但是软件控制比较复杂。
MDDI模式(MobileDisplayDigitalInterface) 高通公司于2004年提出的接口MDDI，通过减少连线可提高移动电话的可靠性并降低功耗，这将取代SPI模式而成为移动领域的高速串行接口。 连线主要是host_data,host_strobe,client_data,client_strobe,power,GND几根线。
DSI模式 该模式串行的双向高速命令传输模式，连线有D0P，D0N，D1P，D1N，CLKP，CLKN。
]]></content>
  </entry>
  
  <entry>
    <title>带你走进Linux内核源码中最常见的数据结构之「mutex」</title>
    <url>/post/linux/linux-kernel-source-code-data-structure-mutex.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>kernel</tag>
      <tag>mutex</tag>
    </tags>
    <content type="html"><![CDATA[定义 互斥锁（英语：Mutual exclusion，缩写 Mutex）是一种用于多线程编程中，防止两条线程同时对同一公共资源（比如全域变量）进行读写的机制。
该目的通过将代码切片成一个一个的**临界区域（critical section）**达成。临界区域指的是一块对公共资源进行存取的代码，并非一种机制或是算法。一个程序、进程、线程可以拥有多个临界区域，但是并不一定会应用互斥锁。
例如：一段代码（甲）正在分步修改一块数据。这时，另一条线程（乙）由于一些原因被唤醒。如果乙此时去读取甲正在修改的数据，而甲碰巧还没有完成整个修改过程，这个时候这块数据的状态就处在极大的不确定状态中，读取到的数据当然也是有问题的。更严重的情况是乙也往这块地方写数据，这样的一来，后果将变得不可收拾。因此，多个线程间共享的数据必须被保护。达到这个目的的方法，就是确保同一时间只有一个临界区域处于运行状态，而其他的临界区域，无论是读是写，都必须被挂起并且不能获得运行机会。
互斥锁实现多线程同步的核心思想是：有线程访问进程空间中的公共资源时，该线程执行“加锁”操作（将资源“锁”起来），阻止其它线程访问。访问完成后，该线程负责完成“解锁”操作，将资源让给其它线程。当有多个线程想访问资源时，谁最先完成“加锁”操作，谁就最先访问资源。
当有多个线程想访问“加锁”状态下的公共资源时，它们只能等待资源“解锁”，所有线程会排成一个等待（阻塞）队列。资源解锁后，操作系统会唤醒等待队列中的所有线程，第一个访问资源的线程会率先将资源“锁”起来，其它线程则继续等待。当有多个线程想访问“加锁”状态下的公共资源时，它们只能等待资源“解锁”，所有线程会排成一个等待（阻塞）队列。资源解锁后，操作系统会唤醒等待队列中的所有线程，第一个访问资源的线程会率先将资源“锁”起来，其它线程则继续等待。
mutex有什么缺点？ 不同于mutex最初的设计与目的，现在的struct mutex是内核中最大的锁之一，比如在x86-64上，它差不多有32bytes的大小，而struct samaphore是24bytes，rw_semaphore为40bytes，更大的数据结构意味着占用更多的CPU缓存和更多的内存占用。
什么时候应该使用mutex？ 除非mutex的严格语义要求不合适或者临界区域阻止锁的共享，否则相较于其他锁原语来说更倾向于使用mutex
mutex与spinlock的区别？ spinlock是让一个尝试获取它的线程在一个循环中等待的锁，线程在等待时会一直查看锁的状态。而mutex是一个可以让多个进程轮流分享相同资源的机制
spinlock通常短时间持有，mutex可以长时间持有
spinlock任务在等待锁释放时不可以睡眠，mutex可以
看到一个非常有意思的解释：
spinlock就像是坐在车后座的熊孩子，一直问“到了吗？到了吗？到了吗？…”
mutex就像一个司机返回的信号，说“我们到了！”
实现 看一下Linux kernel-5.8是如何实现mutex的2 实现
struct mutex { atomic_long_t owner; spinlock_t wait_lock; #ifdef CONFIG_MUTEX_SPIN_ON_OWNER  struct optimistic_spin_queue osq; /* Spinner MCS lock */ #endif  struct list_head wait_list; #ifdef CONFIG_DEBUG_MUTEXES  void *magic; #endif #ifdef CONFIG_DEBUG_LOCK_ALLOC  struct lockdep_map dep_map; #endif }; 可以看到，mutex使用了原子变量owner来追踪锁的状态，owner实际上是指向当前mutex锁拥有者的struct task_struct *指针，所以当锁没有被持有时，owner为NULL。
/* * This is the control structure for tasks blocked on mutex, * which resides on the blocked task&#39;s kernel stack: * 表示等待队列wait_list中进程的结构体 */ struct mutex_waiter { struct list_head list; struct task_struct *task; struct ww_acquire_ctx *ww_ctx; #ifdef CONFIG_DEBUG_MUTEXES  void *magic; #endif }; 上锁 当要获取mutex时，通常有三种路径方式
fastpath: 通过 cmpxchg() 当前任务与所有者来尝试原子性的获取锁。这仅适用于无竞争的情况（cmpxchg() 检查 0UL，因此上面的所有 3 个状态位都必须为 0）。如果锁被争用，它会转到下一个可能的路径。
midpath: 又名乐观旋转（optimistic spinning）—在锁的持有者正在运行并且没有其他具有更高优先级（need_resched）的任务准备运行时，通过旋转来获取锁。理由是如果锁的所有者正在运行，它很可能很快就会释放锁。mutex spinner使用 MCS 锁排队，因此只有一个spinner可以竞争mutex。
MCS 锁（由 Mellor-Crummey 和 Scott 提出）是一个简单的自旋锁，具有公平的理想属性，每个 cpu 都试图获取在本地变量上旋转的锁，排队采用的是链表实现的FIFO。它避免了常见的test-and-set自旋锁实现引起的昂贵的cacheline bouncing。类似MCS的锁是专门为睡眠锁的乐观旋转而量身定制的（毕竟如果只是短暂的自旋比休眠效率要高）。自定义 MCS 锁的一个重要特性是它具有额外的属性，即当spinner需要重新调度时，它们能够直接退出 MCS 自旋锁队列。这有助于避免需要重新调度的 MCS spinner持续在mutex持有者上自旋，而仅需直接进入慢速路径获取MCS锁。
slowpath: 最后的手段，如果仍然无法获得锁，则将任务添加到等待队列并休眠，直到被解锁路径唤醒。在正常情况下它阻塞为 TASK_UNINTERRUPTIBLE。 虽然正式的内核互斥锁是可休眠的锁，但midpath路径 (ii) 使它们更实际地成为混合类型。通过简单地不中断任务并忙于等待几个周期而不是立即休眠，此锁的性能已被视为显着改善了许多工作负载。请注意，此技术也用于 rw 信号量。
具体代码调用链很长…
/*不可中断的获取锁*/ void __sched mutex_lock(struct mutex *lock) { might_sleep(); /*fastpath*/ if (!__mutex_trylock_fast(lock)) /*midpath and slowpath*/ __mutex_lock_slowpath(lock); } __mutex_trylock_fast(lock) -&gt; atomic_long_try_cmpxchg_acquire(&amp;lock-&gt;owner, &amp;zero, curr) -&gt; atomic64_try_cmpxchg_acquire(v, (s64 *)old, new); __mutex_lock_slowpath(lock)-&gt;__mutex_lock(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_) -&gt; __mutex_lock_common(lock, state, subclass, nest_lock, ip, NULL, false) /*可中断的获取锁*/ int mutex_lock_interruptible(struct mutex *lock); 尝试上锁 int __sched mutex_trylock(struct mutex *lock) { bool locked; #ifdef CONFIG_DEBUG_MUTEXES  DEBUG_LOCKS_WARN_ON(lock-&gt;magic != lock); #endif  locked = __mutex_trylock(lock); if (locked) mutex_acquire(&amp;lock-&gt;dep_map, 0, 1, _RET_IP_); return locked; } static inline bool __mutex_trylock(struct mutex *lock) { return !__mutex_trylock_or_owner(lock); } 释放锁 void __sched mutex_unlock(struct mutex *lock) { #ifndef CONFIG_DEBUG_LOCK_ALLOC  if (__mutex_unlock_fast(lock)) return; #endif  __mutex_unlock_slowpath(lock, _RET_IP_); } 跟加锁对称，也有fastpath, midpath, slowpath三条路径。 判断锁状态
bool mutex_is_locked(struct mutex *lock) { return __mutex_owner(lock) != NULL; } 很显而易见，mutex持有者不为NULL即表示锁定状态。
实际案例 实验：
#include &lt;pthread.h&gt;#include &lt;stdio.h&gt; #define LOOP 1000000  int cnt = 0; int cs1 = 0, cs2 = 0; void* task(void* args) { while(1) { if(cnt &gt;= LOOP) { break; } cnt++; if((int)args == 1) cs1 ++; else cs2++; } return NULL; } int main() { pthread_t tid1; pthread_t tid2; /* create the thread */ pthread_create(&amp;tid1, NULL, task, (void*)1); pthread_create(&amp;tid2, NULL, task, (void*)2); /* wait for thread to exit */ pthread_join(tid1, NULL); pthread_join(tid2, NULL); printf(&#34;cnt = %d cs1=%d cs2=%d total=%d\n&#34;, cnt,cs1,cs2,cs1+cs2); return 0; } 输出：
cnt = 1000000 cs1=958560 cs2=1520226 total=2478786 正确结果不应该是1000000吗？为什么会出错呢，我们可以从汇编角度来分析一下。
$&gt; g++ -E test.c -o test.i $&gt; g++ -S test.i -o test.s $&gt; vim test.s .file &#34;test.c&#34; .globl _cnt .bss .align 4 _cnt: .space 4 .text .globl __Z5task1Pv .def __Z5task1Pv; .scl 2; .type 32; .endef __Z5task1Pv: ... 我们可以看到一个简单的cnt++，对应
movl _cnt, %eax addl $1, %eax movl %eax, _cnt CPU先将cnt的值读到寄存器eax中，然后将[eax] + 1，最后将eax的值返回到cnt中，这些操作不是**原子性质(atomic)**的，这就导致cnt被多个线程操作时，+1过程会被打断。
加入mutex保护临界资源
#include &lt;pthread.h&gt;#include &lt;stdio.h&gt; #define LOOP 1000000  pthread_mutex_t mutex; int cnt = 0; int cs1 = 0, cs2 = 0; void* task(void* args) { while(1) { pthread_mutex_lock(&amp;mutex); if(cnt &gt;= LOOP) { pthread_mutex_unlock(&amp;mutex); break; } cnt++; pthread_mutex_unlock(&amp;mutex); if((int)args == 1) cs1 ++; else cs2++; } return NULL; } int main() { pthread_mutex_init(&amp;mutex , NULL); pthread_t tid1; pthread_t tid2; /* create the thread */ pthread_create(&amp;tid1, NULL, task, (void*)1); pthread_create(&amp;tid2, NULL, task, (void*)2); /* wait for thread to exit */ pthread_join(tid1, NULL); pthread_join(tid2, NULL); printf(&#34;cnt = %d cs1=%d cs2=%d total=%d\n&#34;, cnt,cs1,cs2,cs1+cs2); return 0; } 输出：
cnt = 1000000 cs1=517007 cs2=482993 total=1000000 ]]></content>
  </entry>
  
  <entry>
    <title>openssl命令</title>
    <url>/post/linux/openssl.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>openssl</tag>
      <tag>系统管理</tag>
      <tag>系统安全</tag>
    </tags>
    <content type="html"><![CDATA[ 强大的安全套接字层密码库
 OpenSSL是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及SSL协议，并提供丰富的应用程序供测试或其它目的使用。在OpenSSL被曝出现严重安全漏洞后，发现多数通过SSL协议加密的网站使用名为OpenSSL的开源软件包。由于这是互联网应用最广泛的安全传输方法，被网银、在线支付、电商网站、门户网站、电子邮件等重要网站广泛使用，所以该漏洞影响范围广大。
OpenSSL有两种运行模式：交互模式和批处理模式。
直接输入openssl回车进入交互模式，输入带命令选项的openssl进入批处理模式。
OpenSSL整个软件包大概可以分成三个主要的功能部分：密码算法库、SSL协议库以及应用程序。OpenSSL的目录结构自然也是围绕这三个功能部分进行规划的。 对称加密算法 OpenSSL一共提供了8种对称加密算法，其中7种是分组加密算法，仅有的一种流加密算法是RC4。这7种分组加密算法分别是AES、DES、Blowfish、CAST、IDEA、RC2、RC5，都支持电子密码本模式（ECB）、加密分组链接模式（CBC）、加密反馈模式（CFB）和输出反馈模式（OFB）四种常用的分组密码加密模式。其中，AES使用的加密反馈模式（CFB）和输出反馈模式（OFB）分组长度是128位，其它算法使用的则是64位。事实上，DES算法里面不仅仅是常用的DES算法，还支持三个密钥和两个密钥3DES算法。 非对称加密算法 OpenSSL一共实现了4种非对称加密算法，包括DH算法、RSA算法、DSA算法和椭圆曲线算法（EC）。DH算法一般用户密钥交换。RSA算法既可以用于密钥交换，也可以用于数字签名，当然，如果你能够忍受其缓慢的速度，那么也可以用于数据加密。DSA算法则一般只用于数字签名。 信息摘要算法 OpenSSL实现了5种信息摘要算法，分别是MD2、MD5、MDC2、SHA（SHA1）和RIPEMD。SHA算法事实上包括了SHA和SHA1两种信息摘要算法，此外，OpenSSL还实现了DSS标准中规定的两种信息摘要算法DSS和DSS1。 密钥和证书管理 密钥和证书管理是PKI的一个重要组成部分，OpenSSL为之提供了丰富的功能，支持多种标准。 首先，OpenSSL实现了ASN.1的证书和密钥相关标准，提供了对证书、公钥、私钥、证书请求以及CRL等数据对象的DER、PEM和BASE64的编解码功能。OpenSSL提供了产生各种公开密钥对和对称密钥的方法、函数和应用程序，同时提供了对公钥和私钥的DER编解码功能。并实现了私钥的PKCS#12和PKCS#8的编解码功能。OpenSSL在标准中提供了对私钥的加密保护功能，使得密钥可以安全地进行存储和分发。 在此基础上，OpenSSL实现了对证书的X.509标准编解码、PKCS#12格式的编解码以及PKCS#7的编解码功能。并提供了一种文本数据库，支持证书的管理功能，包括证书密钥产生、请求产生、证书签发、吊销和验证等功能。 事实上，OpenSSL提供的CA应用程序就是一个小型的证书管理中心（CA），实现了证书签发的整个流程和证书管理的大部分机制。
实例 1、消息摘要算法应用例子 用SHA1算法计算文件file.txt的哈西值，输出到stdout：
# openssl dgst -sha1 file.txt 用SHA1算法计算文件file.txt的哈西值，输出到文件digest.txt：
# openssl sha1 -out digest.txt file.txt 用DSS1(SHA1)算法为文件file.txt签名，输出到文件dsasign.bin。签名的private key必须为DSA算法产生的，保存在文件dsakey.pem中。
# openssl dgst -dss1 -sign dsakey.pem -out dsasign.bin file.txt 用dss1算法验证file.txt的数字签名dsasign.bin，验证的private key为DSA算法产生的文件dsakey.pem。
# openssl dgst -dss1 -prverify dsakey.pem -signature dsasign.bin file.txt 用sha1算法为文件file.txt签名,输出到文件rsasign.bin，签名的private key为RSA算法产生的文件rsaprivate.pem。
# openssl sha1 -sign rsaprivate.pem -out rsasign.bin file.txt # 用sha1算法验证file.txt的数字签名rsasign.bin，验证的public key为RSA算法生成的rsapublic.pem。 # openssl sha1 -verify rsapublic.pem -signature rsasign.bin file.txt 2、对称加密应用例子 对称加密应用例子，用DES3算法的CBC模式加密文件plaintext.doc，加密结果输出到文件ciphertext.bin。
# openssl enc -des3 -salt -in plaintext.doc -out ciphertext.bin 用DES3算法的OFB模式解密文件ciphertext.bin，提供的口令为trousers，输出到文件plaintext.doc。注意：因为模式不同，该命令不能对以上的文件进行解密。
# openssl enc -des-ede3-ofb -d -in ciphertext.bin -out plaintext.doc -pass pass:trousers 用Blowfish的CFB模式加密plaintext.doc，口令从环境变量PASSWORD中取，输出到文件ciphertext.bin。
# openssl bf-cfb -salt -in plaintext.doc -out ciphertext.bin -pass env:PASSWORD 给文件ciphertext.bin用base64编码，输出到文件base64.txt。
# openssl base64 -in ciphertext.bin -out base64.txt 用RC5算法的CBC模式加密文件plaintext.doc，输出到文件ciphertext.bin，salt、key和初始化向量(iv)在命令行指定。
# openssl rc5 -in plaintext.doc -out ciphertext.bin -S C62CB1D49F158ADC -iv E9EDACA1BD7090C6 -K 89D4B1678D604FAA3DBFFD030A314B29 3、Diffie-Hellman应用例子 使用生成因子2和随机的1024-bit的素数产生D0ffie-Hellman参数，输出保存到文件dhparam.pem
# openssl dhparam -out dhparam.pem -2 1024 从dhparam.pem中读取Diffie-Hell参数，以C代码的形式，输出到stdout。
# openssl dhparam -in dhparam.pem -noout -C 4、DSA应用例子应用例子 生成1024位DSA参数集，并输出到文件dsaparam.pem。
# openssl dsaparam -out dsaparam.pem 1024 使用参数文件dsaparam.pem生成DSA私钥匙，采用3DES加密后输出到文件dsaprivatekey.pem
# openssl gendsa -out dsaprivatekey.pem -des3 dsaparam.pem 使用私钥匙dsaprivatekey.pem生成公钥匙，输出到dsapublickey.pem
# openssl dsa -in dsaprivatekey.pem -pubout -out dsapublickey.pem 从dsaprivatekey.pem中读取私钥匙，解密并输入新口令进行加密，然后写回文件dsaprivatekey.pem
# openssl dsa -in dsaprivatekey.pem -out dsaprivatekey.pem -des3 -passin 5、RSA应用例子 产生1024位RSA私匙，用3DES加密它，口令为trousers，输出到文件rsaprivatekey.pem
# openssl genrsa -out rsaprivatekey.pem -passout pass:trousers -des3 1024 从文件rsaprivatekey.pem读取私匙，用口令trousers解密，生成的公钥匙输出到文件rsapublickey.pem
# openssl rsa -in rsaprivatekey.pem -passin pass:trousers -pubout -out rsapubckey.pem 用公钥匙rsapublickey.pem加密文件plain.txt，输出到文件cipher.txt
# openssl rsautl -encrypt -pubin -inkey rsapublickey.pem -in plain.txt -out cipher.txt 使用私钥匙rsaprivatekey.pem解密密文cipher.txt，输出到文件plain.txt
# openssl rsautl -decrypt -inkey rsaprivatekey.pem -in cipher.txt -out plain.txt 用私钥匙rsaprivatekey.pem给文件plain.txt签名，输出到文件signature.bin
# openssl rsautl -sign -inkey rsaprivatekey.pem -in plain.txt -out signature.bin 用公钥匙rsapublickey.pem验证签名signature.bin，输出到文件plain.txt
# openssl rsautl -verify -pubin -inkey rsapublickey.pem -in signature.bin -out plain 从X.509证书文件cert.pem中获取公钥匙，用3DES加密mail.txt，输出到文件mail.enc
# openssl smime -encrypt -in mail.txt -des3 -out mail.enc cert.pem 从X.509证书文件cert.pem中获取接收人的公钥匙，用私钥匙key.pem解密S/MIME消息mail.enc，结果输出到文件mail.txt
# openssl smime -decrypt -in mail.enc -recip cert.pem -inkey key.pem -out mail.txt cert.pem为X.509证书文件，用私匙key,pem为mail.txt签名，证书被包含在S/MIME消息中，输出到文件mail.sgn
# openssl smime -sign -in mail.txt -signer cert.pem -inkey key.pem -out mail.sgn 验证S/MIME消息mail.sgn，输出到文件mail.txt，签名者的证书应该作为S/MIME消息的一部分包含在mail.sgn中
# openssl smime -verify -in mail.sgn -out mail.txt ]]></content>
  </entry>
  
  <entry>
    <title>syslog命令</title>
    <url>/post/linux/syslog.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>syslog</tag>
      <tag>系统管理</tag>
      <tag>系统安全</tag>
    </tags>
    <content type="html"><![CDATA[ 系统默认的日志守护进程
 syslog是Linux系统默认的日志守护进程。
概述 默认的syslog配置文件是/etc/syslog.conf文件。程序，守护进程和内核提供了访问系统的日志信息。因此，任何希望生成日志信息的程序都可以向 syslog 接口呼叫生成该信息。
几乎所有的网络设备都可以通过syslog协议，将日志信息以用户数据报协议(UDP)方式传送到远端服务器，远端接收日志服务器必须通过syslogd监听UDP 端口514，并根据 syslog.conf配置文件中的配置处理本机，接收访问系统的日志信息，把指定的事件写入特定文件中，供后台数据库管理和响应之用。意味着可以让任何事件都登录到一台或多台服务器上，以备后台数据库用off-line(离线) 方法分析远端设备的事件。
通常，syslog 接受来自系统的各种功能的信息，每个信息都包括重要级。/etc/syslog.conf 文件通知 syslogd 如何根据设备和信息重要级别来报告信息。
使用方法 在/var/log中创建并写入日志信息是由syslog协议处理的，是由守护进程sylogd负责执行。每个标准的进程都可以用syslog记录日志。可以使用logger命令通过syslogd记录日志。
要向syslog文件/var/log/messages中记录日志信息：
logger this is a test log line 输出： tail -n 1 messages Jan 5 10:07:03 localhost root: this is a test log line 如果要记录特定的标记（tag）可以使用：
logger -t TAG this is a test log line 输出： tail -n 1 messages Jan 5 10:37:14 localhost TAG: this is a test log line ]]></content>
  </entry>
  
  <entry>
    <title>如何在Ubuntu Linux下将mp4转成mp3</title>
    <url>/post/linux/how-to-convert-mp4-to-mp3-in-ubuntu-linux.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>ubuntu</tag>
      <tag>ffmpeg</tag>
      <tag>mp4</tag>
      <tag>mp3</tag>
    </tags>
    <content type="html"><![CDATA[ FFmpeg是一款开源软件，用于生成处理多媒体数据的各类库和程序。FFmpeg可以转码、处理视频和图片（调整视频、图片大小，去噪等）、打包、传输及播放视频。
 本文描述了如何在Ubuntu Linux系统下，通过ffmpeg将mp4文件转成mp3文件。
为什么要将mp4转成mp3 因为这样可以节省空间，一些基本的设备是不支持mp4扩展名的文件，在这个例子里，我们将使用ffmpeg将mp4文件转成mp3文件。
FFmpeg是一个完整的跨平台的解决方案，用来录制，转化以及分流音视频，它包括业界领先的音视频编码库 labavcodec 。
在ubuntu上安装ffmpeg sudo apt-get install ffmpeg libavcodec-extra-53 将mp4转成mp3 基本的命令
ffmpeg -i filename.mp4 filename.mp3 可以用命令`man ffmpeg&rsquo;来查看更多选项
ffmpeg -i filename.mp4 -b:a 192K -vn filename.mp3 一个流的说明符可以匹配一些流，这些选项会适用于所有的流，比如，在-b:a 128k选项中的流说明符可以匹配所有的音频流。
通过脚本 下面这个脚本会将Music目录下的带有.mp4扩展名的文件转成.mp3扩展名的文件。
#!/bin/bash MP4FILE=$(ls ~/Music/ |grep .mp4) for filename in $MP4FILE do name=`echo &#34;$filename&#34; | sed -e &#34;s/.mp4$//g&#34;` ffmpeg -i ~/Music/$filename -b:a 192K -vn ~/Music/$name.mp3 done ]]></content>
  </entry>
  
  <entry>
    <title>风河携手TCS建构5G/Open RAN分布式移动网络基础设施生态系统</title>
    <url>/post/news/windriver-and-TCS-build-5G-Open-Ran-ecos.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>WindRiver</tag>
      <tag>vRan</tag>
      <tag>TCS</tag>
      <tag>5G</tag>
    </tags>
    <content type="html"><![CDATA[ 全球领先的关键任务智能系统软件提供商风河公司®宣布，正在与塔塔咨询服务公司（TCS）合作，在Wind River Studio上托管vRAN解决方案。这项战略合作将创建一个全栈移动基础设施解决方案，在4G-5G vRAN下一代网络中开展TCS部署和工程服务，并以Studio作为云平台。
 TCS网络解决方案与服务副总裁Vimal Kumar表示：“我们很高兴与风河合作，帮助我们的客户借助5G技术改善他们的业务。我们的Cognitive Network Operations平台运行在Wind River Studio之上，由此帮助电信网络运营商运用AI和ML技术来监测网络健康状况，预测可能发生的故障，提供以客户为中心的网络体验，并确保卓越的服务质量。”
风河公司首席产品官Avijit Sinha表示：“运营商正在致力于创造数字化、云原生的未来，他们正在寻求灵活、经济的解决方案，以便降低部署复杂度并进行持续性维护。风河公司提供了成熟的生产就绪产品，与领先运营商实现了实用化部署，其基础正是经过广泛验证的Wind River Studio技术。”
风河公司印度销售主管Rajeev Rawal表示：“与TCS携手，提供敏捷、安全、可靠和超低延迟解决方案，以支持新的应用场景，让云计算、边缘计算和智能化技术承担起更加重要的任务。”
作为5G市场的领导者，风河在世界首次成功5G数据会话和商业vRAN/O-RAN项目中发挥了关键作用，其中包括世界上最大的Open RAN网络。
Wind River Studio提供了一个完全基于云原生、Kubernetes和容器的体系结构，可用于大规模分布式边缘网络的开发、部署、运营和服务。这套平台为地理分布的管理解决方案提供了基础，能够为数千个节点提供单一窗口（SPoG）、零接触的自动化管理，从而简化Day 1和Day 2运营，而且与节点的物理位置无关。Studio解决了部署和管理物理地理分散云原生vRAN基础设施的复杂挑战，在vRAN部署中提供了传统的RAN性能。
 塔塔咨询服务 (TCS）简介
 塔塔咨询服务公司是一家IT服务、咨询和业务解决方案提供商，50多年来一直与许多全球最大企业合作，帮助他们实现转型。TCS提供以咨询为主导、以认知为动力的综合性商业、技术和工程服务以及解决方案。所有这些都通过独特的Location Independent Agile™ 模式来提供，被作为卓越软件开发的基准指标。
作为印度最大的跨国商业集团塔塔集团的一部分，TCS在55个国家拥有超过606,000名训练有素的咨询师。在截至2022年3月31日的财年中，TCS创造了257亿美元的合并营收，并在印度的BSE和NSE上市。
]]></content>
  </entry>
  
  <entry>
    <title>VxWorks 6.8下基于QT的串口编程</title>
    <url>/post/vxworks/vxworks-6.8-qt-uart-programming.html</url>
    <categories><category>VxWorks</category>
    </categories>
    <tags>
      <tag>VxWorks</tag>
      <tag>VxWorks 6.8</tag>
      <tag>UART</tag>
      <tag>QT</tag>
      <tag>串口</tag>
      <tag>编程</tag>
    </tags>
    <content type="html"><![CDATA[文章简要记录了VxWorks 6.8下基于Qt实现的串口编程。
相关的VxWorks 和 串口，请参阅 VxWorks下的串口测试程序设计和源码  。
VxWorks简介 VxWorks 操作系统是美国WindRiver公司于1983年设计开发的一种嵌入式实时操作系统（RTOS），是嵌入式开发环境的关键组成部分。良好的持续发展能力、高性能的内核以及友好的用户开发环境，在嵌入式实时操作系统领域占据一席之地。它以其良好的可靠性和卓越的实时性被广泛地应用在通信、军事、航空、航天等高精尖技术及实时性要求极高的领域中，如卫星通讯、军事演习、弹道制导、飞机导航等。在美国的 F-16、FA-18战斗机、B-2 隐形轰炸机和爱国者导弹上，甚至连1997年4月在火星表面登陆的火星探测器、2008年5月登陆的凤凰号，和2012年8月登陆的好奇号也都使用到了VxWorks。
串口简介 串行接口(Serial Interface) 简称串口，也称串行通信接口或串行通讯接口（通常指COM接口），是采用串行通信方式的扩展接口，指数据一位一位地顺序传送。
串行接口的特点是通信线路简单，只要一对传输线就可以实现双向通信（可以直接利用电话线作为传输线），从而大大降低了成本，特别适用于远距离通信，但传送速度较慢。常见的有一般计算机应用的RS-232（使用 25 针或 9 针连接器）和工业计算机应用的半双工RS-485与全双工RS-422。
我这里使用了232和422传输方式，在我本人理解这两种方式根据需求硬件已经做好的传输方式（也可以在BIOS设置），我们知道是什么传输方式，做到心中有数和如何搭建测试环境，今天在这里教大家个简单的232-9针连接器的接线方式，一般没接触过的拿过来一脸懵逼，好家伙9跟针都不知道是干嘛的，那么我告诉你如果是 232-9针，什么也别管直接找到第2针和第3针用杜邦线回连，这时你就具备环境自己检测板卡串口模块是否好用，如果测试程序一定记得把第5跟针要连接上，否则会出现数据不精准的情况（文章底部有贴图）。
在软件层面上只需要关注数据位、停止位、奇偶效验、读取方式和效率即可；
232串口接线说明 RS232串口接线方法：直连和交叉接法
一般情况下，设备和电脑的连接通讯，需用到RS232串口线直连线；而设备和设备的连接通讯，就会用到RS232串口线的交叉线。用户在选择的时候，应根据两个设备之间连接的实际情况，选择不同接法的RS232串口线。
代码实例 VxWorks串口所需要包含的头文件 #include &#34;vxWorks.h&#34;#include &#34;stdIo.h&#34;#include &#34;ioLib.h&#34;#include &#34;sysLib.h&#34;#include &#34;string.h&#34;#include &#34;taskLib.h&#34;VxWorks串口配置函数 ioctl(m_SeriPort,SIO_HW_OPTS_SET, CLOCAL | CS8 | PARODD | PARENB);	//8位数据位|1位停止位|偶效验 ioctl(m_SeriPort,FIOBAUDRATE,9600);	//波特率9600 ioctl(m_SeriPort,FIOSETOPTIONS,OPT_RAW);	//设置串口raw模式 ioctl(m_SeriPort,FIOFLUSH,0);	//清空输入输出的缓冲区 open函数 #define SERI_NAME &#34;/tyCo/0&#34; int m_SeriPort = open(SERI_NAME ,O_RDWR,0); int m_SeriPort = open(SERI_NAME ,O_WRONLY,0); write函数 char* sendData; int writeCom = write(m_SeriPort, sendData,strlen(sendData)); read函数 char data; int readCom = read(m_SeriPort,&amp;data,1); Seri_Demo_Qt_Vx #ifndef THREAD_H #define THREAD_H #include &lt;QThread&gt;#include &lt;QDebug&gt;#include &#34;vxWorks.h&#34;#include &#34;stdIo.h&#34;#include &#34;ioLib.h&#34;#include &#34;sysLib.h&#34;#include &#34;string.h&#34;#include &#34;taskLib.h&#34;class Thread : public QThread { Q_OBJECT public: explicit Thread(QObject *parent = 0); ~Thread(); void run(); //重写run函数 public: bool openSeri(QString comPort,int baudRate); //打开串口  void closeSeri(); //关闭串口  void writeSeri(char* sendData); //发送数据  void setFlag(bool flag = true); //线程数据标志位 signals: void RecvData(char data); private: bool seriStop; //读取数据标志位 true读取数据 false退出循环  int m_SeriPort; //串口文件描述符  QString m_SeriName; //串口名  int m_baud; //波特率 }; #endif //THREAD_H #include &#34;thread.h&#34; Thread::Thread(QObject *parent) : QThread(parent) { } Thread::~Thread() { } void Thread::run() { sysClkRateSet(1000); char rData; while(1) { int readCom = read(m_SeriPort,&amp;rData,1); if(readCom &gt; 0) { printf(&#34;%c\n&#34;,rData); emit RecvData(rData); if(seriStop == false) { qDebug()&lt;&lt; &#34;isStop == false break&#34;; break; } } else { taskDelay(10); } } } bool Thread::openSeri(QString comPort, int baudRate) { this-&gt;m_SeriName = comPort; this-&gt;m_baud = baudRate; qDebug()&lt;&lt; &#34;Thread::openSeri&#34; &lt;&lt; comPort.toUtf8().data() &lt;&lt; baudRate; m_SeriPort = open(comPort.toUtf8().data(),O_RDWR,0); if(m_SeriPort == ERROR) { qDebug()&lt;&lt; &#34;open :&#34; &lt;&lt; comPort.toUtf8().data() &lt;&lt; &#34; = &#34; &lt;&lt;m_SeriPort &lt;&lt; &#34;failed !&#34;; return false; } ioctl(m_SeriPort,SIO_HW_OPTS_SET, CLOCAL | CS8 | PARODD | PARENB); ioctl(m_SeriPort,FIOBAUDRATE,baudRate); ioctl(m_SeriPort,FIOSETOPTIONS,OPT_RAW); ioctl(m_SeriPort,FIOFLUSH,0); qDebug()&lt;&lt; &#34;open :&#34; &lt;&lt; comPort.toUtf8().data() &lt;&lt; &#34; = &#34; &lt;&lt; m_SeriPort &lt;&lt; &#34;succeeded !&#34;; return true; } void Thread::closeSeri() { if(seriStop == false) { qDebug()&lt;&lt; &#34;Thread::closeSeri&#34;; close(m_SeriPort); } } void Thread::writeSeri(char* sendData) { if(m_SeriPort == ERROR) { openSeri(m_SeriName,m_baud); } int writeCom = write(m_SeriPort, sendData,strlen(sendData)); qDebug()&lt;&lt; sendData &lt;&lt; writeCom; } void Thread::setFlag(bool flag) { this-&gt;seriStop = flag; qDebug()&lt;&lt; &#34;Thread::setFlag&#34; &lt;&lt; flag; } TestSeri_Demo_Qt_Vx_Demo #ifndef SERI_H #define SERI_H  #include &lt;QObject&gt;#include &lt;QDebug&gt;#include &#34;thread.h&#34; class Seri : public QObject { Q_OBJECT public: explicit Seri(QObject *parent = 0); ~Seri(); public: /*	open_Seri	打开串口 * comName	串口名 * comBaud	串口波特率 *	return 成功 true 失败 false */ bool open_Seri(QString comName,int comBaud); /* write_Seri	发送数据 * comData	发送数据内容 */ void write_Seri(QByteArray comData); /*	* close_Seri	关闭串口 */ void close_Seri(); signals: send_Seri(char data); private: Thread* m_pThread; }; #endif // SERI_H #include &#34;Seri.h&#34; Seri::Seri(QObject *parent) : QObject(parent) { m_pThread = new Thread; } Seri::~Seri() { if(m_pThread){ delete m_pThread; m_pThread=NULL; } } bool Seri::open_Seri(QString comName,int comBaud) { if(m_pThread-&gt;openSeri(comName,comBaud))//如果打开成功 	{ m_pThread-&gt;setFlag(true); m_pThread-&gt;start(); } return false; } void Seri::write_Seri(QByteArray comData) { m_pThread-&gt;writeSeri(comData.data()); } void Seri::close_Seri() { if(m_pThread-&gt;isRunning())//如果线程还在运行 --&gt; 退出循环接收数据 --&gt; 关闭串口 --&gt; 退出线程 --&gt; 回收线程 	{ m_pThread-&gt;setFlag(false); m_pThread-&gt;closeSeri(); m_pThread-&gt;quit(); m_pThread-&gt;wait(); } } 程序代码说明：  thread类为配置串口类 seri类为外部使用类 接收到的数据是利用信号槽为接口把数据传输出去 ]]></content>
  </entry>
  
  <entry>
    <title>VxWorks操作系统下的串口读写程序</title>
    <url>/post/vxworks/vxworks-uart-read-write-programming.html</url>
    <categories><category>VxWorks</category>
    </categories>
    <tags>
      <tag>VxWorks</tag>
      <tag>UART</tag>
      <tag>串口</tag>
      <tag>编程</tag>
    </tags>
    <content type="html"><![CDATA[关于传统的串口编程，在各大操作系统下的流程基本是一致的，只是针对不同的操作系统，函数接口可能有所差异而已，下面讲述VxWorks操作系统下对于串口读写的编程步骤和代码
相关的VxWorks 和 串口，请参阅 VxWorks下的串口测试程序设计和源码  。
串口配置过程 打开串口 fd = open(&#34;/tyCo/0&#34;, O_RDWR, 0);  &ldquo;/tyCo/0&rdquo;: 串口1的设备名 O_RDWR: 按照读写方式打开串口  设置串口raw模式，清空输入输出的缓冲区 在VxWorks中配置串口可以直接通过ioctl的控制命令来实现
ioctl(fd,FIOSETOPTIONS,OPT_RAW); ioctl(fd,FIOFLUSH,0); ioctl(int fd,int function,int arg); function的参数如下：
   参数 说明     FIOBAUDRATE 设置波特率，arg为一整数，表示要设定的波特率   FIOGETOPTIONS 取得设备控制字，arg表示读出的内容存放的位置   FIOSETOPTIONS 设置设备控制字，arg表示要设置的选项   FIOGETNAME 取得文件描述符对应的文件名，arg存放文件名的缓冲区   FIOREAD 取得输入缓冲区内未读取的字符数，arg用于接收结果的整型指针   FIOWRITE 取得输出缓冲区内的字符个数，arg用于接收结果的整型指针   FIOFLUSH 清空输入输出缓冲区的字符   FIOCANCEL 取消读和写    设置波特率，数据位，停止位，校验方式 在 VxWorks 中设置串口也是用 &lsquo;ioctl&rsquo; 系统调用加控制命令实现，其控制命令为&rsquo;SIO_HW_OPTS_SET'，第三个参数跟配置参数，如：数据位为8，停止位为1，无奇偶校验位，无流控可以这样配置
ioctl(fd,SIO_HW_OPTS_SET,CS8|PARENB|CLOCAL|CREAD); 具体各项参数意义如下：
   参数 说明     CLOCAL 忽略modem控制信号   CREAD 启动接收器   CSIZE 指定数据位：CS5~CS8   HUPCL 最后关闭时挂断modem连接   STOP8 被设置时指定2位停止位，否则默认为1位停止位   PARENB 被设置时启用奇偶校验，否则默认为无奇偶校验   PARODD 被设置时启用奇校验，否则默认为偶校验(PARENB设置时才有效)    串口读写操作 在VxWorks系统中串口的读写操作非常简单，直接使用系统调用函数 read() 和 write() 就能实现串口的读写操作。
int read(int fd, char *buffer, size_t maxbytes) 参数说明：
 fd: 用open函数打开串口设备返回的文件描述符 buffer: 读取的内容将要存放的地址，为指针变量 maxbytes: 读取的最大字节数  int write(int fd, char *buffer, size_t nbytes) 参数说明：
 fd: 用open函数打开串口设备返回的文件描述符 buffer: 将要写的内容的地址，为指针变量，通常为字符串首地址 nbytes: 将要写入的字节数，通常为要写入的字符串的长度  实例代码 VxWorks系统下串口读写的实例代码，仅供参考。
#include &#34;vxWorks.h&#34;#include &#34;stdio.h&#34;#include &#34;ioLib.h&#34;#include &#34;taskLib.h&#34;#include &#34;sioLib.h&#34;#include &#34;sdLib.h&#34;#include &#34;semLib.h&#34;#include &#34;msgQLib.h&#34; char wbuf[] = &#34;hello&#34;; #define DEV_NAME &#34;/tyCo/2&#34; #define MAX_BUF_SIZE 20 #define SD_COMMDATA_NAME &#34;share_data&#34; #define SD_COMMDATA_MUTEX &#34;share_sem&#34; #define SHARE_DATA_LENGTH 20  typedef struct unix_clock_struct { UINT32 sec; /* ms */ UINT32 msec; /* s */ UINT8 quality; /* 时标质量 */ } UNIX_CLOCK_STRUCT; char *comdata; int set_serial(int fd); SEM_ID mutexComdata; void taskUart(void); int main(void) { int ret; int sdCommId; char r_buff[MAX_BUF_SIZE]; mutexComdata = semOpen(SD_COMMDATA_MUTEX, SEM_TYPE_MUTEX, SEM_FULL, SEM_Q_PRIORITY | SEM_DELETE_SAFE | \ SEM_INVERSION_SAFE, OM_CREATE | OM_DELETE_ON_LAST_CLOSE, NULL); if(mutexComdata == NULL) { /*致命错误，无法创建互斥锁*/ printf(&#34;ERROR TO OPEN SD_COMMDATA_MUTEX\n&#34;); taskExit(0); } /* 申请公共数据共享内存 */ sdCommId = sdOpen(SD_COMMDATA_NAME, SD_LINGER, OM_CREATE, SHARE_DATA_LENGTH, 0, SD_ATTR_RW|SD_CACHE_OFF, &amp;comdata); if(sdCommId == NULL) { /*致命错误，无法分配公共数据内存，报错退出*/ printf(&#34;ERROR TO OPEN SD_COMMDATA\n&#34;); taskExit(0); } if((ret = taskSpawn(&#34;taskUart&#34;,90,0x100, 20000, (FUNCPTR)taskUart,\ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)) &lt; 0) { printf(&#34;taskSpawn failed:ret = %s\n&#34;); } return 0; } void taskUart(void) { int ret; int fd = -1; UNIX_CLOCK_STRUCT w_buff; if((fd = open(DEV_NAME, O_RDWR,0)) &lt; 0) { printf(&#34;open %s failed.\n&#34;,DEV_NAME); } /*配置串口参数*/ if((ret = set_serial(fd)) &lt; 0) { printf(&#34;ret = %d\nset_serial failed.\n&#34;); } while(1) { semRTake(mutexComdata,WAIT_FOREVER); #if 0/*清空输入输出缓冲*/ if((ret = ioctl(fd, FIOFLUSH, 0))&lt;0) { printf(&#34; ret = %d\nset FIOFLUSH failed.\n&#34;,ret); } memset(r_buff,0,sizeof(r_buff)); /*读取串口中的值*/ if((ret = read(fd,r_buff,sizeof(r_buff)))&lt;0) { printf(&#34;ret = %d:read %s failed.\n&#34;,ret,DEV_NAME); } else printf(&#34;Received:%s\n&#34;,r_buff); #endif  #if 1  /*清空输入输出缓冲*/ if((ret = ioctl(fd, FIOFLUSH, 0))&lt;0) { printf(&#34; ret = %d\nset FIOFLUSH failed.\n&#34;,ret); } if(NULL == bzero(&amp;w_buff,sizeof(w_buff))) { printf(&#34;memset failed.\n&#34;); } if(NULL == memcpy(&amp;w_buff,comdata,sizeof(w_buff))) { printf(&#34;memset failed.\n&#34;); } if(&amp;w_buff != NULL) { /*往串口中写值*/ if((ret = write(fd, &amp;w_buff.sec, sizeof(ret)))&lt;0) // if((ret = write(fd, wbuf, sizeof(wbuf)))&lt;0)  { printf(&#34;ret = %d:write %s failed.\n&#34;,ret,DEV_NAME); } else { printf(&#34;write success:%d\n&#34;,w_buff.sec); } } semGive(mutexComdata); #endif  taskDelay(sysClkRateGet()*2); } } int set_serial(int fd) { int error = -1; int ok = 0; int ret; if(fd&lt;0) { printf(&#34;error:fd is %d\n&#34;,fd); } /*设定波特率为9600*/ if((ret = ioctl(fd, FIOBAUDRATE, 9600))&lt;0) { printf(&#34;ret = %d\nset baudrate failed\n&#34;,ret); return error; } /*设定：数据位为8，无奇偶校验，1位停止位*/ /*CLOCAL:忽略modem控制信号 * CREAD：启动接收器 * CS8:设定数据位为8*/ if((ret = ioctl(fd, SIO_HW_OPTS_SET,CREAD|CS8 |CLOCAL))&lt;0) { printf(&#34;ret = %d\nset SIO_HW_OPTS_SET failed.\n&#34;); return error; } return ok; } ]]></content>
  </entry>
  
  <entry>
    <title>针对VxWorks的QT 5.15.10发布了</title>
    <url>/post/vxworks/qt-5-15-10-for-vxworks-released.html</url>
    <categories><category>VxWorks</category>
    </categories>
    <tags>
      <tag>VxWorks</tag>
      <tag>QT</tag>
      <tag>图像</tag>
    </tags>
    <content type="html"><![CDATA[Qt是一个多平台的C++图形用户界面应用程序框架。它提供给应用程序开发者建立艺术级的图形用户界面所需的所用功能。Qt是完全面向对象的编程，所以具有易扩展和组件编程的优势。
相关的VxWorks 和 QT的文章，请参阅 VxWorks 6.8操作系统下QT的安装设置和运行方法  。
我们非常激动地发布了支持VxWorks的QT 5.15.10 支持VxWorks的Qt 5.15.10长期支持的商业发行是基于我们最新的QT 5.15.10(LTS)之上的源代码发布。这个发行从早期的QT 5的版本官方升级了针对VxWorks的QT支持，这是对诸如航空和国防以及医疗等行业的市场需求的积极回应。它提供了QT版本的升级同时也提供了VxWorks系统具体的问题解决，还有别的一些改进。
这次发行支持基于iMX6硬件的Ubuntu主机，我们也同时在准备基于x68和基于Windows主机的支持。此次的发行包开放给拥有QT账户的客户，也同时通过git仓库的形式开放给具有商业许可证的客户，请和我们联系以获取更多细节。
从这儿开始 这儿有关于安装和配置的独立的文章，要获取更多关于QT和支持VxWorks的QT的详细信息，请查看这儿关于QT 5.15的在线文档。
   https://doc.qt.io/qt-5/vxworks.html  
   https://doc.qt.io/qt-5/index.html  
   https://wiki.qt.io/Getting_Commercial_Qt_Sources  
 ]]></content>
  </entry>
  
  <entry>
    <title>北南南北</title>
    <url>/about.html</url>
    <categories>
    </categories>
    <tags>
    </tags>
    <content type="html"><![CDATA[北南南北 是众多使用 VxWorks 嵌入式实时操作系统的网友分享经验的平台，为的就是让 VxWorks 的学习和应用变得相对开放一些，在此也欢迎你的加入！
我们的愿景 技术创新是技术持续发展的生命力，紧跟技术的发展趋势，研究最新的技术，保持对新技术的热情和好奇心，让技术为生产和生活服务。
使用反馈  加入 VxWorks Club   或 Google AI TPU     欢迎你的加入
 ]]></content>
  </entry>
  
  <entry>
    <title>VxWorks实时性能探究</title>
    <url>/post/vxworks/vxworks-real-time-feature-explore.html</url>
    <categories><category>VxWorks</category>
    </categories>
    <tags>
      <tag>VxWorks</tag>
      <tag>实时性</tag>
    </tags>
    <content type="html"><![CDATA[ VxWorks操作系统是一款硬实时操作系统，一直听闻其实时性能非常优秀，但是一直没有一个直观地概念。
 笔者最近在使用 VxWorks  , 由大名鼎鼎的风河（WindRiver）开发。本篇文章就是将VxWorks操作系统和市面上几种其他实时操作系统的实时性能进行对比。
前期知识准备 实时性能和响应时间有关，为此，先对计算机操作系统中的时间概念和时间尺度进行一下介绍。
1 s = 1000 ms = 1000000 us = 1000000000 ns，看不出来1 s时间还是很长的嘛
  时钟周期：主频为4 GHz的CPU的时钟周期为1/4G = 0.25 ns，时钟周期是计算机中最基本的、最小的[时间单位。在一个时钟周期内，CPU仅完成一个最基本的动作。
  CPU周期：CPU周期亦称机器周期，一条指令执行过程被划分为若干阶段，每一阶段完成所需时间。完成一个基本操作所需要的时间称为机器周期。通常用内存中读取一个指令字的最短时间来规定CPU周期。
  指令周期：取出并执行一条指令的时间。想要详细了解可以看这篇文章【浅析】CPU中的指令周期、CPU周期和时钟周期
  内存时钟周期：相比CPU，一般的DDR内存芯片速率仅为400 MHz，时钟周期达2.5 ns, 再加上总线延时，导致内存访问时间达到几十纳秒。CPU运行速率与内存访问速率比大致为100：1。
  硬盘读取时间：硬盘的读写速度就更慢了，一般的机械硬盘的完成一次读写所需要的时间，主要取决寻道时间+旋转时间，完成一次读或者写的时间量级大致为ms级别，因此内存访问速率与磁盘存取速度比大致为1000:1。
  上面是有关硬件方面的时间周期情况，对于操作系统或者应用程序来说，我们一般关注的是算法的时间复杂度和空间复杂度，这是从整理理想的情况来衡量一个算法的优劣。如果想要详细了解每条代码的执行所耗时间，我们需要更深入了解代码是怎么在计算机上执行的。
C语言代码都是经过预处理、编译，产生汇编代码（汇编代码几乎已经接近机器码了），一句高级语言代码相当于汇编语言的几行甚至几十行。而学过汇编语言的都应该知道，不同的汇编代码指令执行所耗费的时间也是不同的。一般来说,移位,加法,取反这种指令只需要一个时钟周期,而乘法,除法等指令需要几个乃至几十个时钟周期执行。
实时操作系统（RTOS）的实时性能评价指标 实时操作系统的实时性能评价指标一般有两个：
 任务切换时间  当多任务应用程序运行在操作系统上时，它把正在运行的任务的状态保存到任务自己的栈区之中，然后把下一个将要运行的任务的当前状态从该任务的栈区装入CPU的寄存器，并开始这个任务的执行，这个过程就叫做任务切换。
 中断响应时间  计算机接收到中断信号到操作系统做出响应，并完成切换转入中断服务程序的时间。
下图是几种实时操作系统的实时性能对比：
可以看出不管是任务切换时间还是中断响应，VxWorks都是最好的，当然VxWorks也是最贵的。
此外我们还可以看出不管是任务切换还是中断响应，时间尺度都是在几个us，根据CPU主频的不同，大概是几千个时钟周期的样子。 下面代码是测试执行100万次简单循环语句所耗费的时间:
int i = 1000000; int j = 0; while(i){ j += 0; i--; } timer = 2033 us //执行100万次该循环所耗时间，可以将执行每次的时间和任务切换的时间进行对比 ]]></content>
  </entry>
  
  <entry>
    <title>风河公司的资本交易历史</title>
    <url>/post/vxworks/windriver-capital-transaction.html</url>
    <categories><category>VxWorks</category>
    </categories>
    <tags>
      <tag>WindRiver</tag>
    </tags>
    <content type="html"><![CDATA[日前，安波福宣布同意以43亿美元现金从私募股权公司TPG Capital收购风河公司（ Wind River  ），以帮助其在多个行业的关键软件领域建立独特地位，继续其智能转型，向边缘支持、软件定义的未来迈进。
该交易预计将于2022年年中完成，在被收购之后，风河将隶属于安波福主动安全与用户体验事业部，继续在公司总裁兼首席执行官Kevin Dallas的领导下作为独立业务单位运营。
实时操作系统 作为实时操作系统领域，全球最优秀的选手，它值得我们所有的溢美之词，无论怎么夸它，都不过分。
VxWorks是风河公司推出的实时多任务操作系统（RTOS）。过去40年间，风河和VxWorks在嵌入式OS领域一直处于领先地位，在航空航天、通信、工业控制等行业有着广泛的应用，在业内被称为嵌入式OS的常青树。
风河公司目前有2个嵌入式OS平台：Linux和VxWorks。
VxWorks是由支持多核、32/64位嵌入式处理器、内存包含和内存管理的VxWorks 6.x和VxWorks5.x，Workbench开发工具（包括多种C/C++编译器和调试器），连接组件（USB、IPv4/v6、多种文件系统等），先进的网络协议和图像多媒体等模块组成。除了通用平台外，VxWorks还包括支持工业、网络、医疗和消费电子等的特定平台产品。
老当益壮 风河成立于1981年，2021年收入大约4亿美元，毛利率超过80%。
1987年风河基于VRTX推出VxWorks，1993年IPO上市，1995年VxWorks在NASA Clementine月球探测器上，被发射入太空。
1997年NASA火星探险者号飞船的实时操作系统，登陆火星。
风河是全球第一大嵌入式RTOS厂家，也是全球第一大嵌入式Linux厂家，硬实时操作系统长达30年的霸主，市场占有率超30%。
它的主要收入来自4个领域：
 宇航与国防 工业与医疗 电信 汽车  宇航与国防所占比例最高，接近50%，各种飞船或者说航天飞行器基本都是风河VxWorks的市场，SpaceX也是它的忠实用户，中国神舟系列的SpaceOS也有借鉴VxWorks653。
除了航天飞行器，AH-64阿帕奇武装直升机、F-16V（全球空军主力机型）、F-18大黄蜂，B-2战略轰炸机，X-47A，波音787都是VxWorks。
美国的F-22猛禽、F-35、B-52轰炸机、B-1B轰炸机、C-17运输机和F-16改进型，以及欧洲的A-400M运输机，X-47B无人机，还有民航空客的A380，爱国者防空导弹，都是Vxworks的忠实用户。
把竞争对手买下来，然后干掉！
1999年风河收购一个主要竞争对手，pSOS的发明者，一家集成系统公司。从那以后风河公司不再支持pSOS产品线，并推荐现存的pSOS客户转向VxWorks。
2004年针对网络和通信市场，推出便携的Linux平台，正式进军嵌入式Linux市场。
VxWorks通过了汽车领域最高的ASIL-D级认证，以及远超汽车标准的DO-178C A级认证，它也通过了，已经准备好了对汽车行业进行降维打击。  卖来卖去 2009年英特尔以8.84亿美元收购风河；
2018年4月英特尔出售风河给投资公司TPG。
英特尔刚刚收购4年不到，就卖给了TPG，英特尔也是颇具渣男属性了。
不过，风河公司貌似还不是最后一个被卖来卖去的此类企业，另外一个汽车级嵌入式系统的大牛供应商，Green Hills也在被卖的路上了，我们接下来的文章会保持对它的追踪，及时报道相关信息。
戳穿实时操作系统 在日常的HIL测试工作中，几乎没有哪个测试任务是因为“实时仿真机的实时性不够高”而导致出问题。
HIL工作最容易出问题的地方，往往是功能定义不明确、工具链不完整、协同自动化测试做不起来、线束掉链子以及项目上各种瞎搞等等。
换句话说，在汽车HIL测试领域，哪怕是最low逼的实时操作系统，也足够了，人家Vector公司用wince做实时机，照样玩得飞起，不耽误事。
HIL实时机诞生的历史环境，已经不复存在了，当年的PC机真是太鸡肋了。
而且，在近些年大火的自动驾驶测试领域，我见过太多实时性差得一批的测试系统，响应滞后得跟PID调节似的，却闭口不谈实时性问题，忽忽悠悠就验收通过了……
资本市场 通过这些收购案例，我们也能看到资本唯利是图的本性，什么来钱快干什么，脑子一热就买了，兴奋劲儿过了之后又卖了，不符合自己的产品路线，也照样卖掉。
]]></content>
  </entry>
  
  <entry>
    <title>友情链接</title>
    <url>/flinks.html</url>
    <categories>
    </categories>
    <tags>
    </tags>
    <content type="html"><![CDATA[如想交换本站友情链接，请在评论区留下你的站点信息，格式参考如下：
name: VxWorks俱乐部 desc: VxWorks实时操作系统 link: https://www.vxworks.net ]]></content>
  </entry>
  
</search>